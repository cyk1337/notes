<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A note for NLP Interview.">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP Basics">
<meta property="og:url" content="https://cyk1337.github.io/notes/2018/06/29/NLP/NER/NLP-Basics-Summary/index.html">
<meta property="og:site_name" content="The Gradient">
<meta property="og:description" content="A note for NLP Interview.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ROC.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/AUC.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/LSTM.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/LSTM-1.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/GRU.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/GRU-1.png">
<meta property="article:published_time" content="2018-06-29T01:23:00.000Z">
<meta property="article:modified_time" content="2018-06-29T01:23:00.000Z">
<meta property="article:author" content="cyk1337">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/ROC.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2018/06/29/NLP/NER/NLP-Basics-Summary/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>NLP Basics | The Gradient</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The Gradient</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Language is not just words.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2018/06/29/NLP/NER/NLP-Basics-Summary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="cyk1337">
      <meta itemprop="description" content="What is now proved was once only imagined.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Gradient">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP Basics
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-29 09:23:00" itemprop="dateCreated datePublished" datetime="2018-06-29T09:23:00+08:00">2018-06-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2018/06/29/NLP/NER/NLP-Basics-Summary/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/06/29/NLP/NER/NLP-Basics-Summary/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A note for NLP Interview.<br><span id="more"></span></p>
<h1 id="Statistical-ML"><a href="#Statistical-ML" class="headerlink" title="Statistical ML"></a>Statistical ML</h1><h2 id="LR-vs-SVM"><a href="#LR-vs-SVM" class="headerlink" title="LR vs SVM"></a>LR vs SVM</h2><p>Difference:</p>
<ol>
<li>LR uses logistic loss, while SVM uses hinge loss.</li>
<li>LR is sensitive to outliers, while SVM is not.</li>
<li>SVM is suitable for small training set, while LR needs much.</li>
<li>LR tries to find a hyperplane that stays far away with all points (all points count), whereas SVM only aims at keeping away support vectors.</li>
<li>LR requires feature enginnering, SVM uses kernel trick.</li>
<li>SVM is non-parametric methods, whereas LR is parametric model.</li>
</ol>
<h3 id="Logistic-Regression-LR"><a href="#Logistic-Regression-LR" class="headerlink" title="Logistic Regression (LR)"></a>Logistic Regression (LR)</h3><p>Logistic Regression (LR) is a linear mapping from features $x$ to labels $y \in \{ 0,1 \}$ with sigmoid function $g(z)=1/(1+\exp(-z))$.</p>
<p>The LR is fomulated as:</p>
<script type="math/tex; mode=display">
\begin{equation}
h_{\theta} = g(\theta^\top x) = \frac{1}{1+\exp(-\theta^\top x)}
\end{equation}</script><p>The derivative of sigmoid function is:</p>
<script type="math/tex; mode=display">
\begin{align}
g^\prime(z) &{}= \frac{d}{dz} \frac{1}{1+\exp(-\theta^\top x)}\\
&{}= \frac{1}{(1+\exp(-\theta^\top x))^2} (\exp(-\theta^\top x))\\
&{}= \frac{1}{1+\exp(-\theta^\top x)} \cdot \bigg( 1- \frac{1}{1+\exp(-\theta^\top x)} \bigg) \\
&{}= g(z)(1-g(z))
\end{align}</script><p>LR can be used for binary classification, thus</p>
<script type="math/tex; mode=display">
\begin{align}
 P(y=1 \vert x; \theta) &{}= h_{\theta} (x)\\
 P(y=0 \vert x; \theta) &{}= (1-h_{\theta} (x))
\end{align}</script><p>That is,</p>
<script type="math/tex; mode=display">
p(y \vert x, \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{(1-y)}</script><p>Given the training data, the features $x = \{ x_1, x_2, \cdots, x_m \}$ and labels $y = \{ y_1, y_2, \cdots, y_m \}$. The maximum likelihood function is:</p>
<script type="math/tex; mode=display">
\begin{align}
\ell (\theta) &{}= \log \mathcal{L}(\theta) \\
&{}= \sum_{i=1}^m y^{(i)} \log h(x^{(i)}) + (1-y^{(i)}) \log (1-h(x^{(i)}))
\end{align}</script><p>With gradient ascend algorithm, we have $\theta : \theta + \alpha \nabla_{\theta}\ell (\theta)$.</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial \theta_j} &{}= \bigg( y \frac{1}{g(\theta^T x)} - (1-y) \frac{1}{1-g(\theta^T x)} \bigg) \frac{\partial}{\partial \theta_j} g(\theta^T x) \\
&{}= \bigg( y \frac{1}{g(\theta^T x)} - (1-y) \frac{1}{1-g(\theta^T x)} \bigg) g(\theta^T x) (1-g(\theta^T x)) \frac{\partial}{\partial \theta_j} \theta^T x \\
&{}= (y (1-g(\theta^T x)) - (1-y)g(\theta^T x)) x_j \\
&{}= (y-h_\theta (x)) x_j
\end{align}</script><p>If we only use one sample to train, the update can be formulated as:</p>
<script type="math/tex; mode=display">
\theta_j: \theta_j + \alpha (y^{(i)} - h_\theta (x^{(i)})) x_{j}^{(i)}</script><p>The loss function is:</p>
<script type="math/tex; mode=display">
\begin{align}
J(w, b) &{}= \frac{1}{m} \sum_{i=1}^m \mathcal{L} (\hat{y}^{(i)}, y^{(i)})\\
&{}= \frac{1}{m} \sum_{i=1}^m (-y \log (\hat{y}^{(i)})- (1-y) \log (1-\hat{y}^{(i)}))
\end{align}</script><h3 id="Linear-SVM"><a href="#Linear-SVM" class="headerlink" title="Linear SVM"></a>Linear SVM</h3><p>Given a training dataset of $m$ points of the form $(\mathbf{x}_1,y_1)， \cdots,(\mathbf{x}_m,y_m)$, where $y \in \{-1,1\}$, each indicating the calss to which the point $x_i$ belongs. We want to find the maximum-margin hyperplane that divides the group of points $\mathbf{x}_i$ into two groups so that the distance between the hyperplane and the nearest point from either group is maximized.</p>
<p>Any hyperplane can be written as the set of points $\mathbf{x}$ satisfying $\mathbf{w}^T\mathbf{x}+\mathbf{b}=0$.</p>
<h4 id="Hard-margin"><a href="#Hard-margin" class="headerlink" title="Hard margin"></a>Hard margin</h4><p>If the training data is linearly separable, we can select to parallel hyperplanes that separate the two clases of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the “margin”, and the maximum-margin hyperplanes is the hyperplane that lies halfway between them.<br>The optimization aims to “minimize $\Vert \mathbf{w} \Vert$ subject to $y_i (w^T x_i + b) \geq 1$ for $\forall i$”:</p>
<ol>
<li>Maximize the margin: $ \min_{\mathbf{w,b}} \frac{1}{2} \Vert \mathbf{w} \Vert^2$</li>
<li>Classify: $y_i (w^T x_i + b) \geq 1, \quad i=1,2,3,\cdots,m$</li>
</ol>
<p>where the $\mathbf{w},\mathbf{b}$ determine our classifier $\mathbf{x} \rightarrow \textrm{sign} (\mathbf{w}^T \mathbf{x} + \mathbf{b})$</p>
<h4 id="Soft-margin"><a href="#Soft-margin" class="headerlink" title="Soft margin"></a>Soft margin</h4><ul>
<li><p>Hinge Loss<br>When the data are not linearly separable, the hinge loss<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: Hinge Loss](https://en.wikipedia.org/wiki/Hinge_loss)
">[1]</span></a></sup> is helpful:</p>
<script type="math/tex; mode=display">
\begin{align}
\max(0, 1- \underbrace{y_i}_\textrm{label} \underbrace{(\mathbf{w}^T \mathbf{x}_i + \mathbf{b})}_\textrm{prediction})
\end{align}</script><p>The hinge loss is zero if the constraint $y_i (w^T x_i + b) \geq 1$ is satisfied, <em>i.e.</em>, if $\mathbf{x}_i$ lies on the correct side of the margin. For data on the wrong side of the margin (-1 vs 1), the hinge loss is proportional to the distance from the margin.</p>
</li>
<li><p>Soft margin objective<br>The optimization goal is to minimize</p>
<script type="math/tex; mode=display">
\begin{align}
\lambda \Vert \mathbf{w} \Vert^2 +  \bigg[ \frac{1}{n} \sum_{i=1}^n \max (0, 1- \underbrace{y_i}_\textrm{label}\underbrace{(\mathbf{w}^T\mathbf{x}_i + \mathbf{b}))}_\textrm{prediction} \bigg]
\end{align}</script><p>where the parameter $\lambda$ determines the trade-off between increasing the margin size and ensuring that the $\mathbf{x}_i$ lie on the correct side of the margin. Thus, for sufficiently small values of $\lambda$, the second term in the loss function will become negligible, hence it will behave similar to the hard-margin SVM.</p>
</li>
</ul>
<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Let $x$ represent the observation, $y$ denote the labels. CRF can be formulated as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y|x) = \frac{\exp(\textrm{score}(x,y))}{\sum_{y'}\exp(\textrm{score}(x,y'))}
\end{aligned}</script><p>where </p>
<script type="math/tex; mode=display">\textrm{score}(x,y) = \sum_{i} T_{y_i, y_{i+1}} + \sum_{i} E_{i, y_i}</script><p>The loss function would be given as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell &{}= -\log (p (y \vert x))\\
&{}= - \textrm{score}(x,y) + \log (\sum_{y'} (\exp(\textrm{score}(x, y'))))
\end{aligned}</script><h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><h3 id="GBDT-Xgboost"><a href="#GBDT-Xgboost" class="headerlink" title="GBDT / Xgboost"></a>GBDT / Xgboost</h3><h2 id="L1-L2-Regularization"><a href="#L1-L2-Regularization" class="headerlink" title="L1/L2 Regularization"></a>L1/L2 Regularization</h2><h3 id="L1-regularization"><a href="#L1-regularization" class="headerlink" title="L1 regularization"></a>L1 regularization</h3><p>The L1 regularization is given as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
C = C_0 + \frac{\lambda}{2n} \sum_w \Vert w \Vert^2 \\
\end{aligned}</script><p>Thus,</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w} = \frac{\partial C}{\partial w} + \frac{\lambda}{n} w</script><p>The weight update is:</p>
<script type="math/tex; mode=display">
\begin{aligned}
w & \rightarrow w - \eta \frac{\partial C_0}{\partial w} - \frac{\eta \lambda}{n} \\
&{}= \underbrace{\big( 1- \frac{\eta \lambda}{n} \big) w }_{\Downarrow decrease }- \eta \frac{\partial C_0}{\partial w}
\end{aligned}</script><h3 id="L2-regularization"><a href="#L2-regularization" class="headerlink" title="L2 regularization"></a>L2 regularization</h3><p>The L2 regularization is given as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
C &{}= C_0 + \frac{\lambda}{2n} \sum_w \vert w \vert 
\end{aligned}</script><p>Thus, </p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w} = \frac{\partial C}{\partial w} + \frac{\lambda}{n} \textrm{sgn} (w)</script><p>The weight update is:</p>
<script type="math/tex; mode=display">
\begin{aligned}
w & \rightarrow w - \frac{\eta \lambda}{n} \textrm{sgn}(w) - \eta \frac{\partial C_0}{\partial w} 
\end{aligned}</script><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h3><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><h2 id="Class-Imbalance-Long-tailed-Learning"><a href="#Class-Imbalance-Long-tailed-Learning" class="headerlink" title="Class Imbalance / Long-tailed Learning"></a>Class Imbalance / Long-tailed Learning</h2><p>Extant <strong>class imbalance</strong><sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Long-Tail Learning via Logit Adjustment](https://arxiv.org/pdf/2007.07314.pdf)
">[12]</span></a></sup><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Data Imbalance blog](https://kexue.fm/archives/7615)">[13]</span></a></sup> methods:</p>
<ol>
<li>the <em>input</em> to a model (Data modification)<ul>
<li>Under-sampling</li>
<li>Over-sampling</li>
<li>Feature Transfer</li>
</ul>
</li>
<li>the <em>output</em> of a model (Post-hoc correction of the decision threshold)<ul>
<li>Modify threshold</li>
<li>Normalize weights</li>
</ul>
</li>
<li>the <em>internals</em> of a model (e.g., loss function)<ul>
<li>Loss balancing</li>
<li>Volume weighting</li>
<li>Average top-k loss</li>
<li>Domain adaptation</li>
<li>Label aware margin</li>
</ul>
</li>
</ol>
<h2 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h2><h3 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL Divergence"></a>KL Divergence</h3><p>Kullback-Leibler (KL) divergence<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
">[9]</span></a></sup> (a.k.a, relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. </p>
<p>Consider two probability distributions $P$ and $Q$. Usually, $P$ represents the data, the observations, or a measured probability distribution. Distribution $Q$ represents instead a theory, a model, a description or an approxmation of $P$. The KL divergence is then interpreted as <strong>the average difference of the number of bits required for encoding samples of $P$ using a code optimized for $Q$ rather than one optimized for $P$</strong>.</p>
<p>For discrete probability distributions $P$ and $Q$ defined on the same probability space $\chi$, the <strong>relative entropy from $Q$ to $P$</strong> is defined to be:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{KL}(P \Vert Q) &{}= \sum_{x \in \chi} P(x) \log \bigg( \frac{P(x)}{Q(x)} \bigg) \\
&{}= - \sum_{x \in \chi} P(x) \log \bigg( \frac{Q(x)}{P(x)} \bigg)
\end{align}</script><p>The relative entropy can be interpreted as the expected message-length per datum that must be communicated if a code that is optimal for a given (wrong) distribution $Q$ is used, compared to using a code based on the true distribution $P$.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{KL} (P \Vert Q) &{}= - \sum_{x \in \chi} p(x) \log q(x) + \sum_{x \in \chi} p(x) \log p(x) \\
&{}= \mathbb{H}(P \vert Q) - \mathbb{H}(P)
\end{align}</script><p>where $\mathbb{H}(P \vert Q)$ indicates the cross entropy of P and Q, $\mathbb{H}(P)$ is the entropy of P.</p>
<h4 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Non-negative</li>
<li>Asymmetric</li>
</ol>
<h3 id="JS-Divergence"><a href="#JS-Divergence" class="headerlink" title="JS Divergence"></a>JS Divergence</h3><p>Jensen-Shannon (JS) divergence is a measure of similarity between two probablity distributions.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{JS}(P \vert Q) = \frac{1}{2} \mathbb{KL}(P \Vert M) + \frac{1}{2} \mathbb{KL}(Q \Vert M)
\end{align}</script><p>where $M = \frac{1}{2}(P+Q)$</p>
<h4 id="Properties-1"><a href="#Properties-1" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Symmetric</li>
<li>Bound $0 \leq JSD \leq 1$</li>
</ol>
<h3 id="Mutual-Information"><a href="#Mutual-Information" class="headerlink" title="Mutual Information"></a>Mutual Information</h3><p>Mutual Information (MI)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)
">[10]</span></a></sup> measures the mutual dependence between the two variables. </p>
<script type="math/tex; mode=display">
\begin{align}
I(X; Y) &{}= \mathbb{KL} (P(X,Y) \Vert P(X)P(Y)) \\
&{}= \mathbb{E}_{X} \{\mathbb{KL}(P(Y \vert X) \Vert P(Y))\}\\
&{}= \mathbb{E}_{Y} \{\mathbb{KL}(P(X \vert Y) \Vert P(X))\}
\end{align}</script><p>For discrete variables $X$ and $Y$ the MI is:</p>
<script type="math/tex; mode=display">
\begin{align}
I(X;Y) = \sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p_{(X,Y)} (x,y) \log \bigg( \frac{p_{(X,Y)}(x,y)}{p_X(x)_Yp(y)} \bigg)
\end{align}</script><h4 id="Properties-2"><a href="#Properties-2" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Non-negative: $I(X;Y) \geq 0$</li>
<li>Symmetry: $I(X;Y) = I(Y;X)$</li>
</ol>
<h2 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h2><h3 id="ROC-AUC"><a href="#ROC-AUC" class="headerlink" title="ROC / AUC"></a>ROC / AUC</h3><h4 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h4><p>Reiceiver Operating Characteristic (ROC) Curve is a plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.</p>
<ul>
<li>x-axis: false positive rate (FPR), a.k.a, sensitivity, recall, probability of detection.</li>
<li>y-axis: true positive rate (TPR), a.k.a. probability of false alarm.</li>
</ul>
<p>ROC is a comparison of two operating characteristics (TPR and FPR) as the criterion changes. </p>
<p>True Positive Rate (TPR) is a synonym for <strong>recall</strong> and is therefore defined as follows:</p>
<script type="math/tex; mode=display">
\textrm{TPR} = \frac{\textrm{TP}}{\textrm{TP+FN}}</script><p>False Positive Rate (FPR) is defined as follows:</p>
<script type="math/tex; mode=display">
\textrm{FPR} = \frac{\textrm{FP}}{\textrm{FP+TN}}</script><p>An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. </p>
<p><img data-src="/notes/images/ROC.png" width='40%' /></p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>Area under the ROC Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.</p>
<p><img data-src="/notes/images/AUC.png" width='40%' /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">y_pred = <span class="built_in">list</span>(np.random.uniform(<span class="number">.4</span>, <span class="number">.6</span>, <span class="number">2000</span>)) + <span class="built_in">list</span>(np.random.uniform(<span class="number">.5</span>, <span class="number">.7</span>, <span class="number">8000</span>))</span><br><span class="line">y_true = [<span class="number">0</span>] * <span class="number">2000</span> + [<span class="number">1</span>] * <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_auc</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    pair = <span class="built_in">list</span>(<span class="built_in">zip</span>(y_true, y_pred))</span><br><span class="line">    pair = <span class="built_in">sorted</span>(pair, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    df = pd.DataFrame([[x[<span class="number">0</span>], x[<span class="number">1</span>], i + <span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(pair)], columns=[<span class="string">&#x27;y_true&#x27;</span>, <span class="string">&#x27;y_pred&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> df[<span class="string">&#x27;y_pred&#x27;</span>].value_counts().items():</span><br><span class="line">        <span class="keyword">if</span> v == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        rank_mean = df[df[<span class="string">&#x27;y_pred&#x27;</span>] == k][<span class="string">&#x27;rank&#x27;</span>].mean()</span><br><span class="line">        df.loc[df[<span class="string">&#x27;y_pred&#x27;</span>] == k, <span class="string">&#x27;rank&#x27;</span>] = rank_mean</span><br><span class="line">    pos_df = df[df[<span class="string">&#x27;y_true&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line">    m = pos_df.shape[<span class="number">0</span>]</span><br><span class="line">    n = df.shape[<span class="number">0</span>] - m</span><br><span class="line">    <span class="keyword">return</span> (pos_df[<span class="string">&#x27;rank&#x27;</span>].<span class="built_in">sum</span>() - m * (m + <span class="number">1</span>) / <span class="number">2</span>) / (m * n)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(calc_auc(y_true, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="built_in">print</span>(roc_auc_score(y_true, y_pred))</span><br></pre></td></tr></table></figure>
<h3 id="F1-Measure"><a href="#F1-Measure" class="headerlink" title="F1-Measure"></a>F1-Measure</h3><ul>
<li>Micro-F1: calculate metrics globally by counting total TP,FN,FP</li>
<li>Macro-F1: calculate metrics for each label =&gt; unweighted mean.</li>
<li>Weighted-F1: calculate metrics for each label =&gt;  average weighted by support (# of true instances for each class)</li>
</ul>
<div class="note warning">
            <p><strong>Comparison between ROC and F1-measure</strong>:</p><ol><li>Both look at the precision scores (TPR): ROC looks at the True Positive Rate (TPR/Recall) and False Positive Rate (FPR) while F1 looks at Positive Predictive Value (PPV/Precision) and True Positive Rate (TPR/Recall).<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[F1 score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?]">[11]</span></a></sup></li><li><strong>F1 score</strong> cares more about the <strong>positive class</strong>, such as highly <strong>imbalanced</strong> dataset where the fraction of positive class is small.</li><li><strong>ROC</strong> cares equally about the <strong>positive and negative class</strong> or the dataset is quite <strong>balanced</strong>.</li></ol>
          </div>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><h2 id="Batch-Norm-vs-Layer-Norm"><a href="#Batch-Norm-vs-Layer-Norm" class="headerlink" title="Batch Norm vs Layer Norm"></a>Batch Norm vs Layer Norm</h2><script type="math/tex; mode=display">
y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</script><ul>
<li>BN normalizes along one batch (first dim), LN does on one sample (last dim). </li>
<li>Refer to <a href="/notes/2019/02/28/NN/Normalization-in-Neural-Networks-a-Summary/">details</a></li>
</ul>
<h2 id="Gradient-Vanishing-Exploding"><a href="#Gradient-Vanishing-Exploding" class="headerlink" title="Gradient Vanishing/Exploding"></a>Gradient Vanishing/Exploding</h2><p>Gradient vanishing/exploding arises from the issues of backpropagation, in other words, the accumulated multiplication of smaller-than-1 or greater-than-1 gradient values.</p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><ol>
<li>Pretraining-Finetuning per layer</li>
<li>Gradient Clip / Weight Regularization</li>
<li>Activation function: avoid to use sigmoid.</li>
<li>Appropriate weight initialization: Xavier-Glorot initialization<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Written Memories: Understanding, Deriving and Extending the LSTM](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)
">[4]</span></a></sup></li>
<li>Batch Norm: reduce the covariant shift of training dataset.</li>
<li>Residual Connection</li>
<li>LSTM: refer to <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Written Memories: Understanding, Deriving and Extending the LSTM](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)
">[4]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[LSTM eased gradient vanishing explanations (in Chinese)](https://www.zhihu.com/question/34878706)
">[5]</span></a></sup>.</li>
</ol>
<h2 id="RNNs"><a href="#RNNs" class="headerlink" title="RNNs"></a>RNNs</h2><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
">[6]</span></a></sup> integrates three gates: input gate, forget gate, and output gate.</p>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  &{}= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c) \\
\mathbf{c}^c_j &{}= \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j} \\
\mathbf{h}_j^c &{}= \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)
\end{align}</script><p><img data-src="/notes/images/LSTM.png" width='80%' /><br><!--![LSTM<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
">[6]</span></a></sup>](/notes/images/LSTM.png)--><br><img data-src="/notes/images/LSTM-1.png" alt="LSTM" width='40%' /></p>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>GRU has three gates: update gate (vs input/output gate in LSTM) and reset gate.<br><img data-src="/notes/images/GRU.png" width='80%' /></p>
<p><img data-src="/notes/images/GRU-1.png" alt="GRU" width='40%' /></p>
<!--![GRU<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
">[6]</span></a></sup>](/notes/images/GRU.png)-->
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>See <a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/">Transformer blog</a></p>
<h2 id="Backprop-with-Softmax-XE"><a href="#Backprop-with-Softmax-XE" class="headerlink" title="Backprop with Softmax + XE"></a>Backprop with Softmax + XE</h2><p>Refer to <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Softmax classification with cross-entropy (2/2)](https://peterroelants.github.io/posts/cross-entropy-softmax/)
">[7]</span></a></sup>.</p>
<h3 id="Softmax-Forward"><a href="#Softmax-Forward" class="headerlink" title="Softmax Forward"></a>Softmax Forward</h3><p>Given the softmax written in:</p>
<script type="math/tex; mode=display">
\textrm{softmax}(a_i) = p_i = \frac{\exp(a_i)}{\sum_{j}^N \exp(a_j)}</script><p>where $a_i, i=1,2,\cdots,N$ is the output logits, $p_i$ is the predicted probability of $i$-th class, and</p>
<script type="math/tex; mode=display">\sum_{i=1}^N p_i = 1</script><h4 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h4><p>The computation of softmax will first reduce the maximum value of $A=[a_1, a_2, \cdots, a_N]$ to avoid the overflow of exp(.).</p>
<p>We have </p>
<script type="math/tex; mode=display">
\begin{align}
p_i &{}= \frac{\exp(a_i)}{\sum_j^N \exp(a_i)} \\
&{}= \frac{C \exp(a_i)}{C \sum_j^N \exp(a_i)} \\
&{}= \frac{\exp(\log C) \exp(a_i)}{\exp(\log C) \sum_j^N \exp(a_i)} \\
&{}= \frac{\exp(a_i + \log C)}{\sum_j^N \exp(a_i + \log C)} \\
&{}= \frac{\exp(a_i - max(A))}{\sum_j^N \exp(a_i - max(A))} \\
\end{align}</script><p>where $C$ is constant.</p>
<h3 id="Cross-Entropy-Forward"><a href="#Cross-Entropy-Forward" class="headerlink" title="Cross Entropy Forward"></a>Cross Entropy Forward</h3><p>Denote the Cross Entropy (XE) loss as $H$:</p>
<script type="math/tex; mode=display">
\ell(y_i, p_i) = H(y_i, p_i) = -\sum_{i}^N y_i \cdot \log p_i</script><h3 id="Softmax-Derivative"><a href="#Softmax-Derivative" class="headerlink" title="Softmax Derivative"></a>Softmax Derivative</h3><p>The derivative of softmax w.r.t $a_i$ is:</p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} = \frac{\partial \big( \frac{\exp(a_i)}{\sum_j^N \exp(a_i)} \big)}{\partial a_j}</script><p>For brevity, let $\sum = \sum_j^N \exp(a_j)$.</p>
<ol>
<li><p>When $i=j$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial p_i}{\partial a_j} &{}= \frac{\exp(a_i) \cdot \sum - \exp(a_i)\cdot \exp(a_j)}{\sum\cdot \sum} \\
&{}= \frac{\exp(a_i) (\sum - \exp(a_i))}{\sum\cdot \sum} \\
&{}= p_i (1-p_j)
\end{align}</script></li>
<li><p>When $i \neq j$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial p_i}{\partial a_j} &{}= \frac{0 \cdot \sum - \exp(a_i)\cdot \exp(a_j)}{\sum\cdot \sum} \\
&{}= - p_i \cdot p_j 
\end{align}</script></li>
</ol>
<h3 id="XE-Softmax-Derivative"><a href="#XE-Softmax-Derivative" class="headerlink" title="XE+Softmax Derivative"></a>XE+Softmax Derivative</h3><p>The derivative of XE is:</p>
<script type="math/tex; mode=display">
\begin{equation}
H^\prime(y_i, p_i) = - \sum_i^N y_i \frac{1}{p_i}
\end{equation}</script><p>According to the chain rule, the derivative w.r.t $a_j$ is:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial H}{\partial a_j} &{}= \frac{\partial H}{\partial p_i} \cdot \frac{\partial p_i}{\partial a_j}\\
&{}= \bigg( -\sum_i y_i \frac{1}{p_i} \bigg) \cdot \frac{\partial p_i}{\partial a_j}  \label{eq:xe_derivative}
\end{align}</script><ol>
<li><p>When $i=j$</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{Eq.} \eqref{eq:xe_derivative} &{}= -\sum_{i=j} y_i \frac{1}{p_i}\cdot p_i \cdot (1-p_j) \\
&{}= -\sum_{i=j} y_i \cdot (1 - p_j) \\
&{}= -y_i + y_i p_i  \label{eq:s1}
\end{align}</script></li>
<li><p>When $i \neq j$, the Eq. $\eqref{eq:xe_derivative}$ is:</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{Eq.} \eqref{eq:xe_derivative}  &{}= -\sum_{i \neq j} y_i \frac{1}{p_i}\cdot (- p_i \cdot p_j) \\
&{}= \sum_{i \neq j} y_i p_j  \label{eq:s2}
\end{align}</script></li>
</ol>
<p>Since above two scenarios are independent, combining Eq. $\eqref{eq:s1}$ and $\eqref{eq:s2}$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{Eq.} \eqref{eq:xe_derivative}  &{}= \textrm{Eq.}\eqref{eq:s1} + \textrm{Eq.}\eqref{eq:s2} \\
&{}= -y_i + y_i p_i + \sum_{i \neq j} y_i p_j \\
&{}= -y_i + (\sum_{i=j}y_i p_j + \sum_{i \neq j} y_i p_j)\\
&{}= -y_i + \sum_i^N y_i p_i \label{eq:one_hot} \\
&{}= p_j - y_i \label{eq:ij}\\
&{}= p_j - y_j
\end{align}</script><p>In Eq.$\eqref{eq:one_hot}$, we have $\sum_i^N y_i = 1$;<br>In Eq.$\eqref{eq:ij}$, we have $\sum_i^N y_i = y_j$.</p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><h2 id="Static-Word-Representation"><a href="#Static-Word-Representation" class="headerlink" title="Static Word Representation"></a>Static Word Representation</h2><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><h4 id="Hierarchical-Softmax-Negative-Sampling"><a href="#Hierarchical-Softmax-Negative-Sampling" class="headerlink" title="Hierarchical Softmax / Negative Sampling"></a>Hierarchical Softmax / Negative Sampling</h4><p>Refer to <a href="/notes/2019/12/13/NN/Efficient-Softmax-Explained/">my blog</a></p>
<ul>
<li>Hierarchical Softmax: $|V| =&gt; \log |V|$ using huffman tree</li>
<li>Negative Sampling</li>
</ul>
<h4 id="W2V-vs-GloVe"><a href="#W2V-vs-GloVe" class="headerlink" title="W2V vs GloVe"></a>W2V vs GloVe</h4><h3 id="BPE-vs-WordPiece"><a href="#BPE-vs-WordPiece" class="headerlink" title="BPE vs WordPiece"></a>BPE vs WordPiece</h3><p>Refer to <a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/">OOV blog</a></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hinge_loss">Wiki: Hinge Loss</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76946313">SVM Blog</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2018-10-17-20">SVM Derivatives (in Chinese)</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">Written Memories: Understanding, Deriving and Extending the LSTM</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34878706">LSTM eased gradient vanishing explanations (in Chinese)</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://peterroelants.github.io/posts/cross-entropy-softmax/">Softmax classification with cross-entropy (2/2)</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://blog.csdn.net/MacKendy/article/details/106391817">Softmax+XE Backpropagation (in Chinese)</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Wiki: KL divergence</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mutual_information">Wiki: Mutual Information</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">[F1 score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?]<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.07314.pdf">Long-Tail Learning via Logit Adjustment</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kexue.fm/archives/7615">Data Imbalance blog</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/NLP/" rel="tag"># NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/notes/2018/11/21/NLP/NER/NER-Evaluation-Metrics/" rel="next" title="Evaluation Metrics of Named Entity Recognition">
      Evaluation Metrics of Named Entity Recognition <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Statistical-ML"><span class="nav-number">1.</span> <span class="nav-text">Statistical ML</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LR-vs-SVM"><span class="nav-number">1.1.</span> <span class="nav-text">LR vs SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-LR"><span class="nav-number">1.1.1.</span> <span class="nav-text">Logistic Regression (LR)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-SVM"><span class="nav-number">1.1.2.</span> <span class="nav-text">Linear SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hard-margin"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Hard margin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Soft-margin"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Soft margin</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CRF"><span class="nav-number">1.2.</span> <span class="nav-text">CRF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss"><span class="nav-number">1.2.1.</span> <span class="nav-text">Loss</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decision-Tree"><span class="nav-number">1.3.</span> <span class="nav-text">Decision Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT-Xgboost"><span class="nav-number">1.3.1.</span> <span class="nav-text">GBDT &#x2F; Xgboost</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1-L2-Regularization"><span class="nav-number">1.4.</span> <span class="nav-text">L1&#x2F;L2 Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-regularization"><span class="nav-number">1.4.1.</span> <span class="nav-text">L1 regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L2-regularization"><span class="nav-number">1.4.2.</span> <span class="nav-text">L2 regularization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementation"><span class="nav-number">1.5.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KMeans"><span class="nav-number">1.5.1.</span> <span class="nav-text">KMeans</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN"><span class="nav-number">1.5.2.</span> <span class="nav-text">KNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Class-Imbalance-Long-tailed-Learning"><span class="nav-number">1.6.</span> <span class="nav-text">Class Imbalance &#x2F; Long-tailed Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Information-Theory"><span class="nav-number">1.7.</span> <span class="nav-text">Information Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KL-Divergence"><span class="nav-number">1.7.1.</span> <span class="nav-text">KL Divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Properties"><span class="nav-number">1.7.1.1.</span> <span class="nav-text">Properties</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JS-Divergence"><span class="nav-number">1.7.2.</span> <span class="nav-text">JS Divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Properties-1"><span class="nav-number">1.7.2.1.</span> <span class="nav-text">Properties</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mutual-Information"><span class="nav-number">1.7.3.</span> <span class="nav-text">Mutual Information</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Properties-2"><span class="nav-number">1.7.3.1.</span> <span class="nav-text">Properties</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Metric"><span class="nav-number">1.8.</span> <span class="nav-text">Evaluation Metric</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ROC-AUC"><span class="nav-number">1.8.1.</span> <span class="nav-text">ROC &#x2F; AUC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ROC"><span class="nav-number">1.8.1.1.</span> <span class="nav-text">ROC</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AUC"><span class="nav-number">1.8.2.</span> <span class="nav-text">AUC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F1-Measure"><span class="nav-number">1.8.3.</span> <span class="nav-text">F1-Measure</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Learning"><span class="nav-number">2.</span> <span class="nav-text">Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Norm-vs-Layer-Norm"><span class="nav-number">2.1.</span> <span class="nav-text">Batch Norm vs Layer Norm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Vanishing-Exploding"><span class="nav-number">2.2.</span> <span class="nav-text">Gradient Vanishing&#x2F;Exploding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Solution"><span class="nav-number">2.2.1.</span> <span class="nav-text">Solution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNNs"><span class="nav-number">2.3.</span> <span class="nav-text">RNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM"><span class="nav-number">2.3.1.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU"><span class="nav-number">2.3.2.</span> <span class="nav-text">GRU</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">2.4.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backprop-with-Softmax-XE"><span class="nav-number">2.5.</span> <span class="nav-text">Backprop with Softmax + XE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-Forward"><span class="nav-number">2.5.1.</span> <span class="nav-text">Softmax Forward</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computation"><span class="nav-number">2.5.1.1.</span> <span class="nav-text">Computation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Entropy-Forward"><span class="nav-number">2.5.2.</span> <span class="nav-text">Cross Entropy Forward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-Derivative"><span class="nav-number">2.5.3.</span> <span class="nav-text">Softmax Derivative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XE-Softmax-Derivative"><span class="nav-number">2.5.4.</span> <span class="nav-text">XE+Softmax Derivative</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP"><span class="nav-number">3.</span> <span class="nav-text">NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Static-Word-Representation"><span class="nav-number">3.1.</span> <span class="nav-text">Static Word Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2Vec"><span class="nav-number">3.1.1.</span> <span class="nav-text">Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hierarchical-Softmax-Negative-Sampling"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">Hierarchical Softmax &#x2F; Negative Sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#W2V-vs-GloVe"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">W2V vs GloVe</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BPE-vs-WordPiece"><span class="nav-number">3.1.2.</span> <span class="nav-text">BPE vs WordPiece</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="cyk1337"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">cyk1337</p>
  <div class="site-description" itemprop="description">What is now proved was once only imagined.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cyk1337</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2018/06/29/NLP/NER/NLP-Basics-Summary/";
    this.page.identifier = "2018/06/29/NLP/NER/NLP-Basics-Summary/";
    this.page.title = "NLP Basics";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
