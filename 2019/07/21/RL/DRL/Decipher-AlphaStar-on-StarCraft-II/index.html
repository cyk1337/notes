<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="On 19 December 2018, AlphaStar has decisively beaten human player MaNa with 5-0 on StarCraft II, one of the most challenging Real-Time-Strategy (RTS) games.  2019-07-23 slides2019-11-12 slides">
<meta property="og:type" content="article">
<meta property="og:title" content="Deciphering AlphaStar on StarCraft II">
<meta property="og:url" content="https://cyk1337.github.io/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/index.html">
<meta property="og:site_name" content="The Gradient">
<meta property="og:description" content="On 19 December 2018, AlphaStar has decisively beaten human player MaNa with 5-0 on StarCraft II, one of the most challenging Real-Time-Strategy (RTS) games.  2019-07-23 slides2019-11-12 slides">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/alphaStarVIsualization.gif">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/alphaStar-league.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/alphaStar_NN.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/nature-alphastar-model.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/AlphaStar-using-camera-based-model.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-relational-inductive-biases.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Pointer-nets.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/GatedResNet.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FiLM.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FiLM-ed-Net.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-COMA.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-COMA-ac.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-policy-distillation.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-policy-distillation-multi-task.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-IMPALA-fig.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-IMPALA-paper-fig.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-IMPALA-model.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PBT-fig.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PBT-alg.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/rl-pbt-network-architecture.png">
<meta property="article:published_time" content="2019-07-21T04:11:00.000Z">
<meta property="article:modified_time" content="2024-07-08T12:01:01.565Z">
<meta property="article:author" content="cyk1337">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="StarCraft II">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/alphaStarVIsualization.gif">

<link rel="canonical" href="https://cyk1337.github.io/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Deciphering AlphaStar on StarCraft II | The Gradient</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The Gradient</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Language is not just words.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="cyk1337">
      <meta itemprop="description" content="What is now proved was once only imagined.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Gradient">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deciphering AlphaStar on StarCraft II
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-07-21 12:11:00" itemprop="dateCreated datePublished" datetime="2019-07-21T12:11:00+08:00">2019-07-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/RL/" itemprop="url" rel="index"><span itemprop="name">RL</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>On 19 December 2018, AlphaStar has decisively beaten human player MaNa with 5-0 on StarCraft II, one of the most challenging Real-Time-Strategy (RTS) games.</p>
<p><img data-src="/notes/images/alphaStarVIsualization.gif" /></p>
<p><a href="https://cyk1337.github.io/slides/2019-07-23-AlphaStar.pdf">2019-07-23 slides</a><br><a href="https://cyk1337.github.io/slides/2019-11-12-AlphaStarII.pdf">2019-11-12 slides</a><br><span id="more"></span></p>
<h1 id="How-AlphaStar-is-trained"><a href="#How-AlphaStar-is-trained" class="headerlink" title="How AlphaStar is trained"></a>How AlphaStar is trained</h1><ul>
<li>Imitation learning (supervised)<ul>
<li>AlphaStar is initially trained with <strong>imitation learning</strong> with anonymous game replays from human experts</li>
<li>This offers a good initialization for neural networks</li>
<li>This initial agent beats the built-in “Elite” level AI ($\approx$ human golden level)</li>
</ul>
</li>
<li>RL<ul>
<li>Seed a multi-agent reinforcement learning process with a continuous league</li>
</ul>
</li>
</ul>
<h2 id="AlphaStar-League"><a href="#AlphaStar-League" class="headerlink" title="AlphaStar League"></a>AlphaStar League</h2><p>Multi-agent RL in the presence of strategy cycles</p>
<ul>
<li>The league contains $m$ agents and $n$ competitors<ul>
<li>Agent: actively learning agent, updating a policy</li>
<li>Competitor: passive policy, not learning</li>
</ul>
</li>
<li>Each agent plays games against other agents/competitors and learns by RL<ul>
<li>Ensuring robust performance against a wide diversity of counter-strategies</li>
<li>Several mechanisms encourage diversity among the league</li>
</ul>
</li>
<li>Agents periodically replicate into a competitor<ul>
<li>The new competitor is added to the league</li>
<li>Ensure that agents do not forget how to defeat old selves</li>
</ul>
</li>
</ul>
<p><img data-src="/notes/images/alphaStar-league.png" alt="AlphaStar League Training"></p>
<h3 id="Agent-Diversity"><a href="#Agent-Diversity" class="headerlink" title="Agent Diversity"></a>Agent Diversity</h3><p>Each agent’s personalized objective is described by:</p>
<ul>
<li>Matchmaking distribution over opponent to play in each game</li>
<li>Intrinsic reward function specifying preferences (e.g. over unit types)</li>
</ul>
<h3 id="Matchmaking-Strategies"><a href="#Matchmaking-Strategies" class="headerlink" title="Matchmaking Strategies"></a>Matchmaking Strategies</h3><p>Prioritised Fictious Self-Play (pFSP)</p>
<ul>
<li>Matchmaking distribution is proportional to win-rate of opponents</li>
<li>Encourages monotonic progress against set of competitors</li>
</ul>
<p>Exploiters</p>
<ul>
<li>Matchmaking distribution is uniform over individual agents</li>
<li>The sole goal of an exploiter is to identify the weaknesses in agents</li>
<li>The agents can then learn to defend against those weakness</li>
</ul>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p>Agent parameters updated by <strong>off-policy</strong> RL from replayed subsequence</p>
<ul>
<li>Advantage Actor-Critic</li>
<li>V-Trace + TD($\lambda$)</li>
<li>Self-imitation learning</li>
<li>Entropy regularisation</li>
<li>Policy distillation</li>
</ul>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><p><strong>Exploration and diversity</strong></p>
<ul>
<li>Solution 1: use <strong>human data</strong> to aid in exploration and to preserve strategic diversity throughout training. After initialization with SL, AlphaStar continually <strong>minimizes the KL divergence between the supervised and current policy</strong>.</li>
<li>Solution 2: apply <strong>pseudo-rewards</strong> to follow a strategy statistic $z$, from randomly sampled human data. The pseudo-rewards measure the edit distance between sampled and execeuted build orders; and the Hamming distances between sampled and executed cumulative statistics.</li>
<li>It shows that the utilzation of human data is critical in final results.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hamming_distance</span>(<span class="params">s1, s2</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot; Return the Hamming distance between equal-length sequences &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(s1)!=<span class="built_in">len</span>(s2): <span class="keyword">raise</span> ValueError(<span class="string">&quot;Non-equal length given!&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(e1!=e2 <span class="keyword">for</span> e1,e2 <span class="keyword">in</span> <span class="built_in">zip</span>(s1,s2))</span><br></pre></td></tr></table></figure>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>Three reasons for supervised learning (SL)</p>
<ul>
<li>Provide simpler evaluation metric than multi-agent RL<ul>
<li>A good network architecture for SL is likely to also be good for RL</li>
</ul>
</li>
<li><strong>Initialization</strong>. Starting from human behaviour speeds up learning<ul>
<li>Initialize all policy weights <script type="math/tex">\theta = \theta^\text{SL}</script></li>
</ul>
</li>
<li>Maintain <strong>diverse exploration</strong>. Staying close to human behaviour ensures exploration of reasonable strategies<ul>
<li>Add <script type="math/tex">\text{KL}(\theta \vert\vert \theta^\text{SL})</script> cost to RL update</li>
<li>This is the solution to a<strong>void naive exploration in micro-tactics</strong> of ground units, e.g. naively build and use air units. The agents are also penalized whenever their action probabilties differ from the supervised policy.</li>
</ul>
</li>
</ul>
<h2 id="AlphaStar-architecture"><a href="#AlphaStar-architecture" class="headerlink" title="AlphaStar architecture"></a>AlphaStar architecture</h2><p><img data-src="/notes/images/alphaStar_NN.png" alt="upload successful"></p>
<p>Nature AlphaStar detailed model. It uses <strong>scatter connection</strong> to combine spatial and non-spatial features.</p>
<p><img data-src="/notes/images/nature-alphastar-model.png" alt="upload successful"></p>
<!--![upload successful](/notes/images/alphaStar-architecture.png)-->
<hr>
<h3 id="Network-Inputs"><a href="#Network-Inputs" class="headerlink" title="Network Inputs"></a>Network Inputs</h3><ul>
<li><code>prev_state</code>: the previous LSTM state<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[AlphaStar Nature paper supplemental data](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1724-z/MediaObjects/41586_2019_1724_MOESM2_ESM.zip)
">[19]</span></a></sup></li>
<li><code>entity_list</code>: entities within the game</li>
<li><code>map</code>: game map</li>
<li><code>scalar_features</code>: player data, game statistics, <strong>build orders</strong>.</li>
<li><code>opponent_observations</code>: opponent’s observations (only used for baselines, not for inference during play).</li>
<li><code>cumulative_score</code>: various score metrics of the game (only used for baselines, not for inference during play). “It includes <em>score, idle production and work time, total value of units and structure, total destroyed value of units and structures, total collected minerals and vespene, rate of minerals and vespene collection, and total spent minerals and vespene</em>“<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[AlphaStar Nature paper supplemental data](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1724-z/MediaObjects/41586_2019_1724_MOESM2_ESM.zip)
">[19]</span></a></sup>.</li>
</ul>
<h3 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h3><h4 id="Entity-encoder"><a href="#Entity-encoder" class="headerlink" title="Entity encoder"></a>Entity encoder</h4><ul>
<li>Input: <code>entity_list</code></li>
<li>Output:<br>  <code>embedded_entity</code> - a 1D tensor (one embedding for all entities)<br>  <code>entity_embeddings</code> - one embedding for each entity, including lots of fields, involving unit_type, unit_attr, alliance, current_health, was_selected, etc.</li>
</ul>
<p>The preprocessed entities (features) and biased are passed into a <strong>transformer</strong> (2 layer with 2-headed self attention, with embedding size 128). Then pass the aggregated values to a <strong>Conv1D</strong> with kernel size 1 to double the number of channels (to 256). Sum the head results and passed to a 2-layer MLP with hidden size 1024 and output size 256.</p>
<ul>
<li><code>entity_embeddings</code>: pass the transformer output through a ReLU, a Conv1D with kernel size 1 and 256 channels, and another ReLU.  </li>
<li><code>embedded_entity</code>: The mean of tansformer output across the units is fed through an MLP of size 256 and a ReLU.</li>
</ul>
<h4 id="Spatial-encoder"><a href="#Spatial-encoder" class="headerlink" title="Spatial encoder"></a>Spatial encoder</h4><ul>
<li>Input: <code>map</code>, <code>entity_embeddings</code></li>
<li>Output:<br>  <code>embedded_spatial</code> - A 1D tensor of the embedded map<br>  <code>map_skip</code> - output tensors of intermediate computation, used for skip connections.</li>
</ul>
<p><code>map</code>: add two features</p>
<ul>
<li>cameral: whether a location is inside/outside the virtual camera;</li>
<li>scattered entities. Pass <code>entity_embeddings</code> through a size 32 conv1D followed by a ReLU, then scattered into a map layer so that the 32 vector at a specific location corresponds to the units placed there.</li>
</ul>
<p>Concatenated all planes including camera, scattered_entities, vasibility, entity_owners, buildable, etc. Project to 32 channels by 2D conv with kernel size 1, followed by a ReLU. Then downsampled from 128x128 to 16x16 through 3 conv2D and ReLUs with different channel sizes (i.e., 64, 128, and 128). </p>
<p><code>embedded_spatial</code>: The ResBlock output is embedded into a 1D tensor of size 256 by a MLP and a ReLU.</p>
<h4 id="Scalar-encoder"><a href="#Scalar-encoder" class="headerlink" title="Scalar encoder"></a>Scalar encoder</h4><ul>
<li>Input: <code>scalar_features</code>, <code>entity_list</code></li>
<li>Output:<br>  <code>embedded_scalar</code> - 1D tensor of embedded scalar features<br>  <code>scalar_context</code> - 1D tensor of certain scalar features as context to use for gating</li>
</ul>
<h3 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h3><ul>
<li>Input: <code>prev_state</code>, <code>embedded_entity</code>, <code>embedded_spatial</code>, <code>embedded_scalar</code></li>
<li>Output:<br>  <code>next_state</code> - The LSTM state for the next step<br>  <code>lstm_output</code> - The output of the LSTM</li>
</ul>
<p>Concatenate <code>embedded_entity</code>, <code>embedded_spatial</code>, and <code>embedded_scalar</code> into a single 1D tensor, and feeds that tensor along with <code>prev_state</code> into an LSTM with 3 hidden layers each of size 384. No projection is used.</p>
<h3 id="Heads"><a href="#Heads" class="headerlink" title="Heads"></a>Heads</h3><h4 id="Action-type-head"><a href="#Action-type-head" class="headerlink" title="Action type head"></a>Action type head</h4><ul>
<li>Input: <code>lstm_output</code>, <code>scalar_context</code></li>
<li>Output:<br>  <code>action_type_logits</code> - action type logits<br>  <code>action_type</code> - The action_type sampled from the action_type_logits using a multinomial with temperature 0.8. During supervised learning, <code>action_type</code> will be the ground truth human action type, and temperature is 1.0<br>  <code>autoregressive_embedding</code> - Embedding that combines information from <code>lstm_output</code> and all previous sampled arguments.</li>
</ul>
<p>It embeds <code>lstm_output</code> into a 1D tensor of size 256, passes it through 16 ResBlocks with LayerNorm each of size 256, and applies a ReLU. The output is converted to a tensor with one logit for each possible action type through a <code>GLU</code> gated by <code>scalar_context</code>.</p>
<p><code>autoregressive_embedding</code>: apply a ReLU and a MLP of size 256 to the one-hot version of <code>action_type</code>, and project to a 1D tensor of size 1024 through a <strong>GLU</strong> gated by <code>scalar_context</code>. The projection is added to <code>lstm_output</code> projection gated by <code>scalar_context</code> to yield <code>autoregressive_embedding</code>.</p>
<h4 id="Delay-head"><a href="#Delay-head" class="headerlink" title="Delay head"></a>Delay head</h4><ul>
<li>Input: <code>autoregressive_embedding</code></li>
<li>Output:<br>  <code>delay_logits</code> - The logits corresponding to the probabilities of each delay<br>  <code>delay</code> - The sampled delay using a multinomial with no teperature.<br>  <code>autoregressive_embedding</code> - Embedding that combines information from <code>lstm_output</code> and all previous sampled arguments. Similarly, project the delay to size 1024 1D tensor through 2-layer MLP with ReLUsk and add to <code>autoregressive_embedding</code>.</li>
</ul>
<h4 id="Queued-head"><a href="#Queued-head" class="headerlink" title="Queued head"></a>Queued head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>embedded_entity</code></li>
<li>Output:<br>  <code>queued_logits</code> - 2-dimensional logits corresponding to the probabilities of queueing and not queueing.<br>  <code>queued</code> - Whether or no to queue this action.<br>  <code>autoregressive_embedding</code> - Embedding that combines information from <code>lstm_output</code> and all previous sampled arguments. Queuing information is not added if queuing is not possible for the chosen <code>action_type</code>.</li>
</ul>
<h4 id="Selected-units-head"><a href="#Selected-units-head" class="headerlink" title="Selected units head"></a>Selected units head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>entity_embeddings</code></li>
<li>Output:<br>  <code>units_logits</code> - The logits corresponding to the probabilities of selecting each unit, repeated for each of the possible 64 unit selections<br>  <code>units</code> - The units selected for this action.<br>  <code>autoregressive_embedding</code></li>
</ul>
<p>If the selected <code>action_type</code> does not require units, then ignore this head.</p>
<p>Otherwise, create one-hot version of entities that satisfy the selected action type, pass it to an MLP and a ReLU, denoted as <code>func_embed</code>. </p>
<ul>
<li>Compute the masked of which units can be selected, initialized to allow selected entities that exist (including enemy units)</li>
<li>Compute a <strong>key</strong> for each entity by feeding <code>entity_embeddings</code> through a conv1D with 32channels and kernel size 1.</li>
<li>Then repeated for selecting up to 64 units, pass <code>autoregressive_embedding</code> through an MLP (size 256), add <code>func_embed</code>, pass through a ReLU and a linear with size 32. The result is fed into a LSTM with size 32 and zero initial state to get a <strong>query</strong>. </li>
<li>The entity <strong>keys</strong> are multiplied by the <strong>query</strong>, and are sampled using the mask and temperature 0.8 to decide which entity to select.</li>
<li>The one-hot position of the selected entity is multiplied by the keys, reduced by the mean across the entities, passed through an MLP of size 1024, and added to subsequent <code>autoregressive_embedding</code>. If <code>action_type</code> does not involve selecting units, skip this head.</li>
</ul>
<h4 id="Target-unit-head"><a href="#Target-unit-head" class="headerlink" title="Target unit head"></a>Target unit head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>entity_embeddings</code></li>
<li>Output:<br>  <code>target_unit_logits</code><br>  <code>target_unit</code> - sampled from <code>target_unit_logits</code> using a multinomial with temperature 0.8</li>
</ul>
<p>No need to return <code>autoregressive_embeddings</code> as the one of the two terminal arguments (as Location Head). </p>
<h4 id="Location-head"><a href="#Location-head" class="headerlink" title="Location head"></a>Location head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>map_skip</code></li>
<li>Output:<br>  <code>target_location_logits</code><br>  <code>target_location</code></li>
</ul>
<p><code>autoregressive_embedding</code> is reshaped as the same as the final skip as <code>map_skip</code> and concatenate together along the channel dimension, pass through a ReLU and conv2D with 128 channels and kernel size 1, then another ReLU. the 3D tensor is then passed through <strong>gated ResBlocks</strong> with 128 channels, kernel size 3, and <strong>FiLM</strong>, gated on <code>autoregressive_embedding</code> and using the <code>map_skip</code> elements in the reverse order. </p>
<p>Afterwards, upsample 2x by each of the transposed 2D convolutions with kernel size 4 and channel sizes 128, 64, 16 and 1 respectively. Those final logits are flattened and sampled with the temperature 0.3 (masking out invalid locations using <code>action_type</code>) to get the actual target position.</p>
<hr>
<p><strong>Another NN trained with camera-based interface</strong> lost the follow-up game against MaNa.</p>
<ul>
<li>Partial observability: only see information in <strong>camera view</strong>, <strong>“saccade” actions</strong></li>
<li>Imperfect information: only see opponent unit within range of own units</li>
<li>Large action space: simutaneous control of hundreds of units</li>
<li>Strategy cycles: counterstrategies discovered by pro players over 20 years</li>
</ul>
<p><img data-src="/notes/images/AlphaStar-using-camera-based-model.png" alt="upload successful"></p>
<h1 id="Related-methods"><a href="#Related-methods" class="headerlink" title="Related methods"></a>Related methods</h1><h2 id="Relational-inductive-biases"><a href="#Relational-inductive-biases" class="headerlink" title="Relational inductive biases"></a>Relational inductive biases</h2><p>Vinicius et. al(2019)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., ... & Shanahan, M. (2018). [Deep reinforcement learning with relational inductive biases](https://pdfs.semanticscholar.org/9ea9/2ebeb7462f2db346cfa3281ad7497b1063d6.pdf?_ga=2.187374248.1439184430.1563761069-43402279.1542977082). ICLR 2019
">[2]</span></a></sup> augmented model-free DRL with a mechanism for relational reasoning over structured representations via <strong>self-attention mechanisms</strong>, improving performance, learning efficiency, generalization and interpretability.</p>
<p>It incorporates relational inductive baises for entity- and relation- centric state representations, and iterated reasoning into the RL agent based on a distributed advantage actor-critic (A2C).</p>
<p>The agent receives the raw visual input pixels as the input, employing the front-end of CNNs to compute the entity embeddings, without depending on any priori knowledge, similar to Visual QA, video understanding tasks. </p>
<h3 id="Embedded-state-representation"><a href="#Embedded-state-representation" class="headerlink" title="Embedded state representation"></a>Embedded state representation</h3><ul>
<li>The agent creates an embedded state representation $S$ from its input observation, which is a spatial feature map returned by a CNN.</li>
</ul>
<h3 id="Relational-module"><a href="#Relational-module" class="headerlink" title="Relational module"></a>Relational module</h3><ul>
<li><code>Feature-to-entity transformation</code>: the relational module reshapes the feature map $S$ (with shape $m \times n \times f$) to entity vectors $E$ (with shape $N \times f$, where $N=m \cdot n$). Each row of $E$, denoted as <script type="math/tex">\mathbf{e}_i</script>, consists of a feature vector <script type="math/tex">\pmb{s}_{x,y}</script> at a particular $x$,$y$ location in each feature map. This allows for non-local computation between entities, unconstrained by their coordinates in the spatial feature map.</li>
<li><code>Self-attention mechanism</code>: apply multi-head dot-product attention(MHDPA) compute the pairwise interactions between each entity and all others (include itself).<script type="math/tex; mode=display">A = \text{softmax}(d^{-1/2}QK^T)V</script>where $d$ is the dimensionality of the query and key vectors.</li>
<li>Finally pass them to a multi-layer perceptron (MLP), with a residual connection.<script type="math/tex; mode=display">\tilde{\pmb{e}}_i = g_\theta (\pmb{a}_i^{h=1:H})</script></li>
</ul>
<h3 id="Output-module"><a href="#Output-module" class="headerlink" title="Output module"></a>Output module</h3><p>$\tilde{E}$ with the shape $N \times f$, is reduced to an $f$-dimensional vector by max-pooling over the entity dimension, followed by an MLP to output $(c+1)$-dimensional vector. The vector contains $c$-dimensional vector of $\pi$’s logits where $c$ is the # of discrete actions, plus a scalar baseline value estimate $B$.</p>
<p><img data-src="/notes/images/rl-relational-inductive-biases.png" alt="upload successful"></p>
<h2 id="Auto-regressive-policy-head"><a href="#Auto-regressive-policy-head" class="headerlink" title="Auto-regressive policy head"></a>Auto-regressive policy head</h2><p>Vinyals et. al (2017)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... & Quan, J. (2017). [Starcraft II: A new challenge for reinforcement learning](https://arxiv.org/pdf/1708.04782). arXiv preprint arXiv:1708.04782.
">[3]</span></a></sup> represented the policy with the <strong>auto-regressive</strong> manner, i.e. predict each action conditioned on previous actions:</p>
<script type="math/tex; mode=display">\pi_\theta(a \vert s) = \prod_{l=0}^L \pi_\theta (a^l \vert a^{<l}, s)</script><p>The auto-regressive policy transforms the problem of choosing a full action $a$ to a sequence of decisions for each argument $a^l$.</p>
<h2 id="Pointer-Networks"><a href="#Pointer-Networks" class="headerlink" title="Pointer Networks"></a>Pointer Networks</h2><p>It can be inferred that the Pointer Net is used to <strong>output the action for each unit</strong>, since the StarCraft involves many units in concert and the # of units changes over time. <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://www.alexirpan.com/2019/02/22/alphastar-part2.html](https://www.alexirpan.com/2019/02/22/alphastar-part2.html)
">[6]</span></a></sup></p>
<p><img data-src="/notes/images/Pointer-nets.png" alt="upload successful"></p>
<p><code>Pointer Net</code> is employed to handle the variablity of the output length:</p>
<ul>
<li><p>Applies the attention mechanism:</p>
<script type="math/tex; mode=display">u_j^i = v^T \tanh (W_1 e_j + W_2 d_i) \quad j \in (1,\cdots,n)</script><script type="math/tex; mode=display">p(C_i \vert C_1, \cdots, C_{i-1}, \mathcal{P}) = \text{softmax}(u^i)</script><p>  where softmax normalizes the vector $u^i$ (of length $n$) to be an output distribution over the dictionary of inputs. And $v$,<script type="math/tex">W_1</script>, <script type="math/tex">W_2</script> are learnable parameters of the output model.</p>
<p>  Here, we do not blend the encoder state <script type="math/tex">e_j</script> to propagate extra information to the decoder $d_i$. Instead, we use <script type="math/tex">u_j^i</script> as <code>pointers to the input elements</code>. </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ptr</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot; Pointer Nets &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Attn, self).__init__()</span><br><span class="line">        self.h_size = h_size</span><br><span class="line">        self.W1 = nn.Linear(h_size, h_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W2 = nn.Linear(h_size, h_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.vt = nn.Linear(h_size, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.tanh = nn.Tanh()</span><br><span class="line">        self.score = nn.Softmax(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec, enc, m</span>):</span></span><br><span class="line">        attn = self.vt(self.tanh(self.W1(enc) + self.W2(dec))).squeeze(-<span class="number">1</span>)</span><br><span class="line">        logits = attn.masked_fill_(m, -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">		<span class="keyword">return</span> self.score(logits)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><code>Ptr Nets</code> can be seen as an application of <strong><code>content-based attention mechanisms</code></strong>.</li></ul>
          </div>
<h2 id="Gated-ResNet"><a href="#Gated-ResNet" class="headerlink" title="Gated ResNet"></a>Gated ResNet</h2><p>Gated Residual Network (Gated ResNet)<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Savarese, P.H. (2017). [Learning Identity Mappings with Residual Gates](https://arxiv.org/pdf/1611.01260.pdf). ICLR
">[20]</span></a></sup> adds a linear gating mechanism to shortcut connections using a scalar parameter to control each gate.</p>
<ul>
<li>Let Residual layer <script type="math/tex">u = g(k) f(x, W) = f_r(x,W)+x</script>, the gated Highway network is:<script type="math/tex; mode=display">
\begin{align}
u &= g(k) f(x,W) + (1- g(k)) x \\
&= g(k) (\underbrace{f_r(x,W)+x}_\text{ResNet}) + (1-g(k))x \\
&= g(k)f_r(x,W) + x
\end{align}</script></li>
</ul>
<p><img data-src="/notes/images/GatedResNet.png" alt="upload successful"></p>
<h2 id="FiLM"><a href="#FiLM" class="headerlink" title="FiLM"></a>FiLM</h2><p><strong>F</strong>eature-w<strong>i</strong>se <strong>L</strong>iner <strong>M</strong>odulation (FiLM)<sup id="fnref:21"><a href="#fn:21" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Perez, E., Strub, F., Vries, H.D., Dumoulin, V., & Courville, A.C. (2017). [FiLM: Visual Reasoning with a General Conditioning Layer](https://arxiv.org/pdf/1709.07871.pdf). AAAI.">[21]</span></a></sup> applies the feature-wise affine transormation to the intermediate features of networks, based on some conditioning inputs. </p>
<p><img data-src="/notes/images/FiLM.png" width="20%" /></p>
<p>FiLM learns functions $f$ and $h$ which output <script type="math/tex">\gamma_{i,c}</script> and <script type="math/tex">\beta_{i,c}</script> as a function of input <script type="math/tex">\mathbf{x}_i</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
\gamma_{ic} & = f_c(\mathbf{x}_i) & \text{coefficient} \\
\beta_{i,c} &= h_c(\mathbf{x}_i) & \text{intercept} \\
\text{FiLM}(\mathbf{F}_{i,c} \vert y_{i,c}, \beta_{i,c}) &= \gamma_{i,c} \mathbf{F}_{i,c} + \beta_{i,c} & \text{FiLM generator}
\end{align}</script><p>where</p>
<ul>
<li>$f$ and $h$ can be arbitary functions such as dense networks, sigmoid/tanh/exponential functions, etc.</li>
<li>Modulation of the target NN can be applied on the same input to that NN or some other inputs. For CNNs, $f$ and $h$ modulate the per-feature-map distribution of activations based on <script type="math/tex">\mathbf{x}_i</script>.</li>
</ul>
<ul>
<li>FiLM-ed network architecture:<br><img data-src="/notes/images/FiLM-ed-Net.png" width="60%" /></li>
</ul>
<h2 id="Centralized-value-baseline"><a href="#Centralized-value-baseline" class="headerlink" title="Centralized value baseline"></a>Centralized value baseline</h2><h3 id="Centralized-critic"><a href="#Centralized-critic" class="headerlink" title="Centralized critic"></a>Centralized critic</h3><ul>
<li><p><strong>Problems</strong>: Conventional <em>independent actor-critic</em> (IAC) independently trains each agent, but the lack of information sharing impedes to learn coordinated strategies that depend on interactions between multiple agents, or to estimate the contribution of single agent’s action to the global rewards. </p>
</li>
<li><p>Solution: use a <strong>centralized critic</strong> that conditions on the true global state $s$, or the joint action-observation histories $\tau$ otherwise. (See the figure below)</p>
</li>
</ul>
<p><img data-src="/notes/images/rl-COMA.png" alt="upload successful"></p>
<p>The critic (red parts in the figure) is used only during learning and only the actor is needed during execution. </p>
<h3 id="Counterfactual-baseline"><a href="#Counterfactual-baseline" class="headerlink" title="Counterfactual baseline"></a>Counterfactual baseline</h3><ul>
<li><strong>Problems</strong>: A naive way is to follow the gradient based on the TD error:<script type="math/tex; mode=display">g = \nabla_{\theta^\pi} \log \pi (\mu \vert \tau_t^a) (r+ \gamma V(s_{t+1})- V(s_t))</script>It fails to address the key credit assignment problem since the TD error only considers global rewards, the gradient from each actor does not explicitly considered based on their respective contribution.</li>
<li>Solution: <strong>counterfactual baseline</strong>.<br>It is inspired by <em>difference reward</em> by computing the change of global reward when the action $a$ of an individual agent is replaced by a default action $c^a$:<script type="math/tex; mode=display">D^a = r(s,\pmb{u}) - r(s, (\pmb{u}^{-a}, c^a))</script></li>
</ul>
<p>But difference baseline requires 1) the access to a simulator <script type="math/tex">r(s, (\pmb{u}^{-a}, c^a))</script> and 2) a use-specific default action $c^a$.</p>
<ul>
<li><strong>counterfactual baseline</strong>. Compute the agent $a$ we can compute the advantage function that compares the $Q$-value for the current action $\mu^a$ to a counterfactual baseline that marginalize out $\mu^a$, while keeping the other agents’ actions $\mu^{-a}$ fixed:<script type="math/tex; mode=display">A^a(s, \mu) = Q(s, \mu) - \sum_{\mu^{'a}} \pi^a(\mu^{'a} \vert \tau^a) Q(s, (\mu^{-a}, \mu^{'a}))</script>where $A^a(s,\mu^a)$ measures the difference when only $a$’s action changes, learn directly from agents’ experiences rather than on extra simulations, a reward model or a use-designed default action.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018, April). [Counterfactual multi-agent policy gradients](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17193/16614). In Thirty-Second AAAI Conference on Artificial Intelligence.
">[5]</span></a></sup></li>
</ul>
<h3 id="Critic-representation"><a href="#Critic-representation" class="headerlink" title="Critic representation"></a>Critic representation</h3><p>The output dimension of networks would be equal to $|U|^n$, where $n$ is the # of agents. COMA uses critic representation in which it also takes the action of other agents <script type="math/tex">u_t^{-a}</script> as part of the input, and output a $Q$-value for each of agent $a$’s action, with the # of output nodes $|U|$. (see Fig.(c) below)</p>
<p><img data-src="/notes/images/rl-COMA-ac.png" alt="upload successful"><br>    Fig. (b) and (c) are the architectures of the actor and critic.</p>
<h2 id="Self-Imitation-Learning"><a href="#Self-Imitation-Learning" class="headerlink" title="Self-Imitation Learning"></a>Self-Imitation Learning</h2><p>Self-Imitation Learning (SIL)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oh, J., Guo, Y., Singh, S., & Lee, H. (2018). [Self-imitation learning](https://arxiv.org/pdf/1806.05635). arXiv preprint arXiv:1806.05635.
">[8]</span></a></sup> learns to imitate the agent’s own past good experiences in the actor-critic framework. It stores experiences with cumulative rewards in a replay buffer: <script type="math/tex">{\mathcal{D}=\{ (s_t, a_t, R_t) \}}</script>, where <script type="math/tex">s_t</script>,<script type="math/tex">a_t</script> are a state and an action at time-step $t$, and <script type="math/tex">R_t = \sum_{k=t}^\infty y^{k-t} r_k</script> is the discounted sum of rewards with a discount factor $\gamma$, learns to imitate state-action pairs in the replay buffer only when the return in the past episode is greater than the agent’s value estimate.</p>
<h3 id="Off-policy-actor-critic-loss"><a href="#Off-policy-actor-critic-loss" class="headerlink" title="Off-policy actor-critic loss"></a>Off-policy actor-critic loss</h3><script type="math/tex; mode=display">
\begin{align*}
\mathcal{L}^\text{sil} &= \mathbb{E}_{s,a,R \in \mathcal{D}} [ \mathcal{L}^\text{sil}_\text{policy} + \beta^\text{sil} \mathcal{L}_\text{value}^\text{sil}] \\
\mathcal{L}_\text{policy}^\text{sil} &= -\log \pi_\theta (a \vert s) \max(R-V_\theta(s), 0) \\
\mathcal{L}_\text{value}^\text{sil} &= \frac{1}{2} || \max(R-V_\theta(s), 0) ||^2
\end{align*}</script><p>where <script type="math/tex">\pi_\theta</script>, <script type="math/tex">V_\theta(s)</script> are the policy (i.e. actor) and the value function, $\beta^\text{sil} \in \mathbb{R}^+$ is a hyperparameter for the value loss.</p>
<p>The <script type="math/tex">\mathcal{L}_\text{policy}^\text{sil}</script> can be interpreted as cross entropy loss with sample weights proportional to the gap between the return and the agent’s value estimate <script type="math/tex">(R-V_\theta)</script>:</p>
<ol>
<li>If the past return is greater than the agent’s value estimate, i.e. <script type="math/tex">R>V_\theta</script>, the agent learns to choose the action chosen in the poast in the given state.</li>
<li>Otherwise (<script type="math/tex">R < V_\theta</script>), such a state-action pair is not used to update due to the $\max$ op.<br>This encourages the agent to imitate its own decisions in the past only when such decisions resulted in larger returns than expected. <script type="math/tex">\mathcal{L}_\text{value}^\text{sil}</script> updates the value estimate towards the off-policy return $R$.</li>
</ol>
<h3 id="Prioritized-experience-replay"><a href="#Prioritized-experience-replay" class="headerlink" title="Prioritized experience replay:"></a><strong>Prioritized experience replay</strong>:</h3><p>Sample transitions from the replay buffer using the clipped advantage <script type="math/tex">\max(R-V_\theta(s),0)</script> as priority, i.e. sampling probablity is prop. to <script type="math/tex">\max(R-V_\theta(s),0)</script>.</p>
<h3 id="Advantage-Actor-Critic-with-SIL-A2C-SIL"><a href="#Advantage-Actor-Critic-with-SIL-A2C-SIL" class="headerlink" title="Advantage Actor-Critic with SIL (A2C + SIL)"></a><strong>Advantage Actor-Critic with SIL (A2C + SIL)</strong></h3><p>A2C + SIL objective: </p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{L}^\text{a2c} &= \mathbb{E}_{s,a\sim \pi_\theta} [\mathcal{L}_\text{policy}^\text{a2c} + \beta^\text{a2c} \mathcal{L}_\text{value}^\text{a2c}] \\
\mathcal{L}_\text{policy}^\text{a2c} &= - \log \pi_\theta (a_t \vert s_t) (V_t^n - V_\theta(s_t)) - \alpha \mathcal{H}_t^{\pi_\theta}\\
\mathcal{L}_\text{value}^\text{a2c} &= \frac{1}{2} ||V_\theta(s_t) - V_t^n||^2
\end{align*}</script><h3 id="SIL-algorithms"><a href="#SIL-algorithms" class="headerlink" title="SIL algorithms"></a>SIL algorithms</h3><ol>
<li>Initialize parameter $\theta$</li>
<li>Initialize replay buffer $\mathcal{D} \leftarrow \emptyset$</li>
<li>Initialize episode buffer $\mathcal{E} \leftarrow \emptyset$</li>
<li>For each iteration do:<ol>
<li><strong>## Collect on-policy samples</strong></li>
<li>for each step do:<ol>
<li>execute an action <script type="math/tex">s_t, a_t, r_{t+1} \sim \pi_\theta(a_t \vert s_t)</script></li>
<li>store transition $\mathcal{E} \leftarrow \mathcal{E} \cup { (s_t, a_t, r_t) }$</li>
</ol>
</li>
<li>if $s_{t+1}$ == TERMINAL:<ol>
<li><strong>## Update replay buffer </strong></li>
<li>compute returns $R_t = \sum_k^\infty \gamma^{k-t} r_k$ for all $t$ in $\mathcal{E}$</li>
<li>$\mathcal{D} \leftarrow \mathcal{D} \cup { (s_t,a_t,R_t) }$ for all $t$ in $\mathcal{E}$</li>
<li>clear episode buffer $\mathcal{E} \leftarrow \emptyset$</li>
</ol>
</li>
<li><strong>## perform actor-critic using on-policy samples</strong></li>
<li>$\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}^\text{a2c}$</li>
<li><em>*## Perform self-imitation learning</em></li>
<li>for m = 1 to $M$:<ol>
<li>Sample a mini-batch ${ (s,a,R) }$ from $\mathcal{D}$</li>
<li>$\theta \leftarrow theta - \eta \nabla_\theta \mathcal{L}^\text{sil}$</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Policy-distillation"><a href="#Policy-distillation" class="headerlink" title="Policy distillation"></a>Policy distillation</h2><p><strong>Policy distillation</strong> is iused to train a new network that performs at the expert level while being dramatically smaller and more efficient <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., ... & Hadsell, R. (2015). [Policy distillation](https://arxiv.org/pdf/1511.06295). arXiv preprint arXiv:1511.06295.
">[9]</span></a></sup>. Andrei <em>et. al</em>(2016) demonstrated that the <strong>multi-task</strong> distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent in the Atari domain.</p>
<h3 id="Distillation"><a href="#Distillation" class="headerlink" title="Distillation"></a>Distillation</h3><p>Distillation is proposed for supervised model compression, by creating a single network from an ensemble model. Model compression trains a student network using the output of a teacher network, compressing a large ensemble model into a single shallow network.</p>
<h3 id="Single-game-policy-distillation"><a href="#Single-game-policy-distillation" class="headerlink" title="Single-game policy distillation"></a>Single-game policy distillation</h3><p>Distillation is to transfer knowledge frm a <em>teacher</em> model $T$ to a <em>student</em> model $S$. </p>
<ul>
<li>The distillation targets from a classification targets from a classification network are typically obtained by passing the weighted sum of the last network layer through a softmax function.</li>
<li>In order to transfer more knowledge of the network, the teacher outputs can be softened by passing the network output through a relaxed (higher temperature) softmax than one that was used for training: <script type="math/tex">\text{softmax}(\frac{q^T}{\tau})</script>, where $q^T$ is the vector of $Q$-values of $T$.</li>
</ul>
<p><img data-src="/notes/images/rl-policy-distillation.png" alt="upload successful"></p>
<p>When transferring $Q$-value rather than a classifier, the scale of the $Q$-values may be hard to learn since it is not bounded and can be quite unstable. Training $S$ to predict only the single best action from $T$ is problematic, since multiple actions may have similar $Q$-values. </p>
<p>Consider policy distillation from $T$ to $S$, hwere the teacher $T$ generates a dataset <script type="math/tex">\mathcal{D}^T = \{ (s_i, q_i) \}_{i=0}^N</script>, where each sample consists of a short observation sequence <script type="math/tex">s_i</script> and unnormalized $Q$-value vector <script type="math/tex">q_i</script>. Here is three approaches:</p>
<ol>
<li>Only use the highest valued action from the teacher <script type="math/tex">a_\text{i, best} = \arg \max(q_i)</script>. $T$ is trained with a negative log likelihood(NLL) to predict the same action:<script type="math/tex; mode=display">L_\text{NLL}(\mathcal{D}^T, \theta_S) = - \sum_{i=1}^{|D|} \log P(a_i=a_\text{i, best} \vert x_i, theta_S)</script></li>
<li>Train with mean-squared-error loss (MSE). It preserves the full set of action-values:<script type="math/tex; mode=display">L_\text{MSE} (\mathcal{D}^T, \theta_S) = \sum_{i=1}^{|D|} || q_i^T - q_i^S ||_2^2</script></li>
<li>KL divergence with temperature $\tau$:<script type="math/tex; mode=display">
\begin{align}
L_\text{KL} (\mathcal{D}^T, \theta_S) &= \text{KL} (\text{softmax}(\frac{q_i^T}{\tau}) || \text{softmax} (q_i^S) ) \\
&= \sum_{i=1}^{|D|} \text{softmax}(\frac{q_i^T}{\tau}) \ln \frac{\text{softmax}(\frac{q_i^T}{\tau})}{\text{softmax}(q_i^S)}
\end{align}</script></li>
</ol>
<h3 id="Multi-task-policy-distillation"><a href="#Multi-task-policy-distillation" class="headerlink" title="Multi-task policy distillation"></a>Multi-task policy distillation</h3><p>Multi-task policy distillation uses $n$ DQN single-game experts, each trained separatedly, providing inputs and targets for $S$. The data is stored in separate memory buffers. The distillation agent $S$ learns from the $n$ data stores sequentially, and different tasks have different output layer(i.e. controller layer). The KL and NLL loss functions are used for multi-task distillation.</p>
<p><img data-src="/notes/images/rl-policy-distillation-multi-task.png" alt="upload successful"></p>
<h2 id="IMPALA"><a href="#IMPALA" class="headerlink" title="IMPALA"></a>IMPALA</h2><p><strong>Imp</strong>ortance Weighted <strong>A</strong>ctor-<strong>L</strong>earning <strong>A</strong>rchitecture (IMPALA) can scale to thousands of machines without reducing data efficiency or resouce utilization. IMPALA achieves exceptionally high data throughput rates of 250,000 frames per second, over 30 times faster than single-machine A3C.<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... & Legg, S. (2018). [Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures](https://arxiv.org/pdf/1802.01561). arXiv preprint arXiv:1802.01561.
">[10]</span></a></sup></p>
<h3 id="IMPALA-archtecture"><a href="#IMPALA-archtecture" class="headerlink" title="IMPALA archtecture"></a>IMPALA archtecture</h3><p><img data-src="/notes/images/rl-IMPALA-fig.png" alt="upload successful"></p>
<p>IMPALA applies an actor-critic setup to laern a policy $\pi$ and a baseline function $V^\pi$. Each actor updates its own local policy $\mu$ (i.e. behavior policy) to the latest learner policy $\pi$ (i.e. target policy), and runs $n$ steps in the environment. Afterwards, store the trajectory of states, actions, rewards, <script type="math/tex">\{(x_i,a_i,r_i) \}_{i=1}^n</script> and policy distributions <script type="math/tex">\mu(a_t \vert x_t)</script> as well as the initial LSTM state to a <strong>queue</strong>(as in left Fig.). This could lead to the <em>policy-lag</em> between actors and the learner. <strong>V-trace</strong> is proposed to correct the lag to get high data throughput while keeping data efficiency.</p>
<p>Also, IMPALA can employ synchronized parameter update (right fig.).<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... & Legg, S. (2018). [Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures](https://arxiv.org/pdf/1802.01561). arXiv preprint arXiv:1802.01561.
">[10]</span></a></sup></p>
<p><img data-src="/notes/images/rl-IMPALA-paper-fig.png" alt="upload successful"></p>
<h3 id="V-trace"><a href="#V-trace" class="headerlink" title="V-trace"></a>V-trace</h3><h4 id="V-trace-target"><a href="#V-trace-target" class="headerlink" title="V-trace target"></a>V-trace target</h4><p>Consider the trajectory <script type="math/tex">(x_t, a_t, r_t)_{t=s}^{t=s+n}</script> generated by the actor following the some policy $\mu$, define $n$-step V-trace target for <script type="math/tex">V(s_s)</script>, the value approximation at state <script type="math/tex">x_s</script>:</p>
<script type="math/tex; mode=display">
\begin{align*}
v_s &\triangleq V(x_s) + \sum_{t=s}^{s+n-1} \gamma^{t-s}(\prod_{i=s}^{t-1}c_i) \pmb{\delta_t V} & \text{value approximation}\\
\pmb{\delta_t V} &\triangleq \rho_t (r_t + \gamma V(x_{t+1}) - V(x_t)) & \text{temporal difference for }V \\
\end{align*}</script><p>where <script type="math/tex">\rho_t \triangleq \min(\bar{\rho}, \frac{\pi(a_t \vert x_t)}{\mu(a_t \vert x_t)})</script> and <script type="math/tex">c_i \triangleq \min(\bar{c}, \frac{\pi(a_i \vert x_i)}{\mu(a_i \vert x_i)})</script> are truncated improtance sampling(IS) weights.</p>
<ul>
<li>The truncated IS weight <script type="math/tex">c_i</script> define the <strong>fixed point</strong> of this update rule.</li>
<li>The weights <script type="math/tex">c_i</script> are similar to the “trace cutting” coefficients in Retrace. The product <script type="math/tex">c_s \cdots c_{t-1}</script> measures the temporal difference <script type="math/tex">\delta_t V</script> observed at time $t$ impacts the value function update at previous time $s$.</li>
</ul>
<h4 id="V-trace-actor-critic-algorithms"><a href="#V-trace-actor-critic-algorithms" class="headerlink" title="V-trace actor-critic algorithms"></a>V-trace actor-critic algorithms</h4><ul>
<li>At training time $s$, the value prameters $\theta$ are updated by gradient descent on $l2$ loss to the target <script type="math/tex">v_s</script>, in the direction of:<script type="math/tex; mode=display">(v_s - V_\theta(x_s)) \nabla_\theta V_\theta(x_s)</script></li>
<li>update the policy params:<script type="math/tex; mode=display">\rho_s \nabla_\omega \log \pi_\omega (a_s \vert x_s) (r_s + \gamma v_{s+1} - V-\theta(x_s))</script></li>
<li>To prevent premature convergence, add an entropy term, along the direction:<script type="math/tex; mode=display">- \nabla_\omega \sum_a \pi_\omega(a \vert x_s) \log \pi_\omega (a \vert x_s)</script></li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>Left: small, 2 Conv layers, 1.2 million parameters; </li>
<li>Right: large, 15 Conv layers, 1.6 million parameters.<br><img data-src="/notes/images/rl-IMPALA-model.png" alt="upload successful"></li>
</ul>
<h2 id="Population-based-training-PBT"><a href="#Population-based-training-PBT" class="headerlink" title="Population-based training(PBT)"></a>Population-based training(PBT)</h2><p>Two common tracks of hyperparameter tuning:</p>
<ol>
<li>parallel search:<ul>
<li>grid search</li>
<li>random search</li>
</ul>
</li>
<li>sequential optimization: requires multiple sequential training runs.<ul>
<li>Bayesian optimization</li>
</ul>
</li>
</ol>
<p><img data-src="/notes/images/PBT-fig.png" alt="upload successful"></p>
<p>PBT starts like parallel search, randomly sampling hyperparameters and weight initializations. However, each training run asynchronously evaluates its performance periodically. If a model in the population is under-performing, it will <em>exploit</em> the rest of the polulation by replacing itself with a better performing model, and it will <em>explore</em> new hyperparameters by modifying a better model’s hyperparameters before training is continued.<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., ... & Fernando, C. (2017). [Population based training of neural networks](https://arxiv.org/abs/1711.09846). arXiv preprint arXiv:1711.09846.
">[13]</span></a></sup></p>
<h3 id="PBT-algorithms"><a href="#PBT-algorithms" class="headerlink" title="PBT algorithms"></a>PBT algorithms</h3><p>Population-based Training(PBT) can be used to optimize neural networks for RL, supervised learning, GAN. PBT offers a way to optimize both the <strong>parameters</strong> $\theta$ and the <strong>hyperparameters</strong> $h$ jointly on the actual metric $\mathcal{Q}$ that we care about.</p>
<p>Training $N$ models <script type="math/tex">\{ \theta^i \}_{i=1}^N</script> forming a population $\mathcal{P}$ which are optimized with different hyperparameters <script type="math/tex">\{ \mathbf{h}^i \}_{i=1}^N</script>. Then use the  collection of partial solutions in the population to perform <em>meta-optimization</em>, where the hyperparameters $h$ and weights $\theta$ are additionally adapted w.r.t the entire population. For each worker (member) in the population, we apply two functions independently:</p>
<ol>
<li><em>expoit</em>: decide whether the worker abandon the current solutions and copy a better one.</li>
<li><em>explore</em>: propose new ones to better explore the solution space.</li>
</ol>
<p>Each worker of the population is trained in paraleel, with iterative calls of the repeated cycle of local iterative training (with <em>step</em>) and exploitation and exploration with the rest of the population (with <em>exploit</em> and <em>explore</em>) until convergence of the model.</p>
<ul>
<li><em>step</em>: a step of gradient descent</li>
<li><em>eval</em>: mean episodic return or validation set performance of the metric to optimize</li>
<li><em>exploit</em>: select another member of the population to copy the weights and hyperparameters from</li>
<li><em>explore</em>: create new hyperparameters for the next steps of gradient based learning by either <strong>perturbing the copied hyperparameters</strong> or <strong>resampling hyperparameters from the original defined prior distribution</strong>.</li>
</ul>
<p>PBT is asynchronous and does not require a centralized process to orchestrate the training of the members of the population. “PBT is an online evolutionary process that adapts internal rewards and hyperparameters and performs model selection by replacing underperforming agents with mutated version of better agents”.<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., ... & Sonnerat, N. (2019). [Human-level performance in 3D multiplayer games with population-based reinforcement learning](https://science.sciencemag.org/content/sci/364/6443/859.full.pdf). Science, 364(6443), 859-865.
">[14]</span></a></sup></p>
<p>Silimar to genetic algorithms: local optimization by SGD -&gt; periodic model selection -&gt; hyperparameter refinement</p>
<p><img data-src="/notes/images/PBT-alg.png" alt="upload successful"></p>
<h4 id="PBT-for-RL"><a href="#PBT-for-RL" class="headerlink" title="PBT for RL"></a>PBT for RL</h4><ul>
<li><strong>Hyperparameters</strong>: learning rate, entropy cost, unroll length for LSTM,…</li>
<li><strong>Step</strong>: each iteration does a step of gradient descent with RMSProp on the model weights</li>
<li><strong>Eval</strong>: evaluate the model with the last 10 episodic rewards during training</li>
<li><strong>Ready</strong>: 1e6 ~ 1e7 agent steps finished</li>
</ul>
<div class="note success">
            <p><strong>Exploit</strong></p><ol><li><strong>T-selection</strong>: uniformly sample another agent in the population, and compare the last 10 episodic rewards using Welch’s t-test. If the sampled agent has a higher mean episodic reward and satisfies the t-test, the <em>weights and hyperparameters</em> are copied to replace the current agent.</li><li><strong>Truncation selection</strong>: <strong>rank all agents</strong> in the population by episodic reward. Replace the bottom 20% agents with unformly sampled agent from the top 20% of the population, by copying the <em>weights and hyperparameters</em>.</li></ol>
          </div>
<div class="note warning">
            <p><strong>Explore</strong> the hyperparameter space:</p><ol><li><strong>Perturb</strong>: randomly perturb each hyperparameter by a factor of 0.8 or 1.2 ($\pm 20\%$)</li><li><strong>Resample</strong>: each hyperparameter is resampled from the original prior distribution defined with some probability.</li></ol>
          </div>
<h4 id="For-The-Win-FTW"><a href="#For-The-Win-FTW" class="headerlink" title="For The Win (FTW)"></a>For The Win (FTW)</h4><p>For The Win (FTW) network architecture:</p>
<ul>
<li>Use a hirarchical RNN consisting of two RNNs, operating on two different timescales. The fast timescale RNN generates the hidden state <script type="math/tex">h_t^q</script> at each time step $t$, while the slow timescale RNN produces the hidden state <script type="math/tex">h_t^p = h^p_{\tau \lfloor \frac{t}{\tau} \rfloor }</script> every $\tau$ time steps.</li>
<li>The observation is encoded with CNNs.</li>
</ul>
<p><img data-src="/notes/images/rl-pbt-network-architecture.png" alt="upload successful"></p>
<p><strong>PBT</strong>:</p>
<ul>
<li>Optimize the hyperparameter $\phi$ of learning rate, slow LSTM time scale $\tau$, the weight of <script type="math/tex">D_\text{KL}</script> term, entropy cost</li>
<li>In FTW, for each agent $i$ periodically sampled any agent $j$ and estimated the win probability of a team $i$ versus a team $j$. If the probabilty to win is less than 70%, the losing agent was replaced by the winner.</li>
<li>The exploration is perturbing the inherited value by $\pm 20\%$ with a probability of $5\%$, except that they uniformly sample the slow LSTM time scale $\tau$ from the integer range $[5,20)$.</li>
</ul>
<h2 id="Evolutionary-computation"><a href="#Evolutionary-computation" class="headerlink" title="Evolutionary computation"></a>Evolutionary computation</h2><h3 id="Lamarckian-Evolution"><a href="#Lamarckian-Evolution" class="headerlink" title="Lamarckian Evolution"></a>Lamarckian Evolution</h3><p>PBT is a memetric algorithm that uses Lamarckian evolution (LE):</p>
<ul>
<li>Innner loop: NNs are trained with backpropagation for individual solutions</li>
<li>Outer loop: evolution is run as the optimization algorithm, where NNs are picked with selection methods, with the winner’s parameters overwriting the loser’s.<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arulkumaran, K., Cully, A., & Togelius, J. (2019). [Alphastar: An evolutionary computation perspective](https://arxiv.org/pdf/1902.01724). arXiv preprint arXiv:1902.01724.
">[16]</span></a></sup></li>
</ul>
<h3 id="Co-evolution"><a href="#Co-evolution" class="headerlink" title="Co-evolution"></a>Co-evolution</h3><p>Competitive co-evolutionary algorithms(CCEAs) can be seen as a superset of self-play, it keep and evluate against an entire population of solutions, rather than keeping only one solution.</p>
<h3 id="Quality-diversity"><a href="#Quality-diversity" class="headerlink" title="Quality diversity"></a>Quality diversity</h3><p>Quality diversity (QD) algorithms explicitly optimize for a single objective(quality), but also searches for a large variety of solution types, via behaviour descriptors (i.e, solution phenotypes), to encourage greater diversity in the population.<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arulkumaran, K., Cully, A., & Togelius, J. (2019). [Alphastar: An evolutionary computation perspective](https://arxiv.org/pdf/1902.01724). arXiv preprint arXiv:1902.01724.
">[16]</span></a></sup></p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;chai2019Decipher-AlphaStar-on-StarCraft-II,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Deciphering AlphaStar on StarCraft II&#125;&#125;,</span><br><span class="line">  year = &#123;2019&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., ... &amp; Shanahan, M. (2018). <a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/9ea9/2ebeb7462f2db346cfa3281ad7497b1063d6.pdf?_ga=2.187374248.1439184430.1563761069-43402279.1542977082">Deep reinforcement learning with relational inductive biases</a>. ICLR 2019<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... &amp; Quan, J. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.04782">Starcraft II: A new challenge for reinforcement learning</a>. arXiv preprint arXiv:1708.04782.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Fortunato, M., &amp; Jaitly, N. (2015). <a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/5866-pointer-networks.pdf">Pointer networks</a>. In Advances in Neural Information Processing Systems (pp. 2692-2700).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., &amp; Whiteson, S. (2018, April). <a target="_blank" rel="noopener" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17193/16614">Counterfactual multi-agent policy gradients</a>. In Thirty-Second AAAI Conference on Artificial Intelligence.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.alexirpan.com/2019/02/22/alphastar-part2.html">https://www.alexirpan.com/2019/02/22/alphastar-part2.html</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/Whiteson.pdf">COMA slides 2017, University of Oxford</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oh, J., Guo, Y., Singh, S., &amp; Lee, H. (2018). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.05635">Self-imitation learning</a>. arXiv preprint arXiv:1806.05635.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., ... &amp; Hadsell, R. (2015). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.06295">Policy distillation</a>. arXiv preprint arXiv:1511.06295.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... &amp; Legg, S. (2018). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.01561">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</a>. arXiv preprint arXiv:1802.01561.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/">https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://deepmind.com/blog/population-based-training-neural-networks/">https://deepmind.com/blog/population-based-training-neural-networks/</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., ... &amp; Fernando, C. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.09846">Population based training of neural networks</a>. arXiv preprint arXiv:1711.09846.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., ... &amp; Sonnerat, N. (2019). <a target="_blank" rel="noopener" href="https://science.sciencemag.org/content/sci/364/6443/859.full.pdf">Human-level performance in 3D multiplayer games with population-based reinforcement learning</a>. Science, 364(6443), 859-865.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://deepmind.com/blog/capture-the-flag-science/">https://deepmind.com/blog/capture-the-flag-science/</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Arulkumaran, K., Cully, A., &amp; Togelius, J. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.01724">Alphastar: An evolutionary computation perspective</a>. arXiv preprint arXiv:1902.01724.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">DeepMind AlphaStar: Mastering the Real-Time Strategy Game StarCraft II</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., ... &amp; Oh, J. (2019). <a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-019-1724-z.pdf">Grandmaster level in StarCraft II using multi-agent reinforcement learning</a>. Nature, 1-5.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1724-z/MediaObjects/41586_2019_1724_MOESM2_ESM.zip">AlphaStar Nature paper supplemental data</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Savarese, P.H. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.01260.pdf">Learning Identity Mappings with Residual Gates</a>. ICLR<a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Perez, E., Strub, F., Vries, H.D., Dumoulin, V., &amp; Courville, A.C. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1709.07871.pdf">FiLM: Visual Reasoning with a General Conditioning Layer</a>. AAAI.<a href="#fnref:21" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/RL/" rel="tag"># RL</a>
              <a href="/notes/tags/StarCraft-II/" rel="tag"># StarCraft II</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2019/07/19/RL/DRL/PG-A-Summary/" rel="prev" title="Policy Gradient: A Summary !">
      <i class="fa fa-chevron-left"></i> Policy Gradient: A Summary !
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2019/08/28/NN/go-deeper-in-Convolutions-a-Peek/" rel="next" title="Go Deeper in Convolutions: a Peek ">
      Go Deeper in Convolutions: a Peek  <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#How-AlphaStar-is-trained"><span class="nav-number">1.</span> <span class="nav-text">How AlphaStar is trained</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AlphaStar-League"><span class="nav-number">1.1.</span> <span class="nav-text">AlphaStar League</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Agent-Diversity"><span class="nav-number">1.1.1.</span> <span class="nav-text">Agent Diversity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matchmaking-Strategies"><span class="nav-number">1.1.2.</span> <span class="nav-text">Matchmaking Strategies</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforcement-Learning"><span class="nav-number">1.2.</span> <span class="nav-text">Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Challenges"><span class="nav-number">1.2.1.</span> <span class="nav-text">Challenges</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Supervised-Learning"><span class="nav-number">1.3.</span> <span class="nav-text">Supervised Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AlphaStar-architecture"><span class="nav-number">1.4.</span> <span class="nav-text">AlphaStar architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Network-Inputs"><span class="nav-number">1.4.1.</span> <span class="nav-text">Network Inputs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoders"><span class="nav-number">1.4.2.</span> <span class="nav-text">Encoders</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Entity-encoder"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">Entity encoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spatial-encoder"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">Spatial encoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scalar-encoder"><span class="nav-number">1.4.2.3.</span> <span class="nav-text">Scalar encoder</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Core"><span class="nav-number">1.4.3.</span> <span class="nav-text">Core</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Heads"><span class="nav-number">1.4.4.</span> <span class="nav-text">Heads</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Action-type-head"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">Action type head</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Delay-head"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">Delay head</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Queued-head"><span class="nav-number">1.4.4.3.</span> <span class="nav-text">Queued head</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selected-units-head"><span class="nav-number">1.4.4.4.</span> <span class="nav-text">Selected units head</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Target-unit-head"><span class="nav-number">1.4.4.5.</span> <span class="nav-text">Target unit head</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Location-head"><span class="nav-number">1.4.4.6.</span> <span class="nav-text">Location head</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Related-methods"><span class="nav-number">2.</span> <span class="nav-text">Related methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Relational-inductive-biases"><span class="nav-number">2.1.</span> <span class="nav-text">Relational inductive biases</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedded-state-representation"><span class="nav-number">2.1.1.</span> <span class="nav-text">Embedded state representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relational-module"><span class="nav-number">2.1.2.</span> <span class="nav-text">Relational module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Output-module"><span class="nav-number">2.1.3.</span> <span class="nav-text">Output module</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Auto-regressive-policy-head"><span class="nav-number">2.2.</span> <span class="nav-text">Auto-regressive policy head</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pointer-Networks"><span class="nav-number">2.3.</span> <span class="nav-text">Pointer Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gated-ResNet"><span class="nav-number">2.4.</span> <span class="nav-text">Gated ResNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FiLM"><span class="nav-number">2.5.</span> <span class="nav-text">FiLM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Centralized-value-baseline"><span class="nav-number">2.6.</span> <span class="nav-text">Centralized value baseline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Centralized-critic"><span class="nav-number">2.6.1.</span> <span class="nav-text">Centralized critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Counterfactual-baseline"><span class="nav-number">2.6.2.</span> <span class="nav-text">Counterfactual baseline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Critic-representation"><span class="nav-number">2.6.3.</span> <span class="nav-text">Critic representation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Self-Imitation-Learning"><span class="nav-number">2.7.</span> <span class="nav-text">Self-Imitation Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Off-policy-actor-critic-loss"><span class="nav-number">2.7.1.</span> <span class="nav-text">Off-policy actor-critic loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prioritized-experience-replay"><span class="nav-number">2.7.2.</span> <span class="nav-text">Prioritized experience replay:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advantage-Actor-Critic-with-SIL-A2C-SIL"><span class="nav-number">2.7.3.</span> <span class="nav-text">Advantage Actor-Critic with SIL (A2C + SIL)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SIL-algorithms"><span class="nav-number">2.7.4.</span> <span class="nav-text">SIL algorithms</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-distillation"><span class="nav-number">2.8.</span> <span class="nav-text">Policy distillation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Distillation"><span class="nav-number">2.8.1.</span> <span class="nav-text">Distillation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-game-policy-distillation"><span class="nav-number">2.8.2.</span> <span class="nav-text">Single-game policy distillation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-task-policy-distillation"><span class="nav-number">2.8.3.</span> <span class="nav-text">Multi-task policy distillation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IMPALA"><span class="nav-number">2.9.</span> <span class="nav-text">IMPALA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IMPALA-archtecture"><span class="nav-number">2.9.1.</span> <span class="nav-text">IMPALA archtecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V-trace"><span class="nav-number">2.9.2.</span> <span class="nav-text">V-trace</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#V-trace-target"><span class="nav-number">2.9.2.1.</span> <span class="nav-text">V-trace target</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#V-trace-actor-critic-algorithms"><span class="nav-number">2.9.2.2.</span> <span class="nav-text">V-trace actor-critic algorithms</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-number">2.9.3.</span> <span class="nav-text">Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Population-based-training-PBT"><span class="nav-number">2.10.</span> <span class="nav-text">Population-based training(PBT)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PBT-algorithms"><span class="nav-number">2.10.1.</span> <span class="nav-text">PBT algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PBT-for-RL"><span class="nav-number">2.10.1.1.</span> <span class="nav-text">PBT for RL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#For-The-Win-FTW"><span class="nav-number">2.10.1.2.</span> <span class="nav-text">For The Win (FTW)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evolutionary-computation"><span class="nav-number">2.11.</span> <span class="nav-text">Evolutionary computation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lamarckian-Evolution"><span class="nav-number">2.11.1.</span> <span class="nav-text">Lamarckian Evolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Co-evolution"><span class="nav-number">2.11.2.</span> <span class="nav-text">Co-evolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quality-diversity"><span class="nav-number">2.11.3.</span> <span class="nav-text">Quality diversity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">3.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="cyk1337"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">cyk1337</p>
  <div class="site-description" itemprop="description">What is now proved was once only imagined.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cyk1337</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/";
    this.page.identifier = "2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/";
    this.page.title = "Deciphering AlphaStar on StarCraft II";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
