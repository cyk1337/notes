<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="The brain has about 1014 synapses and we only live for about 109 seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsuper">
<meta property="og:type" content="article">
<meta property="og:title" content="Likelihood-based Generative Models I: Autoregressive Models">
<meta property="og:url" content="https://cyk1337.github.io/notes/2019/12/17/Generative/Likelihood-based-autoregressive-models/index.html">
<meta property="og:site_name" content="The Gradient">
<meta property="og:description" content="The brain has about 1014 synapses and we only live for about 109 seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsuper">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/taxonomy-of-generative-models.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Generative-image-modeling-RGB.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelRNN-state-mapping.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelRNN-diagonal-biLSTM.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/pixelRNN-skip-connection.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/pixelCNN-visualization.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/MADE.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/pixelCNN.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/gatedPixelCNN.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Gated-PixelCNN.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelCNN++.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/WaveNet-causal-conv.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Dilated-causal-conv.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/WaveNet-Model.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/conv-AR-fast-generation.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelSNAIL-model.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelSNAIL-attn-block.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelSNAIL-residual-blocks.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Image-generative-ordering.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PixelCNN-effective-receptive-field.png">
<meta property="article:published_time" content="2019-12-17T07:31:00.000Z">
<meta property="article:modified_time" content="2019-12-17T07:31:00.000Z">
<meta property="article:author" content="cyk1337">
<meta property="article:tag" content="Unsupervised learning">
<meta property="article:tag" content="Autoregressive models">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/taxonomy-of-generative-models.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2019/12/17/Generative/Likelihood-based-autoregressive-models/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Likelihood-based Generative Models I: Autoregressive Models | The Gradient</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The Gradient</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Language is not just words.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2019/12/17/Generative/Likelihood-based-autoregressive-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="cyk1337">
      <meta itemprop="description" content="What is now proved was once only imagined.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Gradient">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Likelihood-based Generative Models I: Autoregressive Models
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-17 15:31:00" itemprop="dateCreated datePublished" datetime="2019-12-17T15:31:00+08:00">2019-12-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Unsupervised-learning/" itemprop="url" rel="index"><span itemprop="name">Unsupervised learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Unsupervised-learning/Likelihood-based-models/" itemprop="url" rel="index"><span itemprop="name">Likelihood-based models</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Unsupervised-learning/Likelihood-based-models/Autoregressive-models/" itemprop="url" rel="index"><span itemprop="name">Autoregressive models</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2019/12/17/Generative/Likelihood-based-autoregressive-models/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/17/Generative/Likelihood-based-autoregressive-models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>The brain has about 10<sup>14</sup> synapses and we only live for about 10<sup>9</sup> seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception) is the only place we can get 10<sup>5</sup> dimensions of constraint per second.<br><b>(Geoffrey Hinton)</b></p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<span id="more"></span>
<p><strong>Unsupervised learning</strong> can be used to capture rich patterns in raw data with deep networks in a <strong>label-free</strong> way.</p>
<ul>
<li>Generative models: recreate raw data distribution</li>
<li>Goal: learn some underlying hidden structure of the data</li>
<li>Self-supervised learning: “puzzle” tasks that require semantic understanding to improve downstream tasks.</li>
<li>Examples: clustering , dimensionality reduction, compression, feature learning, density estimation</li>
</ul>
<div class="note info">
            <p>Main generative models:</p><ul><li>Autoregressive</li><li>Normalizing flow</li><li>Variational Autoencoder</li><li>Generative Adversarial Networks</li></ul>
          </div>
<p>Real applications:</p>
<ul>
<li>generating data: synthesizing images, videos, speech, texts</li>
<li>compressing data: constructing efficient codes</li>
<li>anomaly detection</li>
</ul>
<p><img data-src="/notes/images/taxonomy-of-generative-models.png" alt="upload successful"></p>
<center> Image source: <sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Stanford cs231n: Generative models](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)">[20]</span></a></sup> </center>


<div class="note info">
            <p><strong>Likelihood-based models</strong>:</p><ul><li>estimate <script type="math/tex">p_\text{data}</script> from samples <script type="math/tex">x^{(1)},\cdots,x^{(n)} \sim p_\text{data}(x)</script></li></ul>
          </div>
<p>Given a dataset <script type="math/tex">\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(n)}</script>, find $\theta$ by solving the optimization problem:</p>
<script type="math/tex; mode=display">\arg\min_\theta J(\theta,\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(n)}) = \frac{1}{n}\sum_{i=1}^n - \log p_\theta (\mathbf{x}^{(i)})</script><p>which is equivalent to minimizing KL divergence between the empirical data distribution and the model:</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{p}_\text{data}(\mathbf{x}) &= \frac{1}{n} \sum_{i=1}^n \mathbb{I}[\mathbf{x}=\mathbf{x}^{(i)}] \\
\mathbb{KL}(\hat{p}_\text{data} \| p_\theta) &= \mathbb{E}_{\mathbb{x}\sim \hat{p}_\text{data}} [- \log p_\theta(\mathbf{x})] - \mathbb{H}(\hat{p}_\text{data})
\end{align}</script><ul>
<li>MLE + SGD</li>
<li>p<sub>$\theta$</sub> $\rightarrow$ NN</li>
</ul>
<h1 id="Autoregressive-models"><a href="#Autoregressive-models" class="headerlink" title="Autoregressive models"></a>Autoregressive models</h1><p>Autoregressive (AR) models share parameters among conditional distributions:</p>
<ol>
<li>RNNs</li>
<li>Masking convolutions &amp; attentions</li>
</ol>
<p>The AR property is that each output <script type="math/tex">x_d</script> only dependes on the previous input units <script type="math/tex">\mathbf{x}_{<d}</script>, but not on the future:</p>
<script type="math/tex; mode=display">p(\mathbf{x}) = \prod_{d=1}^D p(x_d \vert \mathbf{x}_{<d})</script><p>AR models can only model <strong>discrete data</strong>.</p>
<h1 id="RNN-AR-models"><a href="#RNN-AR-models" class="headerlink" title="RNN AR models"></a>RNN AR models</h1><p>RNNs privode a <strong>compact, shared parameterization</strong> of a sequence of conditional distributions, shown to excel in handwriting generation, character prediction, machine translation, etc.</p>
<h2 id="RNN-LM"><a href="#RNN-LM" class="headerlink" title="RNN LM"></a>RNN LM</h2><p>Given sequence of characters $\mathbf{x}$, $i$ indicates the position of characters:</p>
<script type="math/tex; mode=display">\log p(\mathbf{x}) = \sum_{i=1}^d \log p(x_i \vert \mathbf{x}_{1:i-1})</script><p>Raw LSTM layers:</p>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  &= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c) \\
\mathbf{c}^c_j &= \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j} \\
\mathbf{h}_j^c &= \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)
\end{align}</script><h2 id="PixelRNN"><a href="#PixelRNN" class="headerlink" title="PixelRNN"></a>PixelRNN</h2><p>Like in LM, AR models cast the joint distribution of pixels in the images to a product of conditional distributions, turning the joint modeling problem into a sequential problem with factorization, where one learns to predict the next pixel given all preivous generated pixels.</p>
<p><em>PixelRNN</em> leverages two-dimensional RNNs and residual connections<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup> in generative image modeling. </p>
<h3 id="Pixel-by-pixel-generation"><a href="#Pixel-by-pixel-generation" class="headerlink" title="Pixel-by-pixel generation"></a>Pixel-by-pixel generation</h3><p>The probability $p(\mathbf{x})$ to each image $\mathbf{x}$ of $n \times n$ pixels. The image $\mathbf{x}$ is tiled as 1-D sequence <script type="math/tex">x_1, \cdots, x_{n^2}</script> where pixels are taken from the image row by row. The joint distribution $p(\mathbf{x})$ is the product of the conditional probability over pixels:</p>
<script type="math/tex; mode=display">p(\mathbf{x}) = \prod_{i=1}^{n^2} p(x_i \vert x_1, \cdots, x_{i-1})</script><p>where each pixel is conditioned on all the previous generated pixels, whose generation is in the <em>raster scan order</em>: row by row and pixel by pixel within each row.</p>
<p>Taking into account RGB color channels of each pixel, the distribution of pixel $x_i$ is:</p>
<script type="math/tex; mode=display">p(x_i \vert \mathbf{x}_{<i}) = p(\color{red}{x_{i,R}} \vert \mathbf{x}_{<i}) p(\color{green}{x_{i,G}} \vert \mathbf{x}_{<i}, \color{red}{x_{i,R}}) p(\color{blue}{x_{i, B}} \vert \mathbf{x}_{<i}, \color{red}{x_{i, R}}, \color{green}{x_{i, G}})</script><p><img data-src='/notes/images/Generative-image-modeling-RGB.png' width='40%'/></p>
<p><em>PixelRNN</em> employs the discrete distribution with a 256-way softmax. Each channel variable <script type="math/tex">x_{x,*}</script> takes the scalar values from 0 to 255. The advantages:</p>
<ol>
<li>to be arbitrarily multimodal without prior on the shape;</li>
<li>achieve better results than continuous distribution and easy to learn.</li>
</ol>
<div class="note info">
            <ul><li>Training &amp; evaluation -&gt; the pixel distribution is parallel distribution (<em>teacher forcing</em>)</li><li>Generation -&gt; sequential, row by row and pixel by pixel.</li></ul>
          </div>
<p><img data-src="/notes/images/PixelRNN-state-mapping.png" width="80%" /></p>
<center>Image source: <i>PixelRNN</i><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup></center>

<h3 id="Row-LSTM"><a href="#Row-LSTM" class="headerlink" title="Row LSTM"></a>Row LSTM</h3><p><strong>Row LSTM</strong> is a <strong>unidirectional LSTM layer</strong> that takes the image <strong>row by row</strong> from top down to bottom computing features with 1-D convolution for a whole row at once.</p>
<p>It captures a roughly <u><strong>“triangular context”</strong> above the pixel</u> with kernel size of $k \times 1$ in temporal convolutions where $k \leq 3$, of which the larger kernel size captures the broader contexts and weight sharing guarantees the translation invariance.</p>
<div class="note warning">
            <ul><li><em>Row LSTMs</em> have the <strong>triangular receptive field</strong>, unable capturing the entire available context.</li></ul>
          </div>
<p>The computation proceeds as follows.</p>
<ul>
<li>LSTM layers have an <em>input-to-state</em> (is) and recurrent <em>state-to-state</em> (ss) component that together determine the four gates inside the LSTM core.</li>
<li>The <em>input-to-state</em> component is precomputed using 1-D  masked convolution with kernel size $k \times 1$ horizontally, where  <script type="math/tex">\{ \mathbf{i}^c_j, \mathbf{o}^c_j, \mathbf{f}^c_j, \tilde{c}^c_j\} \in \mathbb{R}^{h \times n \times n}</script>, $h$ denotes the # of output feature maps.</li>
<li>The row-wise <em>state-to-state</em> component of the LSTM layer takes the previous hidden and cell state <script type="math/tex">\mathbf{h}_{j-1}^c</script> and <script type="math/tex">\mathbf{c}_{j-1}^c</script>, where <script type="math/tex">\{\mathbf{x}_i, \mathbf{h}_{j-1}^c, \mathbf{c}_{j-1}^c\} \in \mathbb{R}^{h \times n \times 1}</script>, the weights $\mathbf{K}^{ss}$ and $\mathbf{K}^{is}$ represent the kernel weights of <em>state-to-state</em> and <em>input-to-state</em> components, $\circledast$ denotes the convolution operation.</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  &= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  ( \color{red}{ \mathbf{K}^\text{ss} \circledast \mathbf{h}_{j-1}^c} \color{blue}{+} \color{red}{ \mathbf{K}^\text{is} \circledast \mathbf{x}_{j} }) \\
\mathbf{c}^c_j &= \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j} \\
\mathbf{h}_j^c &= \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)
\end{align}</script><h3 id="Diagonal-BiLSTM"><a href="#Diagonal-BiLSTM" class="headerlink" title="Diagonal BiLSTM"></a>Diagonal BiLSTM</h3><p><em>Diagonal BiLSTM</em> is designed to impede the drawbacks of limited triangular receptive fields of <em>Row LSTM</em> and could capture the entire available context.</p>
<p><em>Diagonal BiLSTM</em> skews the input $\mathbf{x} \in \mathbb{R}^{n \times n}$ into $\mathbb{R}^{n \times (2n-1)}$ by shifting the $i$-th row with $(i-1)$ position offsets, i.e. each row is one position right shift compared with the previous row (see below figure).</p>
<ul>
<li>The <em>input-to-state</em> components of each direction adopt a $1 \times 1$ convolution $K^\text{is}$, and the output of $(\mathbf{K}^\text{is} \circledast \mathbf{x}) \in \mathbb{R}^{4h \times n \times n}$</li>
<li>The <em>state-to-state</em> recurrent component uses a <strong>column-wise 1D convolution</strong> $K^\text{ss}$ with kernel size $2 \times 1$.<br>  <small>Why 2x1? Larger sizes do not broaden the already global receptive fields.</small></li>
<li>For bi-LSTMs, the right-to-left directional LSTM is shifted down by one row and added to the left-to-right LSTM outputs.</li>
</ul>
<p><img data-src='/notes/images/PixelRNN-diagonal-biLSTM.png' width='80%'/></p>
<ul>
<li>Train the <em>pixelRNN</em> of up to 12 layers of depth with residual connections and layer-to-output skip connections.</li>
</ul>
<p><img data-src="/notes/images/pixelRNN-skip-connection.png" alt="upload successful"></p>
<p><center> Image source: <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup><center></p>
<h1 id="Masking-based-AR-models"><a href="#Masking-based-AR-models" class="headerlink" title="Masking-based AR models"></a>Masking-based AR models</h1><p>Key property: parallelized computation of all conditions</p>
<ol>
<li>Masked MLP (MADE)</li>
<li>Masked convolutions &amp; self-attention (PixelCNN families and <em>PixelSNAIL</em>)<ul>
<li>also share parameters across time</li>
</ul>
</li>
</ol>
<p><img data-src='/notes/images/pixelCNN-visualization.png' width='50%'/></p>
<h2 id="MADE"><a href="#MADE" class="headerlink" title="MADE"></a>MADE</h2><p>Masked Autoencoder Distribution Estimator (MADE) (Deepmind &amp; Iain Murray)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Germain, M., Gregor, K., Murray, I., & Larochelle, H. (2015, June). [Made: Masked autoencoder for distribution estimation](http://www.jmlr.org/proceedings/papers/v37/germain15.pdf). In International Conference on Machine Learning (pp. 881-889).
">[3]</span></a></sup> masks the autoencoder’s parameters to respect autoregressive properties that each input only reconstructed from previous input in a given ordering.</p>
<p>MADE zeros out the connections of layer connections by elementwise-multiplying a binary mask matrix on the weight matrices, setting the weight connectivities as 0s for removing.</p>
<p>For masked autoencoder with $L&gt;1$ hidden layers, let </p>
<ul>
<li>$D$ denote the dimension of input $\mathbf{x}$, $\mathbf{M}$ denote the connection mask; </li>
<li>in $l$-th layer, $K^l$ be the # of hidden states, $m^l(k)$ represent the maximum number of connected input of the $k$-th unit.</li>
</ul>
<blockquote>
<p>For $l$-th layer in masked autoencoders, the mask of weight matrices $\mathbf{W}$:</p>
<script type="math/tex; mode=display">
\mathbf{M}_{k^\prime, k}^{\mathbf{W}^l} = \mathbf{1}_{m^l(k^\prime) \leq m^{l-1}(k)} = \left\{
                \begin{array}{ll}
                  1 & \text{if} \; m^l(k^\prime) \leq m^{l-1}(k) \\
                  0 & \text{otherwise}
                \end{array}
              \right.</script><p>For the output mask of weight matrices $\mathbf{V}$:</p>
<script type="math/tex; mode=display">
\mathbf{M}_{d, k}^{\mathbf{V}} = \mathbf{1}_{d > m^L(k)} = \left\{
                \begin{array}{ll}
                  1 & \text{if} \; d > m^L(k) \\
                  0 & \text{otherwise}
                \end{array}
              \right.</script></blockquote>
<p><img data-src='/notes/images/MADE.png' width='70%'/></p>
<p><center> Image source:<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Germain, M., Gregor, K., Murray, I., & Larochelle, H. (2015, June). [Made: Masked autoencoder for distribution estimation](http://www.jmlr.org/proceedings/papers/v37/germain15.pdf). In International Conference on Machine Learning (pp. 881-889).
">[3]</span></a></sup> <center></p>
<h2 id="PixelCNN-families"><a href="#PixelCNN-families" class="headerlink" title="PixelCNN families"></a>PixelCNN families</h2><h3 id="PixelCNN"><a href="#PixelCNN" class="headerlink" title="PixelCNN"></a>PixelCNN</h3><p>PixelCNN<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup> adopts multiple conv layers <em>without pooling</em> to preserve the spatial resolution and masks the future context.</p>
<ul>
<li><em>Drawbacks</em>: PixelRNNs cannot consider the pixels on the right side of the current position (as Fig. below).</li>
</ul>
<p><img data-src='/notes/images/pixelCNN.png' width='50%'/></p>
<h3 id="Gated-PixelCNN"><a href="#Gated-PixelCNN" class="headerlink" title="Gated PixelCNN"></a>Gated PixelCNN</h3><p><em>Gated PixelCNN</em> takes into account both the <strong>vertical stack</strong> and the <strong>horizontal stack</strong> by combing both the pixels of region above and those on the left of the current row, wherein the convolutions of vertical stack are not masked. (See <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Tutorial: Gated PixelCNN](http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html)
">[5]</span></a></sup> for the tutorial.)</p>
<ul>
<li>advantages: deal with “blind spot” of the receptive field in <em>PixelCNNs</em>.</li>
</ul>
<p><img data-src='/notes/images/gatedPixelCNN.png' width='50%'/></p>
<p><em>Gated PixelCNNs</em> replace the ReLU between masked convolutions in the original pixelCNN with the gated activation function:</p>
<script type="math/tex; mode=display">\mathbf{y} = \tanh (w_{k,f} \circledast \mathbf{x}) \odot \sigma (W_{k,g} \circledast \mathbf{x})</script><p>where $p$ represents the # of feature maps, $\circledast$ denotes convolution operations, where it is masked in horizontal stack but unmasked in the vertical stack.</p>
<p><img data-src='/notes/images/Gated-PixelCNN.png' width='100%'/></p>
<h4 id="Conditional-PixelCNN"><a href="#Conditional-PixelCNN" class="headerlink" title="Conditional PixelCNN"></a>Conditional PixelCNN</h4><p>Given high-level latent representation $\mathbf{h}$, we model the conditional PixelCNN models:</p>
<script type="math/tex; mode=display">p(\mathbf{x} \vert \mathbf{h}) = \prod_{i=1}^{n^2} p(x_i \vert x_1, \cdots, x_{i-1}, \mathbf{h})</script><p>Add terms pf $\mathbf{h}$ before the non-linearities:</p>
<script type="math/tex; mode=display">\mathbf{y} = \tanh (W_{k,f} \circledast \mathbf{x}  \color{red}{+ V_{k,f}^\top \mathbf{h}} ) \odot \sigma (W_{k,g} \circledast \mathbf{x} \color{red}{+ V_{k,g}^\top \mathbf{h} })</script><p>where $k$ is the layer number.<br>Condition on <em>what</em>: </p>
<ul>
<li>class-dependent: $\mathbf{h} \rightarrow \text{1-hot}$, is equivalent to adding a class-dependent bias at each layer. </li>
</ul>
<p>Condition on <em>where</em>:</p>
<ul>
<li>location-dependent: use Transposed convolution to map $\mathbf{h}$ to a spatial representation $\color{red}{\mathbf{s} = \text{deconv}(\mathbf{h})}$ to produce the output $\mathbf{s}$ of the same shape as the image. It can be seen as adding a location dependent bias:<script type="math/tex; mode=display">\mathbf{y} = \tanh (W_{k,f} \circledast \mathbf{x}  \color{red}{+ V_{k,f} \circledast \mathbf{s}} ) \odot \sigma (W_{k,g} \circledast \mathbf{x} \color{red}{+ V_{k,g} \circledast \mathbf{s} })</script></li>
</ul>
<h3 id="PixelCNN-1"><a href="#PixelCNN-1" class="headerlink" title="PixelCNN++"></a>PixelCNN++</h3><p>Background: </p>
<ul>
<li>previous 256-way softmax is very costly and slow to compute, and makes the gradient w.r.t parameters very sparse.</li>
<li>the model does not know that the value 128 is close to that of 127 and 129. Especially unobserved sub-pixels will be assigned with 0 probability.</li>
</ul>
<p><em>PixelCNN++</em><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). [Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications](https://arxiv.org/pdf/1701.05517). arXiv preprint arXiv:1701.05517.
">[6]</span></a></sup> assumes the latent color intensity $\nu$ with continuous distribution and takes the continuous univariate distribution to be <strong>a mixture of logistic distributions</strong>. </p>
<ul>
<li>$\sigma(x) = -1 / (1+ \exp(- x))$.</li>
<li>For all sub-pixels except the edges 0 and 255:<script type="math/tex; mode=display">
\begin{align}
\nu & \sim \sum_{i=1}^K \pi_i \text{logistic}(\mu_i, s_i) \\
P(x \vert \pi, \mu, s) &= \sum_{i=1}^K \pi_i [ \sigma \bigg( (\frac{x+0.5 - \mu_i}{s_i}) \bigg) - \sigma \bigg( (\frac{x-0.5 - \mu_i}{s_i}) \bigg)  ]
\end{align}</script></li>
<li>For edge cases, <ol>
<li>when $x=0$, set $x-0.5 \rightarrow -\infty$</li>
<li>when $x=255$, set $x+0.5 \rightarrow +\infty$</li>
</ol>
</li>
</ul>
<div class="note info">
            <p><strong>Logistic distribution</strong>:</p><script type="math/tex; mode=display">\begin{align}F(x) &= \big(1+ e^{-\frac{x-\mu}{s}}\big)^{-1} \\&= \frac{1}{2} [1+\tanh (\frac{x-\mu}{2 s})]\end{align}</script><p>where the mean $ \mu \in (-\infty, +\infty)$,  std deviation $s &gt;0$</p>
          </div>
<ul>
<li><em>PixelCNN++</em> does not use deep networks to model the relationship between color channels. For the pixel <script type="math/tex">(r_{i,j}, g_{i,j}, b_{i,j})</script> at the location $(i,j)$ in the image, with the contexts <script type="math/tex">C_{i,j}</script>:</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
p(\color{red}{r}_{i,j}, \color{green}{g}_{i,j}, \color{blue}{b}_{i,j} \vert C_{i,j}) &= P \bigg(\color{red}{r}_{i,j} \vert \mu_\color{red}{r}(C_{i,j}), s_\color{red}{r}(C_{i,j})\bigg) \times P\bigg(\color{green}{g}_{i,j} \vert \mu_\color{green}{g} (C_{i,j}, \color{red}{r}_{i,j}), s_\color{green}{g}(C_{i,j})\bigg) \\ & \times P\bigg(\color{blue}{b}_{i,j} \vert \mu_\color{blue}{b} (C_{i,j}, \color{red}{r}_{i,j}, \color{green}{g}_{i,j}), s_\color{blue}{b}(C_{i,j}) \bigg) \\
\mu_\color{green}{g}(C_{i,j}, \color{red}{r}_{i,j}) &= \mu_\color{green}{g}(C_{i,j}) + \alpha(C_{i,j})\color{red}{r}_{i,j} \\
\mu_\color{blue}{b}(C_{i,j}, \color{red}{r}_{i,j}, \color{green}{g}_{i,j}) &= \mu_\color{blue}{b}(C_{i,j}) + \beta (C_{i,j}) \color{red}{r}_{i,j} + \gamma (C_{i,j}) \color{blue}{b}_{i,j}
\end{align}</script><p>where $\alpha$, $\gamma$, $\beta$ are scalar coefficents depdenting on the mixture component and previous pixels.</p>
<ul>
<li>As shown in the figure, it applies convolutions of stride 2 for downsampling and transposed strided convolution for upsampling. It also uses shor-cut connections recover the information loss from convolutions in the lower layers, similar to VAE<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). [Improved variational inference with inverse autoregressive flow](https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf). In Advances in neural information processing systems (pp. 4743-4751).
">[8]</span></a></sup> and U-Net<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ronneberger, O., Fischer, P., & Brox, T. (2015, October). [U-net: Convolutional networks for biomedical image segmentation](https://arxiv.org/pdf/1505.04597.pdf)%E5%92%8C[Tiramisu](https://arxiv.org/abs/1611.09326). In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.
">[7]</span></a></sup>. </li>
</ul>
<p><img data-src="/notes/images/PixelCNN++.png" alt="upload successful"></p>
<h2 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h2><ul>
<li>Masked convolutions: masked convolution has limited receptive field and thus requires deep stacked layers of a linearly increased number. It requires expand the kernel size or incease the layer depth to enlarge the effective receptive fields. (see below figure)</li>
</ul>
<p><img data-src="/notes/images/WaveNet-causal-conv.png" alt="upload successful"></p>
<p>WaveNet<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). [Wavenet: A generative model for raw audio](https://arxiv.org/pdf/1609.03499.pdf). arXiv preprint arXiv:1609.03499.
">[11]</span></a></sup><sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[(DeepMind blog) WaveNet: A generative model for raw audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)
">[12]</span></a></sup> (van den Ood <em>et al.</em>, DeepMind 2016) leverages dilated masked casual convolution to exponentially increase the receptive field. It is applied in TTS, ASR, music generation, audio modeling, etc.</p>
<p><img data-src="/notes/images/Dilated-causal-conv.png" alt="upload successful"></p>
<h3 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h3><p>It uses the same gated activation unit in <em>PixelCNN</em>, outperforming ReLU:</p>
<script type="math/tex; mode=display">
\mathbf{z} = \tanh (W_{f,k} \circledast \mathbf{x}) \odot \sigma(W_{g,k} \circledast \mathbf{x})</script><p>The overall model structure is:<br><img data-src="/notes/images/WaveNet-Model.png" alt="upload successful"></p>
<h3 id="Conditional-WaveNet"><a href="#Conditional-WaveNet" class="headerlink" title="Conditional WaveNet"></a>Conditional WaveNet</h3><p>Like the conditional <em>Gated PixelCNN</em>, WaveNet can be also conditional on a hidden representation $\mathbf{h}$.</p>
<ul>
<li>Global conditioning on a single representation vector $\mathbf{h}$ that influences the output distribution of all timesteps, e.g. a speaker embedding in a TTS model:<script type="math/tex; mode=display">
\mathbf{z} = \tanh (W_{f,k} \circledast \mathbf{x} + \color{red}{V_{f,k}^\top \mathbf{h}}) \odot \sigma(W_{g,k} \circledast \mathbf{x} + \color{red}{V_{g,k}^\top \mathbf{h}})</script></li>
<li>Local conditioning on a second timeseries <script type="math/tex">h_t</script>, possibly with a lower sampling frequency than the audio, e.g. linguistic features in a TTS model. WaveNet learns the upsampling on this time series using a <strong>transposed convolution</strong>: <script type="math/tex">\mathbf{y} = f(\mathbf{h})</script><script type="math/tex; mode=display">
\mathbf{z} = \tanh (W_{f,k} \circledast \mathbf{x} + \color{red}{V_{f,k}  \circledast f(\mathbf{h})}) \odot \sigma(W_{g,k} \circledast \mathbf{x} + \color{red}{V_{g,k}  \circledast f(\mathbf{h})})</script></li>
</ul>
<h3 id="Softmax-distribution"><a href="#Softmax-distribution" class="headerlink" title="Softmax distribution"></a>Softmax distribution</h3><p>The raw audio output is stored as a sequence of 16-bit scalar values (one per time step), thus the softmax output is 2<sup>16</sup>=65,536 probabilities per timestep. WaveNet applies a <strong>$\mu$-law companding transformation</strong> to the data and thenquantize it to 256 possible values:</p>
<script type="math/tex; mode=display">
f(x_t) = \text{sign} (x_t) \frac{\ln(1 + \mu |x_t|)}{\ln(1 + \mu)}</script><p>where <script type="math/tex">x_t \in (-1,1)</script>, $u=255$. The reconstruction signal after quantization sounded similar to the original.</p>
<h3 id="Fast-generation-via-caching"><a href="#Fast-generation-via-caching" class="headerlink" title="Fast generation via caching"></a>Fast generation via caching</h3><div class="note dander">
            <p><strong>Problems</strong>: During generation, convolutional AR models redundantly compute states, impeding the speed of generation process. Such states can be cached and reused to expedite the generation.<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... & Huang, T. S. (2017). [Fast generation for convolutional autoregressive models](https://arxiv.org/pdf/1704.06001). arXiv preprint arXiv:1704.06001.">[14]</span></a></sup></p>
          </div>
<p>The convolutional autoregressive generative model could cache and reuse the previously computed hidden states to accelerate the generation.</p>
<p>The below figure shows the model with 2 convolutional and 2 transposed convolutional layers with strid of 2, wherein blue dots indicate the cached states and orange bots are computed in the current step. The computation process can be seen as:</p>
<p><img data-src="/notes/images/conv-AR-fast-generation.png" alt="upload successful"></p>
<p><center> Image source: <sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... & Huang, T. S. (2017). [Fast generation for convolutional autoregressive models](https://arxiv.org/pdf/1704.06001). arXiv preprint arXiv:1704.06001.
">[14]</span></a></sup> </center></p>
<ul>
<li>This can also scale to 2D to apply on <strong>PixelCNN families</strong><sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... & Huang, T. S. (2017). [Fast generation for convolutional autoregressive models](https://arxiv.org/pdf/1704.06001). arXiv preprint arXiv:1704.06001.
">[14]</span></a></sup>.</li>
</ul>
<h2 id="PixelSNAIL"><a href="#PixelSNAIL" class="headerlink" title="PixelSNAIL"></a>PixelSNAIL</h2><p><em>PixelSNAIL</em><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, X., Mishra, N., Rohaninejad, M., & Abbeel, P. (2017). [Pixelsnail: An improved autoregressive generative model](https://arxiv.org/pdf/1712.09763). ICML 2018.
">[10]</span></a></sup> adopt masked self-attention approaches inspired by <em>SNAIL</em><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mishra, N., Rohaninejad, M., Chen, X., & Abbeel, P. (2017). [A simple neural attentive meta-learner](https://arxiv.org/pdf/1707.03141). arXiv preprint arXiv:1707.03141.
">[9]</span></a></sup>. </p>
<h3 id="Model-architecture-1"><a href="#Model-architecture-1" class="headerlink" title="Model architecture"></a>Model architecture</h3><p>The overall model structure:</p>
<p><img data-src="/notes/images/PixelSNAIL-model.png" alt="upload successful"></p>
<ul>
<li><p>It uses the self-attention block with shape <script type="math/tex">H \times W \times C_1 \rightarrow H \times W \times C_2</script> (see below figure):</p>
<ul>
<li>Key f<sub>k</sub>: <script type="math/tex">C_1 \rightarrow \text{d}_\text{key}</script></li>
<li>Query f<sub>q</sub>: <script type="math/tex">C_1 \rightarrow \text{d}_\text{key}</script></li>
<li><p>Value f<sub>v</sub>(x): <script type="math/tex">C_1 \rightarrow C_2</script></p>
<p>Given 2D feature map $\mathbf{y}= {y_1, y_2, \cdots, y_N }$, the attention mapping is:</p>
<script type="math/tex; mode=display">
\begin{align}
z_i & = \sum_{j<i} e_{ij} f_v(y_i) \\
e_i &= \text{softmax}([f_k(y_1)^\top f_q(y_i), \cdots, f_k(y_i-1)^\top f_q(y_i)])
\end{align}</script><p>where the summation over all previous history, i.e. $j&lt;i$.</p>
</li>
</ul>
</li>
</ul>
<p><img data-src="/notes/images/PixelSNAIL-attn-block.png" width="60%"/></p>
<ul>
<li>It applies 2D convolutions with gated activation functions as gated &amp;PixelCNN* and residual connections as the figure.</li>
</ul>
<p><img data-src="/notes/images/PixelSNAIL-residual-blocks.png" width="60%"/></p>
<ul>
<li>It adopts the <strong>zigzag ordering</strong> rather than <em>PixelCNN</em>-like raster scan ordering.</li>
</ul>
<p><img data-src="/notes/images/Image-generative-ordering.png" alt="upload successful"></p>
<ul>
<li>It employs the <strong>discretized mixture of logistics</strong> of <em>PixelCNN++</em> as the output distribution.</li>
</ul>
<p>In comparison,</p>
<ul>
<li><em>Gated PixelCNN</em> and <em>PixelCNN++</em> apply causal convolutions (dilated and strided conv, respectively) over the sequence, allowing the <strong>high-brandwidth access</strong> to the previous pixels. However, caual convolutions are limited to the receptive field due to their finite sizes.</li>
<li><strong>PixelSNAIL</strong> achieves a much larger receptive field size (see below figure).</li>
</ul>
<p><img data-src="/notes/images/PixelCNN-effective-receptive-field.png" alt="upload successful"></p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>“The basic difference between AR models with Generative Adversarial Networks (GANs) is that GANs <strong>implicitly learn data distribution</strong> whereas AR models learn the explicit distribution governed by a prior. “<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[(TowardsDataScience blog) Auto-Regressive Generative Models (PixelRNN, PixelCNN++)](https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173)
[^16:] [CS294-158 Lecture 2 slides](https://drive.google.com/file/d/194FouvI7xJM0bG4AaEsHo8PSSOjUozN6/view)
">[15]</span></a></sup><br><div class="note info">
            <p><strong>Pros:</strong></p><ul><li>expressivity (explicit learn): AR factorization is general; can explicitly compute likelihood $p(x)$</li><li>explicit likelihood of training data gives good evaluation metric</li><li>good samples</li><li>generalization: meaningful parameter sharing has good inductive bias</li><li>the training is more stable than GANs</li><li>it works for both discrete and continuous data (It is hard to learn discrete data like text for GANs)</li></ul>
          </div><br><div class="note danger">
            <p><strong>Cons:</strong></p><ul><li>Sequential generation =&gt; slow!</li><li>Low sampling efficency: sampling each pixel = 1 forward pass!</li></ul>
          </div><br><!-- todo: 17 / 18 /19 reading --></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oord, A. V. D., Kalchbrenner, N., &amp; Kavukcuoglu, K. (2016). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1601.06759.pdf">Pixel recurrent neural networks</a>. (Google DeepMind). ICML 2016.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks. Andrej Karpathy blog</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Germain, M., Gregor, K., Murray, I., &amp; Larochelle, H. (2015, June). <a target="_blank" rel="noopener" href="http://www.jmlr.org/proceedings/papers/v37/germain15.pdf">Made: Masked autoencoder for distribution estimation</a>. In International Conference on Machine Learning (pp. 881-889).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., &amp; Graves, A. (2016). <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders.pdf">Conditional image generation with pixelcnn decoders</a>. In Advances in neural information processing systems (pp. 4790-4798).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html">Tutorial: Gated PixelCNN</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Salimans, T., Karpathy, A., Chen, X., &amp; Kingma, D. P. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1701.05517">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</a>. arXiv preprint arXiv:1701.05517.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ronneberger, O., Fischer, P., &amp; Brox, T. (2015, October). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.04597.pdf">U-net: Convolutional networks for biomedical image segmentation</a>%E5%92%8C<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.09326">Tiramisu</a>. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016). <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf">Improved variational inference with inverse autoregressive flow</a>. In Advances in neural information processing systems (pp. 4743-4751).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mishra, N., Rohaninejad, M., Chen, X., &amp; Abbeel, P. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1707.03141">A simple neural attentive meta-learner</a>. arXiv preprint arXiv:1707.03141.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, X., Mishra, N., Rohaninejad, M., &amp; Abbeel, P. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1712.09763">Pixelsnail: An improved autoregressive generative model</a>. ICML 2018.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... &amp; Kavukcuoglu, K. (2016). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.03499.pdf">Wavenet: A generative model for raw audio</a>. arXiv preprint arXiv:1609.03499.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">(DeepMind blog) WaveNet: A generative model for raw audio</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... &amp; Huang, T. S. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.06001">Fast generation for convolutional autoregressive models</a>. arXiv preprint arXiv:1704.06001.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173">(TowardsDataScience blog) Auto-Regressive Generative Models (PixelRNN, PixelCNN++)</a>
[^16:] <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/194FouvI7xJM0bG4AaEsHo8PSSOjUozN6/view">CS294-158 Lecture 2 slides</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.03664.pdf">Parallel Multiscale Autoregressive Density Estimation</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.08185.pdf">PixelCNN Models with Auxiliary Variables for Natural Image Modeling</a><a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.01608.pdf">GENERATING HIGH FIDELITY IMAGES
WITH SUBSCALE PIXEL NETWORKS
AND MULTIDIMENSIONAL UPSCALING</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf">Stanford cs231n: Generative models</a><a href="#fnref:20" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/Unsupervised-learning/" rel="tag"># Unsupervised learning</a>
              <a href="/notes/tags/Autoregressive-models/" rel="tag"># Autoregressive models</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2019/12/15/NN/Neural-Network-Tricks/" rel="prev" title="Neural Network Tricks">
      <i class="fa fa-chevron-left"></i> Neural Network Tricks
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2019/12/22/Generative/Likelihood-based-generative-flow-models/" rel="next" title="Likelihood-based Generative Models II: Flow  Models">
      Likelihood-based Generative Models II: Flow  Models <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Autoregressive-models"><span class="nav-number">1.</span> <span class="nav-text">Autoregressive models</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN-AR-models"><span class="nav-number">2.</span> <span class="nav-text">RNN AR models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-LM"><span class="nav-number">2.1.</span> <span class="nav-text">RNN LM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PixelRNN"><span class="nav-number">2.2.</span> <span class="nav-text">PixelRNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pixel-by-pixel-generation"><span class="nav-number">2.2.1.</span> <span class="nav-text">Pixel-by-pixel generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Row-LSTM"><span class="nav-number">2.2.2.</span> <span class="nav-text">Row LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Diagonal-BiLSTM"><span class="nav-number">2.2.3.</span> <span class="nav-text">Diagonal BiLSTM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Masking-based-AR-models"><span class="nav-number">3.</span> <span class="nav-text">Masking-based AR models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MADE"><span class="nav-number">3.1.</span> <span class="nav-text">MADE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PixelCNN-families"><span class="nav-number">3.2.</span> <span class="nav-text">PixelCNN families</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PixelCNN"><span class="nav-number">3.2.1.</span> <span class="nav-text">PixelCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gated-PixelCNN"><span class="nav-number">3.2.2.</span> <span class="nav-text">Gated PixelCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Conditional-PixelCNN"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">Conditional PixelCNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PixelCNN-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">PixelCNN++</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WaveNet"><span class="nav-number">3.3.</span> <span class="nav-text">WaveNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-architecture"><span class="nav-number">3.3.1.</span> <span class="nav-text">Model architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conditional-WaveNet"><span class="nav-number">3.3.2.</span> <span class="nav-text">Conditional WaveNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-distribution"><span class="nav-number">3.3.3.</span> <span class="nav-text">Softmax distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fast-generation-via-caching"><span class="nav-number">3.3.4.</span> <span class="nav-text">Fast generation via caching</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PixelSNAIL"><span class="nav-number">3.4.</span> <span class="nav-text">PixelSNAIL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-architecture-1"><span class="nav-number">3.4.1.</span> <span class="nav-text">Model architecture</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Analysis"><span class="nav-number">4.</span> <span class="nav-text">Analysis</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="cyk1337"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">cyk1337</p>
  <div class="site-description" itemprop="description">What is now proved was once only imagined.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cyk1337</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2019/12/17/Generative/Likelihood-based-autoregressive-models/";
    this.page.identifier = "2019/12/17/Generative/Likelihood-based-autoregressive-models/";
    this.page.title = "Likelihood-based Generative Models I: Autoregressive Models";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
