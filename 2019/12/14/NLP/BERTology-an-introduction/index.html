<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="This is an introduction of recent BERT families.">
<meta property="og:type" content="article">
<meta property="og:title" content="BERTology: An Introduction!">
<meta property="og:url" content="https://cyk1337.github.io/notes/2019/12/14/NLP/BERTology-an-introduction/index.html">
<meta property="og:site_name" content="Yekun&#39;s Note">
<meta property="og:description" content="This is an introduction of recent BERT families.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/XLNet-PLM.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/content-stream-attention.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/query-stream-attention.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/XLNet-two-stream-attention.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/SpanBERT.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ELECTRA-replaced-token-detection.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ELECTRA-approach.png">
<meta property="article:published_time" content="2019-12-14T11:38:00.000Z">
<meta property="article:modified_time" content="2019-12-14T11:38:00.000Z">
<meta property="article:author" content="Yekun Chai">
<meta property="article:tag" content="Pre-training">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Language model">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/XLNet-PLM.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2019/12/14/NLP/BERTology-an-introduction/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>BERTology: An Introduction! | Yekun's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yekun's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Machine learning notes and writeup.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2019/12/14/NLP/BERTology-an-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="Yekun Chai">
      <meta itemprop="description" content="Language is not just words.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yekun's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BERTology: An Introduction!
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-14 19:38:00" itemprop="dateCreated datePublished" datetime="2019-12-14T19:38:00+08:00">2019-12-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/NLP/Language-model/" itemprop="url" rel="index"><span itemprop="name">Language model</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/NLP/Language-model/BERT/" itemprop="url" rel="index"><span itemprop="name">BERT</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2019/12/14/NLP/BERTology-an-introduction/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/14/NLP/BERTology-an-introduction/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>This is an introduction of recent BERT families.<br><span id="more"></span></p>
<div class="note success">
            <p><strong>Relevant notes</strong>:</p><ol><li><a href="/notes/2019/01/22/NLP/Attention-in-a-nutshell/#Transformer">Transformer detailed explanation</a></li><li><a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/">Transformer variant architectures</a></li><li><a href="/notes/2019/12/14/NLP/BERTology-an-introduction/">BERTology introduction</a></li></ol>
          </div>
<h1 id="LM-pretraining-background"><a href="#LM-pretraining-background" class="headerlink" title="LM pretraining background"></a>LM pretraining background</h1><h2 id="Autoregressive-Language-Modeling"><a href="#Autoregressive-Language-Modeling" class="headerlink" title="Autoregressive Language Modeling"></a>Autoregressive Language Modeling</h2><p>Given a text sequence <script type="math/tex">\pmb{x} = (x_1, \cdots, x_T)</script>, autoregressive (AR) language modeling factorizes the likelihood along a uni direction according to the product rule, either forward:</p>
<script type="math/tex; mode=display">p(\pmb{x})=\prod_{t=1}^T p(x_t \vert x_{<t}))</script><p>or backward:</p>
<script type="math/tex; mode=display">p(\pmb{x})=\prod_{t=T}^1 p(x_t \vert x_{>t}))</script><p>The AR pretraining maximizes the likehood under the forward AR factorization:</p>
<script type="math/tex; mode=display">
\begin{align}
\max_\theta \log p_\theta (\pmb{x}) &= \sum_{t=1}^T \log p_\theta (x_t \vert \pmb{x}_{<t}) \\
& = \sum_{t=1}^T \log \frac{\exp\left(  \overbrace{h_\theta (\pmb{x}_{1:t-1})^T}^\text{context representation } \overbrace{e(x_t)}^\text{ embedding} \right)}{\sum_{x'}\exp\left(  h_\theta (\pmb{x}_{1:t-1})^T e(x') \right)}
\end{align}</script><p>where <script type="math/tex">h_\theta (\pmb{x}_{1:t-1})</script> denotes the context representation by NNs, such as RNNs/Transformers; <script type="math/tex">e(x_t)</script> denotes the embedding of $x$.</p>
<p>It is not effective to model the deep bidirectional contexts.</p>
<h2 id="Autoencoding-based-pretraining"><a href="#Autoencoding-based-pretraining" class="headerlink" title="Autoencoding based pretraining"></a>Autoencoding based pretraining</h2><p>Autoencoding (AE) based pretraining does not perform density estimation, but <strong>recover the original data from corrupted (masked) input</strong>.</p>
<p>Denosing autoencoding based pretraining, such as BERT can <strong>model the bidirectional contexts</strong>. Given a text sequence <script type="math/tex">\pmb{x} = (x_1, \cdots, x_T)</script>, it randomly masks a portion (15%) of tokens $\bar{\pmb{x}}$ in $\pmb{x}$. The training objective is to reconstruct randomly masked token $\bar{\pmb{x}}$ from   corrupted sequence $\hat{\pmb{x}}$:</p>
<script type="math/tex; mode=display">
\begin{align}
\max_\theta \log p_\theta (\bar{\pmb{x}} \vert \hat{\pmb{x}}) & \approx \sum_{t=1}^T m_t \log p_\theta (x_t \vert \hat{\pmb{x}}) \\
& = \sum_{t=1}^T m_t \log \frac{\exp\big( H_\theta (\hat{\pmb{x}})^T_t e(x_t) \big)}{\sum_{x'} \exp\big( H_\theta(\hat{\pmb{x}})_t^T e(x') \big)}
\end{align}</script><p>where </p>
<ul>
<li><script type="math/tex">m_t=1</script> means <script type="math/tex">x_t</script> is masked; </li>
<li>the Transformer <script type="math/tex">H_\theta</script> encodes $\pmb{x}$ into hidden vectors <script type="math/tex">H_\theta(\pmb{x}) = \big[H_\theta(\pmb{x})_1, H_\theta(\pmb{x})_2,  \cdots, H_\theta(\pmb{x})_T \big]</script></li>
</ul>
<p>However, it relies on corrupting the input with masks. The <strong>drawbacks</strong>:</p>
<ol>
<li><strong>Independent assumption</strong>: cannot model joint probability and assume the predicted tokens are independent of each other. It neglects the dependency between the masked positions</li>
<li><strong>pretrain-finetune discrepancy</strong> (input noise): the artificial symbols like [MASK] used by BERT does not exist during the training of downstream tasks.</li>
</ol>
<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p>XLNet<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237). arXiv preprint arXiv:1906.08237.
">[1]</span></a></sup> (CMU &amp; Google brain 2019) leverages both the advatage of AR and AE LM objectives and hinder their drawbacks. </p>
<ul>
<li>The permutation of the factorization order impedes the dependency between masked positions in BERT and still remains the AR-like objectives so as to prevent the pretrain-finetuning discrepancy. </li>
<li>On the other hand, with permutation, it attends to the bi-contextual information as in BERT.</li>
</ul>
<h2 id="Permutation-Language-Model"><a href="#Permutation-Language-Model" class="headerlink" title="Permutation Language Model"></a>Permutation Language Model</h2><p>XLNet<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: XLNet](https://github.com/zihangdai/xlnet/blob/master/data_utils.py)">[13]</span></a></sup> applies permutation language model by autoregressively pretraining bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order.</p>
<p><img data-src="/notes/images/XLNet-PLM.png" alt="Permutation Language Model"></p>
<p>For a sequence $\pmb{x}$ of length $T$, there are $T!$ different orders to perform a valid AR factorization.</p>
<script type="math/tex; mode=display">\max_\theta \mathbb{E}_{\pmb{z}\sim Z_T} \left[ \sum_{t=1}^T \log p_\theta(x_{z_t} \vert \pmb{x}_{z<t}) \right]</script><p>Permutation language modeling not only retains the benefits of AR models but also capture the bidirectional contexts as BERT. It only permutes the factorization order, rather than the sequence order.</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><h4 id="XLNet-Implementation"><a href="#XLNet-Implementation" class="headerlink" title="XLNet Implementation"></a>XLNet Implementation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># XLNet implementation (tf)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_local_perm</span>(<span class="params">inputs, targets, is_masked, perm_size, seq_len</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Sample a permutation of the factorization order, and create an</span></span><br><span class="line"><span class="string">  attention mask accordingly.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    inputs: int64 Tensor in shape [seq_len], input ids.</span></span><br><span class="line"><span class="string">    targets: int64 Tensor in shape [seq_len], target ids.</span></span><br><span class="line"><span class="string">    is_masked: bool Tensor in shape [seq_len]. True means being selected</span></span><br><span class="line"><span class="string">      for partial prediction.</span></span><br><span class="line"><span class="string">    perm_size: the length of longest permutation. Could be set to be reuse_len.</span></span><br><span class="line"><span class="string">      Should not be larger than reuse_len or there will be data leaks.</span></span><br><span class="line"><span class="string">    seq_len: int, sequence length.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Generate permutation indices</span></span><br><span class="line">  index = tf.<span class="built_in">range</span>(seq_len, dtype=tf.int64)</span><br><span class="line">  index = tf.transpose(tf.reshape(index, [-<span class="number">1</span>, perm_size]))</span><br><span class="line">  index = tf.random_shuffle(index)</span><br><span class="line">  index = tf.reshape(tf.transpose(index), [-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `perm_mask` and `target_mask`</span></span><br><span class="line">  <span class="comment"># non-functional tokens</span></span><br><span class="line">  non_func_tokens = tf.logical_not(tf.logical_or(</span><br><span class="line">      tf.equal(inputs, SEP_ID),</span><br><span class="line">      tf.equal(inputs, CLS_ID)))</span><br><span class="line"></span><br><span class="line">  non_mask_tokens = tf.logical_and(tf.logical_not(is_masked), non_func_tokens)</span><br><span class="line">  masked_or_func_tokens = tf.logical_not(non_mask_tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Set the permutation indices of non-masked (&amp; non-funcional) tokens to the</span></span><br><span class="line">  <span class="comment"># smallest index (-1):</span></span><br><span class="line">  <span class="comment"># (1) they can be seen by all other positions</span></span><br><span class="line">  <span class="comment"># (2) they cannot see masked positions, so there won&quot;t be information leak</span></span><br><span class="line">  smallest_index = -tf.ones([seq_len], dtype=tf.int64)</span><br><span class="line">  rev_index = tf.where(non_mask_tokens, smallest_index, index)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create `target_mask`: non-funcional and maksed tokens</span></span><br><span class="line">  <span class="comment"># 1: use mask as input and have loss</span></span><br><span class="line">  <span class="comment"># 0: use token (or [SEP], [CLS]) as input and do not have loss</span></span><br><span class="line">  target_tokens = tf.logical_and(masked_or_func_tokens, non_func_tokens)</span><br><span class="line">  target_mask = tf.cast(target_tokens, tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create `perm_mask`</span></span><br><span class="line">  <span class="comment"># `target_tokens` cannot see themselves</span></span><br><span class="line">  self_rev_index = tf.where(target_tokens, rev_index, rev_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1: cannot attend if i &lt;= j and j is not non-masked (masked_or_func_tokens)</span></span><br><span class="line">  <span class="comment"># 0: can attend if i &gt; j or j is non-masked</span></span><br><span class="line">  perm_mask = tf.logical_and(</span><br><span class="line">      self_rev_index[:, <span class="literal">None</span>] &lt;= rev_index[<span class="literal">None</span>, :],</span><br><span class="line">      masked_or_func_tokens)</span><br><span class="line">  perm_mask = tf.cast(perm_mask, tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># new target: [next token] for LM and [curr token] (self) for PLM</span></span><br><span class="line">  new_targets = tf.concat([inputs[<span class="number">0</span>: <span class="number">1</span>], targets[: -<span class="number">1</span>]],</span><br><span class="line">                          axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># construct inputs_k</span></span><br><span class="line">  inputs_k = inputs</span><br><span class="line"></span><br><span class="line">  <span class="comment"># construct inputs_q</span></span><br><span class="line">  inputs_q = target_mask</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> perm_mask, new_targets, target_mask, inputs_k, inputs_q</span><br></pre></td></tr></table></figure>
<h4 id="Huggingface-Implementation"><a href="#Huggingface-Implementation" class="headerlink" title="Huggingface Implementation"></a>Huggingface Implementation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForPermutationLanguageModeling</span>(<span class="params">DataCollatorMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Data collator used for permutation language modeling.</span></span><br><span class="line"><span class="string">    - collates batches of tensors, honoring their tokenizer&#x27;s pad_token</span></span><br><span class="line"><span class="string">    - preprocesses batches for permutation language modeling with procedures specific to XLNet</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: PreTrainedTokenizerBase</span><br><span class="line">    plm_probability: <span class="built_in">float</span> = <span class="number">1</span> / <span class="number">6</span></span><br><span class="line">    max_span_length: <span class="built_in">int</span> = <span class="number">5</span>  <span class="comment"># maximum length of a span of masked tokens</span></span><br><span class="line">    return_tensors: <span class="built_in">str</span> = <span class="string">&quot;pt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            examples = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        batch = _torch_collate_batch(examples, self.tokenizer)</span><br><span class="line">        inputs, perm_mask, target_mapping, labels = self.torch_mask_tokens(batch)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;perm_mask&quot;</span>: perm_mask, <span class="string">&quot;target_mapping&quot;</span>: target_mapping, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            examples = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        batch = _tf_collate_batch(examples, self.tokenizer)</span><br><span class="line">        inputs, perm_mask, target_mapping, labels = self.tf_mask_tokens(batch)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;perm_mask&quot;</span>: perm_mask, <span class="string">&quot;target_mapping&quot;</span>: target_mapping, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            examples = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        batch = _numpy_collate_batch(examples, self.tokenizer)</span><br><span class="line">        inputs, perm_mask, target_mapping, labels = self.numpy_mask_tokens(batch)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;perm_mask&quot;</span>: perm_mask, <span class="string">&quot;target_mapping&quot;</span>: target_mapping, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:</span></span><br><span class="line"><span class="string">            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line"><span class="string">            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line"><span class="string">            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be</span></span><br><span class="line"><span class="string">               masked</span></span><br><span class="line"><span class="string">            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -</span></span><br><span class="line"><span class="string">               span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line"><span class="string">            4. Set `cur_len = cur_len + context_length`. If `cur_len &lt; max_len` (i.e. there are tokens remaining in the</span></span><br><span class="line"><span class="string">               sequence to be processed), repeat from Step 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for permutation language modeling. Please add a mask token if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.size(<span class="number">1</span>) % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        labels = inputs.clone()</span><br><span class="line">        <span class="comment"># Creating the mask and target_mapping tensors</span></span><br><span class="line">        masked_indices = torch.full(labels.shape, <span class="number">0</span>, dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">        target_mapping = torch.zeros((labels.size(<span class="number">0</span>), labels.size(<span class="number">1</span>), labels.size(<span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.size(<span class="number">0</span>)):</span><br><span class="line">            <span class="comment"># Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line">            cur_len = <span class="number">0</span></span><br><span class="line">            max_len = labels.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> cur_len &lt; max_len:</span><br><span class="line">                <span class="comment"># Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line">                span_length = torch.randint(<span class="number">1</span>, self.max_span_length + <span class="number">1</span>, (<span class="number">1</span>,)).item()</span><br><span class="line">                <span class="comment"># Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked</span></span><br><span class="line">                context_length = <span class="built_in">int</span>(span_length / self.plm_probability)</span><br><span class="line">                <span class="comment"># Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line">                start_index = cur_len + torch.randint(context_length - span_length + <span class="number">1</span>, (<span class="number">1</span>,)).item()</span><br><span class="line">                masked_indices[i, start_index : start_index + span_length] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># Set `cur_len = cur_len + context_length`</span></span><br><span class="line">                cur_len += context_length</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Since we&#x27;re replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,</span></span><br><span class="line">            <span class="comment"># the i-th predict corresponds to the i-th token.</span></span><br><span class="line">            target_mapping[i] = torch.eye(labels.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = torch.tensor(</span><br><span class="line">            [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()],</span><br><span class="line">            dtype=torch.<span class="built_in">bool</span>,</span><br><span class="line">        )</span><br><span class="line">        masked_indices.masked_fill_(special_tokens_mask, value=<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels.eq(self.tokenizer.pad_token_id)</span><br><span class="line">            masked_indices.masked_fill_(padding_mask, value=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.</span></span><br><span class="line">        non_func_mask = ~(padding_mask | special_tokens_mask)</span><br><span class="line"></span><br><span class="line">        inputs[masked_indices] = self.tokenizer.mask_token_id</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        perm_mask = torch.zeros((labels.size(<span class="number">0</span>), labels.size(<span class="number">1</span>), labels.size(<span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.size(<span class="number">0</span>)):</span><br><span class="line">            <span class="comment"># Generate permutation indices i.e. sample a random factorisation order for the sequence. This will</span></span><br><span class="line">            <span class="comment"># determine which tokens a given token can attend to (encoded in `perm_mask`).</span></span><br><span class="line">            <span class="comment"># Note: Length of token sequence being permuted has to be less than or equal to reused sequence length</span></span><br><span class="line">            <span class="comment"># (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,</span></span><br><span class="line">            <span class="comment"># we assume that reused length is half of sequence length and permutation length is equal to reused length.</span></span><br><span class="line">            <span class="comment"># This requires that the sequence length be even.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a linear factorisation order</span></span><br><span class="line">            perm_index = torch.arange(labels.size(<span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Split this into two halves, assuming that half the sequence is reused each time</span></span><br><span class="line">            perm_index = perm_index.reshape((-<span class="number">1</span>, labels.size(<span class="number">1</span>) // <span class="number">2</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># Permute the two halves such that they do not cross over</span></span><br><span class="line">            perm_index = perm_index[torch.randperm(labels.size(<span class="number">1</span>) // <span class="number">2</span>)]</span><br><span class="line">            <span class="comment"># Flatten this out into the desired permuted factorisation order</span></span><br><span class="line">            perm_index = torch.flatten(perm_index.transpose(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Set the permutation indices of non-masked (non-functional) tokens to the</span></span><br><span class="line">            <span class="comment"># smallest index (-1) so that:</span></span><br><span class="line">            <span class="comment"># (1) They can be seen by all other positions</span></span><br><span class="line">            <span class="comment"># (2) They cannot see masked positions, so there won&#x27;t be information leak</span></span><br><span class="line">            perm_index.masked_fill_(~masked_indices[i] &amp; non_func_mask[i], -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># The logic for whether the i-th token can attend on the j-th token based on the factorisation order:</span></span><br><span class="line">            <span class="comment"># 0 (can attend): If perm_index[i] &gt; perm_index[j] or j is neither masked nor a functional token</span></span><br><span class="line">            <span class="comment"># 1 (cannot attend): If perm_index[i] &lt;= perm_index[j] and j is either masked or a functional token</span></span><br><span class="line">            perm_mask[i] = (</span><br><span class="line">                perm_index.reshape((labels.size(<span class="number">1</span>), <span class="number">1</span>)) &lt;= perm_index.reshape((<span class="number">1</span>, labels.size(<span class="number">1</span>)))</span><br><span class="line">            ) &amp; masked_indices[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inputs.long(), perm_mask, target_mapping, labels.long()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:</span></span><br><span class="line"><span class="string">            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line"><span class="string">            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line"><span class="string">            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be</span></span><br><span class="line"><span class="string">               masked</span></span><br><span class="line"><span class="string">            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -</span></span><br><span class="line"><span class="string">               span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line"><span class="string">            4. Set `cur_len = cur_len + context_length`. If `cur_len &lt; max_len` (i.e. there are tokens remaining in the</span></span><br><span class="line"><span class="string">               sequence to be processed), repeat from Step 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for permutation language modeling. Please add a mask token if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tf.shape(inputs)[<span class="number">1</span>] % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        labels = tf.identity(inputs)</span><br><span class="line">        <span class="comment"># Creating the mask and target_mapping tensors</span></span><br><span class="line">        masked_indices = np.full(labels.shape.as_list(), <span class="number">0</span>, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line">        labels_shape = tf.shape(labels)</span><br><span class="line">        target_mapping = np.zeros((labels_shape[<span class="number">0</span>], labels_shape[<span class="number">1</span>], labels_shape[<span class="number">1</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">            <span class="comment"># Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line">            cur_len = <span class="number">0</span></span><br><span class="line">            max_len = tf.shape(labels)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> cur_len &lt; max_len:</span><br><span class="line">                <span class="comment"># Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line">                span_length = randint(<span class="number">1</span>, self.max_span_length + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked</span></span><br><span class="line">                context_length = <span class="built_in">int</span>(span_length / self.plm_probability)</span><br><span class="line">                <span class="comment"># Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line">                start_index = cur_len + randint(<span class="number">0</span>, context_length - span_length + <span class="number">1</span>)</span><br><span class="line">                masked_indices[i, start_index : start_index + span_length] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># Set `cur_len = cur_len + context_length`</span></span><br><span class="line">                cur_len += context_length</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Since we&#x27;re replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,</span></span><br><span class="line">            <span class="comment"># the i-th predict corresponds to the i-th token.</span></span><br><span class="line">            target_mapping[i] = np.eye(labels_shape[<span class="number">1</span>])</span><br><span class="line">        masked_indices = tf.cast(tf.convert_to_tensor(masked_indices), dtype=tf.<span class="built_in">bool</span>)</span><br><span class="line">        target_mapping = tf.convert_to_tensor(target_mapping)</span><br><span class="line">        special_tokens_mask = tf.convert_to_tensor(</span><br><span class="line">            [</span><br><span class="line">                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> val <span class="keyword">in</span> labels.numpy().tolist()</span><br><span class="line">            ],</span><br><span class="line">        )</span><br><span class="line">        special_tokens_mask = tf.cast(special_tokens_mask, dtype=tf.<span class="built_in">bool</span>)</span><br><span class="line">        masked_indices = masked_indices &amp; ~special_tokens_mask</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices = masked_indices &amp; ~padding_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.</span></span><br><span class="line">        non_func_mask = ~(padding_mask | special_tokens_mask)</span><br><span class="line"></span><br><span class="line">        inputs = tf.where(masked_indices, self.tokenizer.mask_token_id, inputs)</span><br><span class="line">        labels = tf.where(masked_indices, labels, -<span class="number">100</span>)  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        perm_mask = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">            <span class="comment"># Generate permutation indices i.e. sample a random factorisation order for the sequence. This will</span></span><br><span class="line">            <span class="comment"># determine which tokens a given token can attend to (encoded in `perm_mask`).</span></span><br><span class="line">            <span class="comment"># Note: Length of token sequence being permuted has to be less than or equal to reused sequence length</span></span><br><span class="line">            <span class="comment"># (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,</span></span><br><span class="line">            <span class="comment"># we assume that reused length is half of sequence length and permutation length is equal to reused length.</span></span><br><span class="line">            <span class="comment"># This requires that the sequence length be even.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a linear factorisation order</span></span><br><span class="line">            <span class="comment"># tf.range is the equivalent of torch.arange</span></span><br><span class="line">            perm_index = tf.<span class="built_in">range</span>(labels_shape[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># Split this into two halves, assuming that half the sequence is reused each time</span></span><br><span class="line">            perm_index = tf.transpose(tf.reshape(perm_index, (-<span class="number">1</span>, labels_shape[<span class="number">1</span>] // <span class="number">2</span>)))</span><br><span class="line">            <span class="comment"># Permute the two halves such that they do not cross over</span></span><br><span class="line">            perm_index = tf.random.shuffle(perm_index)  <span class="comment"># Shuffles along the first dimension</span></span><br><span class="line">            <span class="comment"># Flatten this out into the desired permuted factorisation order</span></span><br><span class="line">            perm_index = tf.reshape(tf.transpose(perm_index), (-<span class="number">1</span>,))</span><br><span class="line">            <span class="comment"># Set the permutation indices of non-masked (non-functional) tokens to the</span></span><br><span class="line">            <span class="comment"># smallest index (-1) so that:</span></span><br><span class="line">            <span class="comment"># (1) They can be seen by all other positions</span></span><br><span class="line">            <span class="comment"># (2) They cannot see masked positions, so there won&#x27;t be information leak</span></span><br><span class="line">            perm_index = tf.where(~masked_indices[i] &amp; non_func_mask[i], -<span class="number">1</span>, perm_index)</span><br><span class="line">            <span class="comment"># The logic for whether the i-th token can attend on the j-th token based on the factorisation order:</span></span><br><span class="line">            <span class="comment"># 0 (can attend): If perm_index[i] &gt; perm_index[j] or j is neither masked nor a functional token</span></span><br><span class="line">            <span class="comment"># 1 (cannot attend): If perm_index[i] &lt;= perm_index[j] and j is either masked or a functional token</span></span><br><span class="line">            perm_mask.append(</span><br><span class="line">                (tf.reshape(perm_index, (labels_shape[<span class="number">1</span>], <span class="number">1</span>)) &lt;= tf.reshape(perm_index, (<span class="number">1</span>, labels_shape[<span class="number">1</span>])))</span><br><span class="line">                &amp; masked_indices[i]</span><br><span class="line">            )</span><br><span class="line">        perm_mask = tf.stack(perm_mask, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.cast(inputs, tf.int64), tf.cast(perm_mask, tf.float32), target_mapping, tf.cast(labels, tf.int64)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:</span></span><br><span class="line"><span class="string">            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line"><span class="string">            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line"><span class="string">            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be</span></span><br><span class="line"><span class="string">               masked</span></span><br><span class="line"><span class="string">            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -</span></span><br><span class="line"><span class="string">               span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line"><span class="string">            4. Set `cur_len = cur_len + context_length`. If `cur_len &lt; max_len` (i.e. there are tokens remaining in the</span></span><br><span class="line"><span class="string">               sequence to be processed), repeat from Step 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for permutation language modeling. Please add a mask token if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.shape[<span class="number">1</span>] % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        labels = np.copy(inputs)</span><br><span class="line">        <span class="comment"># Creating the mask and target_mapping tensors</span></span><br><span class="line">        masked_indices = np.full(labels.shape, <span class="number">0</span>, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line">        target_mapping = np.zeros((labels.shape[<span class="number">0</span>], labels.shape[<span class="number">1</span>], labels.shape[<span class="number">1</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line">            cur_len = <span class="number">0</span></span><br><span class="line">            max_len = labels.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> cur_len &lt; max_len:</span><br><span class="line">                <span class="comment"># Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line">                span_length = randint(<span class="number">1</span>, self.max_span_length + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked</span></span><br><span class="line">                context_length = <span class="built_in">int</span>(span_length / self.plm_probability)</span><br><span class="line">                <span class="comment"># Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line">                start_index = cur_len + randint(<span class="number">0</span>, context_length - span_length + <span class="number">1</span>)</span><br><span class="line">                masked_indices[i, start_index : start_index + span_length] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># Set `cur_len = cur_len + context_length`</span></span><br><span class="line">                cur_len += context_length</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Since we&#x27;re replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,</span></span><br><span class="line">            <span class="comment"># the i-th predict corresponds to the i-th token.</span></span><br><span class="line">            target_mapping[i] = np.eye(labels.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = np.array(</span><br><span class="line">            [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()],</span><br><span class="line">            dtype=np.<span class="built_in">bool</span>,</span><br><span class="line">        )</span><br><span class="line">        masked_indices[special_tokens_mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices[padding_mask] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.</span></span><br><span class="line">        non_func_mask = ~(padding_mask | special_tokens_mask)</span><br><span class="line"></span><br><span class="line">        inputs[masked_indices] = self.tokenizer.mask_token_id</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        perm_mask = np.zeros((labels.shape[<span class="number">0</span>], labels.shape[<span class="number">1</span>], labels.shape[<span class="number">1</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># Generate permutation indices i.e. sample a random factorisation order for the sequence. This will</span></span><br><span class="line">            <span class="comment"># determine which tokens a given token can attend to (encoded in `perm_mask`).</span></span><br><span class="line">            <span class="comment"># Note: Length of token sequence being permuted has to be less than or equal to reused sequence length</span></span><br><span class="line">            <span class="comment"># (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,</span></span><br><span class="line">            <span class="comment"># we assume that reused length is half of sequence length and permutation length is equal to reused length.</span></span><br><span class="line">            <span class="comment"># This requires that the sequence length be even.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a linear factorisation order</span></span><br><span class="line">            perm_index = np.arange(labels.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># Split this into two halves, assuming that half the sequence is reused each time</span></span><br><span class="line">            perm_index = perm_index.reshape((-<span class="number">1</span>, labels.shape[<span class="number">1</span>] // <span class="number">2</span>)).T</span><br><span class="line">            <span class="comment"># Permute the two halves such that they do not cross over</span></span><br><span class="line">            np.random.shuffle(perm_index)</span><br><span class="line">            <span class="comment"># Flatten this out into the desired permuted factorisation order</span></span><br><span class="line">            perm_index = perm_index.T.flatten()</span><br><span class="line">            <span class="comment"># Set the permutation indices of non-masked (non-functional) tokens to the</span></span><br><span class="line">            <span class="comment"># smallest index (-1) so that:</span></span><br><span class="line">            <span class="comment"># (1) They can be seen by all other positions</span></span><br><span class="line">            <span class="comment"># (2) They cannot see masked positions, so there won&#x27;t be information leak</span></span><br><span class="line">            perm_index[~masked_indices[i] &amp; non_func_mask[i]] = -<span class="number">1</span></span><br><span class="line">            <span class="comment"># The logic for whether the i-th token can attend on the j-th token based on the factorisation order:</span></span><br><span class="line">            <span class="comment"># 0 (can attend): If perm_index[i] &gt; perm_index[j] or j is neither masked nor a functional token</span></span><br><span class="line">            <span class="comment"># 1 (cannot attend): If perm_index[i] &lt;= perm_index[j] and j is either masked or a functional token</span></span><br><span class="line">            perm_mask[i] = (</span><br><span class="line">                perm_index.reshape((labels.shape[<span class="number">1</span>], <span class="number">1</span>)) &lt;= perm_index.reshape((<span class="number">1</span>, labels.shape[<span class="number">1</span>]))</span><br><span class="line">            ) &amp; masked_indices[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inputs.astype(np.int64), perm_mask, target_mapping, labels.astype(np.int64)</span><br></pre></td></tr></table></figure>
<h3 id="Two-stream-self-attention"><a href="#Two-stream-self-attention" class="headerlink" title="Two-stream self-attention"></a>Two-stream self-attention</h3><p>The next-token distribution with the standard softmax formulation:</p>
<script type="math/tex; mode=display">p_\theta(X_{z_t}=x \vert \pmb{x}_{\pmb{z}<t}) = \frac{\exp \big( e(x)^T h_\theta (\pmb{x}_{\pmb{z}<t}) \big)}{\sum_{x'}\exp \big( e(x')^T h_\theta (\pmb{x}_{\pmb{z}<t}) \big)}</script><p>where <script type="math/tex">h_\theta(\pmb{z}_{\pmb{z}<t})</script>, abbr. <script type="math/tex">h_{z_t}</script>, denotes <strong>content representation</strong>, which encodes both context and <script type="math/tex">x_{z_t}</script> itself, as hidden states in Transformer, i.e. standard self-attention, see below figure(a).</p>
<p><img data-src='/notes/images/content-stream-attention.png' width='50%'/></p>
<p>However, the previous $t-1$ sequence cannot implies the unique predicted target since different target words might have the same previous sequence in the permutated AR factorization. Hence, XLNet also consider the target position information:</p>
<script type="math/tex; mode=display">p_\theta(X_{z_t}=x \vert \pmb{x}_{z<t}) = \frac{\exp\left( e(x)^T g_\theta(\pmb{x}_{\pmb{z}<t}, \color{red}{z_t}) \right)}{\sum_{x'} \exp\left( e(x')^T g_\theta(\pmb{x}_{\pmb{z}<t}, \color{red}{z_t}) \right)}</script><p>where <script type="math/tex">g_\theta(\pmb{x}_{\pmb{z}<t}, z_t)</script>, abbr. <script type="math/tex">g_{z_t}</script> denotes a <strong>query representation</strong>, only using the position <script type="math/tex">\color{red}{z_t}</script> and not the context <script type="math/tex">\mathbf{x_{z_t}}</script>, as below figure(b).</p>
<p><img data-src='/notes/images/query-stream-attention.png' width='50%'/></p>
<ul>
<li><strong>query stream</strong> uses <script type="math/tex">z_t</script> but cannot see <script type="math/tex">x_{z_t}</script>:<script type="math/tex; mode=display">g_{z_t}^m \leftarrow \text{attention}\big(\pmb{Q}=g_{z_t}^{(m-1)}, \pmb{KV} = \pmb{h}_{\pmb{z}<t}^{(m-1)} ;\theta \big)</script></li>
<li><strong>content stream</strong> uses both <script type="math/tex">z_t</script> and <script type="math/tex">x_{z_t}</script>:<script type="math/tex; mode=display">h_{z_t}^m \leftarrow \text{attention}\big(\pmb{Q}=h_{z_t}^{(m-1)}, \pmb{KV} = \pmb{h}_{\pmb{z}\leq t}^{(m-1)} ;\theta \big)</script></li>
</ul>
<p>Here $\pmb{Q}$,$\pmb{K}$,$\pmb{V}$ denot the query, key, value in an attention op.<br><div class="note info">
            <ul><li>During finetuning, we can simply <strong>drop the query stream</strong> and <strong>use the content stream</strong> as a normal Transformer(-XL).</li><li>The permutation implementation is relying on the <strong>attention mask</strong>, as shown in the figure, which does not affect the original sequecen orders.</li></ul>
          </div></p>
<p><img data-src="/notes/images/XLNet-two-stream-attention.png" alt="upload successful"></p>
<h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h2><p>Borrow <strong>relative positional encoding</strong> and <strong>segment recurrence mechanism</strong> from Transformer-XL<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). [Transformer-xl: Attentive language models beyond a fixed-length context](https://arxiv.org/pdf/1901.02860). arXiv preprint arXiv:1901.02860.
">[2]</span></a></sup>.<br>The next segment with memory is:</p>
<script type="math/tex; mode=display">h_{z_t}^{(m)} \leftarrow \text{Att}( \mathbf{Q}= h_{z_t}^{(m-1)}, \, \mathbf{KV}= \big[ \mathbf{\tilde{h}}^{(m-1)}, \color{green}{\mathbf{h}_{z \leq t}^{(m-1)}} \big] ; \theta )</script><h2 id="Relative-segment-encoding"><a href="#Relative-segment-encoding" class="headerlink" title="Relative segment encoding"></a>Relative segment encoding</h2><p>XLnet only considers whether the two positions in segments are <strong>within the same segment</strong> as opposed to considering <em>which specific segments they are from</em>. </p>
<p>The idea of relative encodings is only modeling the relationships between positions <script type="math/tex">s_{ij}</script>, denoting the segment encoding beween position i to j.<br>The attention weight <script type="math/tex">a_{ij} = (\mathbf{q}_i + \mathbf{b})^\top s_{ij}</script>, where <script type="math/tex">\mathbf{q}_i</script> is the query vector in std attention and $\mathbf{b}$ is a learnable head-specific bias vector. Finally add <script type="math/tex">a_{ij}</script> to the normal attention weight.</p>
<p>The advantage of relative segment encodings:</p>
<ol>
<li>to introduce inductive biases to improve generalization;</li>
<li>to allow for the multiple input segments in finetuning on tasks.</li>
</ol>
<h1 id="RoBERTa-BERT-is-undertrained"><a href="#RoBERTa-BERT-is-undertrained" class="headerlink" title="RoBERTa: BERT is undertrained!"></a>RoBERTa: BERT is undertrained!</h1><p>RoBERTa<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). [Roberta: A robustly optimized bert pretraining approach](https://arxiv.org/pdf/1907.11692). arXiv preprint arXiv:1907.11692.
">[5]</span></a></sup> (Fair &amp; UW 2019) (<u><b>R</b></u>obustly <u><b>o</b></u>ptimized <u><b>BERT</b></u> <u><b>a</b></u>pproach) redesigned the BERT experiments<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91). arXiv preprint arXiv:1810.04805.
">[6]</span></a></sup>, illustrating <strong>BERT is underfitted</strong>. It showed that BERT pretraining with a larger batch size over more data for more training steps could lead to a better pretraining results. </p>
<div class="note info">
            <p>Recent works<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237). arXiv preprint arXiv:1906.08237.">[1]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). [Roberta: A robustly optimized bert pretraining approach](https://arxiv.org/pdf/1907.11692). arXiv preprint arXiv:1907.11692.">[5]</span></a></sup> questioned the effectiveness of Next Sentence Prediction (NSP) pretraining task proposed by BERT<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91). arXiv preprint arXiv:1810.04805.">[6]</span></a></sup>.</p>
          </div>
<h1 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h1><p>SpanBERT<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., & Levy, O. (2019). [Spanbert: Improving pre-training by representing and predicting spans](https://arxiv.org/pdf/1907.10529). arXiv preprint arXiv:1907.10529.
">[7]</span></a></sup> (UW &amp; Fair) proposed a span-level pretraining approach by <strong>masking contiguous random spans</strong> rather than individual tokens as in BERT. It consistently surpass BERT and substantially outweights on span selection tasks involving question answering and coreference resolution. The NSP auxiliary objective is removed.</p>
<div class="note warning">
            <p>In comparison, </p><ul><li>The concurrent work <strong>ERNIE</strong><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., ... & Wu, H. (2019). [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223). arXiv preprint arXiv:1904.09223.">[8]</span></a></sup> (Baidu 2019) that masked linguistically-informed spans in Chinese, i.e. masking phrase and named entity, achieve improvements on Chinese NLP tasks.</li></ul>
          </div>
<h2 id="Span-masking"><a href="#Span-masking" class="headerlink" title="Span masking"></a>Span masking</h2><p>At each iteration, the spans length is samplled from a geometric distribution $\mathscr{l} \sim Geo(p) = (1-p)^{(k-1)} p$; the starting point of spans are uniformly random selected from the sequence. (In SpanBERT, p=0.2, and clip <script type="math/tex">l_\max = 10</script>.) </p>
<p>15% of tokens <strong>in span-level</strong> are masked: of which masking 80%, replacing 10% with noise, keeping the rest 10%.</p>
<p><img data-src="/notes/images/SpanBERT.png" alt="upload successful"></p>
<h2 id="Span-boundary-objective-SBO"><a href="#Span-boundary-objective-SBO" class="headerlink" title="Span boundary objective (SBO)"></a>Span boundary objective (SBO)</h2><p>Given a masked span <script type="math/tex">(x_s, \cdots, x_e) \in Y</script>, where (s,e) denotes the start and ending positions. Each token <script type="math/tex">x_i</script> in the span are represented using the encodings of the outside boundary tokens <script type="math/tex">x_{s-1}</script> and <script type="math/tex">x_{e+1}</script> (i.e., <script type="math/tex">x_4</script> and <script type="math/tex">x_9</script> in the figure) and the target positional embedding of target token <script type="math/tex">\mathbf{p}_i</script>, that is:</p>
<script type="math/tex; mode=display">\mathbf{y}_i = f( \mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_i)</script><p>where $f(\cdot)$ indicates the 2-layer FFNN with layer normalizations and Gelu activations.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{h} & = \text{LayerNorm} (\text{Gelu} (W_q \cdot [\mathbf{x}_{s-1}; \mathbf{x}_{e+1}; \mathbf{p}_{i}] )) \\
f(\cdot) & =  \text{LayerNorm} (\text{Gelu} (W_2 \cdot \mathbf{h}))
\end{align}</script><p>The representions of span tokens <script type="math/tex">\mathbf{y}_i</script> is used to predict <script type="math/tex">\mathbf{x})i</script> and compute the corss entropy loss like MLM objective in BERT.</p>
<h1 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h1><p>ALBERT<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). [Albert: A lite bert for self-supervised learning of language representations](https://arxiv.org/pdf/1909.11942). arXiv preprint arXiv:1909.11942.
">[9]</span></a></sup> (<strong>A</strong> <strong>L</strong>ite <strong>BERT</strong>) (Google 2019) adopted <strong>factorized embedding parameterization</strong> and <strong>cross-layer parameter sharing</strong> techiniques to reuduce the memory cost of BERT architecture.</p>
<h2 id="Factorized-embedding-parameterization"><a href="#Factorized-embedding-parameterization" class="headerlink" title="Factorized embedding parameterization"></a>Factorized embedding parameterization</h2><p>ALBERT decomposed the embedding parameters with higher dimension to smaller matrices, by firstly projecting the inputs into a lower dimensional embedding of size E, followed by the second projection to the hidden space. The embedding paprameters are reduced from $O(V \times H)$ to $O(V \times E + E \times H)$, which is obvious when $H \gg E$</p>
<h2 id="Cross-layer-parameter-sharing"><a href="#Cross-layer-parameter-sharing" class="headerlink" title="Cross-layer parameter sharing"></a>Cross-layer parameter sharing</h2><p>All parameters across layers on both self-attentions and FFNs are shared. It is empirically showed that the L2 distance and cosine similarity between the input and output are oscillating rather than converging, which is different than that in Deep Equilibrium Model (DEQ)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Deep Equilibrium Models](https://arxiv.org/pdf/1909.01377.pdf). arXiv 2019
">[10]</span></a></sup>.</p>
<h2 id="Sentence-order-prediction-SOP"><a href="#Sentence-order-prediction-SOP" class="headerlink" title="Sentence-order prediction (SOP)"></a>Sentence-order prediction (SOP)</h2><p>ALBERT use two consecutive setences as positive samples as in NSP, and <strong>swap the order of the same ajacent segments directly as the negative samples</strong>, consistently showing a better results for multi-sentence encoding tasks.</p>
<h1 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h1><p>ELECTRA<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ELECTRA: Pre-training Text Encoders
as Discriminators rather than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)
">[11]</span></a></sup> (<strong>E</strong>fficiently <strong>L</strong>earning an <strong>E</strong>ncoder that <strong>C</strong>lassifies <strong>T</strong>oken <strong>R</strong>eplacements <strong>A</strong>ccurately) (Standford NLP) proposed a more sample-efficient pre-training approach, <strong>replaced token detection</strong> to efficiently boost the pretraining efficiency, which solves the pretraining-finetuning discrepancy led by [MASK] symbols.</p>
<p><img data-src="/notes/images/ELECTRA-replaced-token-detection.png" alt="upload successful"></p>
<h2 id="Replaced-token-detection"><a href="#Replaced-token-detection" class="headerlink" title="Replaced token detection"></a>Replaced token detection</h2><ul>
<li>Rather than randomly masked tokens with the probability 15% as in BERT, <strong>replaced token detection</strong> replaces tokens with plausible alternatives that sampled from the output of a small generator network.</li>
<li>Then adopt a discriminator to predict whether each token was corrupted with a sampled replacement.</li>
</ul>
<p><img data-src='/notes/images/ELECTRA-approach.png' width='80%'/></p>
<p>ELECTRA trains two NNs, a generator $G$ and a discriminator $D$. Each one primarily consists of an encoder that maps a sequence on input tokens <script type="math/tex">\mathbf{x}= [x_1,\cdots,x_n]</script> into the contextual representation <script type="math/tex">h(\mathbf{x}) = [h_1, \cdots, h_n]</script>. </p>
<ol>
<li><p>The generator is used to to do Masked Language Model (MLM) as in BERT<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91). arXiv preprint arXiv:1810.04805.
">[6]</span></a></sup>. For the position $t$, the generator outputs the distribution of <script type="math/tex">\mathbf{x}_t</script> via a softmax layer:</p>
<script type="math/tex; mode=display">p_G (x_t \vert \mathbf{x}) = \frac{\exp (e(x_t)\top h_G(\mathbf{x})_t)}{\sum_{x^\prime} \exp(e(x^\prime)^\top h_G(\mathbf{x})_t)}</script><p>where $e$ is the word embeddings.</p>
</li>
<li><p>For the discriminator $\mathscr{D}$, it discriminates whether the token <script type="math/tex">x_t</script> at position $t$ is replaced.</p>
<script type="math/tex; mode=display">\mathscr{D}(\mathbf{x},t) = \sigma (w\top h_D(\mathbf{x})_t)</script></li>
</ol>
<div class="note success">
            <ul><li>MLM of BERT first randomly selects the  positions to mask <script type="math/tex">\mathbf{m} = [m_1, \cdots, m_k]</script>, wherein tokens at masked positions are replaced with a [MASK] token:<script type="math/tex; mode=display">\begin{align}m_i  &\sim \text{Uniform}\{1,n\} \\\mathbf{x}^\text{masked} &= \text{Replace}(\mathbf{x}, \mathbf{m}, \text{[MASK]})\end{align}</script></li><li>In contrast, the replaced token detection uses the generator G to learn the MLE of masked tokens whilst the discriminator D is applied to detect the fakeness.<script type="math/tex; mode=display">\begin{align}\color{red}{\hat{x}_i}  &\sim p_G(x_i \vert \mathbf{x}^\text{masked}) \; \text{for}\, i\in \mathbf{m} \\\mathbf{x}^\text{corrupted} &= \text{Replace}(\mathbf{x}, \mathbf{m}, \color{red}{\hat{\mathbf{x}}})\end{align}</script></li></ul>
          </div>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>The loss functions are:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_\text{MLM}(\mathbf{x}, \theta_G) &= \mathbb{E} \bigg( \sum_{i \in \mathbf{m}} - \log p_G (x_i \vert \mathbf{x}^\text{masked}) \bigg) \\
\mathcal{L}_\text{D} (\mathbf{x}, \theta_D) &= \mathbb{E} \bigg( \sum_{t=1}^n \mathbb{I} (x_t^\text{corrupt} = x_t) \log D(\mathbf{x}^\text{corrupt}, t) + \mathbb{I} (x_t^\text{corrupt} \neq x_t) \log ( 1- D\big(\mathbf{x}^\text{corrupt}, t\big) ) \bigg)
\end{align}</script><p>The combined loss is minimized:</p>
<script type="math/tex; mode=display">\min_{\theta_G, \theta_D} \sum_{\mathbb{x} \in \chi} \mathcal{L}_\text{MLM} (\mathbf{x}, \theta_G) + \lambda \mathcal{L}_\text{D}(\mathbf{x}, \theta_D)</script><p>where $\chi$ denotes the corpus.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><h4 id="Weight-sharing"><a href="#Weight-sharing" class="headerlink" title="Weight sharing"></a>Weight sharing</h4><ul>
<li>Share the embeddings (both token embeddings and position embeddings) of the generator and discriminator.</li>
<li>Weight tying strategy<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., & Wolf, L. (2016). [Using the output embedding to improve language models](https://arxiv.org/pdf/1608.05859). arXiv preprint arXiv:1608.05859.
">[12]</span></a></sup> -&gt; only tied embeddings.</li>
</ul>
<h4 id="Two-stage-training"><a href="#Two-stage-training" class="headerlink" title="Two-stage training"></a>Two-stage training</h4><ol>
<li>Train only the geenrator with <script type="math/tex">\mathcal{L}_\text{MLM}</script> for $n$ steps</li>
<li>Initialize the weights of the $D$ with $G$ and train $D$ with <script type="math/tex">\mathcal{L}_\text{Disc}</script> for $n$ steps, keeping the generators weight frozen.</li>
</ol>
<p>After pretraining, throw away the generator and fine-tune the discriminator on downstream tasks.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., &amp; Le, Q. V. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>. arXiv preprint arXiv:1906.08237.<a href="#fnref:1" rev="footnote"> </a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., &amp; Salakhutdinov, R. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.02860">Transformer-xl: Attentive language models beyond a fixed-length context</a>. arXiv preprint arXiv:1901.02860.<a href="#fnref:2" rev="footnote"> </a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shaw, P., Uszkoreit, J., &amp; Vaswani, A. (2018). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02155">Self-attention with relative position representations</a>. arXiv preprint arXiv:1803.02155.<a href="#fnref:3" rev="footnote"> </a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).<a href="#fnref:4" rev="footnote"> </a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... &amp; Stoyanov, V. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.11692">Roberta: A robustly optimized bert pretraining approach</a>. arXiv preprint arXiv:1907.11692.<a href="#fnref:5" rev="footnote"> </a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf%E3%80%91">Bert: Pre-training of deep bidirectional transformers for language understanding</a>. arXiv preprint arXiv:1810.04805.<a href="#fnref:6" rev="footnote"> </a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., &amp; Levy, O. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10529">Spanbert: Improving pre-training by representing and predicting spans</a>. arXiv preprint arXiv:1907.10529.<a href="#fnref:7" rev="footnote"> </a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., ... &amp; Wu, H. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.09223">ERNIE: Enhanced Representation through Knowledge Integration</a>. arXiv preprint arXiv:1904.09223.<a href="#fnref:8" rev="footnote"> </a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.11942">Albert: A lite bert for self-supervised learning of language representations</a>. arXiv preprint arXiv:1909.11942.<a href="#fnref:9" rev="footnote"> </a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.01377.pdf">Deep Equilibrium Models</a>. arXiv 2019<a href="#fnref:10" rev="footnote"> </a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders
as Discriminators rather than Generators</a><a href="#fnref:11" rev="footnote"> </a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, O., &amp; Wolf, L. (2016). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.05859">Using the output embedding to improve language models</a>. arXiv preprint arXiv:1608.05859.<a href="#fnref:12" rev="footnote"> </a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/zihangdai/xlnet/blob/master/data_utils.py">GitHub: XLNet</a><a href="#fnref:13" rev="footnote"> </a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/Pre-training/" rel="tag"># Pre-training</a>
              <a href="/notes/tags/NLP/" rel="tag"># NLP</a>
              <a href="/notes/tags/Language-model/" rel="tag"># Language model</a>
              <a href="/notes/tags/BERT/" rel="tag"># BERT</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2019/12/13/NN/Efficient-Softmax-Explained/" rel="prev" title="Efficient Softmax Explained">
      <i class="fa fa-chevron-left"></i> Efficient Softmax Explained
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2019/12/15/NN/Neural-Network-Tricks/" rel="next" title="Neural Network Tricks">
      Neural Network Tricks <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LM-pretraining-background"><span class="nav-number">1.</span> <span class="nav-text">LM pretraining background</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Autoregressive-Language-Modeling"><span class="nav-number">1.1.</span> <span class="nav-text">Autoregressive Language Modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Autoencoding-based-pretraining"><span class="nav-number">1.2.</span> <span class="nav-text">Autoencoding based pretraining</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#XLNet"><span class="nav-number">2.</span> <span class="nav-text">XLNet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Permutation-Language-Model"><span class="nav-number">2.1.</span> <span class="nav-text">Permutation Language Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementation"><span class="nav-number">2.1.1.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#XLNet-Implementation"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">XLNet Implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Huggingface-Implementation"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Huggingface Implementation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Two-stream-self-attention"><span class="nav-number">2.1.2.</span> <span class="nav-text">Two-stream self-attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-XL"><span class="nav-number">2.2.</span> <span class="nav-text">Transformer-XL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Relative-segment-encoding"><span class="nav-number">2.3.</span> <span class="nav-text">Relative segment encoding</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RoBERTa-%E2%80%9CBERT-is-undertrained%E2%80%9D"><span class="nav-number">3.</span> <span class="nav-text">RoBERTa: BERT is undertrained!</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SpanBERT"><span class="nav-number">4.</span> <span class="nav-text">SpanBERT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Span-masking"><span class="nav-number">4.1.</span> <span class="nav-text">Span masking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Span-boundary-objective-SBO"><span class="nav-number">4.2.</span> <span class="nav-text">Span boundary objective (SBO)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ALBERT"><span class="nav-number">5.</span> <span class="nav-text">ALBERT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Factorized-embedding-parameterization"><span class="nav-number">5.1.</span> <span class="nav-text">Factorized embedding parameterization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-layer-parameter-sharing"><span class="nav-number">5.2.</span> <span class="nav-text">Cross-layer parameter sharing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sentence-order-prediction-SOP"><span class="nav-number">5.3.</span> <span class="nav-text">Sentence-order prediction (SOP)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ELECTRA"><span class="nav-number">6.</span> <span class="nav-text">ELECTRA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Replaced-token-detection"><span class="nav-number">6.1.</span> <span class="nav-text">Replaced token detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-function"><span class="nav-number">6.1.1.</span> <span class="nav-text">Loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training"><span class="nav-number">6.1.2.</span> <span class="nav-text">Training</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Weight-sharing"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">Weight sharing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Two-stage-training"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">Two-stage training</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">7.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yekun Chai"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">Yekun Chai</p>
  <div class="site-description" itemprop="description">Language is not just words.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home  https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub  https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail  mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter  https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow  https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yekun Chai</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //copyright
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2019/12/14/NLP/BERTology-an-introduction/";
    this.page.identifier = "2019/12/14/NLP/BERTology-an-introduction/";
    this.page.title = "BERTology: An Introduction!";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
