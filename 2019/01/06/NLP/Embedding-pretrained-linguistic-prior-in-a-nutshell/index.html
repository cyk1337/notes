<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Pretraining on ImageNet followed by domain-specific fine-tuning has illustrated compelling improvements in computer vision research. Similarly, Natual Language Processing (NLP) tasks could borrow idea">
<meta property="og:type" content="article">
<meta property="og:title" content="Embedding Pretrained Linguistic Prior in a Nutshell">
<meta property="og:url" content="https://cyk1337.github.io/notes/2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/index.html">
<meta property="og:site_name" content="The Gradient">
<meta property="og:description" content="Pretraining on ImageNet followed by domain-specific fine-tuning has illustrated compelling improvements in computer vision research. Similarly, Natual Language Processing (NLP) tasks could borrow idea">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/embedding.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/NNLM.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/asr-intro.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/cbow.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/skip-gram.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/GloVe.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ULMFit.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ELMo.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ELMo-embedding.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/OpenAI-GPT.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/bert.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/bert-on-downstream-tasks.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/bert-extractor.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/bert-example.png">
<meta property="article:published_time" content="2019-01-06T14:40:00.000Z">
<meta property="article:modified_time" content="2019-01-06T14:40:00.000Z">
<meta property="article:author" content="cyk1337">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Survey">
<meta property="article:tag" content="Language model">
<meta property="article:tag" content="Word representation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/embedding.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Embedding Pretrained Linguistic Prior in a Nutshell | The Gradient</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The Gradient</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Language is not just words.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="cyk1337">
      <meta itemprop="description" content="What is now proved was once only imagined.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Gradient">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Embedding Pretrained Linguistic Prior in a Nutshell
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-01-06 22:40:00" itemprop="dateCreated datePublished" datetime="2019-01-06T22:40:00+08:00">2019-01-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/NLP/Language-model/" itemprop="url" rel="index"><span itemprop="name">Language model</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Pretraining on <em>ImageNet</em> followed by domain-specific fine-tuning has illustrated compelling improvements in computer vision research. Similarly, Natual Language Processing (NLP) tasks could borrow ideas from this. </p>
<p>Employing pretrained word representations or even langugage models to <strong>introduce linguistic prior knowledge</strong> has been common sense in amounts of NLP tasks with deep learning.</p>
<p><img data-src="/notes/images/embedding.png" alt="Pretrained word representation topology"></p>
<span id="more"></span>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Language-Model-LM"><a href="#Language-Model-LM" class="headerlink" title="Language Model (LM)"></a>Language Model (LM)</h2><div class="note primary">
            <p>Probability of a sequence of words.<br>   Goal: learn the joint probability function of sequences of words in a language      <script type="math/tex">\prod_{i=1}^N P(w_i | w_{1},..., w_{i-1})</script></p>
          </div>
<div class="note danger">
            <p>Challenge: <strong>the curse of dimensionality</strong></p>
          </div>
<h3 id="Discrete-n-gram"><a href="#Discrete-n-gram" class="headerlink" title="Discrete n-gram"></a>Discrete n-gram</h3><p>For a sentence S of length n: <script type="math/tex">w_1,...,w_m</script>:</p>
<script type="math/tex; mode=display">P(S) = P(w_1,...,w_n)  =  \prod_{i=1}^m P(w_i|w_0,...,w_{i-1}) 
\\ \approx \prod_{i=1}^n \underbrace{P(w_i|w_{i-1},...,w_{i-n+1})}_\textrm{Markov assumption}</script><div class="note success">
            <p><strong>Strong Markov assumption</strong>: for each word i, the probability <em>only</em> depends on previous n-1 words:</p><ul><li>zerogram: uniform distribution</li><li>unigram:  word frenquency</li><li>bigram: <script type="math/tex">x_i</script> depends only on <script type="math/tex">x_{i-1}</script></li><li>trigram: <script type="math/tex">x_i</script> depends only on <script type="math/tex">x_{i-2}, x_{i-1}</script></li></ul>
          </div>
<p>n-gram models:</p>
<script type="math/tex; mode=display">P(w_i|w_1,w_2,...,w_{i-1}) \approx P(w_i | w_{i-(n-1)},...,w_{i-1})</script><p>MLE by counting:</p>
<script type="math/tex; mode=display">P(w_i | w_{i-(n-1)},...,w_{i-1}) = \frac{ \text{count}(w_{i-(n-1)},...,w_{i-1}) }{ \text{count}(w_{i-(n-1)},...,w_{i}) }</script><div class="note danger">
            <p><strong>Problems</strong>: cound-based methods cannot deal with <strong>out-of-vocabulary</strong> (OOV, i.e. unseen words <UNK>)</p>
          </div>
<div class="note warning">
            <p><strong>Solution</strong>: smoothing (discounting)<br>Core idea: reserve part of probability mass for unseen events.</p>
          </div>
<p>Methods:</p>
<ul>
<li>Add-1 smoothing (Laplace smoothing) </li>
<li>Add-$\alpha$ smoothing</li>
<li>Stupid backoff</li>
<li>Interpolation</li>
<li>Kneser-Ney smoothing</li>
</ul>
<h3 id="Continuous-n-gram"><a href="#Continuous-n-gram" class="headerlink" title="Continuous n-gram"></a>Continuous n-gram</h3><div class="note primary">
            <p>Rather than discounting, rely on <code>similarity in internal representation</code> for estimating <code>unseen</code> events.</p>
          </div>
<div class="note info">
            <p><strong>Relationship between LM and word representation</strong> (personal thoughts): neural network(NN) is another way to do <code>matrix factorization but with non-linear transformation</code>. LMs aims to learn the <strong>joint distribution function of word sequences</strong>, which accumulates conditional probability word by word. Before passing the softmax for normalization, the <em>compact</em> projection layer hidden states could provide effective insights to tell the difference among vocabularies (after softmax normalization). As the subsidiary product of LMs, low dimensional projection states could mitigate the curse of dimensionality and serve for NLP transfer learning.</p>
          </div>
<p>The earliest idea using NN for LM is not (Bengio et al., 2003). Previous work e.g. (Miikkulainen and Dyer, 1991).</p>
<h4 id="NNLM-Bengio-et-al-2003"><a href="#NNLM-Bengio-et-al-2003" class="headerlink" title="NNLM (Bengio et al., 2003)"></a>NNLM (Bengio et al., 2003)</h4><p>Employed NN in <strong>statistical n-gram LM</strong>.</p>
<p>Learns simultaneously </p>
<ol>
<li>Distributed representation (see following section for details) for each word;</li>
<li>Joint probability function <script type="math/tex">f(w_t,w_{t-1},...,w_{t-n+1})</script> for word sequences.</li>
</ol>
<div class="note primary">
            <ul><li>Input: n-1 context words</li><li>Output: probability distribution of the next word</li><li>Model: <code>a linear projection layer + a non-linear hidden layer</code>. 1 non-linear hidden layer beyond the word feature mapping (i.e. embedding lookup). Optionally, <em>direct feed lookup word vectors to final layer</em> (<code>Implicitly ResNets!</code>). When the hidden states W is set to 0, there is no direct connection.</li><li>Parameter set: <script type="math/tex">\Theta = (C, \omega)</script>, where C is word vector mapping, $\omega$ denotes parameters.</li><li>Loss function:<br><script type="math/tex">L=\frac{1}{T} \sum_t log f(w_t,w_{t-1},...,w_{t-n+1};\Theta) + R(\Theta)</script>, where $R(\Theta)$ is a regularization term.</li></ul>
          </div>
<p>As below figure, NNLMs decompose the n-gram joint probabilty function <script type="math/tex">f(w_t,...,w_{t-n+1}) = \hat{P}(w_t|w_1^{t-1})</script>:</p>
<ol>
<li>Mapping matrix C with dimension $ |V| \times m $, represents the distributed feature vector associated with each word in the vocabulary (<strong>embedding matrix</strong>, a.k.a embedding loopup table). </li>
<li>probability function over words:<script type="math/tex; mode=display">f(w_t,w_{t-1},...,w_{t-n+1}) = g(i, C(w_{t-1}),...,C(w_{t-n+1}))</script> where C(i) is the i-th word feature vector.</li>
</ol>
<p><img data-src="/notes/images/NNLM.png" width='60%'></p>
<div class="note danger">
            <p>Solved issue: <code>OOV</code></p><p>Drawbacks: </p><ol><li><code>Limited context length (fixed n)</code> that needs to be specified <em>ad hoc</em> before training, only consider previous n-1 words;</li><li>Simple NN architecture;</li><li>Word feature representation (embedding) cannot deal with <code>polysemy</code>, which assign each word a single point in a continuous semantic space. Proposed future solution: assign each word sense with different points. </li></ol>
          </div>
<h4 id="RNNLM-Mikolov-et-al-2010"><a href="#RNNLM-Mikolov-et-al-2010" class="headerlink" title="RNNLM (Mikolov et al., 2010)"></a>RNNLM (Mikolov et al., 2010)</h4><div class="note success">
            <p>NNLMs utilize <code>fixed context length</code> which needs to be pre-specified. RNNLMs encode temporal information implicitly for <code>contexts with arbitrary lengths</code>.</p>
          </div>
<ul>
<li>Motivation: condition on arbitrarily long context $\rightarrow$ no Markov assumption</li>
<li>Input: 1-of-K encoding over the vocabulary with size |V|</li>
<li>Read in one word at a time, and update hidden state incrementally;</li>
<li>Hidden state is initialized as empty vectors at time step 0;</li>
<li>Parameters<ul>
<li>Embedding matrix $E$</li>
<li>Feedforward matrices <script type="math/tex">W_1</script>, <script type="math/tex">W_2</script></li>
<li>Recurrent maxtrix $U$</li>
</ul>
</li>
<li>Training: BPTT</li>
</ul>
<p>Simple RNNs:</p>
<script type="math/tex; mode=display">h_i=\left\{
                \begin{array}{ll}
                  0, \quad if \ i = 0\\
                  \tanh( W_1Ex_i + Uh_{i-1} ), if\  i > 0\quad
                \end{array}
              \right.</script><script type="math/tex; mode=display">y_i = \text{softmax} (W_2 h_{i-1})</script><p>Basic LSTM RNNs:</p>
<script type="math/tex; mode=display">\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  = \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c)</script><script type="math/tex; mode=display">\mathbf{c}^c_j = \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j}</script><script type="math/tex; mode=display">\mathbf{h}_j^c = \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)</script><p>where $\mathbf{i}^c_j$, $\mathbf{f}^c_j$, $\mathbf{o}^c_j$ denotes a set of input, forget and output gates, respectively. $\mathbf{c}^c_j$ denotes the char cell vector, $\mathbf{h}^c_j$ denotes the hidden vector on each char $c_j$, $\mathbf{W}^{c^T}$, $\mathbf{b}^c$ are parameters.</p>
<hr>
<h3 id="Intrinsic-evaluation-of-LM"><a href="#Intrinsic-evaluation-of-LM" class="headerlink" title="Intrinsic evaluation of LM"></a>Intrinsic evaluation of LM</h3><p><strong>Perplexity (PP)</strong>:<br><div class="note warning">
            <p>Intuitional interpretation: <strong>weighted average branching factor</strong></p><script type="math/tex; mode=display">\begin{aligned}PP (S) &{}= P(w_1,...,w_N) = P(w_1,w_2,...,w_N)^{-\frac{1}{N}} \\        &{}=  \sqrt[N]{\frac{1}{P(w_1w_2...w_N)}} \\         &{}= \sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_1...w_{i-1})}}\end{aligned}</script>
          </div></p>
<p>For bigram models:</p>
<script type="math/tex; mode=display">PP (S) = \sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_{i-1})}}</script><hr>
<h2 id="LM-application"><a href="#LM-application" class="headerlink" title="LM application"></a>LM application</h2><div class="note warning">
            <p>LMs serve as a component in various NLP tasks, such as ASR, MT. </p>
          </div>
<h3 id="Automatic-Speech-Recognition"><a href="#Automatic-Speech-Recognition" class="headerlink" title="Automatic Speech Recognition"></a>Automatic Speech Recognition</h3><p><img data-src='/notes/images/asr-intro.png' width="60%" /></p>
<center><small>Image source: Steve Renals, Edinburgh Informatics (INFR11033) </small></center>

<h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>Let T denote a target sentence of length m :<script type="math/tex">x_1, ..., x_m</script>, S denote a source sentence n: <script type="math/tex">y_1,...,y_n</script>. The machine translation can be expressed as:</p>
<script type="math/tex; mode=display">T^* = \arg\max_T P(T|S) \\=  \arg\max_T P(y_1,...,y_n|x_1,...,x_m) \\= \prod_{i=1}^n P(y_i | y_{1}, ...,y_{i-1}, x_1,...,x_m)</script><p>With Bayes’ theorem, we can get:</p>
<script type="math/tex; mode=display">T^* = \arg\max_T P(S|T) \underbrace{P(T)}_{\text{LM}}</script><h2 id="Word-representation"><a href="#Word-representation" class="headerlink" title="Word representation"></a>Word representation</h2><h3 id="Distributional-representation"><a href="#Distributional-representation" class="headerlink" title="Distributional representation"></a>Distributional representation</h3><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>distributional hypothesis</strong>: linguistic items with similar distributions have similar meanings  </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<div class="note warning">
            <ul><li>Statistical (count-based) method;</li><li><code>high-dimensional</code> vector representation obtained from the rows of the <strong>word-context co-occurrence matrix</strong>, whose dimension size equals to the <strong>vocabulary size</strong> of the corpus.</li></ul>
          </div>
<p>Approaches:</p>
<ul>
<li>One-hot encoding (a.k.a 1-of-K encoding)</li>
<li>TF-IDF (Term Frequency - Inverse Document Frequency)<br>N $\rightarrow$ # of ducuments, $df_t$ $\rightarrow$ Term frequency for term $t$, $d$ $\rightarrow$ document.<script type="math/tex; mode=display">\text{tf}_{t,d} = 1 + \log_{10} \text{count} (t,d) \quad \text{if count} (t,d)>0, \text{ else } 0</script><script type="math/tex; mode=display">\text{idf}_{t} = \log (\frac{N}{\text{df}_t})</script><script type="math/tex; mode=display">w_{t,d} = \text{tf}_{t,d} \times \text{idf}_{t}</script></li>
<li>PPMI<br>PMI association between a target word w and a context word c is:<script type="math/tex; mode=display">\text{PMI}(w,c) = \log_2 \frac{P(w,c)}{P(w)P(c)}</script></li>
</ul>
<script type="math/tex; mode=display">\text{PPMI} = \max(PMI(w,c), 0)</script><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>PMI</strong>: The numerator tells us how often we observed the two words together (assuming we compute probability by using the MLE). The denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently, so their probabilities could just be multiplied. Thus, the ratio gives us an estimate of how much more the target and feature co-occur than we expect by chance.</p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<!-- 
**Latent Dirichlet allocation (LDA)**
**Hyperspace Analogue to Language (HAL)**
1. Syntagmatic models
"combinatorial relations between words (i.e., syntagmatic relations), which relate words that co-occur within the
same text region (e.g., sentence, paragraph or document)." ()
a. Build words-by-documents co-occurrence matrix
b. low-rank decomposition
2. Paradigmatic models concern substitutional
<blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p> relations between words (i.e., paradigmatic relations), which relate words that occur in the same<br>context but may not at the same time </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>  -->
<h3 id="Distributed-static-word-representation"><a href="#Distributed-static-word-representation" class="headerlink" title="Distributed (static) word representation"></a>Distributed (static) word representation</h3><p><strong>Distributed representations of words in a vector space</strong> have become an effective way to capturing <strong>fine-grained linguistic regularities</strong>.</p>
<div class="note success">
            <p><strong>low-dimensional</strong>, dense, compact vector representation.</p><ul><li><strong>NN-based</strong> model (such as <strong>word2vec</strong>, Collobert and Weston embeddings, HLBL embeddings) ,</li><li><strong>Matrix factorization based</strong> model on the <strong>word-context co-occurrence matrix</strong> (such as the <em>Glove</em> from Stanford using direct matrix factorization, the <strong>Latent Semantic Analysis</strong> using SVD factorization).</li></ul>
          </div>
<div class="note default">
            <p>You shall know a word by the company it keeps (John Rupert Firth, 1957).</p>
          </div>
<blockquote>
<p>Learning word representations using Language modeling. (Dr. Adam Lopez’s 2018 lecture)</p>
</blockquote>
<div class="note danger">
            <p>Inherent limitation of word representations: </p><ul><li><em>indifference to word order and inability to represent idiomatic phrases</em>.</li><li>cannot tackle <em>polysemy</em></li></ul>
          </div>
<h4 id="Word2Vec-Mikolov-2013-Google"><a href="#Word2Vec-Mikolov-2013-Google" class="headerlink" title="Word2Vec (Mikolov 2013; Google)"></a>Word2Vec (Mikolov 2013; Google)</h4><div class="note danger">
            <p><strong>Issue</strong><br>NLP systems <em>treat words as atomic units</em>: map words to the <strong>indices in the vocabulary</strong>, not considering similarity between words. </p><ul><li>Pros: simplicity, rebustness, simple models</li><li>Cons: require <em>huge amount of data</em>, which is unrealistic in some occasions, e.g. ASR and NMT.</li></ul>
          </div>
<h5 id="Continuous-Bag-of-Words-CBOW"><a href="#Continuous-Bag-of-Words-CBOW" class="headerlink" title="Continuous Bag of Words (CBOW)"></a><strong>Continuous Bag of Words (CBOW)</strong></h5><div class="note default">
            <ul><li>Intuition: predicting the current word based on its context.</li><li>Archtecture: <strong>linear projection layer</strong>. Feedforward NNLM remove the non-linear hidden layer. </li><li>All words get projected into the same position (vectors are averaged). Called bag-of-word model since the <em>word order in the history doesnot influence the projection</em>.</li><li>Same as NNLMs, weights for different positions are shared. </li><li>Computationally much more efficient than NNLMs.</li></ul>
          </div>
<p><img data-src="/notes/images/cbow.png" width='60%'></p>
<center>CBOW <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Weng L. (2017, Oct 15). Learning Word Embedding [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html
">[9]</span></a></sup></center>

<h5 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a><strong>Skip-gram</strong></h5><div class="note primary">
            <ul><li>Intuition: “maximize classification of a word based on another word in the same sentence”, i.e. input each current word to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.</li><li><code>Objective</code>: maximize the <strong>average log probability</strong>, with context size c:<script type="math/tex; mode=display">\frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j}|w_t)</script></li><li><em>Simple vector addition</em> can often produce meaningful results. e.g. vec(“Russia”) + vec(“river”) is close to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). </li><li><strong>Find word representations useful for predicting surrounding words</strong> in a sentence or documentation</li></ul>
          </div>
<p><img data-src="/notes/images/skip-gram.png" width='60%'></p>
<center>SkipGram <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Weng L. (2017, Oct 15). Learning Word Embedding [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html
">[9]</span></a></sup></center>

<ul>
<li><strong>Issue 1</strong>: Inability to represent idiomatic phrases that are not compositions of the individual words. </li>
<li><p><strong>Solution</strong>: Find out the phrases and <em>treat the phrases as individual tokens</em> during training. Typical analogy pair: vec(“Montreal Canadiens”) - vec(“Montreal”) + vec(“Toronto”) is vec(“Toronto Maple Leafs”).</p>
</li>
<li><p><strong>Issue 2</strong>: very large dimension in softmax layer (size equals to vocabulary size |V|)</p>
</li>
<li><strong>Solution</strong>:<br>Hierarchical softmax (<a target="_blank" rel="noopener" href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf">Morin and Bengio, 2005</a>),<br>Negative sampling (NEG) (<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al. 2013</a>)</li>
</ul>
<div class="note danger">
            <p>Basic softmax (impractical due to computing <script type="math/tex">\nabla log p(w_O|w_I)</script> cost $\propto$ W (~ $10^5 - 10^7$ terms) ): </p><script type="math/tex; mode=display">p(w_O|w_I) = \frac{\exp( {v'}_{WO}^T v_{WI} )}{\sum_{w=1}^W \exp({v'}_{W}^T v_{WI})}</script><p>where $v_w$ and ${v’}_w$ are input and output vector representations of w, W is the vocabulary size. </p>
          </div>
<div class="note success">
            <ul><li><p>Solution 1 $\rightarrow$ <code>Hierarchical softmax</code><br>Instead of computing vocabulary size output nodes in NNs, only need to evaluate ~ <script type="math/tex">log_2(W)</script> nodes.<br>(<strong>Binary Huffman tree</strong> structure)</p></li><li><p>Solution 2 $\rightarrow$ <code>Negative sampling</code>: Noise Contrastive Estimation (NCE)<br>Differentiate data from noise by means of <code>logistic regression</code> classifier (<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Gutmann and Hyvärinen, 2010</a>).</p></li></ul>
          </div>
<p> Distributed representations capture <code>syntactic</code> and <code>semantic</code> information.</p>
<blockquote>
<p>Additional word2vec limitations:<br>    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">closed vocabulary assumption</span><br><span class="line">cannot exploit functional relationships in learning </span><br></pre></td></tr></table></figure></p>
</blockquote>
<!--
#### Collobert and Weston embeddings
#### HLBL embeddings -->
<h4 id="FastText-Mikolov-et-al-2017-Facebook"><a href="#FastText-Mikolov-et-al-2017-Facebook" class="headerlink" title="FastText (Mikolov et al. 2017; Facebook)"></a>FastText (Mikolov et al. 2017; Facebook)</h4><div class="note danger">
            <p>Issue: Popular models ignores the mophology of words, by assigning distinct vector to each word. Previous popular models <code>ignore the internal structure of words</code>, which is an important limitation for <strong>morphologically rich languages</strong>, such as Turkish, Finnish.</p>
          </div>
<div class="note success">
            <p>Bag-of-words $\rightarrow$ <code>Bag of features</code></p><p>Fasttext solution: employ <code>the sum of bag of character n-grams</code> as well as itself for each word, as an extension of skip-gram models. Taking into account <code>subword information</code>.</p>
          </div>
<p>Let <code>&lt;</code> and <code>&gt;</code> denote the beginning and ending of tokens to <em>distinguish prefixes and suffixes</em> from other character sequences. Taking <code>&lt;where&gt;</code> for example, we use char trigram(n=3), we can get:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">char trigram: &lt;wh, whe, her, ere, re&gt;</span><br><span class="line">itself: &lt;where&gt;</span><br></pre></td></tr></table></figure>
<p>Fasttext represents a word by the <strong>sum of the vector representations of its n-grams</strong>.<br>Let vector representation $\mathbf{z}_g$ to each n-gram $g$ ($g \in G$) The scoring function:</p>
<script type="math/tex; mode=display">s(w,c) = \sum_{g \in \mathbf{G}} \mathbf{z}_g^T \mathbf{v}_c</script><!--#### LSA (Latent Semantic Analysis)
- TODO
#### LDA (Latent Dirichlet Allocation)
- TODO
Cons: computationally very expensive on large data sets
-->
<h4 id="GloVe-Pennington-2014-Stanford"><a href="#GloVe-Pennington-2014-Stanford" class="headerlink" title="GloVe (Pennington 2014; Stanford)"></a>GloVe (Pennington 2014; Stanford)</h4><p>GloVe<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Pennington, J., Socher, R. & Manning, C. D. (2014). [Glove: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162). EMNLP 2014.
">[8]</span></a></sup> leverages <code>statistical information</code> by training only on the nonzero elements in a <code>word-word co-occurrence matrix</code>.</p>
<div class="note info">
            <p>Define the co-occurence probability as:</p><script type="math/tex; mode=display">p_{\text{co}}(w_k \vert w_i) = \frac{C(w_i, w_k)}{C(w_i)}</script><p><strong>Intuition</strong>: the word meanings are captured by the <code>ratios of co-occurrence probabilities</code> rather than the probabilities themselves. The global vector models the relationship between words i,j towards the thrid context word k:</p><script type="math/tex; mode=display">F(w_i, w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script><p>For words <em>k</em> like water or fashion, that are either related to both <em>ice</em> and <em>steam</em>, or to neither, the <strong>ratio should be close to one</strong>. </p><ul><li>build <code>word-word co-occurrence matrix</code></li><li>do <code>global matrix factorization</code></li></ul>
          </div>
<p><img data-src="/notes/images/GloVe.png" width='60%'></p>
<hr>
<h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><ul>
<li><p><strong>polysemy</strong><br>Frozen representations, can not express <strong>polysemy</strong>.</p>
</li>
<li><p>For languages where tokens are not delimited, such as Chinese and Japanese, NLP pipelines require <code>word segmentation</code> ahead. As we know, error generated by upstream tasks would amplified during the following propagation process. Hence, the performance of Chinese word segmentation also counts.</p>
</li>
</ul>
<h1 id="Pretraining-dynamic-word-representation"><a href="#Pretraining-dynamic-word-representation" class="headerlink" title="Pretraining (dynamic word representation)"></a>Pretraining (dynamic word representation)</h1><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>NLP’s ImageNet moment has arrived</strong><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ruder S. (2018, Jul 08) NLP's ImageNet moment has arrived [Blog post]. Retrieved from https://thegradient.pub/nlp-imagenet/
">[10]</span></a></sup><br>“At the core of the recent advances of ULMFiT, ELMo, and the OpenAI transformer is one key paradigm shift: going from just <strong>initializing the first layer</strong> of our models to <strong>pretraining the entire model with hierarchical representations</strong>. If learning word vectors is like only learning edges, these approaches are like learning the full hierarchy of features, from edges to shapes to high-level semantic concepts.”</p>
<p><strong>CV</strong>: “Interestingly, pretraining <em>entire models</em> to learn both low and high level features has been practiced for years by the computer vision (CV) community.” </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<h2 id="Feature-based-pretraining-frozen-representation"><a href="#Feature-based-pretraining-frozen-representation" class="headerlink" title="Feature-based pretraining (frozen representation)"></a>Feature-based pretraining (frozen representation)</h2><h3 id="ULMFiT"><a href="#ULMFiT" class="headerlink" title="ULMFiT"></a>ULMFiT</h3><p><strong>ULMFiT</strong>: Universal Language Model Fine-tuning</p>
<p><strong>Problem</strong>: LMs overfit to small datasets and suffered <code>catastrophic forgetting</code> when fine-tuned with a classifier.</p>
<p><strong>Solution</strong></p>
<ul>
<li>Inductive transfer learning.</li>
<li>Model: AWD-LSTM <sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182). arXiv preprint arXiv:1708.02182 .
">[19]</span></a></sup>, a regular LSTM (without attention, shot-cut connections)</li>
</ul>
<p><strong>Three stages</strong>:</p>
<ul>
<li>General domain LM pretraining<br>To capture the general features of the language in different layers;</li>
<li>Target task LM fine-tuning<ul>
<li>Trick: <em>discriminative fine-tuning</em>, <em>slanted triangular learning rates</em></li>
</ul>
</li>
<li>Target task classifier fine-tuning <ul>
<li>Trick: <em>concat pooling</em>, <em>gradual unfreezing</em></li>
</ul>
</li>
</ul>
<p><strong>Sloved issue</strong></p>
<blockquote>
<p>prevent catastrophic forgetting and enable robust transfer learning.</p>
</blockquote>
<p><img data-src="/notes/images/ULMFit.png" width='100%'></p>
<h3 id="ELMo-NAACL-2018-AllenAI"><a href="#ELMo-NAACL-2018-AllenAI" class="headerlink" title="ELMo (NAACL 2018, AllenAI)"></a>ELMo (NAACL 2018, AllenAI)</h3><h4 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h4><p>Some word representations are <code>context-independent</code>, only model complex charateristics of word use (e.g. syntax and semantics), ignoring how these uses vary across linguistic context (i.e. polysemy).</p>
<p><strong>Previous improvement</strong>:</p>
<ul>
<li>Enriching with <strong>subword information</strong> <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wieting, J., Bansal, M., Gimpel, K., & Livescu, K. (2016). [Charagram: Embedding words and sentences via character n-grams](https://arxiv.org/pdf/1607.02789). arXiv preprint arXiv:1607.02789.
">[15]</span></a></sup> <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T. (2017). [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010). Transactions of the Association for Computational Linguistics, 5, 135--146. 
">[7]</span></a></sup></li>
<li>Learning <strong>separate vectors for each word sense</strong> <sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Neelakantan, A., Shankar, J., Passos, A., & McCallum, A. (2015). [Efficient non-parametric estimation of multiple embeddings per word in vector space](https://arxiv.org/pdf/1504.06654). arXiv preprint arXiv:1504.06654.
">[16]</span></a></sup> (as suggested in the conclusion section in the NNLM paper<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). [A neural probabilistic language model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). Journal of machine learning research, 3(Feb), 1137-1155.
">[1]</span></a></sup>)</li>
</ul>
<p><strong>Solved issue</strong>:</p>
<ul>
<li>Seamlessly incorporate <code>multi-sense information</code> into downstream tasks <code>without explicitly training to predefined sense classes</code>.</li>
</ul>
<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><ul>
<li><strong>ELMo</strong> (Embeddings from Language Models) models <strong>polysemy</strong> by extracting <code>context-sensitive features</code>.</li>
<li>Elmo representations are deep $\rightarrow$ <strong>a function of all of the internal layers of the biLM</strong>. </li>
<li>Also incorporate subword information, using char ConvNets in the input and output.</li>
<li>Learn a linear combination of the vectors above each input word. This manner allows for very rich word representations.</li>
<li>Computing on top of two-layer biLMs with <strong>char ConvNets</strong>, as a linear function of internal network states.</li>
<li>After pretrainining the biLM with unlabeled data, ELMo <strong>fixes the weights</strong> and add additional task-specific model.</li>
</ul>
<p><strong>High-level</strong> LSTM states capture <code>context-dependent aspects of word meaning</code> (perform well on WSD tasks), while <strong>lower-level</strong> states model aspects of <code>syntax</code> (POS tagging). Simultaneously exposing <em>all of internal states</em> is highly beneficial, <em>allowing the learned models select the types of semi-supervision that are most useful for each end task</em>.<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365). arXiv preprint arXiv:1802.05365.
">[11]</span></a></sup></p>
<p><strong>BiLM</strong>: Given a sequence of N tokens <script type="math/tex">(t_1, t_2,..., t_N)</script></p>
<ul>
<li>Forward LM models the probability of tokens <script type="math/tex">t_k</script> given the history <script type="math/tex">(t_1,...,t_{k-1})</script>:<script type="math/tex; mode=display">p(t_1, t_2,...,t_N) = \prod_{k=1}^N p(t_k \vert t_1,t_2,...,t_{k-1})</script></li>
<li>Backward LM predicts the previous token given the future context:<script type="math/tex; mode=display">p(t_1,t_2,...,t_N) = \prod_{k=1}^N p(t_k \vert t_{k+1},t_{k+2},...,t_N)</script></li>
<li>biLM combines both of above, by jointly maxmizing the log likelihood of the forward and backward directions:<script type="math/tex; mode=display">\sum_{k=1}^N ( \log p(t_k|t_1,...,t_{k-1}; \Theta_x,  \overrightarrow{\Theta}_{LSTM}, \Theta_s ) + \log p(t_k|t_{k+1},...,t_N; \Theta_x,  \overleftarrow{\Theta}_{LSTM}, \Theta_s )  )</script>where <script type="math/tex">\Theta_x \rightarrow</script> token representation, $\Theta_s \rightarrow$ softmax layer.</li>
</ul>
<h4 id="ELMo-representation"><a href="#ELMo-representation" class="headerlink" title="ELMo representation"></a>ELMo representation</h4><div class="note success">
            <p>ELMo is a <strong>task-specific combination of the intermediate layer representations</strong> in the biLM.</p><p>For each token $t_k$, a L-layer biLM computes a set of 2L+1 representations:</p><script type="math/tex; mode=display">R_k = \{ \mathbf{x}_K^{LM},  \overrightarrow{h}_{k,j}^{LM},  \overleftarrow{h}_{k,j}^{LM} \vert j = 1,...,L \}= \{  \overrightarrow{h}_{k,j}^{LM} \vert j=0,...,L \}</script><p>where <script type="math/tex">\overrightarrow{h}_{k,0}^{LM}</script> is the token layer (j=0) and <script type="math/tex">\overrightarrow{h}_{k,j}^{LM} = [ \overrightarrow{h}_{k,j}^{LM} ; \overleftarrow{h}_{k,j}^{LM} ]</script> for each bi-LSTM layer.</p><p>ELMo collapses all alyers in representation set $\mathbf{R}$ into a single  vector <script type="math/tex">\mathbf{ELMo}_k = E(R_k; \Theta_e)</script>. </p><p><img data-src="/notes/images/ELMo.png" width='80%'></p><blockquote><p>Here previous work like <strong>TagLM</strong><sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Peters, M. E., Ammar, W., Bhagavatula, C., & Power, R. (2017). [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/pdf/1705.00108). arXiv preprint arXiv:1705.00108.">[17]</span></a></sup>, <strong>CoVe</strong><sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bryan McCann, James Bradbury, Caiming Xiong, andRichard Socher. 2017. [Learned in translation: Contextualized word vectors](https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf). In NIPS 2017.">[18]</span></a></sup> just selects the top layer.</p></blockquote><p>ELMo computes a task-specific weighting of all BiLM layer representations:</p><script type="math/tex; mode=display">ELMo_{o_k}^{task} = E(R_k ; \Theta^{task}) = \gamma^{task} \sum_{j=0}^L s_j^{task} \mathbf{h}_{k,j}^{LM}</script><p>where s_{task} are <strong>softmax-normalized weight</strong>, and scalar param $y^{task}$ allows the task model to scale the entire ELMo vector.</p>
          </div>
<p><img data-src="/notes/images/ELMo-embedding.png" width='80%'></p>
<center>ELMo embedding<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Alammar J. (2018, Dec 3). The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [Blog post]. Retrieved from https://jalammar.github.io/illustrated-bert/
">[23]</span></a></sup></center>

<h4 id="How-to-utilize-ELMo-into-downstream-supervised-tasks"><a href="#How-to-utilize-ELMo-into-downstream-supervised-tasks" class="headerlink" title="How to utilize ELMo into downstream supervised tasks?"></a>How to utilize ELMo into downstream supervised tasks?</h4><p><strong>Given</strong>: </p>
<ul>
<li>pretrained biLM with residual conection between LSTM layers</li>
<li>a supervised task-specific model</li>
</ul>
<p>Let $\mathbf{x}_k$ denote context-independent token representation (traditional embedding, like w2v, or compositional char cnn embeddings)<br><div class="note warning">
            <p><strong>How to use EMLo?</strong></p><ul><li>Freeze the biLM weights, run the biLM, and record all the layer representations for each word; </li><li>End task model learn previous linear weights $s^{task}$<ul><li>Usage: concat ELMo vector $\mathbf{ELMo}_k^{task}$ with $\mathbf{x}_k$, and  feed concatenated $ [\mathbf{x}_k, \mathbf{ELMo}_k^{task}] $ into the task-specific model.</li><li>Partially empirically practical on SNLI, SQuAD, replace the output $\mathbf{h}_k$ with $ [\mathbf{h}_k, \mathbf{ELMo}_k^{task}]$, followed by  one more domain-specific linear layer.</li></ul></li></ul>
          </div></p>
<h2 id="Fine-tuning-pretraining"><a href="#Fine-tuning-pretraining" class="headerlink" title="Fine-tuning pretraining"></a>Fine-tuning pretraining</h2><h3 id="OpenAI-Transformer-GPT-generative-pre-training"><a href="#OpenAI-Transformer-GPT-generative-pre-training" class="headerlink" title="OpenAI Transformer GPT (generative pre-training)"></a>OpenAI Transformer GPT (generative pre-training)</h3><p><strong>Problems</strong>: NLP with NN suffers from a dearth of annotated resources.</p>
<p><strong>model</strong>: </p>
<ul>
<li>Left-To-Right google Transformer<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). In Advances in Neural Information Processing Systems (pp. 5998-6008).
">[20]</span></a></sup> </li>
<li>Use BPE vocabulary<sup id="fnref:21"><a href="#fn:21" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="R. Sennrich, B. Haddow, and A. Birch. [Neural machine translation of rare words with subword units](https://arxiv.org/pdf/1508.07909). arXiv preprint arXiv:1508.07909, 2015.
">[21]</span></a></sup></li>
<li>Activation function: Gaussian Error Linear Unit (GELU) <sup id="fnref:22"><a href="#fn:22" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="D. Hendrycks and K. Gimpel. [Bridging nonlinearities and stochastic regularizers with gaussian error linear units](https://arxiv.org/pdf/1606.08415). arXiv preprint arXiv:1606.08415, 2016.
">[22]</span></a></sup></li>
</ul>
<p>Two stage:<br><strong>1) Unsupervised training on unannotated data with LM objective</strong></p>
<ul>
<li>Aim: <strong>find a good initialization point</strong>.</li>
<li>LM objective:<script type="math/tex; mode=display">L_1 (U) = \sum_i \log P(u_i \vert u_{i-k},...,u_{i-1};\Theta)</script>where k is the context window size</li>
<li>LM model: a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers:<script type="math/tex; mode=display">h_0 = UW_e + W_p</script><script type="math/tex; mode=display">h_l = \text{transformer block}(h_{l-1}) \quad  \forall \quad i \in [1,n])</script><script type="math/tex; mode=display">P(u) = \text{softmax} (h_nW_e^T)</script>where <script type="math/tex">U= (u_{-k},...,u_{-1})</script> is the context vector of tokens, n is the # of layers, $W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.</li>
</ul>
<p><strong>2) Supervised training on target tasks</strong></p>
<p>Given a labeled dataset C of input tokens <script type="math/tex">x^1,...,x^m</script> along with a label y, and pretrained LM.</p>
<p>Feed the input through our pretrained LM to get the final transformer block’s activation <script type="math/tex">h_l^m</script>. Then pass them to an added linear output layer with parameter <script type="math/tex">W_y</script> to predict y:</p>
<script type="math/tex; mode=display">P(y|x^1,...,x^m) = \text{softmax} (h_l^m W_y)</script><p>The objective is:</p>
<script type="math/tex; mode=display">L_2 (C) = \sum_{x,y} \log P(y \vert x^1,...,x^m)</script><blockquote>
<p>Found including LM as an auxiliary objective can <em>improve generalization </em>of supervised model and <em>accelerate convergence</em>. Hence, the objective is:</p>
<script type="math/tex; mode=display">L_3(C) = L_2(C) + \lambda \times L_1(C)</script></blockquote>
<p><img data-src="/notes/images/OpenAI-GPT.png" alt="OpenAI-GPT"></p>
<p>As the figure shows, Transformer GPT model could be applied in different discriminative NLP tasks. For tasks that contains more than 1 sentence, a delimiter token ($) is added in between.</p>
<ul>
<li>Classification: directly feed text</li>
<li>RTE: concat premise $p$ and hypothesis $h$ with a delimiter token(dollar) in between: [p; $ ; h]</li>
<li>Similarity: no inherent ordering between sentences. We concat both possible sentence orderings (with a delimiter token $ in between)</li>
<li>QA and Commonsense Reasoning: given context document $z$, a question $q$, and a set of possible answer <script type="math/tex">a_k</script>, concatenate the document context and question with each possible answer, adding delimiter in between: <script type="math/tex">[z;q;\$;a_k]</script>.</li>
</ul>
<h3 id="BERT-Google-2018"><a href="#BERT-Google-2018" class="headerlink" title="BERT (Google 2018)"></a>BERT (Google 2018)</h3><p>BERT: <strong>Bidirectional Encoder Representations from Transformers</strong></p>
<div class="note success">
            <ul><li>Model architecture: multi-layer bi-directional Transformer encoder</li><li>Activation functions: GElu (same as OpenAI GPT)</li><li>Most important improvements: <code>MLM</code> pretraining!</li></ul>
          </div>
<h4 id="BERT’s-pretraining-on-2-tasks"><a href="#BERT’s-pretraining-on-2-tasks" class="headerlink" title="BERT’s pretraining on 2 tasks"></a>BERT’s pretraining on 2 tasks</h4><h5 id="Task-1-Masked-LM-MLM"><a href="#Task-1-Masked-LM-MLM" class="headerlink" title="Task#1: Masked LM (MLM)"></a>Task#1: <strong>Masked LM (MLM)</strong></h5><p>Masked 15% of the input tokens <em>at random</em>, and predicting masked tokens.</p>
<p><strong>Potential problems</strong>:</p>
<ol>
<li>Masked tokens are never seen, leading to mismatch between pretraining and fine-tuning.<br> <strong>Solution</strong>: 80% of the time, replace the word with [MASK] token; 10% with random words; the rest 10% unchanged (to bias the representation towards actual observed word).</li>
<li>Masking 15% tokens requires more convergence time in pretraining steps.<br> <strong>Solution</strong>: it deserves compare with empirical improvements.</li>
</ol>
<h5 id="Task-2-Next-Sentence-Prediction"><a href="#Task-2-Next-Sentence-Prediction" class="headerlink" title="Task#2: Next Sentence Prediction"></a>Task#2: Next Sentence Prediction</h5><p><strong>Task</strong>: binarized next sentence prediction (in order to handle relationships between multiple sentences)</p>
<ul>
<li>Downstream tasks like QA and NLI requires understanding the relationship between two ajacent sentences. Specifically, predicting whether sentence A is followed by B or not.</li>
</ul>
<p><img data-src='/notes/images/bert.png' width='40%'/></p>
<h4 id="How-to-employ-BERT-on-downstream-tasks"><a href="#How-to-employ-BERT-on-downstream-tasks" class="headerlink" title="How to employ BERT on downstream tasks?"></a>How to employ BERT on downstream tasks?</h4><p><img data-src="/notes/images/bert-on-downstream-tasks.png" alt="upload successful"></p>
<p>In above figure, $E$ represents the input embedding, <script type="math/tex">T_i</script> represents the contextual representation of token i, [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences.</p>
<ul>
<li>For <strong>sequence-level</strong> classification tasks, take the final hidden state (i.e. the Transformer output)  for the first token in the input. Feed it into a classification FFNN followed by a softmax.</li>
<li>For <strong>span-level</strong> or <strong>token-level</strong> prediction tasks, as shown in the figure.</li>
<li><code>BERT + FFNN + softmax</code></li>
</ul>
<h4 id="Bert-as-a-feature-extractor"><a href="#Bert-as-a-feature-extractor" class="headerlink" title="Bert as a feature extractor"></a>Bert as a feature extractor</h4><p>Like ELMo as a feature-based approach, use pretrained BERT to create <code>ELMo-like contextualized word embeddings</code>, and feed them to a domain-specific model.</p>
<p><img data-src='/notes/images/bert-extractor.png' width='90%'/></p>
<div class="note success">
            <p>Concating the token representations from the top 4 hidden layers of pretrained Transformer, is only 0.3 F1 behind the fine-tuning BERT. It can be seen that <code>BERT is effective for both the finetuning and feature based approaches</code>.</p>
          </div>
<p><img data-src='/notes/images/bert-example.png' width='90%'/></p>
<center>Bert example<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Alammar J. (2018, Dec 3). The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [Blog post]. Retrieved from https://jalammar.github.io/illustrated-bert/
">[23]</span></a></sup></center>


<h4 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h4><p>Investigate the linguitic phonomena that may or may not be captured by BERT.</p>
<h3 id="Comparison-between-ELMo-OpenAI-GPT-and-BERT"><a href="#Comparison-between-ELMo-OpenAI-GPT-and-BERT" class="headerlink" title="Comparison between ELMo, OpenAI GPT and BERT"></a>Comparison between ELMo, OpenAI GPT and BERT</h3><table style="border-collapse:collapse;border-spacing:0;border-color:#bbb" class="tg"><tr><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">Model</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">architecture</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">pretraining task</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">Usage style</th></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">ELMo</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">bi-LSTM</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">LM</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">feature-based</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">OpenAI GPT</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">left-to-right Transformer</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">LM</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">fine-tuning</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">BERT</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">bi-Transformer</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">MLM, NSP</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">fine-tuning</td></tr></table>

<p>Training time: Transformer &lt; ConvNets &lt; Simple RNNs &lt; LSTMs .</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). <a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A neural probabilistic language model</a>. Journal of machine learning research, 3(Feb), 1137-1155.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">R. Miikkulainen and M.G. Dyer. Natural language processing with modular neural networks and distributed lexicon. Cognitive Science, 15:343–399, 1991.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.quora.com/Whats-the-difference-between-distributed-and-distributional-semantic-representations">What’s the difference between distributed and distributional (semantic) representations?</a> Quora. Retrieved January 7, 2019<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zellig Harris. 1954. <a target="_blank" rel="noopener" href="https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520">Distributional structure. Word</a>, 10(23):146–162.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1301.3781.pdf?">Efficient estimation of word representations in vector space</a>. arXiv preprint arXiv:1301.3781.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed representations of words and phrases and their compositionality</a>. In Advances in neural information processing systems (pp. 3111-3119).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bojanowski, P., Grave, E., Joulin, A. &amp; Mikolov, T. (2017). <a target="_blank" rel="noopener" href="http://aclweb.org/anthology/Q17-1010">Enriching Word Vectors with Subword Information</a>. Transactions of the Association for Computational Linguistics, 5, 135--146.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Pennington, J., Socher, R. &amp; Manning, C. D. (2014). <a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/D14-1162">Glove: Global Vectors for Word Representation</a>. EMNLP 2014.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Weng L. (2017, Oct 15). Learning Word Embedding [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ruder S. (2018, Jul 08) NLP's ImageNet moment has arrived [Blog post]. Retrieved from https://thegradient.pub/nlp-imagenet/<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05365">Deep contextualized word representations</a>. arXiv preprint arXiv:1802.05365.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Howard, J., &amp; Ruder, S. (2018). <a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/P18-1031">Universal language model fine-tuning for text classification</a>. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 328-339).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <a target="_blank" rel="noopener" href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Improving language understanding by generative pre-training</a>.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf?fbclid=IwAR3FQiWQzP7stmPWZ4kzrGmiUaN81UpiNeq4GWthrxmwgX0B9f1CvuXJC2E">Bert: Pre-training of deep bidirectional transformers for language understanding</a>. arXiv preprint arXiv:1810.04805.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wieting, J., Bansal, M., Gimpel, K., &amp; Livescu, K. (2016). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1607.02789">Charagram: Embedding words and sentences via character n-grams</a>. arXiv preprint arXiv:1607.02789.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Neelakantan, A., Shankar, J., Passos, A., &amp; McCallum, A. (2015). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.06654">Efficient non-parametric estimation of multiple embeddings per word in vector space</a>. arXiv preprint arXiv:1504.06654.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.00108">Semi-supervised sequence tagging with bidirectional language models</a>. arXiv preprint arXiv:1705.00108.<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf">Learned in translation: Contextualized word vectors</a>. In NIPS 2017.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.02182">Regularizing and Optimizing LSTM Language Models</a>. arXiv preprint arXiv:1708.02182 .<a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in Neural Information Processing Systems (pp. 5998-6008).<a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">R. Sennrich, B. Haddow, and A. Birch. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.07909">Neural machine translation of rare words with subword units</a>. arXiv preprint arXiv:1508.07909, 2015.<a href="#fnref:21" rev="footnote"> ↩</a></span></li><li id="fn:22"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">D. Hendrycks and K. Gimpel. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.08415">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</a>. arXiv preprint arXiv:1606.08415, 2016.<a href="#fnref:22" rev="footnote"> ↩</a></span></li><li id="fn:23"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Alammar J. (2018, Dec 3). The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [Blog post]. Retrieved from https://jalammar.github.io/illustrated-bert/<a href="#fnref:23" rev="footnote"> ↩</a></span></li><li id="fn:24"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598">Towards data science: FROM Pre-trained Word Embeddings TO Pre-trained Language Models — Focus on BERT</a><a href="#fnref:24" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/NLP/" rel="tag"># NLP</a>
              <a href="/notes/tags/Survey/" rel="tag"># Survey</a>
              <a href="/notes/tags/Language-model/" rel="tag"># Language model</a>
              <a href="/notes/tags/Word-representation/" rel="tag"># Word representation</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2018/12/19/NLP/NER/Industrial-Named-Entity-Recognition/" rel="prev" title="Industrial Tricks for Named Entity Recognition">
      <i class="fa fa-chevron-left"></i> Industrial Tricks for Named Entity Recognition
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2019/01/18/NLP/Text-classification-overview/" rel="next" title="Text Classification: An Overview">
      Text Classification: An Overview <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Background"><span class="nav-number">1.</span> <span class="nav-text">Background</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-Model-LM"><span class="nav-number">1.1.</span> <span class="nav-text">Language Model (LM)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Discrete-n-gram"><span class="nav-number">1.1.1.</span> <span class="nav-text">Discrete n-gram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Continuous-n-gram"><span class="nav-number">1.1.2.</span> <span class="nav-text">Continuous n-gram</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NNLM-Bengio-et-al-2003"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">NNLM (Bengio et al., 2003)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNNLM-Mikolov-et-al-2010"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">RNNLM (Mikolov et al., 2010)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Intrinsic-evaluation-of-LM"><span class="nav-number">1.1.3.</span> <span class="nav-text">Intrinsic evaluation of LM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LM-application"><span class="nav-number">1.2.</span> <span class="nav-text">LM application</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatic-Speech-Recognition"><span class="nav-number">1.2.1.</span> <span class="nav-text">Automatic Speech Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-Translation"><span class="nav-number">1.2.2.</span> <span class="nav-text">Machine Translation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Word-representation"><span class="nav-number">1.3.</span> <span class="nav-text">Word representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Distributional-representation"><span class="nav-number">1.3.1.</span> <span class="nav-text">Distributional representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distributed-static-word-representation"><span class="nav-number">1.3.2.</span> <span class="nav-text">Distributed (static) word representation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Word2Vec-Mikolov-2013-Google"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Word2Vec (Mikolov 2013; Google)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Continuous-Bag-of-Words-CBOW"><span class="nav-number">1.3.2.1.1.</span> <span class="nav-text">Continuous Bag of Words (CBOW)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Skip-gram"><span class="nav-number">1.3.2.1.2.</span> <span class="nav-text">Skip-gram</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FastText-Mikolov-et-al-2017-Facebook"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">FastText (Mikolov et al. 2017; Facebook)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GloVe-Pennington-2014-Stanford"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">GloVe (Pennington 2014; Stanford)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Challenge"><span class="nav-number">2.</span> <span class="nav-text">Challenge</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pretraining-dynamic-word-representation"><span class="nav-number">3.</span> <span class="nav-text">Pretraining (dynamic word representation)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-based-pretraining-frozen-representation"><span class="nav-number">3.1.</span> <span class="nav-text">Feature-based pretraining (frozen representation)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ULMFiT"><span class="nav-number">3.1.1.</span> <span class="nav-text">ULMFiT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ELMo-NAACL-2018-AllenAI"><span class="nav-number">3.1.2.</span> <span class="nav-text">ELMo (NAACL 2018, AllenAI)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problems"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Solution"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">Solution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ELMo-representation"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">ELMo representation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-to-utilize-ELMo-into-downstream-supervised-tasks"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">How to utilize ELMo into downstream supervised tasks?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fine-tuning-pretraining"><span class="nav-number">3.2.</span> <span class="nav-text">Fine-tuning pretraining</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#OpenAI-Transformer-GPT-generative-pre-training"><span class="nav-number">3.2.1.</span> <span class="nav-text">OpenAI Transformer GPT (generative pre-training)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT-Google-2018"><span class="nav-number">3.2.2.</span> <span class="nav-text">BERT (Google 2018)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT%E2%80%99s-pretraining-on-2-tasks"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">BERT’s pretraining on 2 tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Task-1-Masked-LM-MLM"><span class="nav-number">3.2.2.1.1.</span> <span class="nav-text">Task#1: Masked LM (MLM)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Task-2-Next-Sentence-Prediction"><span class="nav-number">3.2.2.1.2.</span> <span class="nav-text">Task#2: Next Sentence Prediction</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-to-employ-BERT-on-downstream-tasks"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">How to employ BERT on downstream tasks?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bert-as-a-feature-extractor"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">Bert as a feature extractor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Future-work"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">Future work</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Comparison-between-ELMo-OpenAI-GPT-and-BERT"><span class="nav-number">3.2.3.</span> <span class="nav-text">Comparison between ELMo, OpenAI GPT and BERT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="cyk1337"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">cyk1337</p>
  <div class="site-description" itemprop="description">What is now proved was once only imagined.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cyk1337</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/";
    this.page.identifier = "2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/";
    this.page.title = "Embedding Pretrained Linguistic Prior in a Nutshell";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
