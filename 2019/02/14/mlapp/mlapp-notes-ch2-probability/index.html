<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A brief introduction of the probability theory and the information theory.">
<meta property="og:type" content="article">
<meta property="og:title" content="A Review of Probability">
<meta property="og:url" content="https://cyk1337.github.io/notes/2019/02/14/mlapp/mlapp-notes-ch2-probability/index.html">
<meta property="og:site_name" content="Yekun&#39;s Note">
<meta property="og:description" content="A brief introduction of the probability theory and the information theory.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/dist-cmp.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/gamma-dist.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/beta-dist.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/pareto-dist.png">
<meta property="article:published_time" content="2019-02-14T09:37:38.000Z">
<meta property="article:modified_time" content="2019-02-14T09:37:38.000Z">
<meta property="article:author" content="Yekun Chai">
<meta property="article:tag" content="Probability theory">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/dist-cmp.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2019/02/14/mlapp/mlapp-notes-ch2-probability/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>A Review of Probability | Yekun's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yekun's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Machine learning notes and writeup.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2019/02/14/mlapp/mlapp-notes-ch2-probability/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="Yekun Chai">
      <meta itemprop="description" content="Language is not just words.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yekun's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Review of Probability
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-02-14 17:37:38" itemprop="dateCreated datePublished" datetime="2019-02-14T17:37:38+08:00">2019-02-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Mathematics/" itemprop="url" rel="index"><span itemprop="name">Mathematics</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Mathematics/Probability-theory/" itemprop="url" rel="index"><span itemprop="name">Probability theory</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2019/02/14/mlapp/mlapp-notes-ch2-probability/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/02/14/mlapp/mlapp-notes-ch2-probability/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A brief introduction of the probability theory and the information theory.<br><span id="more"></span></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Bayesian</strong> interpretation models the <strong>uncertainty</strong> about the events.</p>
<h1 id="Probability-theory"><a href="#Probability-theory" class="headerlink" title="Probability theory"></a>Probability theory</h1><h2 id="Discrete-random-variables"><a href="#Discrete-random-variables" class="headerlink" title="Discrete random variables"></a>Discrete random variables</h2><p>We denote the probability of the event that $X=x$ by $p(X=x)$, ro just $p(x)$ for short. The expression $p(A)$ denotes the probability that the event $A$ is true. Here $p()$ is called a <strong>probability mass function</strong> (<strong>pmf</strong>). </p>
<h3 id="Fundamental-rules"><a href="#Fundamental-rules" class="headerlink" title="Fundamental rules"></a>Fundamental rules</h3><ul>
<li><p>Union of two event:</p>
<script type="math/tex; mode=display">p(A \cup B) = p(A) + p(B) - p(A \cap B)\\ = p(A) + p(B) \text{ if A and B are mutually exclusive}</script></li>
<li><p>Joint probabilities:</p>
<script type="math/tex; mode=display">p(A,B) = p(A \cap B) = p(A|B) p(B)</script></li>
<li><p>Marginal distribution:</p>
<script type="math/tex; mode=display">p(A) = \sum_b p(A,B) = \sum_b p(A|B=b)p(B=b)</script></li>
<li><p>Chain rule of probability:</p>
<script type="math/tex; mode=display">p(X_{1:D}) = p(X_1) p(X_2|X_1) p(X_3|X_2,X_1) p(X_4|X_3,X_2,X_1) ... p(X_D|X_{1:d-1})</script></li>
<li><p>Conditional probability:</p>
<script type="math/tex; mode=display">p(A|B) = \frac{p(A,B)}{p(B) \text{ if p(B) > 0}}</script></li>
</ul>
<h2 id="Bayes-rule-a-k-a-Bayes-Theorem"><a href="#Bayes-rule-a-k-a-Bayes-Theorem" class="headerlink" title="Bayes rule (a.k.a Bayes Theorem):"></a>Bayes rule (a.k.a Bayes Theorem):</h2><script type="math/tex; mode=display">p(X=x|Y=y) = \frac{p(X=x,Y=y)}{p(Y=y)} = \frac{p(X=x)p(Y=y|X=x)}{\sum_{x'}p(X=x')p(Y=y|X=x')}</script><h2 id="Continuous-random-variables"><a href="#Continuous-random-variables" class="headerlink" title="Continuous random variables"></a>Continuous random variables</h2><p><strong>Cumulative distribution function</strong> (<strong>cdf</strong>) of $X$: define <script type="math/tex">F(q) \triangleq p(X \leq q)</script>. </p>
<script type="math/tex; mode=display">P(a < X \leq b) = F(b) - F(a)</script><p><strong>probability density function</strong> (<strong>pdf</strong>): define <script type="math/tex">f(x) = \frac{d}{dx} F(x)</script></p>
<script type="math/tex; mode=display">P(a < X \leq b) =  \int_a^b f(x)dx</script><h2 id="Mean-and-Variance"><a href="#Mean-and-Variance" class="headerlink" title="Mean and Variance"></a>Mean and Variance</h2><h3 id="Mean-expected-value"><a href="#Mean-expected-value" class="headerlink" title="Mean (expected value)"></a>Mean (expected value)</h3><p>Let $\mu$ denote the mean(expected value).</p>
<ul>
<li>For discrete rv’s:<script type="math/tex; mode=display">\mathbb{E}[X] \triangleq \sum_{x \in \chi} x p(x)</script></li>
<li>For continuous rv’s:<script type="math/tex; mode=display">\mathbb{E}[X] \triangleq \int_{\chi} x p(x) dx</script></li>
</ul>
<h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p>Let <script type="math/tex">\sigma^2</script> denote the measure of the “spread of a distribution”.</p>
<script type="math/tex; mode=display">\text{var}[X]  \triangleq \mathbb{E} (X-\mu)^2 = \int (x-\mu)^2 p(x)dx \\= \int x^2 p(x)dx+ \mu^2 \int p(x)dx - 2 \mu \int x p(x) dx = \mathbb{E}[X^2] - \mu^2</script><p>Derive a useful result:</p>
<script type="math/tex; mode=display">\mathbb{E}[X^2] = \mu^2 + \sigma^2</script><p>The standard deviation is:</p>
<script type="math/tex; mode=display">\text{std}[X] \triangleq \sqrt{\text{var}[X]}</script><h1 id="Common-discrete-distributions"><a href="#Common-discrete-distributions" class="headerlink" title="Common discrete distributions"></a>Common discrete distributions</h1><h2 id="The-binomial-and-Bernoulli-distributions"><a href="#The-binomial-and-Bernoulli-distributions" class="headerlink" title="The binomial and Bernoulli distributions"></a>The binomial and Bernoulli distributions</h2><p>Suppose we toss a coin $n$ times. Let $X \in { 0,…,n }$ be the number of heads. If the probability of heads is $\theta$, then X has a binomial distribution: $\text{X} ~ \text{Bin}(n, \theta)$</p>
<script type="math/tex; mode=display">Bin(k|n, \theta) \triangleq {n \choose k} \theta^k (1-\theta)^{n-k}</script><p>where</p>
<script type="math/tex; mode=display">{n \choose k} \triangleq \frac{n!}{(n-k)!k!}</script><p>is the number of ways to choose $k$ items from $n$ (a.k.a. binomial coefficient, pronounced “n choose k”).</p>
<script type="math/tex; mode=display">\mu = \theta, \quad \sigma^2 = n \theta (1-\theta)</script><p>Suppose we only toss a coin only once. Let <script type="math/tex">X \in \{0,1\}</script> be a binary random variable, with the probability of “success” or “heads” of $\theta$. We say that $X$ has a <strong>Bernoulli distribution</strong>: <script type="math/tex">X \sim \text{Ber}(\theta)</script>, where the pmf is defined as:</p>
<script type="math/tex; mode=display">\text{Ber}(x|\theta) = \theta^{\mathbb{I}(x=1)} (1-\theta)^{\mathbb{I}(x=0)}</script><h2 id="The-multinomial-and-multinoulli-distributions"><a href="#The-multinomial-and-multinoulli-distributions" class="headerlink" title="The multinomial and multinoulli distributions"></a>The multinomial and multinoulli distributions</h2><p>The binomial distribution only model the outcomes of coin tosses (2 results per round). We use <strong>multinomial</strong> distribution to model the outcomes of tossing a $K$-sided die. Let $\mathbf{x} = (x_1,…,x_K)$ be a random vector, where $x_j$ is the number of times side $j$ of the die occurs. The pmd is:</p>
<script type="math/tex; mode=display">\text{Mu}(x|n,\theta) \triangleq {n \choose {x_1...x_K}} \prod_{j=1}^K \theta_j^{x_j}</script><p>where <script type="math/tex">\theta_j</script> is the probability that side $j$ shows up, and the <strong>multinomial coefficient</strong> (<script type="math/tex">n = \sum_{k=1}^K x_k</script>) is:</p>
<script type="math/tex; mode=display">{n \choose {x_1...x_K}}  \triangleq \frac{n!}{x_1!x_2!...x_K!}</script><p>Suppose $n = 1$, we rolling a $K$-sided dice once. This is called <strong>one-hot encoding</strong> .</p>
<table style="border-collapse:collapse;border-spacing:0;border-color:#999" class="tg"><tr><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:left">Name</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:center">$n$</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:center;vertical-align:top">$K$</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:center;vertical-align:top">$x$</th></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Multinomial</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1,...,n \}^K, \sum_{k=1}^K x_k = n$$</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Multinoulli</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1\}^K, \sum_{k=1}^K x_k = 1$$ (1-of-$K$ encoding)</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Binomial</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1,...,n \}$$</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Bernoulli</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1\}$$</td></tr></table>



<h2 id="The-Poisson-distribution"><a href="#The-Poisson-distribution" class="headerlink" title="The Poisson distribution"></a>The Poisson distribution</h2><p>With parameter $\lambda &gt; 0$, $X \sim \text{Poi}(\lambda)$, if its pmf is:</p>
<script type="math/tex; mode=display">\text{Poi}(x|\lambda) = e^{-\lambda} \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(A)</script><p>where the first term is just the normalization constant.</p>
<h2 id="The-empirical-distribution"><a href="#The-empirical-distribution" class="headerlink" title="The empirical distribution"></a>The empirical distribution</h2><p>Given a set of data <script type="math/tex">\mathscr{D} = \{ x_1,...,x_N \}</script>, define the empirical distribution (a.k.a. empirical measure):</p>
<script type="math/tex; mode=display">p_{emp}(A) \triangleq \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(A)</script><p>where <script type="math/tex">\delta_x(A)</script> is the <strong>Dirac measure</strong>, defined by:</p>
<script type="math/tex; mode=display">
    \delta_x(A)=\left\{
                \begin{array}{ll}
                  0 \text{ if } x \notin A \\
                  1 \text{ if } x \in A 
                \end{array}
              \right.</script><h1 id="Common-continuous-distributions"><a href="#Common-continuous-distributions" class="headerlink" title="Common continuous distributions"></a>Common continuous distributions</h1><h2 id="Gaussian-normal-distribution"><a href="#Gaussian-normal-distribution" class="headerlink" title="Gaussian (normal) distribution"></a>Gaussian (normal) distribution</h2><p>Let $X \sim \mathcal{N}(\mu, \sigma^2)$ denote </p>
<script type="math/tex; mode=display">\mathcal{N}(x|\mu, \sigma^2) \triangleq \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2 (x-\mu)^2}}</script><p>The <strong>precision</strong> of a Gaussian, i.e. the inverse variance <script type="math/tex">\lambda = 1/\sigma^2</script>. A high precision means a narrow distribution (low variance) centered on $\mu$.</p>
<ul>
<li>Problems: Gaussian distribution is sensitive to outliers, since the log-probability only decays quadratically with the distance from the center.</li>
</ul>
<p>More robust: <strong>Student</strong> $t$ <strong>distribution</strong>. </p>
<script type="math/tex; mode=display">\tau(x|\mu,\sigma^2,v) \propto [1+ \frac{1}{v} (\frac{x-\mu}{\sigma})^2]^{-\frac{v+1}{2}}</script><script type="math/tex; mode=display">\text{mean} = \mu, \text{mode} = \mu, \text{var} = \frac{v \sigma^2}{(v-2)}</script><h2 id="Laplace-distribution"><a href="#Laplace-distribution" class="headerlink" title="Laplace distribution"></a>Laplace distribution</h2><p>A.k.a. <strong>double-sided exponential</strong> distribution.</p>
<script type="math/tex; mode=display">Lap(x|\mu,b) \triangleq \frac{1}{2b} exp(-\frac{|x-\mu|}{b})</script><p>Here $\mu$ is a location parameter and $b&gt;0$ is a scale parameter.</p>
<script type="math/tex; mode=display">\text{mean} = \mu, \text{mode} = \mu, \text{var} = 2 b^2</script><p><img data-src="/notes/images/dist-cmp.png" alt="upload successful"></p>
<h2 id="Gamma-distribution"><a href="#Gamma-distribution" class="headerlink" title="Gamma distribution"></a>Gamma distribution</h2><p>The <strong>gamma distribution</strong> is a flexible distribution for positive real valued rv’s, $x&gt;0$. </p>
<script type="math/tex; mode=display">Ga(T|\text{shape}=a, \text{rate}=b) \triangleq \frac{b^a}{\Gamma(a)} T^{a-1} e^{-Tb}</script><p>where $\gamma(a)$ is the gamma function:</p>
<script type="math/tex; mode=display">\Gamma(x) \triangleq \int_0^{\infty} \mu^{x-1} e^{-u} du</script><script type="math/tex; mode=display">\text{mean} = \frac{a}{b}, \text{mode} = \frac{a-1}{b}, \text{var} = \frac{a}{b^2}</script><script type="math/tex; mode=display">\text{mean} = \frac{a}{b}, \text{mode} = \frac{a-1}{b}, \text{var} = \frac{a}{b^2}</script><p><img data-src="/notes/images/gamma-dist.png" alt="Gamma distribution."></p>
<h2 id="Beta-distribution"><a href="#Beta-distribution" class="headerlink" title="Beta distribution"></a>Beta distribution</h2><p>The <strong>beta distribution</strong> has support over the interval [0,1]:</p>
<script type="math/tex; mode=display">\text{Beta}(x|a,b) = \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1}</script><p>Here $B(p,q)$ is the beta function,</p>
<script type="math/tex; mode=display">B(a,b) \triangleq \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}</script><script type="math/tex; mode=display">\text{mean} = \frac{a}{a+b}, \text{mode} = \frac{a-1}{a+b-2}, \text{var} = \frac{ab}{(a+b)^2(a+b+1)}</script><p><img data-src="/notes/images/beta-dist.png" alt="upload successful"></p>
<h2 id="Pareto-distribution"><a href="#Pareto-distribution" class="headerlink" title="Pareto distribution"></a>Pareto distribution</h2><p>The <strong>Pareto distribution</strong> is used to model the distribution of quantities that exhibit <strong>long tails</strong> (heavy tails).</p>
<p>For example, the most frequent ward in English occurs approximately twice as often as the second most frequent word, which occurs twice as oten as the fourth most frequent word. This is <strong>Zipf’s law</strong>.</p>
<p>Its pdf:</p>
<script type="math/tex; mode=display">\text{Pareto(x|k,m)} = km^k x^{-(k+1)} \mathbb{I}(x \geq m)</script><script type="math/tex; mode=display">\text{mean} = \frac{km}{k-1} \text{ if k>1}, \text{mode} = m, \text{var} = \frac{m^2 k}{(k-1)^2 (k-2)} \text{ if k>2}</script><p><img data-src="/notes/images/pareto-dist.png" alt="upload successful"></p>
<h1 id="Joint-probability-distributions"><a href="#Joint-probability-distributions" class="headerlink" title="Joint probability distributions"></a>Joint probability distributions</h1><p>A <strong>joint probability distribution</strong> has the form <script type="math/tex">p(x_1,...,x_D)</script> for a set of $D &gt; 1$ variables, and models the (stochastic) relationships between the variables.</p>
<h2 id="Covariance-and-correlation"><a href="#Covariance-and-correlation" class="headerlink" title="Covariance and correlation"></a>Covariance and correlation</h2><p><strong>Covariance</strong> between two rv’s X and Y measures the degree to which X and Y are (linearly) related.</p>
<script type="math/tex; mode=display">\text{cov}[X,Y] \triangleq  \mathbb{E}[(X- \mathbb{E}[X])(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]</script><p>If $\mathbf{x}$ is a $d$-dimensional random vector, its <strong>covariance matrix</strong> is defined to be symmetric, positive definite matrix:</p>
<script type="math/tex; mode=display">\text{cov}[\mathbf{x}] \triangleq \mathbb{E}[(\mathbf{x} - \mathbb{E}[\mathbf{x}])(\mathbf{x} - \mathbb{E}[\mathbf{x}])^T ]]  = 
   \begin{bmatrix}
     \text{var}[X_1] & \text{cov}[X_1,X_2] & \cdots & \text{cov}[X_1,X_d] \\
     \text{cov}[X_2,X_1] & \text{var}[X_2] & \cdots & \text{cov}[X_2,X_d] \\
     \vdots & \vdots & \ddots & \vdots \\
     \text{cov}[X_d,X_1] & \text{cov}[X_d,X_2] & \cdots & \text{var}[X_d]
   \end{bmatrix}</script><p>Covariance $\in [0, \infty]$. <strong>Pearson correlation coefficient</strong> use normalized measure with a finite upper bound:</p>
<script type="math/tex; mode=display">\text{corr}[X, Y] \triangleq \frac{\text{cov}[X,Y]}{\sqrt{\text{var}[X] \text{var}[Y]}}</script><p>A<strong>correlation matrix</strong> has the form:</p>
<script type="math/tex; mode=display">\mathbf{R} =    \begin{bmatrix}
     \text{corr}[X_1, X_1] & \text{corr}[X_1,X_2] & \cdots & \text{corr}[X_1,X_d] \\
     \vdots & \vdots & \ddots & \vdots \\
     \text{corr}[X_d,X_1] & \text{corr}[X_d,X_2] & \cdots & \text{corr}[X_d, X_d]
   \end{bmatrix}</script><p>where $\text{corr}[X,Y] \in [-1, 1]$.</p>
<div class="note primary">
            <p>independent $\Rightarrow$ uncorrelated,<br>uncorrelated $\nRightarrow$ independent.</p><p>Measure the dependence between rv’s: <strong>mutual information</strong>.</p>
          </div>
<h2 id="Multivariate-Gaussian"><a href="#Multivariate-Gaussian" class="headerlink" title="Multivariate Gaussian"></a>Multivariate Gaussian</h2><p>The <strong>Multivariate Gaussian</strong> or <strong>multivariate normal (MVN)</strong> is the most widely used pdf for continuous variables.<br>Its pdf:</p>
<script type="math/tex; mode=display">\mathcal{N}(\mathbf{x}|\mathbf{\mu}, \mathbf{\Sigma}) \triangleq \frac{1}{(2\pi)^{-D/2} |\mathbf{\Lambda}^{1/2}|} \text{exp}[-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \sum^{-1} (\mathbf{x} - \mathbf{\mu}) ]</script><p>where $\mathbf{\mu} = \mathbb{E}[\mathbf{x}] \in \mathbb{R}^D$ is the mean vector, and $\Sigma = \text{cov}[\mathbf{x}]$  is the $D /times D$ covariance matrix. The <strong>precision matrix</strong> or <strong>concentration matrix</strong> is the inverse covariance matrix, <script type="math/tex">\Lambda = \Sigma^{-1}</script>. The normalization constant $(2\pi)^{-D/2} |\mathbf{\Lambda}^{1/2}|$ just ensure that the pdf integrates to 1.</p>
<h2 id="Multivariate-Student-t-distribution"><a href="#Multivariate-Student-t-distribution" class="headerlink" title="Multivariate Student $t$ distribution"></a>Multivariate Student $t$ distribution</h2><h2 id="Dirichlet-distribution"><a href="#Dirichlet-distribution" class="headerlink" title="Dirichlet distribution"></a>Dirichlet distribution</h2><h1 id="Transformations-of-random-varianbles"><a href="#Transformations-of-random-varianbles" class="headerlink" title="Transformations of random varianbles"></a>Transformations of random varianbles</h1><h2 id="Linear-transformation"><a href="#Linear-transformation" class="headerlink" title="Linear transformation"></a>Linear transformation</h2><p>Suppose $f()$ is a linear function:</p>
<script type="math/tex; mode=display">\mathbf{y} = f(\mathbf{x}) = \mathbf{A} \mathbf{x} + \mathbf{b}</script><p><strong>Linearity of expectation</strong>:</p>
<script type="math/tex; mode=display">\mathbb{E}[\mathbf{y}] = \mathbb{E}[\mathbf{A}\mathbf{x} + \mathbf{b}] = \mathbf{A}\mathbf{\mu} + \mathbf{b}</script><p>where $\mathbf{\mu}=\mathbb{E}[\mathbf{x}]$.</p>
<p><strong>Covariance</strong>: </p>
<script type="math/tex; mode=display">\text{cov}[\mathbf{y}] = \text{cov}[\mathbf{A}\mathbf{x}] = \mathbf{A}\Sigma\mathbf{A}^T</script><p>where $\Sigma = \text{cov}[\mathbf{x}]$</p>
<p>If $f()$ is a scalar-valued function, $f(\mathbf{x}) = \mathbf{a}^T \mathbf{x} + b$, the mean is: </p>
<script type="math/tex; mode=display">\mathbb{E}[\mathbf{a}^T \mathbf{x} + b] = \mathbf{a}^T \mathbf{\mu} + b</script><p>The covariance：</p>
<script type="math/tex; mode=display">\text{var}[y] = \text{var}[\mathbf{a}^T \mathbf{x} + b] = \mathbf{a}^T\Sigma \mathbf{a}</script><h2 id="Central-limit-theorem"><a href="#Central-limit-theorem" class="headerlink" title="Central limit theorem"></a>Central limit theorem</h2><p>Consider $N$ random variables with pdf’s $p(x<em>i)$, each with mean $\mu$ and variance $\sigma^2$. We assume each variable is <strong>iid</strong>(independent and identically distributed). Let $$S_N = \sum</em>{i=1}^N X_i$$ be the sum of the rv’s.</p>
<p>As $N$ increases, the distribution of this sum approaches</p>
<script type="math/tex; mode=display">p(S_N = s) = \frac{1}{\sqrt{2\pi N \sigma^2}} \text{exp} (- \frac{(s-N\mu)^2}{2N\sigma^2})</script><p>Hence the distribution of the quantity</p>
<script type="math/tex; mode=display">Z_N \triangleq \frac{S_N - N \mu}{\sigma\sqrt{N}} = \frac{\bar{X} - \mu}{ \sigma / \sqrt{N}}</script><p>converges to the standard normal, where <script type="math/tex">\bar{X} = \frac{1}{N} \sum_{i=1}^N x_i</script> is the sample mean. (<strong>central limit theorem</strong>)</p>
<h1 id="Monte-Carlo-approximation"><a href="#Monte-Carlo-approximation" class="headerlink" title="Monte Carlo approximation"></a>Monte Carlo approximation</h1><p>Computing the distribution of a function of an rv using the change of variables formula is difficult.<br><strong>Monte Carlo</strong> approximation: First generate $S$ samples from the distribution, called them <script type="math/tex">x_1,...,x_S</script>. Given the samples, we approximate the distribution of $f(X)$ by using the empirical distribution of <script type="math/tex">\{f(x_s)\}^S_{s=1}</script>.</p>
<p>Approximate the expected value with the arithmetic mean of the function applied to the samples:</p>
<script type="math/tex; mode=display">\mathbb{E}[f(X)] = \int f(x)p(x)dx \approx \frac{1}{S}\sum_{s=1}^S f(x_s)</script><p>where <script type="math/tex">x_s \sim p(X)</script>. This is called Monte Carlo integration.</p>
<script type="math/tex; mode=display">\bar{x} = \frac{1}{S} \sum_{s=1}^S x_s \rightarrow \mathbb{E}[X]</script><script type="math/tex; mode=display">\frac{1}{S} \sum_{s=1}^S (x_s - \bar{x})^2 \rightarrow \text{var}[X]</script><script type="math/tex; mode=display">\frac{1}{S}\#\{x_s \leq c\} \rightarrow p(X \leq c)</script><script type="math/tex; mode=display">\text{median}\{x_1,...,x_S\} \rightarrow \text{median}(X)</script><h1 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h1><p><strong>Information theory</strong> represents data in a compact fashion, as well as with transmitting and storing it in a way that is robust to errors.</p>
<h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h2><p><strong>Entropy</strong> of a rv $X$ with distribution $p$, denoted by $\mathbb{H}(X)$, is a measure of its uncertainty. For a discrete rv with $K$ states:</p>
<script type="math/tex; mode=display">\mathbb{H}(X) \triangleq -\sum_{k=1}^K p(X=k) log_2 p(X=k)</script><p>Usually we use log base 2, where it is called <strong>bits</strong> (short for binary digits); whereas log base $e$ is called <strong>nats</strong>. The discrete distribution with maimum entropy is the uniform distribution.</p>
<h2 id="KL-divergence"><a href="#KL-divergence" class="headerlink" title="KL divergence"></a>KL divergence</h2><p><strong>Kullback-Leibler (KL) divergence</strong> or <strong>relative entropy</strong>: measures the dissimilarity of two probability distributions, $p$ and $q$.</p>
<script type="math/tex; mode=display">\mathbb{KL}(p||q) \triangleq \sum_{k=1}^K p_k \text{log} \frac{p_k}{q_k}</script><p>where the sum gets replaced by an integral for pdf’s.</p>
<script type="math/tex; mode=display">\mathbb{KL}(p||q) = \sum_k p_k \text{log} p_k - \sum_k p_k \text{log} q_k = - \mathbb{H}(p) + \mathbb{H}(p,q)</script><p>where $\mathbb{H}(p,q)$ is called the <strong>cross entropy</strong>.</p>
<script type="math/tex; mode=display">\mathbb{H}(p,q) \triangleq - \sum_k p_k \text{log}q_k</script><div class="note success">
            <p><strong>Cross entropy</strong>: the average number of bits needed to encoder data coming from a source with distribution $p$ when we use model $q$ to define our codebook, i.e. KL divergence is the avarage number of <em>extra</em> bits needed to encode the data, due to the fact that we use distribution $q$ to encoder the data instead of the true distribution $p$.</p>
          </div>
<p>The “extra number of bits” interpretation implies that $\mathbb{KL}(p||q) \geq 0 $, and KL is only equal to zero iff $q = p$.</p>
<p><strong>Information inequality</strong>: <script type="math/tex">\mathbb{KL}(p||q) \geq 0 \text{ with equality iff } p=q</script></p>
<h2 id="Mutual-information"><a href="#Mutual-information" class="headerlink" title="Mutual information"></a>Mutual information</h2><p><strong>Mutual Information (MI)</strong>: determine how similar the joint distribution $p(X,Y)$ is to the factored distribution $p(X)p(Y)$.</p>
<script type="math/tex; mode=display">\mathbb{I}(X;Y) \triangleq \mathbb{KL}(p(X,Y)||p(X) p(Y)) = \sum_x\sum_y p(x,y) \text{log} \frac{p(x,y)}{p(x)p(y)}</script><p>where $\mathbb{I}(X;Y) \geq 0$ woth equality iff $p(X,Y) = p(X) p(Y)$, i.e. the MI is zero iff the variables are independent.</p>
<script type="math/tex; mode=display">\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X|Y) = \mathbb{H}(Y) - \mathbb{H}(Y|X)</script><p>where <script type="math/tex">\mathbb{H}(Y|X)</script> is the <strong>conditional entropy</strong>, defined as <script type="math/tex">\mathbb{H}(Y|X) = \sum_x p(x)\mathbb{H}(Y|X=x)</script>.<br>Hence we interpret the MI between $X$ and $Y$ as the reduction in uncertainty about $X$ after observing $Y$, or by symmetry, the reduction in uncertainty about $Y$ after observing $X$.</p>
<p><strong>Pointwise mutual information (PMI)</strong>: measures the discrepancy between events $x$ and $y$ occurring together compared to what would be expected by chance. </p>
<script type="math/tex; mode=display">PMI(x,y) \triangleq \text{log} \frac{p(x,y)}{p(x)p(y)} = \text{log} \frac{p(x|y)}{p(x)} = \text{log} \frac{p(y|x)}{p(y)}</script><div class="note success">
            <p>The MI of $X$ and $Y$ is the expected value of the PMI.</p><script type="math/tex; mode=display">PMI(x,y)= \text{log} \frac{p(x|y)}{p(x)} = \text{log} \frac{p(y|x)}{p(y)}</script><p>We can interpret that PMI is the amount we update the prior $p(x)$ into the posterior $p(x|y)$, or equivalently update the prior $p(y)$ into $p(y|x)$.</p>
          </div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/Probability-theory/" rel="tag"># Probability theory</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2019/02/14/Programming/Tricks-of-Git/" rel="prev" title="Tricks of Git">
      <i class="fa fa-chevron-left"></i> Tricks of Git
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2019/02/23/RL/David%20Silver/RL-notes-1/" rel="next" title="Introduction to Reinforcement Learning">
      Introduction to Reinforcement Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Probability-theory"><span class="nav-number">2.</span> <span class="nav-text">Probability theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Discrete-random-variables"><span class="nav-number">2.1.</span> <span class="nav-text">Discrete random variables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fundamental-rules"><span class="nav-number">2.1.1.</span> <span class="nav-text">Fundamental rules</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayes-rule-a-k-a-Bayes-Theorem"><span class="nav-number">2.2.</span> <span class="nav-text">Bayes rule (a.k.a Bayes Theorem):</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Continuous-random-variables"><span class="nav-number">2.3.</span> <span class="nav-text">Continuous random variables</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mean-and-Variance"><span class="nav-number">2.4.</span> <span class="nav-text">Mean and Variance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-expected-value"><span class="nav-number">2.4.1.</span> <span class="nav-text">Mean (expected value)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Variance"><span class="nav-number">2.4.2.</span> <span class="nav-text">Variance</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Common-discrete-distributions"><span class="nav-number">3.</span> <span class="nav-text">Common discrete distributions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-binomial-and-Bernoulli-distributions"><span class="nav-number">3.1.</span> <span class="nav-text">The binomial and Bernoulli distributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-multinomial-and-multinoulli-distributions"><span class="nav-number">3.2.</span> <span class="nav-text">The multinomial and multinoulli distributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Poisson-distribution"><span class="nav-number">3.3.</span> <span class="nav-text">The Poisson distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-empirical-distribution"><span class="nav-number">3.4.</span> <span class="nav-text">The empirical distribution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Common-continuous-distributions"><span class="nav-number">4.</span> <span class="nav-text">Common continuous distributions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gaussian-normal-distribution"><span class="nav-number">4.1.</span> <span class="nav-text">Gaussian (normal) distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Laplace-distribution"><span class="nav-number">4.2.</span> <span class="nav-text">Laplace distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gamma-distribution"><span class="nav-number">4.3.</span> <span class="nav-text">Gamma distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beta-distribution"><span class="nav-number">4.4.</span> <span class="nav-text">Beta distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pareto-distribution"><span class="nav-number">4.5.</span> <span class="nav-text">Pareto distribution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Joint-probability-distributions"><span class="nav-number">5.</span> <span class="nav-text">Joint probability distributions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Covariance-and-correlation"><span class="nav-number">5.1.</span> <span class="nav-text">Covariance and correlation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multivariate-Gaussian"><span class="nav-number">5.2.</span> <span class="nav-text">Multivariate Gaussian</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multivariate-Student-t-distribution"><span class="nav-number">5.3.</span> <span class="nav-text">Multivariate Student $t$ distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dirichlet-distribution"><span class="nav-number">5.4.</span> <span class="nav-text">Dirichlet distribution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformations-of-random-varianbles"><span class="nav-number">6.</span> <span class="nav-text">Transformations of random varianbles</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-transformation"><span class="nav-number">6.1.</span> <span class="nav-text">Linear transformation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Central-limit-theorem"><span class="nav-number">6.2.</span> <span class="nav-text">Central limit theorem</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monte-Carlo-approximation"><span class="nav-number">7.</span> <span class="nav-text">Monte Carlo approximation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Information-theory"><span class="nav-number">8.</span> <span class="nav-text">Information theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Entropy"><span class="nav-number">8.1.</span> <span class="nav-text">Entropy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KL-divergence"><span class="nav-number">8.2.</span> <span class="nav-text">KL divergence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mutual-information"><span class="nav-number">8.3.</span> <span class="nav-text">Mutual information</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yekun Chai"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">Yekun Chai</p>
  <div class="site-description" itemprop="description">Language is not just words.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yekun Chai</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2019/02/14/mlapp/mlapp-notes-ch2-probability/";
    this.page.identifier = "2019/02/14/mlapp/mlapp-notes-ch2-probability/";
    this.page.title = "A Review of Probability";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
