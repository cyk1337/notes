<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A note of distributed training methods for large neural models.">
<meta property="og:type" content="article">
<meta property="og:title" content="Efficient Large-Scale Distributed Training">
<meta property="og:url" content="https://cyk1337.github.io/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/index.html">
<meta property="og:site_name" content="The Gradient">
<meta property="og:description" content="A note of distributed training methods for large neural models.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/model_parallelism.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/mp-communication.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Gpipe.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/GPipe-2.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PipeDream-1F1B.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PipeDream.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Sequence%20Parallelism.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/TP-megatron3.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ZeRO.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FSDP.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FSDP-full-sharding.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FDSP%20hybrid%20sharding.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/mixed-precision-trend.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Mixed%20Precision%20Training.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/floating-point.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FlashAttn-Illustration.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/FlashAttn-Alg.png">
<meta property="article:published_time" content="2022-04-17T13:47:00.000Z">
<meta property="article:modified_time" content="2025-03-11T17:57:38.878Z">
<meta property="article:author" content="cyk1337">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Pre-training">
<meta property="article:tag" content="Distributed Training">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/model_parallelism.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Efficient Large-Scale Distributed Training | The Gradient</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">The Gradient</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Language is not just words.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="cyk1337">
      <meta itemprop="description" content="What is now proved was once only imagined.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Gradient">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Efficient Large-Scale Distributed Training
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-17 21:47:00" itemprop="dateCreated datePublished" datetime="2022-04-17T21:47:00+08:00">2022-04-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Pre-training/" itemprop="url" rel="index"><span itemprop="name">Pre-training</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Pre-training/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Pre-training/LLM/Distributed-Training/" itemprop="url" rel="index"><span itemprop="name">Distributed Training</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/04/17/Efficient-Large-Scale-Distributed-Training/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A note of distributed training methods for large neural models.<br><span id="more"></span></p>
<h2 id="Training-Parallelism"><a href="#Training-Parallelism" class="headerlink" title="Training Parallelism"></a>Training Parallelism</h2><h3 id="Data-Parallelism"><a href="#Data-Parallelism" class="headerlink" title="Data Parallelism"></a>Data Parallelism</h3><p>Data parallelism (DP) is a technique where we replicate the entire model’s parameters across multiple devices. During training, the mini-batch of data is partitioned evenly across all participating devices. This means that each device, or DP process, operates on a distinct subset of the data samples.</p>
<p>The training process in data parallelism involves each device executing its own forward and backward propagation. This computes the gradients based on the subset of data it has been assigned. Once the gradients are computed, they are averaged across all devices to ensure a consistent update to the model parameters.</p>
<h3 id="Model-Parallelism"><a href="#Model-Parallelism" class="headerlink" title="Model Parallelism"></a>Model Parallelism</h3><p>Model Parallelism (MP)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019). [Megatron-lm: Training multi-billion parameter language models using model parallelism](https://deepsense.ai/wp-content/uploads/2023/04/1909.08053.pdf). arXiv preprint arXiv:1909.08053.">[1]</span></a></sup> offers a way to scale neural network training beyond the memory limitations of a single device by distributing the model’s computation across multiple processes. This strategy is particularly useful for large transformer models that would otherwise be too large to fit on a single GPU.</p>
<p>Tensor-level Model Parallelism (MP) divides the model’s computation vertically among different devices or processes. </p>
<h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h4><p>To illustrate how MP works, let’s consider the standard Multilayer Perceptron (MLP) block within a transformer model, which is represented by the following equations:</p>
<p>\begin{equation}<br>    Y=\mathrm{GeLU}(XA)<br>\end{equation}</p>
<p>\begin{equation}<br>    Z=\mathrm{Dropout}(YB)<br>\end{equation}</p>
<p><img data-src="/notes/images/model_parallelism.png" alt="Megatron-1: MP"></p>
<p>For the MLP block, tensor-level MP splits the weight matrix  $A$ into columns $A = [A_1, A_2]$. By partitioning<br>$A$, the GeLU activation function can be applied independently to the outputs of each partitioned matrix multiplication (GEMM):</p>
<p>\begin{equation}<br>    [Y_1,Y_2]=[\text{GeLU}(XA_1),\text{GeLU}(XA_2)]<br>\end{equation}</p>
<p>The subsequent GEMM, represented by matrix $B$, is split along its rows. This enables direct input from the GeLU activations without the need for inter-process communication, as depicted in the figure above.</p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>The self-attention mechanism is a cornerstone of transformer models, described by the following equation:</p>
<p>\begin{equation}<br>    \text{Attention}(X,Q,K,V)=\text{softmax}(\frac{(XQ)(XK)^\top}{\sqrt{d_k}})XV<br>\end{equation}</p>
<p>For self-attention black, MP partitions the GEMMs for key ($K$), query ($Q$), and value ($V$) matrices along their columns. This allows for the matrix multiplication of each attention head to be distributed across individual GPUs. The output linear layer’s GEMM is then split along its rows, facilitating an efficient transformer layer that requires only two all-reduce operations in both the forward and backward passes.</p>
<p><img data-src="/notes/images/mp-communication.png" alt="Communication op for MP."></p>
<div class="note info">
            <p>When it comes to components like <strong>dropout, layer normalization, and residual connections</strong>, MP adopts a different approach. Instead of splitting these operations, MP replicates their computations across GPUs. This ensures that the output of the MP region can seamlessly integrate with these operations without additional device communication.</p><p>To achieve this, MP maintains duplicate copies of the layer normalization parameters on each GPU. As a result, each GPU can perform dropout and residual connection operations independently, taking the output from the MP region and processing it locally.</p>
          </div>
<p>Model Parallelism, by partitioning the model’s computation across multiple devices, effectively enables training of large-scale transformer models that would otherwise exceed the memory capabilities of a single device. Careful consideration of how operations like dropout and layer normalization are handled ensures that MP remains efficient without compromising the integrity of the model’s training.</p>
<h3 id="Pipeline-Parallelism"><a href="#Pipeline-Parallelism" class="headerlink" title="Pipeline Parallelism"></a>Pipeline Parallelism</h3><p>Pipeline Parallelism (PP)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Huang, Yanping, et al. "GPipe: Easy Scaling with Micro-Batch Pipel ine Parallelism." Computer Vision and Pattern Recognition (2019).">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Harlap, Aaron, et al. "Pipedream: Fast and efficient pipeline parallel dnn training." arXiv preprint arXiv:1806.03377 (2018).">[6]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Narayanan, Deepak, et al. "[Efficient large-scale language model training on gpu clusters using megatron-lm](https://arxiv.org/pdf/2104.04473)." Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.">[2]</span></a></sup> splits the forward and backward pipelines into multiple stages, each assigned to a different device, and data flows through these stages sequentially, enabling efficient utilization of resources and faster training of large models.</p>
<h4 id="GPipe"><a href="#GPipe" class="headerlink" title="GPipe"></a>GPipe</h4><p>GPipe<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Huang, Yanping, et al. "GPipe: Easy Scaling with Micro-Batch Pipel ine Parallelism." Computer Vision and Pattern Recognition (2019).">[5]</span></a></sup> pipelines different sub-sequences of layers on separate accelerators, where consecutive groups of layers can be partitioned into cells. GPipe divides the input mini-batch into smaller micro-batches, enabling different accelerators to work on different micro-batches simutaneously.</p>
<p><img data-src="/notes/images/Gpipe.png" alt="GPipe"></p>
<p><strong>Pipeline Bubble</strong> (bubble size): Given the PP stages $p$ (PP degree), the sequence of $L$ layers can be partitioned into $p$ composite layers, or cells. The numbder of micro-batches in a batch as $m$. The PP bubble consists of $p-1$ forward passes at the start of a batch, and $p-1$ backward passes at the end. Thus, the pipeline bubble size (bubble time fraction) is defined as:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \text{Bubble time fraction (bubble size)}=1- \frac{mp}{(m+p-1)d}=\frac{p-1}{m+p-1}.
\end{equation}</script><p><img data-src="/notes/images/GPipe-2.png" alt="GPipe"></p>
<p>When $m &gt; 4d$, the bubble overhead is negligible.</p>
<h4 id="PipeDream"><a href="#PipeDream" class="headerlink" title="PipeDream"></a>PipeDream</h4><p>PipeDream<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Harlap, Aaron, et al. "Pipedream: Fast and efficient pipeline parallel dnn training." arXiv preprint arXiv:1806.03377 (2018).">[6]</span></a></sup> revolutionizes the efficiency of pipeline parallelism in deep learning with its one-forward-one-backward (1F1B) strategy. This approach guarantees that no GPU remains idle during the steady state, thereby ensuring continuous forward progress with each minibatch. It achieves this by immediately initiating the backward pass for a minibatch as soon as its forward pass is completed.</p>
<p><img data-src="/notes/images/PipeDream-1F1B.png" alt="PipeDream-1F1B"> </p>
<p>The 1F1B strategy interleaves the forward and backward computations at the minibatch level. This tight coupling of passes optimizes the use of GPU resources and accelerates the learning process, as each minibatch benefits from immediate backward propagation, leading to quicker gradient updates and model improvements.</p>
<p><img data-src="/notes/images/PipeDream.png" alt="PipeDream-1F1B"></p>
<p>For interleaved schedules in PipeDream, it allows each device to handle multiple subsets of layers, referred to as model chunks, rather than being restricted to a single, contiguous block of layers. As a result, each device in the pipeline is responsible for multiple pipeline stages, dramatically increasing the efficiency of the computation distribution.</p>
<p>The figure above illustrates that if each device manages $v$ stages, or model chunks, the time required for processing a minibatch through both the forward and backward passes is reduced to $\frac{1}{v}$  of the time it would have previously taken. Consequently, this reduction in processing time diminishes the pipeline bubble—the period when some GPUs might otherwise be idle—resulting in a more streamlined and efficient training process. The bubble size can be quantified as follows:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \text{Bubble time fraction (bubble size)}=\frac{1}{v} \cdot \frac{p-1}{m+p-1}.
\end{equation}</script><p>Here, $d$ represents the degree of pipeline parallelism (number of devices), and $m$ is the number of microbatches in a batch. </p>
<!--
PipeDream-flush
PipeDream-2BW <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Narayanan, Deepak, et al. "Memory-efficient pipeline-parallel dnn training." International Conference on Machine Learning. PMLR, 2021.">[7]</span></a></sup>
-->
<h3 id="Sequence-Parallelism"><a href="#Sequence-Parallelism" class="headerlink" title="Sequence Parallelism"></a>Sequence Parallelism</h3><p>Model parallelism (MP) retains critical components like layer normalization, dropout, and residual connections across the MP group intact. A key insight presented in <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Korthikanti, Vijay Anand, et al. "[Reducing activation recomputation in large transformer models](https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf)." Proceedings of Machine Learning and Systems 5 (2023): 341-353">[3]</span></a></sup> is that in certain regions of transformer blocks, operations are independent along the sequence dimension. SP <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Korthikanti, Vijay Anand, et al. "[Reducing activation recomputation in large transformer models](https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf)." Proceedings of Machine Learning and Systems 5 (2023): 341-353">[3]</span></a></sup> partitions these regions along the sequence dimension for enhanced parallel processing.</p>
<p><img data-src="/notes/images/Sequence Parallelism.png" alt="Sequence parallesim (Megatron-3)"></p>
<p>Consider the following standard non-parallel block within a transformer layer:</p>
<script type="math/tex; mode=display">
\begin{align}
Y& =\mathrm{LayerNorm}(X) \\
Z& =\mathrm{GeLU}(YA), \\
W& =ZB, \\
V &=\mathrm{Dropout}(W), 
\end{align}</script><p>Sequence parallelism splits the input to the layer normalization along the sequence dimension: <script type="math/tex">X=[X_{1}^{s},X_{2}^{s}]</script>. Consequently, the output of the layer normalization is also parallel along the sequence dimension: <script type="math/tex">Y=[Y_{1}^{s},Y_{2}^{s}]</script>. The subsequent linear layer with GeLU activations requires the complete input $Y$, necessitating an all-gather operation. The matrices <script type="math/tex">A=[A_{1}^{c},A_{2}^{c}]</script> and <script type="math/tex">B=[B_{1}^{r},B_{2}^{t}]</script> are partitioned along their columns and rows, respectively. This partitioning strategy helps to minimize communication overhead and allows us to compute $W_1$ and $W_2$ independently. Afterwards, $W=W_1+W_2$ is combined and passed through the dropout layer using reduce-scatter to maintain parallelism along the sequence dimension.</p>
<p><img data-src="/notes/images/TP-megatron3.png" alt="Tensor Parallesim (Megatron-3)"></p>
<p>Putting it all together, we articulate the SP processing steps as follows:</p>
<script type="math/tex; mode=display">
\begin{align}
[Y_{1}^{s},Y_{2}^{s}]& =\mathrm{LayerNorm}([X_{1}^{s},X_{2}^{s}]), \\
\text{Y}& =g(Y_1^s,Y_2^s), \\
[Z_1^h,Z_2^h]& =[\mathrm{GeLU}(YA_{1}^{c}), \mathrm{GeLU}(YA_{2}^{c})], \\
W_{1}& =Z_{1}^{h}B_{1}^{r} \quad
W_{2}=Z_{2}^{h}B_{2}^{r}, \\
[W_{1}^{s},W_{2}^{s}]& =\bar{g}(W_1,W_2), \\
[V_{1}^{s},V_{2}^{s}]& =[\mathrm{Dropout}(W_{1}^{s}), \mathrm{Dropout}(W_{2}^{s})]. 
\end{align}</script><p>SP divides and conquers the workload along the sequence dimension without compromising the integrity of the underlying operations.</p>
<!--
### Megatron 1/2/3
4D parallel
-->
<h3 id="Memory-Efficient-Optimizer-ZeRO"><a href="#Memory-Efficient-Optimizer-ZeRO" class="headerlink" title="Memory-Efficient Optimizer (ZeRO)"></a>Memory-Efficient Optimizer (ZeRO)</h3><p>Zero Redundancy Optimizer (ZeRO)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. "Zero: Memory optimizations toward training trillion parameter models." In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.">[4]</span></a></sup> optimizes the memory by removing the memory state redundancies across DP processes by partitioning the model states instead of replicating them. ZeRO-DP has three main statges, corresponding to the partitioning of optimizer states, gradients, and parameters.</p>
<ol>
<li>Optimizer state partitioning ($P_\text{os}$);</li>
<li>Add gradient partitioning ($P_\text{os+g}$));</li>
<li>Add parameter partitioning ($P_\text{os+g+p}$));</li>
</ol>
<p><img data-src="/notes/images/ZeRO.png" alt="ZeRO-DP"></p>
<p>Details refer to <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/tutorials/zero/">https://www.deepspeed.ai/tutorials/zero/</a>.</p>
<h3 id="Pytorch-FSDP"><a href="#Pytorch-FSDP" class="headerlink" title="Pytorch FSDP"></a>Pytorch FSDP</h3><p>PyTorch Fully Sharded Data Parallel (FSDP)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhao, Yanli, et al. "Pytorch fsdp: experiences on scaling fully sharded data parallel." arXiv preprint arXiv:2304.11277 (2023).">[8]</span></a></sup> is designed to accommodate extremely large models that exceed the memory capacity of a single GPU. By decomposing a model instance into smaller fragments, FSDP manages each fragment independently. During the forward and backward computations, FSDP strategically materializes only the unsharded parameters and gradients for one fragment at a time, while keeping the rest of the parameters and gradients in their sharded state.</p>
<p>This resource management means that FSDP only fully materializes the parameters and gradients for a single fragment at any given time, allowing the remaining fragments to remain sharded and thus minimizing memory usage.</p>
<p><img data-src="/notes/images/FSDP.png" alt="Pytorch FSDP"></p>
<p>FSDP employs a sharding factor $F$ ato determine the number of ranks over which the parameters are distributed:</p>
<ol>
<li>When $F=1$, FSDP replicates the entire model across all devices, reducing to the conventional Data Parallel (DP) approach, which relies on all-reduce operations for gradient synchronization.</li>
<li>For $F=W$,, where $W$ is the global world size, FSDP fully shards the model so that each device maintains only $\frac{1}{W}$ of the total model parameters. </li>
<li>When $F \in (1, W)$, FSDP enables hybrid sharding, balancing between replication and full sharding.</li>
</ol>
<p><img data-src="/notes/images/FSDP-full-sharding.png" alt="FSDP full sharding"></p>
<p><strong>Sharding strategy: flatten-concat-chunk algorithm</strong>. FSDP uses a sharding strategy known as the flatten-concat-chunk algorithm. This technique entails organizing all parameters within an FSDP unit into a single contiguous <code>FlatParameter</code>. This <code>FlatParameter</code>, which is a one-dimensional tensor, is created by concatenating and flattening the individual parameters, with padding added as necessary to ensure the size is divisible by the sharding factor $F$. The <code>FlatParameter</code> is then divided into equal-sized chunks, with the number of chunks corresponding to the sharding factor, and each chunk is assigned to a different rank.</p>
<p>By leveraging this strategy, FSDP streamlines communication between the parameters and ensures an even distribution of the model across the ranks. This allows for efficient scaling of model training across multiple GPUs, making it possible to train models that were previously too large to fit in the memory of a single device.</p>
<p><img data-src="/notes/images/FDSP hybrid sharding.png" alt="FDSP hybrid sharding"></p>
<h2 id="Mixed-Precision-Training"><a href="#Mixed-Precision-Training" class="headerlink" title="Mixed Precision Training"></a>Mixed Precision Training</h2><p>Mixed precision methods<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="NVIDIA. [Train with mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)">[9]</span></a></sup><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Micikevicius, Paulius, et al. "Mixed precision training." arXiv preprint arXiv:1710.03740 (2017).">[10]</span></a></sup> utilize different numerical formats within a single computational workload, optimizing operations by executing them in half-precision (FP16) format. This approach not only accelerates training but also reduces memory usage, allowing for larger models or batch sizes.</p>
<p>During mixed precision training, weights, activations, and gradients are predominantly stored as FP16 to benefit from the reduced precision’s efficiency. However, to maintain the training stability and model quality, an FP32 master copy of weights is kept. This master copy is updated with weight gradients during the optimization step. For each iteration, an FP16 copy of the master weights is used for the forward and backward passes.</p>
<p><img data-src="/notes/images/mixed-precision-trend.png" alt="Mixed precision training"></p>
<h3 id="Mixed-Precision"><a href="#Mixed-Precision" class="headerlink" title="Mixed Precision"></a>Mixed Precision</h3><p>Mixed precision methods<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="NVIDIA. [Train with mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)">[9]</span></a></sup><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Micikevicius, Paulius, et al. "Mixed precision training." arXiv preprint arXiv:1710.03740 (2017).">[10]</span></a></sup> combine the use of different numerical formats in one computational workload.<br><img data-src="/notes/images/Mixed Precision Training.png" alt="Mixed Precision Training"></p>
<p>The training procedure for mixed precision training can be summarized as follows:</p>
<div class="note info">
            <p><strong>Mixed precision training:</strong></p><ol><li>Keep a master copy of weights in full precision (FP32).</li><li>For each iteration:<br> a. Make an FP16 copy of the weights;<br> b. Forward propagation (FP16 weights and activations).<br> c. Multiply the results loss with the scaling factor $S$.<br> d. Backward propagation (FP16 weights, activations, and their gradients).<br> e. Multiply (scaling down) the weight gradient with 1/S.<br> f. Updating the master weights in FP32, applying necessary adjustments like gradient clipping.</li></ol>
          </div>
<h3 id="Dynamic-loss-scaling"><a href="#Dynamic-loss-scaling" class="headerlink" title="Dynamic loss scaling"></a>Dynamic loss scaling</h3><p>This technique involves starting with a large scaling factor and adjusting it dynamically throughout the training process. If no numerical overflow occurs for a predefined number of iterations $N$, the scaling factor $S$ is increased. Conversely, if an overflow is detected, the current weight update is skipped, and $S$ is decreased to prevent future overflows.</p>
<div class="note info">
            <p><strong>Mixed precision training:</strong></p><ol><li>Maintain a primary copy of weights in FP32.</li><li>Initialize $S$ to a large value.</li><li>For each iteration:<br> a. Make an FP16 copy of the weights;<br> b. Forward propagation (FP16 weights and activations).<br> c. Multiply the results loss with the scaling factor $S$.<br> d. Backward propagation (FP16 weights, activations, and their gradients).<br> e. If there is an <code>inf</code> or <code>nan</code> in weights gradients:<pre><code> (1) Reduce S. (2) Skip the weight update and move to the next iteration.</code></pre> f. Multiply the weight gradient with 1/S.<br> g. Complete the weight update (including gradient clipping, etc.)<br> h. If there has not been an <code>inf</code> or <code>nan</code> in the last $N$ iterations, increase $S$.</li></ol>
          </div>
<p>Implementation:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creates model and optimizer in default precision</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a GradScaler once at the beginning of training.</span></span><br><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Runs the forward pass with autocasting.</span></span><br><span class="line">        <span class="keyword">with</span> autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class="line">        <span class="comment"># Backward passes under autocast are not recommended.</span></span><br><span class="line">        <span class="comment"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.</span></span><br><span class="line">        <span class="comment"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span><br><span class="line">        <span class="comment"># otherwise, optimizer.step() is skipped.</span></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Updates the scale for next iteration.</span></span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure></p>
<p>Automatic Mixed Precision (AMP) settings of <code>apex.amp</code>:<br><code>opt_level:</code></p>
<ul>
<li>O0: FP32 training</li>
<li><p>O1: Mixed precision (recommended). Use a whitelist-blacklist model. Whitelist ops (e.g., tensor core-freindly ops like GEMM and convolutions) are performed in FP16; blacklist ops that benefit from FP32 precision (e.g, softmax) are performaed in F32. <code>O1</code> uses dynamic loss scaling unless overridden.</p>
</li>
<li><p>O2: “Almost FP16” Mixed Precision. O2 <strong>casts the model weights to FP16</strong>, patches the model’s ‘forward’ mothod to cast input data to FP16, keeps batchnorms in FP32, maintains FP32 master weights, update the optimizer’s <code>param_groups</code> so that the <code>optimizer.step</code> acts directly on FP32 weights, and uses dynamic loss scaling. Unlike O1, O2 does not patch Torch functions or Tensor methods.</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>O0: FP32 Training</strong></th>
<th><strong>O1: Mixed Precision</strong></th>
<th><strong>O2: “Almost FP16” Mixed Precision</strong></th>
<th><strong>O3: FP16 Training</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Description</strong></td>
<td>Full FP32 training, useful for establishing an accuracy baseline.</td>
<td>Mixed precision with dynamic casting of operations based on a whitelist-blacklist model. Recommended for typical use.</td>
<td>Almost FP16 training, with model weights and inputs cast to FP16, but BatchNorm and master weights in FP32.</td>
<td>Full FP16 training, useful for establishing a speed baseline. Less stable than O1/O2.</td>
</tr>
<tr>
<td><strong>cast_model_type</strong></td>
<td><code>torch.float32</code></td>
<td><code>None</code> (not applicable, model weights remain FP32)</td>
<td><code>torch.float16</code> (model weights cast to FP16)</td>
<td><code>torch.float16</code> (model weights cast to FP16)</td>
</tr>
<tr>
<td><strong>patch_torch_functions</strong></td>
<td><code>False</code></td>
<td><code>True</code> (patches Torch functions and Tensor methods for dynamic FP16/FP32 casting)</td>
<td><code>False</code> (no patching, explicit control of precision)</td>
<td><code>False</code> (no patching, explicit control of precision)</td>
</tr>
<tr>
<td><strong>keep_batchnorm_fp32</strong></td>
<td><code>None</code> (not applicable, everything is FP32)</td>
<td><code>None</code> (not applicable, model weights remain FP32)</td>
<td><code>True</code> (BatchNorm layers remain in FP32)</td>
<td><code>False</code> (BatchNorm in FP16, unless overridden with <code>keep_batchnorm_fp32=True</code>)</td>
</tr>
<tr>
<td><strong>master_weights</strong></td>
<td><code>False</code></td>
<td><code>None</code> (not applicable, model weights remain FP32)</td>
<td><code>True</code> (maintains FP32 master weights for optimizer updates)</td>
<td><code>False</code> (no FP32 master weights)</td>
</tr>
<tr>
<td><strong>loss_scale</strong></td>
<td><code>1.0</code> (no loss scaling)</td>
<td><code>&quot;dynamic&quot;</code> (dynamic loss scaling to prevent underflow)</td>
<td><code>&quot;dynamic&quot;</code> (dynamic loss scaling to prevent underflow)</td>
<td><code>1.0</code> (no loss scaling)</td>
</tr>
<tr>
<td><strong>Use Case</strong></td>
<td>Baseline for accuracy.</td>
<td>Recommended for typical mixed precision training.</td>
<td>Aggressive mixed precision training with FP16 weights and inputs, but FP32 BatchNorm and master weights.</td>
<td>Baseline for speed. Less stable, useful for comparison with O1/O2.</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Key Differences</strong>:</p>
<ul>
<li><strong>O0</strong>: Full FP32 training, no mixed precision. Used for accuracy baselines.</li>
<li><strong>O1</strong>: Dynamic mixed precision with patched Torch functions and Tensor methods. Balances speed and stability.</li>
<li><strong>O2</strong>: Almost FP16 training, with explicit control of precision (no patching). Maintains FP32 BatchNorm and master weights.</li>
<li><strong>O3</strong>: Full FP16 training, no mixed precision. Used for speed baselines but less stable.</li>
</ul>
<p><strong>Precision</strong>: fp32 (E8M23) fp16 (E5M10)/bf16 (E8M7)/fp8</p>
<p><img data-src="/notes/images/floating-point.png" alt=""></p>
<h2 id="Memory-efficient-methods"><a href="#Memory-efficient-methods" class="headerlink" title="Memory-efficient methods"></a>Memory-efficient methods</h2><h3 id="CPU-Offload"><a href="#CPU-Offload" class="headerlink" title="CPU Offload"></a>CPU Offload</h3><p><strong>CPU offload</strong>: Offloading model states to GPU memory. When GPU memory reaches its capacity, a potential solution is to transfer data that is not immediately required to the CPU, retrieving it when necessary at a later stage.</p>
<h3 id="Activation-Recomputation"><a href="#Activation-Recomputation" class="headerlink" title="Activation Recomputation"></a>Activation Recomputation</h3><p><strong>Activation recomputation / Selective recomputation</strong>: Only selected activations are stored for backpropagation while most activations are discarded as they can be recomputed again during the backpropagation. This strategy involves selectively preserving only a subset of activations for use in the backpropagation process. The majority of activations, deemed less critical, are not stored; instead, they are dynamically recalculated as needed during the backpropagation phase.</p>
<h3 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash Attention"></a>Flash Attention</h3><!--
## Flash Attention
Flash Attention v1/2/3
## Decoding
vLLM / Mooncake
-->
<!--
![](/notes/images/FlashAttn.png)-->
<p>Flash Attention<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dao, T., et al. "Fast and memory-efficient exact attention with io-awareness, 2022." URL https://arxiv. org/abs/2205.14135.">[11]</span></a></sup> reduces the frequency of visiting HBM and use tiling computation on self-attention computation. It uses K/V tiling in the outer loop and Q in inner loop in Flash Attention 1, and uses K/V for the inner loop to reduce the frequent SRAM visit.</p>
<p><img data-src="/notes/images/FlashAttn-Illustration.png" alt=""></p>
<p><img data-src="/notes/images/FlashAttn-Alg.png" alt=""></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., &amp; Catanzaro, B. (2019). <a target="_blank" rel="noopener" href="https://deepsense.ai/wp-content/uploads/2023/04/1909.08053.pdf">Megatron-lm: Training multi-billion parameter language models using model parallelism</a>. arXiv preprint arXiv:1909.08053.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Narayanan, Deepak, et al. &quot;<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.04473">Efficient large-scale language model training on gpu clusters using megatron-lm</a>.&quot; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Korthikanti, Vijay Anand, et al. &quot;<a target="_blank" rel="noopener" href="https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf">Reducing activation recomputation in large transformer models</a>.&quot; Proceedings of Machine Learning and Systems 5 (2023): 341-353<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. &quot;Zero: Memory optimizations toward training trillion parameter models.&quot; In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Huang, Yanping, et al. &quot;GPipe: Easy Scaling with Micro-Batch Pipel ine Parallelism.&quot; Computer Vision and Pattern Recognition (2019).<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Harlap, Aaron, et al. &quot;Pipedream: Fast and efficient pipeline parallel dnn training.&quot; arXiv preprint arXiv:1806.03377 (2018).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Narayanan, Deepak, et al. &quot;Memory-efficient pipeline-parallel dnn training.&quot; International Conference on Machine Learning. PMLR, 2021.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhao, Yanli, et al. &quot;Pytorch fsdp: experiences on scaling fully sharded data parallel.&quot; arXiv preprint arXiv:2304.11277 (2023).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">NVIDIA. <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">Train with mixed Precision</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Micikevicius, Paulius, et al. &quot;Mixed precision training.&quot; arXiv preprint arXiv:1710.03740 (2017).<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dao, T., et al. &quot;Fast and memory-efficient exact attention with io-awareness, 2022.&quot; URL https://arxiv. org/abs/2205.14135.<a href="#fnref:11" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/LLM/" rel="tag"># LLM</a>
              <a href="/notes/tags/Pre-training/" rel="tag"># Pre-training</a>
              <a href="/notes/tags/Distributed-Training/" rel="tag"># Distributed Training</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/" rel="prev" title="Mask Denoising Strategy for Pre-trained Language Models">
      <i class="fa fa-chevron-left"></i> Mask Denoising Strategy for Pre-trained Language Models
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2022/05/13/Large-Language-Models-for-Programming-Languages/" rel="next" title="Large Language Models for Programming Languages">
      Large Language Models for Programming Languages <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Parallelism"><span class="nav-number">1.</span> <span class="nav-text">Training Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Parallelism"><span class="nav-number">1.1.</span> <span class="nav-text">Data Parallelism</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Parallelism"><span class="nav-number">1.2.</span> <span class="nav-text">Model Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MLP"><span class="nav-number">1.2.1.</span> <span class="nav-text">MLP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attention"><span class="nav-number">1.2.2.</span> <span class="nav-text">Self-Attention</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pipeline-Parallelism"><span class="nav-number">1.3.</span> <span class="nav-text">Pipeline Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPipe"><span class="nav-number">1.3.1.</span> <span class="nav-text">GPipe</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PipeDream"><span class="nav-number">1.3.2.</span> <span class="nav-text">PipeDream</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sequence-Parallelism"><span class="nav-number">1.4.</span> <span class="nav-text">Sequence Parallelism</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memory-Efficient-Optimizer-ZeRO"><span class="nav-number">1.5.</span> <span class="nav-text">Memory-Efficient Optimizer (ZeRO)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch-FSDP"><span class="nav-number">1.6.</span> <span class="nav-text">Pytorch FSDP</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mixed-Precision-Training"><span class="nav-number">2.</span> <span class="nav-text">Mixed Precision Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mixed-Precision"><span class="nav-number">2.1.</span> <span class="nav-text">Mixed Precision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-loss-scaling"><span class="nav-number">2.2.</span> <span class="nav-text">Dynamic loss scaling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Memory-efficient-methods"><span class="nav-number">3.</span> <span class="nav-text">Memory-efficient methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU-Offload"><span class="nav-number">3.1.</span> <span class="nav-text">CPU Offload</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Activation-Recomputation"><span class="nav-number">3.2.</span> <span class="nav-text">Activation Recomputation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flash-Attention"><span class="nav-number">3.3.</span> <span class="nav-text">Flash Attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="cyk1337"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">cyk1337</p>
  <div class="site-description" itemprop="description">What is now proved was once only imagined.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cyk1337</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/";
    this.page.identifier = "2022/04/17/Efficient-Large-Scale-Distributed-Training/";
    this.page.title = "Efficient Large-Scale Distributed Training";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
