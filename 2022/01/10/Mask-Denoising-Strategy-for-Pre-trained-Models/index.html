<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Mask modeling is a crucial role in pre-training language models. This note provides a short summary.">
<meta property="og:type" content="article">
<meta property="og:title" content="Mask Denoising Strategy for Pre-trained Language Models">
<meta property="og:url" content="https://cyk1337.github.io/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/index.html">
<meta property="og:site_name" content="Yekun&#39;s Note">
<meta property="og:description" content="Mask modeling is a crucial role in pre-training language models. This note provides a short summary.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/SpanBERT-span-length.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/SpanBERT.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/SpanBERT-masking-scheme-comparison.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/MASS%20Mask.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/BART-Mask.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/T5-mask.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/T5-results-on-objectives.png">
<meta property="article:published_time" content="2022-01-10T09:31:00.000Z">
<meta property="article:modified_time" content="2024-07-08T11:47:46.835Z">
<meta property="article:author" content="Yekun Chai">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Pre-training">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/SpanBERT-span-length.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Mask Denoising Strategy for Pre-trained Language Models | Yekun's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yekun's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Machine learning notes and writeup.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="Yekun Chai">
      <meta itemprop="description" content="Language is not just words.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yekun's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Mask Denoising Strategy for Pre-trained Language Models
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-10 17:31:00" itemprop="dateCreated datePublished" datetime="2022-01-10T17:31:00+08:00">2022-01-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/LLM/Pre-training/" itemprop="url" rel="index"><span itemprop="name">Pre-training</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Mask modeling is a crucial role in pre-training language models. This note provides a short summary.</p>
<span id="more"></span>
<h2 id="BERT-RoBERTa-Mask"><a href="#BERT-RoBERTa-Mask" class="headerlink" title="BERT/RoBERTa Mask"></a>BERT/RoBERTa Mask</h2><p>BERT<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: Google BERT](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/create_pretraining_data.py#L342)
">[1]</span></a></sup> applies <strong>masked language modeling (MLM)</strong> on the sequence of text segments. Specifically, BERT uses a uniform masking rate of 15% after <strong>WordPiece tokenization</strong>, where it replace the masked tokens with<br>1) <strong>[MASK]</strong> 80% of time time,<br>2) with a random word 10% of the time, and<br>3) 10% unchanged, to bias the representation towards the actual observed word.</p>
<p>The random replacement only occurs for 15% of all tokens (<em>i.e.</em>, 10% of 15%), this does not seem to harm the model’s language understanding capacity.</p>
<div class="note info">
            <p><strong>BERT</strong> applies <strong>static masking</strong> for multiple runs ahead of time and keeps unchanged afterwards; while <strong>RoBERTa</strong> adopts <strong>dynamic masking</strong> in an on-the-fly manner during training.</p>
          </div>
<h3 id="Google-BERT-Implementation"><a href="#Google-BERT-Implementation" class="headerlink" title="Google BERT Implementation"></a>Google BERT Implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Google BERT implementation. (w/ wwm)</span></span><br><span class="line">MaskedLmInstance = collections.namedtuple(<span class="string">&quot;MaskedLmInstance&quot;</span>,</span><br><span class="line">                                          [<span class="string">&quot;index&quot;</span>, <span class="string">&quot;label&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span>(<span class="params">tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 max_predictions_per_seq, vocab_words, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates the predictions for the masked LM objective.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&quot;[CLS]&quot;</span> <span class="keyword">or</span> token == <span class="string">&quot;[SEP]&quot;</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> <span class="built_in">len</span>(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">&quot;##&quot;</span>)):</span><br><span class="line">      cand_indexes[-<span class="number">1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line"></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  output_tokens = <span class="built_in">list</span>(tokens)</span><br><span class="line"></span><br><span class="line">  num_to_predict = <span class="built_in">min</span>(max_predictions_per_seq,</span><br><span class="line">                       <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">int</span>(<span class="built_in">round</span>(<span class="built_in">len</span>(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = <span class="built_in">set</span>()</span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) + <span class="built_in">len</span>(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">&quot;[MASK]&quot;</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> <span class="built_in">len</span>(masked_lms) &lt;= num_to_predict</span><br><span class="line">  masked_lms = <span class="built_in">sorted</span>(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure>
<h3 id="Huggingface-Implementation"><a href="#Huggingface-Implementation" class="headerlink" title="Huggingface Implementation"></a>Huggingface Implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Huggingface implementation: https://github.com/huggingface/transformers/blob/d72343d2b804d0304d93bac1c1b58e0dafd5e820/src/transformers/data/data_collator.py#L606</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForLanguageModeling</span>(<span class="params">DataCollatorMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they</span></span><br><span class="line"><span class="string">    are not all of the same length.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):</span></span><br><span class="line"><span class="string">            The tokenizer used for encoding the data.</span></span><br><span class="line"><span class="string">        mlm (`bool`, *optional*, defaults to `True`):</span></span><br><span class="line"><span class="string">            Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs</span></span><br><span class="line"><span class="string">            with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked</span></span><br><span class="line"><span class="string">            tokens and the value to predict for the masked token.</span></span><br><span class="line"><span class="string">        mlm_probability (`float`, *optional*, defaults to 0.15):</span></span><br><span class="line"><span class="string">            The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.</span></span><br><span class="line"><span class="string">        pad_to_multiple_of (`int`, *optional*):</span></span><br><span class="line"><span class="string">            If set will pad the sequence to a multiple of the provided value.</span></span><br><span class="line"><span class="string">        return_tensors (`str`):</span></span><br><span class="line"><span class="string">            The type of Tensor to return. Allowable values are &quot;np&quot;, &quot;pt&quot; and &quot;tf&quot;.</span></span><br><span class="line"><span class="string">    &lt;Tip&gt;</span></span><br><span class="line"><span class="string">    For best performance, this data collator should be used with a dataset having items that are dictionaries or</span></span><br><span class="line"><span class="string">    BatchEncoding, with the `&quot;special_tokens_mask&quot;` key, as returned by a [`PreTrainedTokenizer`] or a</span></span><br><span class="line"><span class="string">    [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.</span></span><br><span class="line"><span class="string">    &lt;/Tip&gt;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: PreTrainedTokenizerBase</span><br><span class="line">    mlm: <span class="built_in">bool</span> = <span class="literal">True</span></span><br><span class="line">    mlm_probability: <span class="built_in">float</span> = <span class="number">0.15</span></span><br><span class="line">    pad_to_multiple_of: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span><br><span class="line">    tf_experimental_compile: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line">    return_tensors: <span class="built_in">str</span> = <span class="string">&quot;pt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__post_init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.mlm <span class="keyword">and</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. &quot;</span></span><br><span class="line">                <span class="string">&quot;You should pass `mlm=False` to train on causal language modeling instead.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">if</span> self.tf_experimental_compile:</span><br><span class="line">            <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">            self.tf_mask_tokens = tf.function(self.tf_mask_tokens, jit_compile=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_bernoulli</span>(<span class="params">shape, probability</span>):</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        prob_matrix = tf.fill(shape, probability)</span><br><span class="line">        <span class="keyword">return</span> tf.cast(prob_matrix - tf.random.uniform(shape, <span class="number">0</span>, <span class="number">1</span>) &gt;= <span class="number">0</span>, tf.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_mask_tokens</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, inputs: <span class="type">Any</span>, vocab_size, mask_token_id, special_tokens_mask: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        input_shape = tf.shape(inputs)</span><br><span class="line">        <span class="comment"># 1 for a special token, 0 for a normal token in the special tokens mask</span></span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)</span></span><br><span class="line">        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability) &amp; ~special_tokens_mask</span><br><span class="line">        <span class="comment"># Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens</span></span><br><span class="line">        labels = tf.where(masked_indices, inputs, -<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = self.tf_bernoulli(input_shape, <span class="number">0.8</span>) &amp; masked_indices</span><br><span class="line"></span><br><span class="line">        inputs = tf.where(indices_replaced, mask_token_id, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = self.tf_bernoulli(input_shape, <span class="number">0.1</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=tf.int64)</span><br><span class="line">        inputs = tf.where(indices_random, random_words, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Handle dict or lists with proper padding and conversion to tensor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            batch = self.tokenizer.pad(examples, return_tensors=<span class="string">&quot;tf&quot;</span>, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = &#123;</span><br><span class="line">                <span class="string">&quot;input_ids&quot;</span>: _tf_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If special token mask has been preprocessed, pop it from the dict.</span></span><br><span class="line">        special_tokens_mask = batch.pop(<span class="string">&quot;special_tokens_mask&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> self.mlm:</span><br><span class="line">            <span class="keyword">if</span> special_tokens_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                special_tokens_mask = [</span><br><span class="line">                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>)</span><br><span class="line">                    <span class="keyword">for</span> val <span class="keyword">in</span> batch[<span class="string">&quot;input_ids&quot;</span>].numpy().tolist()</span><br><span class="line">                ]</span><br><span class="line">                <span class="comment"># Cannot directly create as bool</span></span><br><span class="line">                special_tokens_mask = tf.cast(tf.convert_to_tensor(special_tokens_mask, dtype=tf.int64), tf.<span class="built_in">bool</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                special_tokens_mask = tf.cast(special_tokens_mask, tf.<span class="built_in">bool</span>)</span><br><span class="line">            batch[<span class="string">&quot;input_ids&quot;</span>], batch[<span class="string">&quot;labels&quot;</span>] = self.tf_mask_tokens(</span><br><span class="line">                tf.cast(batch[<span class="string">&quot;input_ids&quot;</span>], tf.int64),</span><br><span class="line">                special_tokens_mask=special_tokens_mask,</span><br><span class="line">                mask_token_id=self.tokenizer.mask_token_id,</span><br><span class="line">                vocab_size=<span class="built_in">len</span>(self.tokenizer),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = batch[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">            <span class="keyword">if</span> self.tokenizer.pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># Replace self.tokenizer.pad_token_id with -100</span></span><br><span class="line">                labels = tf.where(labels == self.tokenizer.pad_token_id, -<span class="number">100</span>, labels)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels = tf.identity(labels)  <span class="comment"># Makes a copy, just in case</span></span><br><span class="line">            batch[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="comment"># Handle dict or lists with proper padding and conversion to tensor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            batch = self.tokenizer.pad(examples, return_tensors=<span class="string">&quot;pt&quot;</span>, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = &#123;</span><br><span class="line">                <span class="string">&quot;input_ids&quot;</span>: _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If special token mask has been preprocessed, pop it from the dict.</span></span><br><span class="line">        special_tokens_mask = batch.pop(<span class="string">&quot;special_tokens_mask&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> self.mlm:</span><br><span class="line">            batch[<span class="string">&quot;input_ids&quot;</span>], batch[<span class="string">&quot;labels&quot;</span>] = self.torch_mask_tokens(</span><br><span class="line">                batch[<span class="string">&quot;input_ids&quot;</span>], special_tokens_mask=special_tokens_mask</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = batch[<span class="string">&quot;input_ids&quot;</span>].clone()</span><br><span class="line">            <span class="keyword">if</span> self.tokenizer.pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                labels[labels == self.tokenizer.pad_token_id] = -<span class="number">100</span></span><br><span class="line">            batch[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, special_tokens_mask: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">        labels = inputs.clone()</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)</span></span><br><span class="line">        probability_matrix = torch.full(labels.shape, self.mlm_probability)</span><br><span class="line">        <span class="keyword">if</span> special_tokens_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            special_tokens_mask = [</span><br><span class="line">                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">            ]</span><br><span class="line">            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            special_tokens_mask = special_tokens_mask.<span class="built_in">bool</span>()</span><br><span class="line"></span><br><span class="line">        probability_matrix.masked_fill_(special_tokens_mask, value=<span class="number">0.0</span>)</span><br><span class="line">        masked_indices = torch.bernoulli(probability_matrix).<span class="built_in">bool</span>()</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = torch.bernoulli(torch.full(labels.shape, <span class="number">0.8</span>)).<span class="built_in">bool</span>() &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = torch.bernoulli(torch.full(labels.shape, <span class="number">0.5</span>)).<span class="built_in">bool</span>() &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = torch.randint(<span class="built_in">len</span>(self.tokenizer), labels.shape, dtype=torch.long)</span><br><span class="line">        inputs[indices_random] = random_words[indices_random]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Handle dict or lists with proper padding and conversion to tensor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            batch = self.tokenizer.pad(examples, return_tensors=<span class="string">&quot;np&quot;</span>, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = &#123;</span><br><span class="line">                <span class="string">&quot;input_ids&quot;</span>: _numpy_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If special token mask has been preprocessed, pop it from the dict.</span></span><br><span class="line">        special_tokens_mask = batch.pop(<span class="string">&quot;special_tokens_mask&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> self.mlm:</span><br><span class="line">            batch[<span class="string">&quot;input_ids&quot;</span>], batch[<span class="string">&quot;labels&quot;</span>] = self.numpy_mask_tokens(</span><br><span class="line">                batch[<span class="string">&quot;input_ids&quot;</span>], special_tokens_mask=special_tokens_mask</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = np.copy(batch[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">            <span class="keyword">if</span> self.tokenizer.pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                labels[labels == self.tokenizer.pad_token_id] = -<span class="number">100</span></span><br><span class="line">            batch[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, special_tokens_mask: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        labels = np.copy(inputs)</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)</span></span><br><span class="line">        probability_matrix = np.full(labels.shape, self.mlm_probability)</span><br><span class="line">        <span class="keyword">if</span> special_tokens_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            special_tokens_mask = [</span><br><span class="line">                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">            ]</span><br><span class="line">            special_tokens_mask = np.array(special_tokens_mask, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            special_tokens_mask = special_tokens_mask.astype(np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">        probability_matrix[special_tokens_mask] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># Numpy doesn&#x27;t have bernoulli, so we use a binomial with 1 trial</span></span><br><span class="line">        masked_indices = np.random.binomial(<span class="number">1</span>, probability_matrix, size=probability_matrix.shape).astype(np.<span class="built_in">bool</span>)</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = np.random.binomial(<span class="number">1</span>, <span class="number">0.8</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.mask_token_id</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        <span class="comment"># indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced</span></span><br><span class="line">        indices_random = (</span><br><span class="line">            np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        )</span><br><span class="line">        random_words = np.random.randint(</span><br><span class="line">            low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64</span><br><span class="line">        )</span><br><span class="line">        inputs[indices_random] = random_words</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># w/ wwm</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForWholeWordMask</span>(<span class="params">DataCollatorForLanguageModeling</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Data collator used for language modeling that masks entire words.</span></span><br><span class="line"><span class="string">    - collates batches of tensors, honoring their tokenizer&#x27;s pad_token</span></span><br><span class="line"><span class="string">    - preprocesses batches for masked language modeling</span></span><br><span class="line"><span class="string">    &lt;Tip&gt;</span></span><br><span class="line"><span class="string">    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically</span></span><br><span class="line"><span class="string">    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will</span></span><br><span class="line"><span class="string">    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].</span></span><br><span class="line"><span class="string">    &lt;/Tip&gt;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            input_ids = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_ids = examples</span><br><span class="line">            examples = [&#123;<span class="string">&quot;input_ids&quot;</span>: e&#125; <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line"></span><br><span class="line">        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line"></span><br><span class="line">        mask_labels = []</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> examples:</span><br><span class="line">            ref_tokens = []</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> tolist(e[<span class="string">&quot;input_ids&quot;</span>]):</span><br><span class="line">                token = self.tokenizer._convert_id_to_token(<span class="built_in">id</span>)</span><br><span class="line">                ref_tokens.append(token)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-&gt; [喜，##欢]</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;chinese_ref&quot;</span> <span class="keyword">in</span> e:</span><br><span class="line">                ref_pos = tolist(e[<span class="string">&quot;chinese_ref&quot;</span>])</span><br><span class="line">                len_seq = <span class="built_in">len</span>(e[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_seq):</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> ref_pos:</span><br><span class="line">                        ref_tokens[i] = <span class="string">&quot;##&quot;</span> + ref_tokens[i]</span><br><span class="line">            mask_labels.append(self._whole_word_mask(ref_tokens))</span><br><span class="line">        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            input_ids = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_ids = examples</span><br><span class="line">            examples = [&#123;<span class="string">&quot;input_ids&quot;</span>: e&#125; <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line"></span><br><span class="line">        batch_input = _tf_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line"></span><br><span class="line">        mask_labels = []</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> examples:</span><br><span class="line">            ref_tokens = []</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> tolist(e[<span class="string">&quot;input_ids&quot;</span>]):</span><br><span class="line">                token = self.tokenizer._convert_id_to_token(<span class="built_in">id</span>)</span><br><span class="line">                ref_tokens.append(token)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-&gt; [喜，##欢]</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;chinese_ref&quot;</span> <span class="keyword">in</span> e:</span><br><span class="line">                ref_pos = tolist(e[<span class="string">&quot;chinese_ref&quot;</span>])</span><br><span class="line">                len_seq = <span class="built_in">len</span>(e[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_seq):</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> ref_pos:</span><br><span class="line">                        ref_tokens[i] = <span class="string">&quot;##&quot;</span> + ref_tokens[i]</span><br><span class="line">            mask_labels.append(self._whole_word_mask(ref_tokens))</span><br><span class="line">        batch_mask = _tf_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        inputs, labels = self.tf_mask_tokens(batch_input, batch_mask)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            input_ids = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_ids = examples</span><br><span class="line">            examples = [&#123;<span class="string">&quot;input_ids&quot;</span>: e&#125; <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line"></span><br><span class="line">        batch_input = _numpy_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line"></span><br><span class="line">        mask_labels = []</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> examples:</span><br><span class="line">            ref_tokens = []</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> tolist(e[<span class="string">&quot;input_ids&quot;</span>]):</span><br><span class="line">                token = self.tokenizer._convert_id_to_token(<span class="built_in">id</span>)</span><br><span class="line">                ref_tokens.append(token)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-&gt; [喜，##欢]</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;chinese_ref&quot;</span> <span class="keyword">in</span> e:</span><br><span class="line">                ref_pos = tolist(e[<span class="string">&quot;chinese_ref&quot;</span>])</span><br><span class="line">                len_seq = <span class="built_in">len</span>(e[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_seq):</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> ref_pos:</span><br><span class="line">                        ref_tokens[i] = <span class="string">&quot;##&quot;</span> + ref_tokens[i]</span><br><span class="line">            mask_labels.append(self._whole_word_mask(ref_tokens))</span><br><span class="line">        batch_mask = _numpy_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        inputs, labels = self.numpy_mask_tokens(batch_input, batch_mask)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_whole_word_mask</span>(<span class="params">self, input_tokens: <span class="type">List</span>[<span class="built_in">str</span>], max_predictions=<span class="number">512</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Get 0/1 labels for masked tokens with whole word mask proxy</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.tokenizer, (BertTokenizer, BertTokenizerFast)):</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. &quot;</span></span><br><span class="line">                <span class="string">&quot;Please refer to the documentation for more information.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        cand_indexes = []</span><br><span class="line">        <span class="keyword">for</span> (i, token) <span class="keyword">in</span> <span class="built_in">enumerate</span>(input_tokens):</span><br><span class="line">            <span class="keyword">if</span> token == <span class="string">&quot;[CLS]&quot;</span> <span class="keyword">or</span> token == <span class="string">&quot;[SEP]&quot;</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span> token.startswith(<span class="string">&quot;##&quot;</span>):</span><br><span class="line">                cand_indexes[-<span class="number">1</span>].append(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cand_indexes.append([i])</span><br><span class="line"></span><br><span class="line">        random.shuffle(cand_indexes)</span><br><span class="line">        num_to_predict = <span class="built_in">min</span>(max_predictions, <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">int</span>(<span class="built_in">round</span>(<span class="built_in">len</span>(input_tokens) * self.mlm_probability))))</span><br><span class="line">        masked_lms = []</span><br><span class="line">        covered_indexes = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) &gt;= num_to_predict:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">            <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) + <span class="built_in">len</span>(index_set) &gt; num_to_predict:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            is_any_index_covered = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">                <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">                    is_any_index_covered = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">                covered_indexes.add(index)</span><br><span class="line">                masked_lms.append(index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(covered_indexes) != <span class="built_in">len</span>(masked_lms):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Length of covered_indexes is not equal to length of masked_lms.&quot;</span>)</span><br><span class="line">        mask_labels = [<span class="number">1</span> <span class="keyword">if</span> i <span class="keyword">in</span> covered_indexes <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(input_tokens))]</span><br><span class="line">        <span class="keyword">return</span> mask_labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, mask_labels: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set</span></span><br><span class="line"><span class="string">        &#x27;mask_labels&#x27; means we use whole word mask (wwm), we directly mask idxs according to it&#x27;s ref.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        labels = inputs.clone()</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)</span></span><br><span class="line"></span><br><span class="line">        probability_matrix = mask_labels</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = [</span><br><span class="line">            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">        ]</span><br><span class="line">        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.<span class="built_in">bool</span>), value=<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels.eq(self.tokenizer.pad_token_id)</span><br><span class="line">            probability_matrix.masked_fill_(padding_mask, value=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        masked_indices = probability_matrix.<span class="built_in">bool</span>()</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = torch.bernoulli(torch.full(labels.shape, <span class="number">0.8</span>)).<span class="built_in">bool</span>() &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = torch.bernoulli(torch.full(labels.shape, <span class="number">0.5</span>)).<span class="built_in">bool</span>() &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = torch.randint(<span class="built_in">len</span>(self.tokenizer), labels.shape, dtype=torch.long)</span><br><span class="line">        inputs[indices_random] = random_words[indices_random]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, mask_labels: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set</span></span><br><span class="line"><span class="string">        &#x27;mask_labels&#x27; means we use whole word mask (wwm), we directly mask idxs according to it&#x27;s ref.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        input_shape = tf.shape(inputs)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        labels = tf.identity(inputs)</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)</span></span><br><span class="line"></span><br><span class="line">        masked_indices = tf.cast(mask_labels, tf.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = [</span><br><span class="line">            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels</span><br><span class="line">        ]</span><br><span class="line">        masked_indices = masked_indices &amp; ~tf.cast(special_tokens_mask, dtype=tf.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = inputs == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices = masked_indices &amp; ~padding_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens</span></span><br><span class="line">        labels = tf.where(masked_indices, inputs, -<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = self.tf_bernoulli(input_shape, <span class="number">0.8</span>) &amp; masked_indices</span><br><span class="line"></span><br><span class="line">        inputs = tf.where(indices_replaced, self.tokenizer.mask_token_id, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = self.tf_bernoulli(input_shape, <span class="number">0.1</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = tf.random.uniform(input_shape, maxval=<span class="built_in">len</span>(self.tokenizer), dtype=tf.int64)</span><br><span class="line">        inputs = tf.where(indices_random, random_words, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, mask_labels: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set</span></span><br><span class="line"><span class="string">        &#x27;mask_labels&#x27; means we use whole word mask (wwm), we directly mask idxs according to it&#x27;s ref.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        labels = np.copy(inputs)</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)</span></span><br><span class="line"></span><br><span class="line">        masked_indices = mask_labels.astype(np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = [</span><br><span class="line">            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">        ]</span><br><span class="line">        masked_indices[np.array(special_tokens_mask, dtype=np.<span class="built_in">bool</span>)] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices[padding_mask] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = np.random.binomial(<span class="number">1</span>, <span class="number">0.8</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        <span class="comment"># indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced</span></span><br><span class="line">        indices_random = (</span><br><span class="line">            np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        )</span><br><span class="line">        random_words = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.tokenizer), size=labels.shape, dtype=np.int64)</span><br><span class="line">        inputs[indices_random] = random_words[indices_random]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br></pre></td></tr></table></figure>
<h3 id="SpanBERT-Implementation"><a href="#SpanBERT-Implementation" class="headerlink" title="SpanBERT Implementation"></a>SpanBERT Implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. SpanBERT implementation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRandomMaskingScheme</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args, tokens, pad, mask_id</span>):</span></span><br><span class="line">        self.args = args</span><br><span class="line">        self.mask_ratio = <span class="built_in">getattr</span>(self.args, <span class="string">&#x27;mask_ratio&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.pad = pad</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.mask_id = mask_id</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mask</span>(<span class="params">self, sentence, tagmap=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;mask tokens for masked language model training</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sentence: 1d tensor, token list to be masked</span></span><br><span class="line"><span class="string">            mask_ratio: ratio of tokens to be masked in the sentence</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            masked_sent: masked sentence</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">        mask_num = math.ceil(sent_length * self.mask_ratio)</span><br><span class="line">        mask = np.random.choice(sent_length, mask_num, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> bert_masking(sentence, mask, self.tokens, self.pad, self.mask_id)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bert_masking</span>(<span class="params">sentence, mask, tokens, pad, mask_id</span>):</span></span><br><span class="line">    sentence = np.copy(sentence)</span><br><span class="line">    sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">    target = np.copy(sentence)</span><br><span class="line">    mask = <span class="built_in">set</span>(mask)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sent_length):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mask:</span><br><span class="line">            rand = np.random.random()</span><br><span class="line">            <span class="keyword">if</span> rand &lt; <span class="number">0.8</span>:</span><br><span class="line">                sentence[i] = mask_id</span><br><span class="line">            <span class="keyword">elif</span> rand &lt; <span class="number">0.9</span>:</span><br><span class="line">                <span class="comment"># sample random token according to input distribution</span></span><br><span class="line">                sentence[i] = np.random.choice(tokens)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target[i] = pad</span><br><span class="line">    <span class="keyword">return</span> sentence, target, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Span-Mask"><a href="#Span-Mask" class="headerlink" title="Span Mask"></a>Span Mask</h2><div class="note info">
            <p>Span masking consists of random masking, named entity masking, etc. </p><ol><li>ERNIE<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Yu et al. [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf). ArXiv abs/1904.09223 (2019)">[6]</span></a></sup> applies knowledge masking on the input sequence including <strong>entity-</strong> and <strong>phrase-</strong> level masking to inject knowledge composition.</li><li>SpanBERT<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: SpanBERT](https://github.com/facebookresearch/SpanBERT/blob/0670d8b6a38f6714b85ea7a033f16bd8cc162676/pretraining/fairseq/data/masking.py)">[2]</span></a></sup> employs random span masking under a clamped geometric distribution.</li><li>BERT-WWM<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cui, Yiming et al. [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/pdf/1906.08101.pdf)">[7]</span></a></sup> uses whole word masking (for Chinese BERT) rather than randomly masking subword pieces to retain the whole meaning of a word.</li></ol>
          </div>
<p>SpanBERT<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: SpanBERT](https://github.com/facebookresearch/SpanBERT/blob/0670d8b6a38f6714b85ea7a033f16bd8cc162676/pretraining/fairseq/data/masking.py)
">[2]</span></a></sup> iteratively samples span’s length under a (clamped) geometric distribution <script type="math/tex">\mathcal{l} \sim \textrm{Geo}(p)</script>, <em>i.e.</em>, </p>
<script type="math/tex; mode=display">P(x=p)=(1-p)^k p</script><p>which is skewed towards shorter spans ($p=0.2$). It also clips $\mathcal{l}$ with <script type="math/tex">\mathcal{l} = \min (\mathcal{l}, 10)</script>, yielding a mean span length of $\bar{\mathcal{l}}=3.8$. SpanBERT measures span length in complete words, not subword tokens, making the masked spans even longer.</p>
<p>The masking strategies are the same as BERT: masking 15% in total, where replacing 80% of tokens with [MASK], 10% with random tokens, and 10% unchanged.</p>
<p><img data-src="/notes/images/SpanBERT-span-length.png" width="40%"></p>
<p><img data-src="/notes/images/SpanBERT.png" alt="SpanBERT"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SpanBERT implementation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PairWithSpanMaskingScheme</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args, tokens, pad, mask_id, paragraph_info</span>):</span> </span><br><span class="line">        self.args = args</span><br><span class="line">        self.mask_ratio = <span class="built_in">getattr</span>(self.args, <span class="string">&#x27;mask_ratio&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.args = args</span><br><span class="line">        self.max_pair_targets = args.max_pair_targets</span><br><span class="line">        self.lower = args.span_lower</span><br><span class="line">        self.upper = args.span_upper</span><br><span class="line">        self.pad = pad</span><br><span class="line">        self.mask_id = mask_id</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.paragraph_info = paragraph_info</span><br><span class="line">        self.lens = <span class="built_in">list</span>(<span class="built_in">range</span>(self.lower, self.upper + <span class="number">1</span>))</span><br><span class="line">        self.p = args.geometric_p</span><br><span class="line">        self.len_distrib = [self.p * (<span class="number">1</span>-self.p)**(i - self.lower) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.lower, self.upper + <span class="number">1</span>)] <span class="keyword">if</span> self.p &gt;= <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.len_distrib = [x / (<span class="built_in">sum</span>(self.len_distrib)) <span class="keyword">for</span> x <span class="keyword">in</span> self.len_distrib]</span><br><span class="line">        <span class="built_in">print</span>(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mask</span>(<span class="params">self, sentence, tagmap=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;mask tokens for masked language model training</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sentence: 1d tensor, token list to be masked</span></span><br><span class="line"><span class="string">            mask_ratio: ratio of tokens to be masked in the sentence</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            masked_sent: masked sentence</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">        mask_num = math.ceil(sent_length * self.mask_ratio)</span><br><span class="line">        mask = <span class="built_in">set</span>()</span><br><span class="line">        word_piece_map = self.paragraph_info.get_word_piece_map(sentence)</span><br><span class="line">        spans = []</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(mask) &lt; mask_num:</span><br><span class="line">            span_len = np.random.choice(self.lens, p=self.len_distrib)</span><br><span class="line">            tagged_indices = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> tagmap <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                tagged_indices = [<span class="built_in">max</span>(<span class="number">0</span>, i - np.random.randint(<span class="number">0</span>, span_len)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tagmap.length()) <span class="keyword">if</span> tagmap[i]]</span><br><span class="line">                tagged_indices += [np.random.choice(sent_length)] * <span class="built_in">int</span>(<span class="built_in">len</span>(tagged_indices) == <span class="number">0</span>)</span><br><span class="line">            anchor  = np.random.choice(sent_length) <span class="keyword">if</span> np.random.rand() &gt;= self.args.tagged_anchor_prob <span class="keyword">else</span> np.random.choice(tagged_indices)</span><br><span class="line">            <span class="keyword">if</span> anchor <span class="keyword">in</span> mask:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># find word start, end</span></span><br><span class="line">            left1, right1 = self.paragraph_info.get_word_start(sentence, anchor, word_piece_map), self.paragraph_info.get_word_end(sentence, anchor, word_piece_map)</span><br><span class="line">            spans.append([left1, left1])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left1, right1):</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(mask) &gt;= mask_num:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                mask.add(i)</span><br><span class="line">                spans[-<span class="number">1</span>][-<span class="number">1</span>] = i</span><br><span class="line">            num_words = <span class="number">1</span></span><br><span class="line">            right2 = right1</span><br><span class="line">            <span class="keyword">while</span> num_words &lt; span_len <span class="keyword">and</span> right2 &lt; <span class="built_in">len</span>(sentence) <span class="keyword">and</span> <span class="built_in">len</span>(mask) &lt; mask_num:</span><br><span class="line">                <span class="comment"># complete current word</span></span><br><span class="line">                left2 = right2</span><br><span class="line">                right2 = self.paragraph_info.get_word_end(sentence, right2, word_piece_map)</span><br><span class="line">                num_words += <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left2, right2):</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(mask) &gt;= mask_num:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    mask.add(i)</span><br><span class="line">                    spans[-<span class="number">1</span>][-<span class="number">1</span>] = i</span><br><span class="line">        sentence, target, pair_targets = span_masking(sentence, spans, self.tokens, self.pad, self.mask_id, self.max_pair_targets, mask, replacement=self.args.replacement_method, endpoints=self.args.endpoints)</span><br><span class="line">        <span class="keyword">if</span> self.args.return_only_spans:</span><br><span class="line">            pair_targets = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> sentence, target, pair_targets</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParagraphInfo</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dictionary</span>):</span></span><br><span class="line">        self.dictionary = dictionary</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_piece_map</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [self.dictionary.is_start_word(i) <span class="keyword">for</span> i <span class="keyword">in</span> sentence]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_at_k</span>(<span class="params">self, sentence, left, right, k, word_piece_map=<span class="literal">None</span></span>):</span></span><br><span class="line">        num_words = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> num_words &lt; k <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(sentence):</span><br><span class="line">            <span class="comment"># complete current word</span></span><br><span class="line">            left = right</span><br><span class="line">            right = self.get_word_end(sentence, right, word_piece_map)</span><br><span class="line">            num_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left, right</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_start</span>(<span class="params">self, sentence, anchor, word_piece_map=<span class="literal">None</span></span>):</span></span><br><span class="line">        word_piece_map = word_piece_map <span class="keyword">if</span> word_piece_map <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.get_word_piece_map(sentence)</span><br><span class="line">        left  = anchor</span><br><span class="line">        <span class="keyword">while</span> left &gt; <span class="number">0</span> <span class="keyword">and</span> word_piece_map[left] == <span class="literal">False</span>:</span><br><span class="line">            left -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br><span class="line">    <span class="comment"># word end is next word start</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_end</span>(<span class="params">self, sentence, anchor, word_piece_map=<span class="literal">None</span></span>):</span></span><br><span class="line">        word_piece_map = word_piece_map <span class="keyword">if</span> word_piece_map <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.get_word_piece_map(sentence)</span><br><span class="line">        right = anchor + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> right &lt; <span class="built_in">len</span>(sentence) <span class="keyword">and</span> word_piece_map[right] == <span class="literal">False</span>:</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> right</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">span_masking</span>(<span class="params">sentence, spans, tokens, pad, mask_id, pad_len, mask, replacement=<span class="string">&#x27;word_piece&#x27;</span>, endpoints=<span class="string">&#x27;external&#x27;</span></span>):</span></span><br><span class="line">    sentence = np.copy(sentence)</span><br><span class="line">    sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">    target = np.full(sent_length, pad)</span><br><span class="line">    pair_targets = []</span><br><span class="line">    spans = merge_intervals(spans)</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(mask) == <span class="built_in">sum</span>([e - s + <span class="number">1</span> <span class="keyword">for</span> s,e <span class="keyword">in</span> spans])</span><br><span class="line">    <span class="comment"># print(list(enumerate(sentence)))</span></span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> spans:</span><br><span class="line">        lower_limit = <span class="number">0</span> <span class="keyword">if</span> endpoints == <span class="string">&#x27;external&#x27;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        upper_limit = sent_length - <span class="number">1</span> <span class="keyword">if</span> endpoints == <span class="string">&#x27;external&#x27;</span> <span class="keyword">else</span> sent_length</span><br><span class="line">        <span class="keyword">if</span> start &gt; lower_limit <span class="keyword">and</span> end &lt; upper_limit:</span><br><span class="line">            <span class="keyword">if</span> endpoints == <span class="string">&#x27;external&#x27;</span>:</span><br><span class="line">                pair_targets += [[start - <span class="number">1</span>, end + <span class="number">1</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pair_targets += [[start, end]]</span><br><span class="line">            pair_targets[-<span class="number">1</span>] += [sentence[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>)]</span><br><span class="line">        rand = np.random.random()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">assert</span> i <span class="keyword">in</span> mask</span><br><span class="line">            target[i] = sentence[i]</span><br><span class="line">            <span class="keyword">if</span> replacement == <span class="string">&#x27;word_piece&#x27;</span>:</span><br><span class="line">                rand = np.random.random()</span><br><span class="line">            <span class="keyword">if</span> rand &lt; <span class="number">0.8</span>:</span><br><span class="line">                sentence[i] = mask_id</span><br><span class="line">            <span class="keyword">elif</span> rand &lt; <span class="number">0.9</span>:</span><br><span class="line">                <span class="comment"># sample random token according to input distribution</span></span><br><span class="line">                sentence[i] = np.random.choice(tokens)</span><br><span class="line">    pair_targets = pad_to_len(pair_targets, pad, pad_len + <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># if pair_targets is None:</span></span><br><span class="line">    <span class="keyword">return</span> sentence, target, pair_targets</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_intervals</span>(<span class="params">intervals</span>):</span></span><br><span class="line">    intervals = <span class="built_in">sorted</span>(intervals, key=<span class="keyword">lambda</span> x : x[<span class="number">0</span>])</span><br><span class="line">    merged = []</span><br><span class="line">    <span class="keyword">for</span> interval <span class="keyword">in</span> intervals:</span><br><span class="line">        <span class="comment"># if the list of merged intervals is empty or if the current</span></span><br><span class="line">        <span class="comment"># interval does not overlap with the previous, simply append it.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> merged <span class="keyword">or</span> merged[-<span class="number">1</span>][<span class="number">1</span>] + <span class="number">1</span> &lt; interval[<span class="number">0</span>]:</span><br><span class="line">            merged.append(interval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># otherwise, there is overlap, so we merge the current and previous</span></span><br><span class="line">        <span class="comment"># intervals.</span></span><br><span class="line">            merged[-<span class="number">1</span>][<span class="number">1</span>] = <span class="built_in">max</span>(merged[-<span class="number">1</span>][<span class="number">1</span>], interval[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> merged</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_to_len</span>(<span class="params">pair_targets, pad, max_pair_target_len</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(pair_targets)):</span><br><span class="line">        pair_targets[i] = pair_targets[i][:max_pair_target_len]</span><br><span class="line">        this_len = <span class="built_in">len</span>(pair_targets[i])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(max_pair_target_len - this_len):</span><br><span class="line">            pair_targets[i].append(pad)</span><br><span class="line">    <span class="keyword">return</span> pair_targets</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/SpanBERT-masking-scheme-comparison.png" alt="Results of SpanBERT mask scheme."></p>
<p>It can be seen from the table that with the exception of coreference resolution, masking random spans is preferable to other strategies. Although <strong>linguistic masking schemes (named entities and noun phrases)</strong> are often competitive with random spans, their performance is not consistent. For <strong>coreference resolution</strong>, <strong>masking random subword toekns</strong> is preferable to any form of span masking.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NER mask</span></span><br><span class="line">class NERSpanMaskingScheme(object):</span><br><span class="line">    def __init__(self, args, tokens, pad, mask_id, paragraph_info): </span><br><span class="line">        self.args = args</span><br><span class="line">        self.mask_ratio = getattr(self.args, <span class="string">&#x27;mask_ratio&#x27;</span>, None)</span><br><span class="line">        self.max_pair_targets = args.max_pair_targets</span><br><span class="line">        self.lower = args.span_lower</span><br><span class="line">        self.upper = args.span_upper</span><br><span class="line">        self.pad = pad</span><br><span class="line">        self.mask_id = mask_id</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.paragraph_info = paragraph_info</span><br><span class="line">        self.lens = list(range(self.lower, self.upper + 1))</span><br><span class="line">        self.p = args.geometric_p</span><br><span class="line">        self.len_distrib = [self.p * (1-self.p)**(i - self.lower) <span class="keyword">for</span> i <span class="keyword">in</span> range(self.lower, self.upper + 1)] <span class="keyword">if</span> self.p &gt;= 0 <span class="keyword">else</span> None</span><br><span class="line">        self.len_distrib = [x / (sum(self.len_distrib)) <span class="keyword">for</span> x <span class="keyword">in</span> self.len_distrib]</span><br><span class="line">        <span class="built_in">print</span>(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line">    def mask_random_span(self, sentence, mask_num, word_piece_map, spans, mask, span_len, anchor):</span><br><span class="line">        <span class="comment"># find word start, end</span></span><br><span class="line">        left1, right1 = self.paragraph_info.get_word_start(sentence, anchor, word_piece_map), self.paragraph_info.get_word_end(sentence, anchor, word_piece_map)</span><br><span class="line">        spans.append([left1, left1])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(left1, right1):</span><br><span class="line">            <span class="keyword">if</span> len(mask) &gt;= mask_num:</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            mask.add(i)</span><br><span class="line">            spans[-1][-1] = i</span><br><span class="line">        num_words = 1</span><br><span class="line">        right2 = right1</span><br><span class="line">        <span class="keyword">while</span> num_words &lt; span_len and right2 &lt; len(sentence) and len(mask) &lt; mask_num:</span><br><span class="line">            <span class="comment"># complete current word</span></span><br><span class="line">            left2 = right2</span><br><span class="line">            right2 = self.paragraph_info.get_word_end(sentence, right2, word_piece_map)</span><br><span class="line">            num_words += 1</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(left2, right2):</span><br><span class="line">                <span class="keyword">if</span> len(mask) &gt;= mask_num:</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line">                mask.add(i)</span><br><span class="line">                spans[-1][-1] = i</span><br><span class="line"></span><br><span class="line">    def mask_entity(self, sentence, mask_num, word_piece_map, spans, mask, entity_spans):</span><br><span class="line">        <span class="keyword">if</span> len(entity_spans) &gt; 0:</span><br><span class="line">            entity_span = entity_spans[np.random.choice(range(len(entity_spans)))]</span><br><span class="line">            spans.append([entity_span[0], entity_span[0]])</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(entity_span[0], entity_span[1] + 1):</span><br><span class="line">                <span class="keyword">if</span> len(mask) &gt;= mask_num:</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line">                spans[-1][-1] = idx</span><br><span class="line">                mask.add(idx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def mask(self, sentence, entity_map=None):</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;mask tokens for masked language model training</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sentence: 1d tensor, token list to be masked</span></span><br><span class="line"><span class="string">            mask_ratio: ratio of tokens to be masked in the sentence</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            masked_sent: masked sentence</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        sent_length = len(sentence)</span><br><span class="line">        mask_num = math.ceil(sent_length * self.mask_ratio)</span><br><span class="line">        mask = <span class="built_in">set</span>()</span><br><span class="line">        word_piece_map = self.paragraph_info.get_word_piece_map(sentence)</span><br><span class="line">        <span class="comment"># get entity spans</span></span><br><span class="line">        entity_spans, spans = [], []</span><br><span class="line">        new_entity = True</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(entity_map.length()):</span><br><span class="line">            <span class="keyword">if</span> entity_map[i] and new_entity:</span><br><span class="line">                entity_spans.append([i, i])</span><br><span class="line">                new_entity = False</span><br><span class="line">            <span class="keyword">elif</span> entity_map[i] and not new_entity:</span><br><span class="line">                entity_spans[-1][-1] = i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_entity = True</span><br><span class="line">        <span class="keyword">while</span> len(mask) &lt; mask_num:</span><br><span class="line">            <span class="keyword">if</span> np.random.random() &lt;= self.args.ner_masking_prob:</span><br><span class="line">                self.mask_entity(sentence, mask_num, word_piece_map, spans, mask, entity_spans)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                span_len = np.random.choice(self.lens, p=self.len_distrib)</span><br><span class="line">                anchor  = np.random.choice(sent_length)</span><br><span class="line">                <span class="keyword">if</span> anchor <span class="keyword">in</span> mask:</span><br><span class="line">                    <span class="built_in">continue</span></span><br><span class="line">                self.mask_random_span(sentence, mask_num, word_piece_map, spans, mask, span_len, anchor)</span><br><span class="line">        sentence, target, pair_targets = span_masking(sentence, spans, self.tokens, self.pad, self.mask_id, self.max_pair_targets, mask, replacement=self.args.replacement_method, endpoints=self.args.endpoints)</span><br><span class="line">        <span class="keyword">if</span> self.args.return_only_spans:</span><br><span class="line">            pair_targets = None</span><br><span class="line">        <span class="built_in">return</span> sentence, target, pair_targets</span><br></pre></td></tr></table></figure>
<h2 id="MASS-Mask"><a href="#MASS-Mask" class="headerlink" title="MASS Mask"></a>MASS Mask</h2><p>MASS<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Song, Kaitao et al. [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/pdf/1905.02450.pdf). ICML (2019).
">[3]</span></a></sup> encoder replaces each masked token by a special [MASK] token, leading to unchanged length overall. Then the decoder predicts the masked tokens autoregressively.</p>
<p><img data-src="/notes/images/MASS Mask.png" alt="MASS Mask"></p>
<h2 id="BART-Mask"><a href="#BART-Mask" class="headerlink" title="BART Mask"></a>BART Mask</h2><p>BART<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lewis, Mike et al. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://aclanthology.org/2020.acl-main.703.pdf). ACL 2020.
">[4]</span></a></sup> replaces corrputed continuous spans of the encoder input as single [MASK], and trains the decoder in an autogressive way using a transformer encoder-decoder architecture.</p>
<p><img data-src="/notes/images/BART-Mask.png" alt="BART masking"></p>
<p>BART allows any type of document corruption, including:</p>
<ul>
<li><strong>Token Masking</strong>: BERT masking.</li>
<li><strong>Token Deletion</strong>: random tokens are deleted from the input.</li>
<li><strong>Text Infilling</strong>: amounts of text spans are corrupted, with span length drawn from a Poission distribution ($\lambda=3$). Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of [MASK] tokens.</li>
<li><strong>Sentence Permutation</strong>: Divide a document into peices of sentences based on full stops, and randomly shuffle them.</li>
<li><strong>Document Rotation</strong>: uniformly chose a token at random to rotate the document.</li>
</ul>
<h2 id="T5-Span-Mask"><a href="#T5-Span-Mask" class="headerlink" title="T5 Span Mask"></a>T5 Span Mask</h2><p>T5<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Raffel, Colin et al. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf). JMLR 2020.
">[5]</span></a></sup> replaces <span class="label success">with unique sentinel</span> <span class="label warning"> the corrupted spans</span> in the input sequence, and predicts the <span class="label success">concatenation of corrupted spans prefixed by the sentinal token</span> used in the input. Specifically, T5 first <strong>replaces the entirety of each consecutive span of corrupted tokens with a unique mask token</strong>. Then, the target sequence becomes the <strong>concatenation of the corrupted spans, each prefixed by the mask token</strong> used to replace it in the input.</p>
<p><img data-src="/notes/images/T5-mask.png" width="60%"></p>
<p><img data-src="/notes/images/T5-results-on-objectives.png" alt=""></p>
<p>As shown in the table, BERT-syle objective simply replaces 15% of the input tokens without the original random token swapping step, and reconstruct the original uncorrupted sequence.</p>
<p>The <span class="label primary">first two rows</span> (<em>i.e.,</em> BERT-style and MASS-style objectives) predict the entire uncorrupted text span which <span class="label danger">requires self-attention over long sequences in the decoder</span>. To avoid this, T5 applies the strategies in the last two rows. The <span class="label primary">last row</span>(<em>i.e.</em>, Drop corrupted tokens) simply <span class="label primary">drops the corrupted tokens from the input sequence completely</span> and task the model with reconstructing the dropped tokens in order. </p>
<p>It can be seen from the table that <span class="label success">"dropping corrupted spans"</span> completely produced a small improvement in the GLUE score thanks to the significatly higher score on CoLA.<br>The <span class="label primary">first two rows</span> (<em>i.e.,</em> BERT-style and MASS-style objectives) predict the entire uncorrupted text span which <span class="label danger">requires self-attention over long sequences in the decoder</span>. To avoid this, T5 applies the strategies in the last two rows. The <span class="label primary">last row</span>(<em>i.e.</em>, Drop corrupted tokens) simply <span class="label primary">drops the corrupted tokens from the input sequence completely</span> and task the model with reconstructing the dropped tokens in order. (60.45 vs avg. baseline 53.84). However, dropping tokens completely performed worse than replacing with sentinel tokens on SuperGLUE. The last two rows’ variants <span class="label success">make the target sequence shorter and consequently make training faster</span>. </p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;chai2022mask-PTMs,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Mask Strategy for Pre-trained Models&#125;&#125;,</span><br><span class="line">  year = &#123;2022&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/create_pretraining_data.py#L342">GitHub: Google BERT</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/SpanBERT/blob/0670d8b6a38f6714b85ea7a033f16bd8cc162676/pretraining/fairseq/data/masking.py">GitHub: SpanBERT</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Song, Kaitao et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.02450.pdf">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>. ICML (2019).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lewis, Mike et al. <a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.703.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>. ACL 2020.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raffel, Colin et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. JMLR 2020.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Yu et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.09223.pdf">ERNIE: Enhanced Representation through Knowledge Integration</a>. ArXiv abs/1904.09223 (2019)<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cui, Yiming et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.08101.pdf">Pre-Training with Whole Word Masking for Chinese BERT</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/LLM/" rel="tag"># LLM</a>
              <a href="/notes/tags/Pre-training/" rel="tag"># Pre-training</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2021/11/29/Subword-Tokenization-in-NLP/" rel="prev" title="Subword Tokenization in Natural Language Processing">
      <i class="fa fa-chevron-left"></i> Subword Tokenization in Natural Language Processing
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/" rel="next" title="Efficient Large-Scale Distributed Training">
      Efficient Large-Scale Distributed Training <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT-RoBERTa-Mask"><span class="nav-number">1.</span> <span class="nav-text">BERT&#x2F;RoBERTa Mask</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Google-BERT-Implementation"><span class="nav-number">1.1.</span> <span class="nav-text">Google BERT Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Huggingface-Implementation"><span class="nav-number">1.2.</span> <span class="nav-text">Huggingface Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SpanBERT-Implementation"><span class="nav-number">1.3.</span> <span class="nav-text">SpanBERT Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Span-Mask"><span class="nav-number">2.</span> <span class="nav-text">Span Mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MASS-Mask"><span class="nav-number">3.</span> <span class="nav-text">MASS Mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BART-Mask"><span class="nav-number">4.</span> <span class="nav-text">BART Mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#T5-Span-Mask"><span class="nav-number">5.</span> <span class="nav-text">T5 Span Mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">6.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yekun Chai"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">Yekun Chai</p>
  <div class="site-description" itemprop="description">Language is not just words.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yekun Chai</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/";
    this.page.identifier = "2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/";
    this.page.title = "Mask Denoising Strategy for Pre-trained Language Models";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
