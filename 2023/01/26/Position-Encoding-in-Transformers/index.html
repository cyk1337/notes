<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="We summarize the positional encoding approaches in transformers. Summary    PE Relative Trainable Each Layer Extrapolation     Sinusoidal ✘ ✘ ✘ ✘   T5 bias ✔ ✔ ✔ ✔   RoPE ✔ ✔ ✔ ✘   ALiBi ✔ ✘ ✔ ✔   KER">
<meta property="og:type" content="article">
<meta property="og:title" content="Inductive Positions in Transformers">
<meta property="og:url" content="https://cyk1337.github.io/notes/2023/01/26/Position-Encoding-in-Transformers/index.html">
<meta property="og:site_name" content="Yekun&#39;s Note">
<meta property="og:description" content="We summarize the positional encoding approaches in transformers. Summary    PE Relative Trainable Each Layer Extrapolation     Sinusoidal ✘ ✘ ✘ ✘   T5 bias ✔ ✔ ✔ ✔   RoPE ✔ ✔ ✔ ✘   ALiBi ✔ ✘ ✔ ✔   KER">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Transformer-positional-encoding.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/RoPE.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ALiBi.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ALiBI_perf.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/XPOS.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/RandomPos.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/RandomPosResults.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/NoPos.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/NoPosResults.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Position%20Interpolation.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/PI-comp.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/ntk-rope-trend.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Dynamic-NTK-RoPE.png">
<meta property="article:published_time" content="2023-01-26T07:44:00.000Z">
<meta property="article:modified_time" content="2024-07-08T12:01:01.565Z">
<meta property="article:author" content="Yekun Chai">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/Transformer-positional-encoding.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2023/01/26/Position-Encoding-in-Transformers/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Inductive Positions in Transformers | Yekun's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yekun's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Machine learning notes and writeup.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2023/01/26/Position-Encoding-in-Transformers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="Yekun Chai">
      <meta itemprop="description" content="Language is not just words.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yekun's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Inductive Positions in Transformers
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-01-26 15:44:00" itemprop="dateCreated datePublished" datetime="2023-01-26T15:44:00+08:00">2023-01-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2023/01/26/Position-Encoding-in-Transformers/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/01/26/Position-Encoding-in-Transformers/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>We summarize the positional encoding approaches in transformers.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>PE</strong></th>
<th style="text-align:center"><strong>Relative</strong></th>
<th style="text-align:center"><strong>Trainable</strong></th>
<th style="text-align:center"><strong>Each Layer</strong></th>
<th style="text-align:center"><strong>Extrapolation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Sinusoidal</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
</tr>
<tr>
<td style="text-align:center">T5 bias</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">RoPE</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
</tr>
<tr>
<td style="text-align:center">ALiBi</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">KERPLE</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Sandwich</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">xPos</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><h3 id="Sinusoidal-Position-Embeddings"><a href="#Sinusoidal-Position-Embeddings" class="headerlink" title="Sinusoidal Position Embeddings"></a>Sinusoidal Position Embeddings</h3><p>Sinusoidal position embeddings<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “[Attention is All you Need](https://arxiv.org/pdf/1706.03762.pdf).” NeurIPS (2017).
">[1]</span></a></sup> are constantly encoded vectors to be added on token embeddings of the first transformer layer. </p>
<script type="math/tex; mode=display">\text{PE}_{(\text{pos}, 2i)} = \sin(\frac{\text{pos}}{10000^{2i/d_\text{model}}})</script><script type="math/tex; mode=display">\text{PE}_{(\text{pos}, 2i+1)} = \cos(\frac{\text{pos}}{10000^{2i/d_\text{model}}})</script><p>where $\text{pos}$ is the position in the sentence and $i$ is the order along the  embedding vector dimension. Assume this allows to learn to attend by relative positions, since for and fixed offset $k$, <script type="math/tex">\text{PE}_{\text{pos}+k}</script> can be represented as the linear function of <script type="math/tex">\text{PE}_{\text{pos}}</script> .</p>
<p><img data-src="/notes/images/Transformer-positional-encoding.png" alt="Position encoding"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Positional encoding layer in PyTorch</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0.</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># generate with maximum length</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>, d_model, <span class="number">2</span>) * - (math.log(<span class="number">1e4</span>) / d_model))</span><br><span class="line">        pe[:, ::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        seq_len = x.size(<span class="number">1</span>) <span class="comment"># take the sequence length</span></span><br><span class="line">        x = x + Variable(self.pe[:, :seq_len], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li>Refer to <a href="/notes/2019/01/22/NLP/Attention-in-a-nutshell/#Absolute-Positional-Encoding">Attention in a Nutshell</a>.</li><li>Relative position in Transformer-XL: refer to <a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/#Relative-positional-encoding">Transformer Variants: A Peek</a></li></ul>
          </div>
<h3 id="Rotary-Position-Embedding-RoPE"><a href="#Rotary-Position-Embedding-RoPE" class="headerlink" title="Rotary Position Embedding (RoPE)"></a>Rotary Position Embedding (RoPE)</h3><p>Rotary Position Embedding (RoPE) <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Su, Jianlin, Yu Lu, Shengfeng Pan, Bo Wen and Yunfeng Liu. “[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf).” ArXiv abs/2104.09864 (2021).
">[3]</span></a></sup><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Rotary Embeddings: A Relative Revolution](https://blog.eleuther.ai/rotary-embeddings/)
">[4]</span></a></sup> proposes to use complex numbers as the base field of encoding space. Instead of working in $\mathbb{R}^d$, it uses consecutive pairs of elements of the query and key vectors in $\mathbb{C}^{d/2}$ to be  a single complex number.</p>
<p>Specifically, instead of viewing <script type="math/tex">\mathbf{q}=(q_1,q_2,q_3,q_4,\ldots,q_{d})</script> as a $d$-dimensional real vector, RoPE views it as <script type="math/tex">\mathbf{q}=(q_1+iq_2, q_3+iq_4,\ldots q_{d-1} + iq_{d})\in\mathbb{C}^{d/2}</script>. If $d$ is odd, RoPE pads it with a dummy coordinate to ensure things line up correctly. Alternatives, it simply increases $d$ by one.</p>
<h4 id="Derivation"><a href="#Derivation" class="headerlink" title="Derivation"></a>Derivation</h4><p>The complex number format of RoPE is written as:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{q}, m) = R_f(\mathbf{q}, m)e^{i\Theta_f(\mathbf{q}, m)}=\mathbf{q}e^{i(\Theta(\mathbf{q})+m\mathbf{\theta})} = \sum_{j=1}^{d/2} q_je^{im\theta_j} \vec{e_j}
\end{align}</script><p>It is convenient to convert into matrix equation:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{q}, m) =
\begin{pmatrix}
M_1 & & & \\
& M_2 & & \\
& & \ddots & \\
& & & M_{d/2}
\end{pmatrix}
\begin{pmatrix}
q_1\\
q_2\\
\vdots\\
q_d
\end{pmatrix} = \mathbf{\Theta_m Q_m} = \mathbf{\Theta_m W_q X_m}
\end{align}</script><p>where <script type="math/tex">M_j=\begin{pmatrix}\cos m\theta_j & -\sin m\theta_j \\sin m\theta_j & \cos m\theta_j\end{pmatrix}</script> , $\mathbf{\Theta_m}$ is the block diagonal rotation matrix, $\mathbf{W_q}$ is learned query weights, and $\mathbf{X_m}$ is the embedding of the $m$-th token.</p>
<p>Due to the high computation cost of sparse matrix, it is implemented as:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{q}, m) = \begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1} 
\end{pmatrix}\odot\begin{pmatrix}\cos m\theta_0 \\ \cos m\theta_0 \\ \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2-1} \\ \cos m\theta_{d/2-1} 
\end{pmatrix} + \begin{pmatrix}-q_1 \\ q_0 \\ -q_3 \\ q_2 \\ \vdots \\ -q_{d-1} \\ q_{d-2} 
\end{pmatrix}\odot\begin{pmatrix}\sin m\theta_0 \\ \sin m\theta_0 \\ \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2-1} \\ \sin m\theta_{d/2-1} 
\end{pmatrix}
\end{align}</script><p>where $\odot$ denotes the element-wise product (*).</p>
<p><img data-src="/notes/images/RoPE.png" alt="RoPE"></p>
<ul>
<li>Extension to Multiple Dimensions</li>
</ul>
<script type="math/tex; mode=display">\begin{align*} \langle f(\mathbf{q}, m, i),f(\mathbf{k}, n, j) \rangle &= \langle f_1(\mathbf{q}{:d/2}, m),f_1(\mathbf{k}{:d/2}, n) \rangle + \langle f_2(\mathbf{q}{d/2:}, i),f_2(\mathbf{k}{d/2:}, j) \rangle \ &= g_1(\mathbf{q}{:d/2}, \mathbf{k}{:d/2}, m - n) + g_2(\mathbf{q}{d/2:}, \mathbf{k}{d/2:}, i - j) \ &= g(\mathbf{q}, \mathbf{k}, m - n, i - j) \end{align*}</script><div class="note info">
            <p><strong>Difference from sinusoidal embedding</strong><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Rotary Embeddings: A Relative Revolution](https://blog.eleuther.ai/rotary-embeddings/)">[4]</span></a></sup></p><ol><li>Sinusoidal embeddings apply to each coordinate individually, while RoPE mixes pairs of coordinates.</li><li>Sinusoidal embeddings add a $\cos(m\theta)$ or $\sin(m\theta)$ term, while RoPE uses a multiplicative factor.</li></ol>
          </div>
<h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># source: https://huggingface.co/transformers/v4.8.0/_modules/transformers/models/roformer/modeling_roformer.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian-&gt;RoFormer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoFormerSinusoidalPositionalEmbedding</span>(<span class="params">nn.Embedding</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;This module produces sinusoidal positional embeddings of any length.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_positions: <span class="built_in">int</span>, embedding_dim: <span class="built_in">int</span>, padding_idx: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(num_positions, embedding_dim)</span><br><span class="line">        self.weight = self._init_weight(self.weight)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weight</span>(<span class="params">out: nn.Parameter</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in</span></span><br><span class="line"><span class="string">        the 2nd half of the vector. [dim // 2:]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n_pos, dim = out.shape</span><br><span class="line">        position_enc = np.array(</span><br><span class="line">            [[pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * (j // <span class="number">2</span>) / dim) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(dim)] <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(n_pos)]</span><br><span class="line">        )</span><br><span class="line">        out.requires_grad = <span class="literal">False</span>  <span class="comment"># set early to avoid an error in pytorch-1.8+</span></span><br><span class="line">        sentinel = dim // <span class="number">2</span> <span class="keyword">if</span> dim % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> (dim // <span class="number">2</span>) + <span class="number">1</span></span><br><span class="line">        out[:, <span class="number">0</span>:sentinel] = torch.FloatTensor(np.sin(position_enc[:, <span class="number">0</span>::<span class="number">2</span>]))</span><br><span class="line">        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, <span class="number">1</span>::<span class="number">2</span>]))</span><br><span class="line">        out.detach_()</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids_shape: torch.Size, past_key_values_length: <span class="built_in">int</span> = <span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;`input_ids_shape` is expected to be [bsz x seqlen].&quot;&quot;&quot;</span></span><br><span class="line">        bsz, seq_len = input_ids_shape[:<span class="number">2</span>]</span><br><span class="line">        positions = torch.arange(</span><br><span class="line">            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().forward(positions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rotary_position_embeddings</span>(<span class="params">sinusoidal_pos, query_layer, key_layer, value_layer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># https://kexue.fm/archives/8265</span></span><br><span class="line">    <span class="comment"># sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]</span></span><br><span class="line">    <span class="comment"># cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]</span></span><br><span class="line">    sin, cos = sinusoidal_pos.chunk(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># sin [θ0,θ1,θ2......θd/2-1] -&gt; sin_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]</span></span><br><span class="line">    sin_pos = torch.repeat_interleave(sin, <span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># cos [θ0,θ1,θ2......θd/2-1] -&gt; cos_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]</span></span><br><span class="line">    cos_pos = torch.repeat_interleave(cos, <span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]</span></span><br><span class="line">    rotate_half_query_layer = torch.stack([-query_layer[..., <span class="number">1</span>::<span class="number">2</span>], query_layer[..., ::<span class="number">2</span>]], dim=-<span class="number">1</span>).reshape_as(</span><br><span class="line">        query_layer</span><br><span class="line">    )</span><br><span class="line">    query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos</span><br><span class="line">    <span class="comment"># rotate_half_key_layer [-k1,k0,-k3,k2......,-kd-1,kd-2]</span></span><br><span class="line">    rotate_half_key_layer = torch.stack([-key_layer[..., <span class="number">1</span>::<span class="number">2</span>], key_layer[..., ::<span class="number">2</span>]], dim=-<span class="number">1</span>).reshape_as(key_layer)</span><br><span class="line">    key_layer = key_layer * cos_pos + rotate_half_key_layer * sin_pos</span><br><span class="line">    <span class="keyword">if</span> value_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># rotate_half_value_layer [-v1,v0,-v3,v2......,-vd-1,vd-2]</span></span><br><span class="line">        rotate_half_value_layer = torch.stack([-value_layer[..., <span class="number">1</span>::<span class="number">2</span>], value_layer[..., ::<span class="number">2</span>]], dim=-<span class="number">1</span>).reshape_as(</span><br><span class="line">            value_layer</span><br><span class="line">        )</span><br><span class="line">        value_layer = value_layer * cos_pos + rotate_half_value_layer * sin_pos</span><br><span class="line">        <span class="keyword">return</span> query_layer, key_layer, value_layer</span><br><span class="line">    <span class="keyword">return</span> query_layer, key_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sinusoidal_pos = RoFormerSinusoidalPositionalEmbedding(</span><br><span class="line">    max_position_embeddings, </span><br><span class="line">    config.hidden_size // config.num_attention_heads</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">q, k = apply_rotary_position_embeddings(sinusoidal_pos, q, k)</span><br></pre></td></tr></table></figure>
<p>GPT-NeoX (PyTorch) implementation.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rotary</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, base=<span class="number">10000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>() / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">        self.seq_len_cached = <span class="literal">None</span></span><br><span class="line">        self.cos_cached = <span class="literal">None</span></span><br><span class="line">        self.sin_cached = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_dim=<span class="number">1</span></span>):</span></span><br><span class="line">        seq_len = x.shape[seq_dim]</span><br><span class="line">        <span class="keyword">if</span> seq_len != self.seq_len_cached:</span><br><span class="line">            self.seq_len_cached = seq_len</span><br><span class="line">            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.cos_cached = emb.cos()[:, <span class="literal">None</span>, <span class="literal">None</span>, :]</span><br><span class="line">            self.sin_cached = emb.sin()[:, <span class="literal">None</span>, <span class="literal">None</span>, :]</span><br><span class="line">        <span class="keyword">return</span> self.cos_cached, self.sin_cached</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rotary pos emb helpers:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_half</span>(<span class="params">x</span>):</span></span><br><span class="line">    x1, x2 = x[..., : x.shape[-<span class="number">1</span>] // <span class="number">2</span>], x[..., x.shape[-<span class="number">1</span>] // <span class="number">2</span> :]</span><br><span class="line">    <span class="keyword">return</span> torch.cat(</span><br><span class="line">        (-x2, x1), dim=x1.ndim - <span class="number">1</span></span><br><span class="line">    )  <span class="comment"># dim=-1 triggers a bug in torch &lt; 1.8.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.jit.script</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rotary_pos_emb</span>(<span class="params">q, k, cos, sin</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)</span><br></pre></td></tr></table></figure></p>
<h4 id="RoPE-with-Bias"><a href="#RoPE-with-Bias" class="headerlink" title="RoPE with Bias"></a>RoPE with Bias</h4><p><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Blog- Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)
">[6]</span></a></sup> finds that RoPE w/ Bias can increase the capability of length extrapolation.</p>
<script type="math/tex; mode=display">
\begin{equation}\boldsymbol{q}_m^{\top}\boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n\boldsymbol{k}_n \quad\to\quad (\boldsymbol{q}_m + \boldsymbol{a})^{\top}\boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n(\boldsymbol{k}_n + \boldsymbol{b})\end{equation}</script><p>where <script type="math/tex">\boldsymbol{\mathcal{R}}_m, \boldsymbol{\mathcal{R}}_n</script> are rotation matrix, $\boldsymbol{a}, \boldsymbol{b}$ are learnable bias.</p>
<div class="note success">
            <p><strong>NB</strong>: Pure self-attention softmax gets equivalent results with or without bias term, since it can be cancelled by the softmax normalization.</p><script type="math/tex; mode=display">\begin{equation}\frac{e^{\boldsymbol{q}\cdot(\boldsymbol{k}_n + \boldsymbol{b})}}{\sum\limits_n e^{\boldsymbol{q}\cdot(\boldsymbol{k}_n + \boldsymbol{b})}} = \frac{e^{\boldsymbol{q}\cdot\boldsymbol{k}_n}e^{\boldsymbol{q}\cdot\boldsymbol{b}}}{\sum\limits_n e^{\boldsymbol{q}\cdot\boldsymbol{k}_n} e^{\boldsymbol{q}\cdot\boldsymbol{b}}}= \frac{e^{\boldsymbol{q}\cdot\boldsymbol{k}_n}}{\sum\limits_n e^{\boldsymbol{q}\cdot\boldsymbol{k}_n}}\end{equation}</script><p>But reducing the bias term for self-attention with RoPE cannot obtain the same results.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Blog- Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)">[6]</span></a></sup></p>
          </div>
<h3 id="T5-Bias"><a href="#T5-Bias" class="headerlink" title="T5 Bias"></a>T5 Bias</h3><p>T5<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2020). [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/volume21/20-074/20-074.pdf). JMLR.
">[7]</span></a></sup> adds no position encoding to word embeddings. Instead, it add a learned, shared bias to each query-key self-attention score that is dependent on just the distance between the query and key. In which multiple different distances share the same learned bias, which might be beneficial for length interpolation. Specifically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.</p>
<p>T5 uses a bucket of 32 learnable parameters and assign the relative position bias with a <strong>log-binning</strong> strategy:</p>
<script type="math/tex; mode=display">
\begin{align}
b_{m-n} = \left\{
                \begin{array}{ll}
                  \text{bucket}[0] &{} \text{if } m-n<0\\
                  \text{bucket}[m-n] &{} \text{if } 0 \leq m-n < 16\\
                  \text{bucket}[\min(31, \lfloor \frac{\log \frac{m-n}{16}}{\frac{128}{16}} \cdot 16 \rfloor)] &{} \text{if }  m-n \geq 16
                \end{array}
    \right.
\end{align}</script><p><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., Smith, N.A., & Lewis, M. (2022). [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). ICLR.
">[8]</span></a></sup> finds that T5 bias enables length extrapolation.</p>
<div class="note info">
            <p>T5 uses 32 embeddings for all models with ranges that increase in size logarithmically up to an offset of 128 beyond which it assigns all relative positions to the same embedding. All position embeddings are shared across all layers in T5, though within a given layer each attention head uses a different learned position embedding.</p>
          </div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">T5Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config: T5Config, has_relative_attention_bias=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line">        self.has_relative_attention_bias = has_relative_attention_bias</span><br><span class="line">        self.relative_attention_num_buckets = config.relative_attention_num_buckets</span><br><span class="line">        self.relative_attention_max_distance = config.relative_attention_max_distance</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.has_relative_attention_bias:</span><br><span class="line">            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_relative_position_bucket</span>(<span class="params">relative_position, bidirectional=<span class="literal">True</span>, num_buckets=<span class="number">32</span>, max_distance=<span class="number">128</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Adapted from Mesh Tensorflow:</span></span><br><span class="line"><span class="string">        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Translate relative position to a bucket number for relative attention. The relative position is defined as</span></span><br><span class="line"><span class="string">        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to</span></span><br><span class="line"><span class="string">        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for</span></span><br><span class="line"><span class="string">        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative</span></span><br><span class="line"><span class="string">        positions &gt;=max_distance map to the same bucket. All relative positions &lt;=-max_distance map to the same bucket.</span></span><br><span class="line"><span class="string">        This should allow for more graceful generalization to longer sequences than the model has been trained on</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            relative_position: an int32 Tensor</span></span><br><span class="line"><span class="string">            bidirectional: a boolean - whether the attention is bidirectional</span></span><br><span class="line"><span class="string">            num_buckets: an integer</span></span><br><span class="line"><span class="string">            max_distance: an integer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        relative_buckets = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            num_buckets //= <span class="number">2</span></span><br><span class="line">            relative_buckets += (relative_position &gt; <span class="number">0</span>).to(torch.long) * num_buckets</span><br><span class="line">            relative_position = torch.<span class="built_in">abs</span>(relative_position)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            relative_position = -torch.<span class="built_in">min</span>(relative_position, torch.zeros_like(relative_position))</span><br><span class="line">        <span class="comment"># now relative_position is in the range [0, inf)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># half of the buckets are for exact increments in positions</span></span><br><span class="line">        max_exact = num_buckets // <span class="number">2</span></span><br><span class="line">        is_small = relative_position &lt; max_exact</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The other half of the buckets are for logarithmically bigger bins in positions up to max_distance</span></span><br><span class="line">        relative_position_if_large = max_exact + (</span><br><span class="line">            torch.log(relative_position.<span class="built_in">float</span>() / max_exact)</span><br><span class="line">            / math.log(max_distance / max_exact)</span><br><span class="line">            * (num_buckets - max_exact)</span><br><span class="line">        ).to(torch.long)</span><br><span class="line">        relative_position_if_large = torch.<span class="built_in">min</span>(</span><br><span class="line">            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)</span><br><span class="line">        <span class="keyword">return</span> relative_buckets</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_bias</span>(<span class="params">self, query_length, key_length, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute binned relative position bias&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            device = self.relative_attention_bias.weight.device</span><br><span class="line">        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, <span class="literal">None</span>]</span><br><span class="line">        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[<span class="literal">None</span>, :]</span><br><span class="line">        relative_position = memory_position - context_position  <span class="comment"># shape (query_length, key_length)</span></span><br><span class="line">        relative_position_bucket = self._relative_position_bucket(</span><br><span class="line">            relative_position,  <span class="comment"># shape (query_length, key_length)</span></span><br><span class="line">            bidirectional=(<span class="keyword">not</span> self.is_decoder),</span><br><span class="line">            num_buckets=self.relative_attention_num_buckets,</span><br><span class="line">            max_distance=self.relative_attention_max_distance,</span><br><span class="line">        )</span><br><span class="line">        values = self.relative_attention_bias(relative_position_bucket)  <span class="comment"># shape (query_length, key_length, num_heads)</span></span><br><span class="line">        values = values.permute([<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]).unsqueeze(<span class="number">0</span>)  <span class="comment"># shape (1, num_heads, query_length, key_length)</span></span><br><span class="line">        <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure>
<h3 id="ALiBi-Attention-with-Linear-Biases"><a href="#ALiBi-Attention-with-Linear-Biases" class="headerlink" title="ALiBi (Attention with Linear Biases)"></a>ALiBi (Attention with Linear Biases)</h3><div class="note warning">
            <p>Length extrapolation allows transformers training on short sequences while testing on substantially long sequences, by means of relative positional encoding. </p>
          </div>
<p>ALiBi<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., Smith, N.A., & Lewis, M. (2022). [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). ICLR.
">[8]</span></a></sup> adds a static, non-learnable bias to the query-key dot product. As is done in T5 bias and RoPE, it adds position information to keys and querys at each layer.</p>
<script type="math/tex; mode=display">
\mathbf{q}_m^T \mathbf{k}_n + \alpha \vert m-n \vert</script><p>where $\alpha$ is a head-specific slope (fixed). For $i$-th heads, the value of slope takes <script type="math/tex">\alpha_i = 2^{\frac{-8}{i}}</script>.</p>
<p><img data-src="/notes/images/ALiBi.png" alt="ALiBi"></p>
<div class="note info">
            <p>ALiBi bias is not multiplied by the <script type="math/tex">\sqrt{d_k}</script> scaling factor as in the original transformer.</p>
          </div>
<p><img data-src="/notes/images/ALiBI_perf.png" alt="Extrapolation: (validation-set’s) input sequence length (x-axis), versus perplexity (y-axis, lower is better)."></p>
<p>It is observed that ALiBi and T5 bias show length extrapolation ability<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., Smith, N.A., & Lewis, M. (2022). [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). ICLR.
">[8]</span></a></sup>, while RoPE and sinusoidal position do not have. </p>
<h3 id="KERPLE"><a href="#KERPLE" class="headerlink" title="KERPLE"></a>KERPLE</h3><p>KErnelize Relative Positional Embedding for Length Extrapolation (KERPLE)<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chi, T., Fan, T., Ramadge, P.J., & Rudnicky, A.I. (2022). [KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation](https://arxiv.org/pdf/2205.09921.pdf). NeurIPS.
">[9]</span></a></sup> proposes kernelized positional embeddings as follows:</p>
<script type="math/tex; mode=display">
\begin{align}
a_{m,n} = \frac{\exp(\frac{\mathbf{q}_m^T \mathbf{k}_n + \tilde{k}_{r_1,\cdots, r_{\mathcal{l}}(m, n)}}{\sqrt{d}})}{\sum_{i=1}^L \exp(\frac{\mathbf{q}_m^T \mathbf{k}_i + \tilde{k}_{r_1,\cdots, r_{\mathcal{l}}(m, i)}}{\sqrt{d}})}
\end{align}</script><p>where <script type="math/tex">r_1,\cdots, r_{\mathcal{l}}</script> are learnable parameters.</p>
<script type="math/tex; mode=display">
\begin{align}
a_{m,n} = \left\{
    \begin{array}{ll}
    \text{(power)}&{}
\mathbf{q}_m^{\top}\mathbf{k}_n - r_1|m - n|^{r_2} ,&{} r_1 >0, 0 < r_2 \leq 2\\ 
\text{(logarithmic)}&{} \mathbf{q}_m^{\top}\mathbf{k}_n - r_1\log(1+r_2|m - n|),&{} r_1, r_2 > 0 
\end{array}
\right.\label{eq:kerple}\end{align}</script><div class="note info">
            <p>Triangle kernel: <script type="math/tex">c-|m-n|</script>. It reduces to ALiBi.</p>
          </div>
<h3 id="xPos"><a href="#xPos" class="headerlink" title="xPos"></a>xPos</h3><p>Extrapolatable Position Embedding (XPOS)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., & Wei, F. (2022). [A Length-Extrapolatable Transformer](https://arxiv.org/pdf/2212.10554.pdf). ArXiv, abs/2212.10554.
">[10]</span></a></sup> proposes:</p>
<script type="math/tex; mode=display">
\begin{align}
f_q (q,n) &{}= A_q qe^{\lambda n}  &{} \text{let }\lambda = \xi+i\theta \in \mathbb{C}^{d/2} \\&{}:= qe^{\xi n + i\theta n} &{} \text{Remove linear factor $A_q$} \\ \nonumber \\

f_k (k,n) &{}= A_k ke^{-\lambda n}  &{} \text{let }\lambda = \xi+i\theta \in \mathbb{C}^{d/2} \\&{}:= ke^{\xi n + i\theta n} &{} \text{Remove linear factor $A_k$}
\end{align}</script><p><img data-src="/notes/images/XPOS.png" alt="xPos"></p>
<h3 id="Sandwich"><a href="#Sandwich" class="headerlink" title="Sandwich"></a>Sandwich</h3><p>The self-attention is calculated as:</p>
<script type="math/tex; mode=display">
\begin{align}
(\mathbf{W}_q(\mathbf{e}_m + \mathbf{p}_m))^T (\mathbf{W}_k(\mathbf{e}_n + \mathbf{p}_n)) &{} \approx \underbrace{\mathbf{e}_m^T\mathbf{W}_q^T\mathbf{W}_k\mathbf{e}_n^T}_{\text{semantic}} + \underbrace{\mathbf{p}_m^T\mathbf{p}_n}_{\text{position}}
\end{align}</script><p>The temporal bias terms is:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{p}_m^T\mathbf{p}_n =&{} \sum_{i=1}^{\bar{d}/2} \sin \big( \frac{m}{10000^{2i/\bar{d}}} \big) \sin \big( \frac{n}{10000^{2i/\bar{d}}} \big) + \cos \big( \frac{m}{10000^{2i/\bar{d}}} \big) \cos \big( \frac{n}{10000^{2i/\bar{d}}} \big)\\
=&{} \sum_{i=1}^{\bar{d}/2} \cos \big( \frac{m-n}{10000^{2i/\bar{d}}} \big)
\end{align}</script><p>Sandwich<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chi, T., Fan, T., Rudnicky, A., & Ramadge, P.J. (2022). [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://arxiv.org/pdf/2212.10356.pdf).
">[11]</span></a></sup> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">base = <span class="number">1e4</span> </span><br><span class="line">heads = <span class="number">12</span></span><br><span class="line">seq_len = <span class="number">8192</span></span><br><span class="line">positions = np.arange(seq_len)[..., <span class="literal">None</span>] </span><br><span class="line">bar_d = <span class="number">128</span> <span class="comment"># This is the hyperparameter of Sandwich </span></span><br><span class="line">i = np.arange(bar_d // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">pos_embs = np.concatenate([np.sin(positions / base ** (<span class="number">2</span> * i / bar_d)), </span><br><span class="line">                           np.cos(positions / base ** (<span class="number">2</span> * i / bar_d))], </span><br><span class="line">                           axis=-<span class="number">1</span>)</span><br><span class="line">sandwich = np.matmul(pos_embs, pos_embs.T) </span><br><span class="line">compression_ratio = np.arange(<span class="number">1</span>, heads + <span class="number">1</span>) * <span class="number">8</span> / heads </span><br><span class="line">multi_head_sandwich = sandwich[<span class="literal">None</span>, ...] / compression_ratio[..., <span class="literal">None</span>, <span class="literal">None</span>]</span><br></pre></td></tr></table></figure>
<h3 id="Randomized-Position"><a href="#Randomized-Position" class="headerlink" title="Randomized Position"></a>Randomized Position</h3><p><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ruoss, A., Del'etang, G., Genewein, T., Grau-Moya, J., Csordás, R., Abbana Bennani, M., Legg, S., & Veness, J. (2023). [Randomized Positional Encodings Boost Length Generalization of Transformers](https://arxiv.org/pdf/2305.16843.pdf). ACL.
">[13]</span></a></sup> introduce randomized position encoding that simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length.</p>
<p><img data-src="/notes/images/RandomPos.png" alt="Randomized Position"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source code: https://github.com/deepmind/randomized_positional_encodings/blob/main/models/positional_encodings.py<span class="comment">#L160</span></span><br></pre></td></tr></table></figure>
<p>It allows transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average) across 15 algorithmic reasoning tasks.</p>
<p><img data-src="/notes/images/RandomPosResults.png" alt="Randomized position results."></p>
<h3 id="No-Position"><a href="#No-Position" class="headerlink" title="No Position"></a>No Position</h3><p><sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Haviv, Adi et al. “[Transformer Language Models without Positional Encodings Still Learn Positional Information.](https://arxiv.org/pdf/2203.16634.pdf)” Conference on Empirical Methods in Natural Language Processing (2022).
">[12]</span></a></sup> observe that LMs without any explicit position encoding (NoPos) are still competitive with standard transformers across datasets, model sizes, and sequence length. It shows that causal LMs might derive positional awareness not only from the explicit positioning mechanism, but also from the causal mask effects.</p>
<p><img data-src="/notes/images/NoPos.png" alt="No position"></p>
<div class="note success">
            <p>Causal transformer LM can achieve competitive results with original LMs, while the bidirectional masked LMs fail to converge. This may be because that causal LMs can learn positions from the autoregressive nature (left-to-right) but masked LMs are order-invariant.</p>
          </div>
<p><img data-src="/notes/images/NoPosResults.png" alt="Comparison results across different parameter sizes."></p>
<div class="note warning">
            <p><strong>NB</strong>: LMs without explicit positional encodings (NoPos) are always slightly worse, suggesting the importance of inductive positional bias.</p>
          </div>
<h3 id="Position-Interpolation"><a href="#Position-Interpolation" class="headerlink" title="Position Interpolation"></a>Position Interpolation</h3><p>Instead of extrapolation, <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Shouyuan, Sherman Wong, Liangjian Chen and Yuandong Tian. [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf). ArXiv abs/2306.15595 (2023).
">[15]</span></a></sup><sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Github discussion: Position Interpolation](https://github.com/ggerganov/llama.cpp/discussions/1965)
">[16]</span></a></sup><sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Reddit: A simple way to "Extending Context to 8K"](https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/)
">[17]</span></a></sup><sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Things I’m Learning While Training SuperHOT](https://kaiokendev.github.io/til#extending-context-to-8k)
">[18]</span></a></sup> presents position interpolation (PI) that directly downscales the non-integer position indices (RoPE-based) so that the maximum position index matches the previous context window limit in pre-training.</p>
<p><img data-src="/notes/images/Position Interpolation.png" alt="Position Interpolation"></p>
<p>It simply adds two lines of code.<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Things I’m Learning While Training SuperHOT](https://kaiokendev.github.io/til#extending-context-to-8k)
">[18]</span></a></sup><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">        </span><br><span class="line">        max_position_embeddings = <span class="number">8192</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(</span><br><span class="line">            self.max_seq_len_cached,</span><br><span class="line">            device=self.inv_freq.device,</span><br><span class="line">            dtype=self.inv_freq.dtype,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># These two lines:</span></span><br><span class="line">        self.scale = <span class="number">1</span> / <span class="number">4</span></span><br><span class="line">        t *= self.scale</span><br></pre></td></tr></table></figure></p>
<p>SuperHOT-13B<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Reddit: A simple way to "Extending Context to 8K"](https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/)
">[17]</span></a></sup> uptrained on scaling factor of 0.25, compared to base LLaMa 13B and a test LoRA trained on 6K sequence length with no scaling.</p>
<p><img data-src="/notes/images/PI-comp.png" alt="PPL evaluation"></p>
<div class="note warning">
            <p><strong>Note</strong> that PI requires further fine-tuning to take effects for length extrapolation.</p>
          </div>
<h3 id="NTK-Aware-Scaled-RoPE"><a href="#NTK-Aware-Scaled-RoPE" class="headerlink" title="NTK-Aware Scaled RoPE"></a>NTK-Aware Scaled RoPE</h3><div class="note danger">
            <p><strong>Background</strong></p><ol><li>“Simply interpolating the RoPE’s fourier space “linearly” is very sub-optimal, as it prevents the network to distinguish the order and positions of tokens that are very close by.”<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-Aware Scaled RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)">[19]</span></a></sup></li><li>“Scaling down the fourier features too much will eventually even prevent succesful finetunes (this is corroborated by the recent paper by Meta<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Shouyuan, Sherman Wong, Liangjian Chen and Yuandong Tian. [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf). ArXiv abs/2306.15595 (2023).">[15]</span></a></sup> that suggests an upper bound of ~600x)”<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-Aware Scaled RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)">[19]</span></a></sup></li></ol>
          </div>
<p>NTK-Aware Scaled RoPE<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-Aware Scaled RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
">[19]</span></a></sup><sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[RoPE is a β-ary encoding (Chinese)](https://spaces.ac.cn/archives/9675)
">[20]</span></a></sup> designs a nonlinear interpolation scheme using Neural Tangent Kernel (NTK) theory. It changes the base of the RoPE instead of the scale, which intuitively changes the “spinning” speed from which each of the RoPE’s dimension vectors shifts to the next.</p>
<p>Implementation <sup id="fnref:21"><a href="#fn:21" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-aware RoPE colab](https://colab.research.google.com/drive/1VI2nhlyKvd5cw4-zHvAIk00cAVj2lCCC#scrollTo=b80b3f37)
">[21]</span></a></sup><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"></span><br><span class="line">old_init = transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.__init__</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ntk_scaled_init</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#The method is just these three lines</span></span><br><span class="line">    max_position_embeddings = <span class="number">16384</span></span><br><span class="line">    a = <span class="number">8</span> <span class="comment"># Alpha value</span></span><br><span class="line">    base = base * a ** (dim / (dim-<span class="number">2</span>)) <span class="comment">#Base change formula</span></span><br><span class="line"></span><br><span class="line">    old_init(self, dim, max_position_embeddings, base, device)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># apply ntk-sclaed init patch</span></span><br><span class="line">transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.__init__ = ntk_scaled_init</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>Average perplexity of LLaMA-7B on a set of 40 very long prompts (12k+ context size).</p>
<p><img data-src="/notes/images/ntk-rope-trend.png" alt="PI-comparison" width="70%"/></p>
<h3 id="Dynamic-Linear-RoPE"><a href="#Dynamic-Linear-RoPE" class="headerlink" title="Dynamic Linear RoPE"></a>Dynamic Linear RoPE</h3><p><em>Dynamic linear RoPE</em>: set the <code>scale</code> to <code>max_seq_len/current position length</code>, which can slowly increase the scale.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaLinearScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, scale=<span class="number">1</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)</span><br><span class="line">        t /= self.scale</span><br><span class="line">        freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">        <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">        dtype = torch.get_default_dtype()</span><br><span class="line">        self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_len=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># x: [bs, num_attention_heads, seq_len, head_size]</span></span><br><span class="line">        <span class="comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; self.max_seq_len_cached:</span><br><span class="line">            self.max_seq_len_cached = seq_len</span><br><span class="line">            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">            t /= self.scale</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Dynamic-NTK-Aware-Scaled-RoPE"><a href="#Dynamic-NTK-Aware-Scaled-RoPE" class="headerlink" title="Dynamic NTK-Aware Scaled RoPE"></a>Dynamic NTK-Aware Scaled RoPE</h3><div class="note warning">
            <p><em>Cons</em>: </p><ul><li>Compared to dynamic linear scaling, NTK-Aware has higher perplexity for shorter sequences, but better perplexity at the tail end of the sequence lengths. </li><li>NTK-aware RoPE suffers from catastrophic perplexity blowup, like regular RoPE and static linear scaling.</li></ul>
          </div>
<p><sup id="fnref:22"><a href="#fn:22" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Dynamic NTK-aware RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)
">[22]</span></a></sup> introduces dynamic NTK-aware scaling. The scaling of $\alpha$ is set to:</p>
<script type="math/tex; mode=display">(\alpha * \text{current sequence length} / \text{original model context length} ) - (\alpha -1)</script><p>This dynamically scales the $\alpha$ as the sequence length increases.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaDynamicScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, ntk=<span class="literal">False</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ntk = ntk</span><br><span class="line">        self.base = base</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)</span><br><span class="line">        freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">        <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">        dtype = torch.get_default_dtype()</span><br><span class="line">        self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_len=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># x: [bs, num_attention_heads, seq_len, head_size]</span></span><br><span class="line">        <span class="comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; self.max_seq_len_cached:</span><br><span class="line">            self.max_seq_len_cached = seq_len</span><br><span class="line">            <span class="keyword">if</span> self.ntk: <span class="comment">### dynamic NTK</span></span><br><span class="line">                base = self.base * ((self.ntk * seq_len / self.max_position_embeddings) - (self.ntk - <span class="number">1</span>)) ** (self.dim / (self.dim-<span class="number">2</span>))</span><br><span class="line">                inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, self.dim, <span class="number">2</span>).<span class="built_in">float</span>().to(x.device) / self.dim))</span><br><span class="line">                self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.ntk:</span><br><span class="line">                t *= self.max_position_embeddings / seq_len</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/Dynamic-NTK-RoPE.png" alt="Dynamic NTK RoPE&lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;[Dynamic NTK-aware RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)
&quot;&gt;[22]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<h4 id="Partial-NTK-Scaled-RoPE"><a href="#Partial-NTK-Scaled-RoPE" class="headerlink" title="Partial NTK Scaled RoPE"></a>Partial NTK Scaled RoPE</h4><p>Combine RoPE, Linear, NTK.<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: Dynamic RoPE.](https://github.com/jquesnelle/scaled-rope/tree/master/scaled_rope)
">[23]</span></a></sup></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_correction_factor</span>(<span class="params">num_rotations, dim, base=<span class="number">10000</span>, max_position_embeddings=<span class="number">2048</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> (dim * math.log(max_position_embeddings/(num_rotations * <span class="number">2</span> * math.pi)))/(<span class="number">2</span> * math.log(base)) <span class="comment">#Inverse dim formula to find number of rotations</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_correction_range</span>(<span class="params">low_rot, high_rot, dim, base=<span class="number">10000</span>, max_position_embeddings=<span class="number">2048</span></span>):</span></span><br><span class="line">    low = math.floor(find_correction_factor(low_rot, dim, base, max_position_embeddings))</span><br><span class="line">    high = math.ceil(find_correction_factor(high_rot, dim, base, max_position_embeddings))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(low, <span class="number">0</span>), <span class="built_in">min</span>(high, dim-<span class="number">1</span>) <span class="comment">#Clamp values just in case</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_ramp_mask</span>(<span class="params"><span class="built_in">min</span>, <span class="built_in">max</span>, dim</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">min</span> == <span class="built_in">max</span>:</span><br><span class="line">        <span class="built_in">max</span> += <span class="number">0.001</span> <span class="comment">#Prevent singularity</span></span><br><span class="line"></span><br><span class="line">    linear_func = (torch.arange(dim, dtype=torch.float32) - <span class="built_in">min</span>) / (<span class="built_in">max</span> - <span class="built_in">min</span>)</span><br><span class="line">    ramp_func = torch.clamp(linear_func, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ramp_func</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_newbase_ntk</span>(<span class="params">dim, base=<span class="number">10000</span>, scale=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> base * scale ** (dim / (dim-<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaPartNTKScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, scale=<span class="number">1</span>, ntk_factor=<span class="number">1</span>, extrapolation_factor=<span class="number">1</span>, original_max_position_embeddings=<span class="number">2048</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#Interpolation constants found experimentally for LLaMA (might not be totally optimal though)</span></span><br><span class="line">        <span class="comment">#Do not change unless there is a good reason for doing so!</span></span><br><span class="line">        beta_0 = <span class="number">1.25</span></span><br><span class="line">        beta_1 = <span class="number">0.75</span></span><br><span class="line">        gamma_0 = <span class="number">16</span></span><br><span class="line">        gamma_1 = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#Three RoPE extrapolation/interpolation methods</span></span><br><span class="line">        inv_freq_base = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        inv_freq_linear = <span class="number">1.0</span> / (scale * (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim)))</span><br><span class="line">        inv_freq_ntk = <span class="number">1.0</span> / (find_newbase_ntk(dim, base, scale) ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line"></span><br><span class="line">        current_dtype = inv_freq_ntk.dtype</span><br><span class="line">        current_device = inv_freq_ntk.device</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#Combine NTK and Linear</span></span><br><span class="line">        low, high = find_correction_range(beta_0, beta_1, dim, base, original_max_position_embeddings)</span><br><span class="line">        inv_freq_mask = (<span class="number">1</span> - linear_ramp_mask(low, high, dim // <span class="number">2</span>).<span class="built_in">type</span>(current_dtype).to(current_device)) * ntk_factor</span><br><span class="line">        inv_freq = inv_freq_linear * (<span class="number">1</span> - inv_freq_mask) + inv_freq_ntk * inv_freq_mask</span><br><span class="line">    </span><br><span class="line">        <span class="comment">#Combine Extrapolation and NTK and Linear</span></span><br><span class="line">        low, high = find_correction_range(gamma_0, gamma_1, dim, base, original_max_position_embeddings)</span><br><span class="line">        inv_freq_mask = (<span class="number">1</span> - linear_ramp_mask(low, high, dim // <span class="number">2</span>).<span class="built_in">type</span>(current_dtype).to(current_device)) * extrapolation_factor</span><br><span class="line">        inv_freq = inv_freq * (<span class="number">1</span> - inv_freq_mask) + inv_freq_base * inv_freq_mask</span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)</span><br><span class="line">        freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">        <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">        dtype = torch.get_default_dtype()</span><br><span class="line">        self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_len=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># x: [bs, num_attention_heads, seq_len, head_size]</span></span><br><span class="line">        <span class="comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; self.max_seq_len_cached:</span><br><span class="line">            self.max_seq_len_cached = seq_len</span><br><span class="line">            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="beta-ary-RoPE"><a href="#beta-ary-RoPE" class="headerlink" title="$\beta$-ary RoPE"></a>$\beta$-ary RoPE</h4><p><sup id="fnref:24"><a href="#fn:24" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[β-ary RoPE (in Chinese)](https://kexue.fm/archives/9706)
">[24]</span></a></sup></p>
<h4 id="ReRoPE"><a href="#ReRoPE" class="headerlink" title="ReRoPE"></a>ReRoPE</h4><p><sup id="fnref:25"><a href="#fn:25" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ReROPE (in Chinese)](https://kexue.fm/archives/9708)">[25]</span></a></sup></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you Need</a>.” NeurIPS (2017).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, Ofir, Noah A. Smith and Mike Lewis. “<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.12409.pdf">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.</a>” ICLR 2022.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Su, Jianlin, Yu Lu, Shengfeng Pan, Bo Wen and Yunfeng Liu. “<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864.pdf">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>.” ArXiv abs/2104.09864 (2021).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://blog.eleuther.ai/rotary-embeddings/">Rotary Embeddings: A Relative Revolution</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kexue.fm/archives/8265">RoFormer blog (Chinese)- Transformer升级之路：2、博采众长的旋转式位置编码</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kexue.fm/archives/9577">Blog- Bias项的神奇作用：RoPE + Bias = 更好的长度外推性</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P.J. (2020). <a target="_blank" rel="noopener" href="https://jmlr.org/papers/volume21/20-074/20-074.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. JMLR.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, O., Smith, N.A., &amp; Lewis, M. (2022). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.12409.pdf">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>. ICLR.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chi, T., Fan, T., Ramadge, P.J., &amp; Rudnicky, A.I. (2022). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.09921.pdf">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation</a>. NeurIPS.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., &amp; Wei, F. (2022). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10554.pdf">A Length-Extrapolatable Transformer</a>. ArXiv, abs/2212.10554.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chi, T., Fan, T., Rudnicky, A., &amp; Ramadge, P.J. (2022). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10356.pdf">Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis</a>.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Haviv, Adi et al. “<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.16634.pdf">Transformer Language Models without Positional Encodings Still Learn Positional Information.</a>” Conference on Empirical Methods in Natural Language Processing (2022).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ruoss, A., Del'etang, G., Genewein, T., Grau-Moya, J., Csordás, R., Abbana Bennani, M., Legg, S., &amp; Veness, J. (2023). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.16843.pdf">Randomized Positional Encodings Boost Length Generalization of Transformers</a>. ACL.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kexue.fm/archives/9444">Blog: Transformer升级之路：8、长度外推性与位置鲁棒性</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, Shouyuan, Sherman Wong, Liangjian Chen and Yuandong Tian. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.15595.pdf">Extending Context Window of Large Language Models via Positional Interpolation</a>. ArXiv abs/2306.15595 (2023).<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/discussions/1965">Github discussion: Position Interpolation</a><a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/">Reddit: A simple way to &quot;Extending Context to 8K&quot;</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kaiokendev.github.io/til#extending-context-to-8k">Things I’m Learning While Training SuperHOT</a><a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-Aware Scaled RoPE</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/9675">RoPE is a β-ary encoding (Chinese)</a><a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1VI2nhlyKvd5cw4-zHvAIk00cAVj2lCCC#scrollTo=b80b3f37">NTK-aware RoPE colab</a><a href="#fnref:21" rev="footnote"> ↩</a></span></li><li id="fn:22"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/">Dynamic NTK-aware RoPE</a><a href="#fnref:22" rev="footnote"> ↩</a></span></li><li id="fn:23"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/jquesnelle/scaled-rope/tree/master/scaled_rope">GitHub: Dynamic RoPE.</a><a href="#fnref:23" rev="footnote"> ↩</a></span></li><li id="fn:24"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kexue.fm/archives/9706">β-ary RoPE (in Chinese)</a><a href="#fnref:24" rev="footnote"> ↩</a></span></li><li id="fn:25"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">25.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://kexue.fm/archives/9708">ReROPE (in Chinese)</a><a href="#fnref:25" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/Transformer/" rel="tag"># Transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2022/12/12/Diffusion-Models-Math-Guide/" rel="prev" title="Diffusion Models: A Mathematical Note from Scratch">
      <i class="fa fa-chevron-left"></i> Diffusion Models: A Mathematical Note from Scratch
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2024/05/24/Tokenization-with-Vector-Quantization/" rel="next" title="Multimodal Tokenization with Vector Quantization: A Review">
      Multimodal Tokenization with Vector Quantization: A Review <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">1.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Position-Encoding"><span class="nav-number">2.</span> <span class="nav-text">Position Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sinusoidal-Position-Embeddings"><span class="nav-number">2.1.</span> <span class="nav-text">Sinusoidal Position Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rotary-Position-Embedding-RoPE"><span class="nav-number">2.2.</span> <span class="nav-text">Rotary Position Embedding (RoPE)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Derivation"><span class="nav-number">2.2.1.</span> <span class="nav-text">Derivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementation"><span class="nav-number">2.2.2.</span> <span class="nav-text">Implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RoPE-with-Bias"><span class="nav-number">2.2.3.</span> <span class="nav-text">RoPE with Bias</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#T5-Bias"><span class="nav-number">2.3.</span> <span class="nav-text">T5 Bias</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ALiBi-Attention-with-Linear-Biases"><span class="nav-number">2.4.</span> <span class="nav-text">ALiBi (Attention with Linear Biases)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KERPLE"><span class="nav-number">2.5.</span> <span class="nav-text">KERPLE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xPos"><span class="nav-number">2.6.</span> <span class="nav-text">xPos</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sandwich"><span class="nav-number">2.7.</span> <span class="nav-text">Sandwich</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Randomized-Position"><span class="nav-number">2.8.</span> <span class="nav-text">Randomized Position</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#No-Position"><span class="nav-number">2.9.</span> <span class="nav-text">No Position</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Position-Interpolation"><span class="nav-number">2.10.</span> <span class="nav-text">Position Interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NTK-Aware-Scaled-RoPE"><span class="nav-number">2.11.</span> <span class="nav-text">NTK-Aware Scaled RoPE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-Linear-RoPE"><span class="nav-number">2.12.</span> <span class="nav-text">Dynamic Linear RoPE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-NTK-Aware-Scaled-RoPE"><span class="nav-number">2.13.</span> <span class="nav-text">Dynamic NTK-Aware Scaled RoPE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Partial-NTK-Scaled-RoPE"><span class="nav-number">2.13.1.</span> <span class="nav-text">Partial NTK Scaled RoPE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#beta-ary-RoPE"><span class="nav-number">2.13.2.</span> <span class="nav-text">$\beta$-ary RoPE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ReRoPE"><span class="nav-number">2.13.3.</span> <span class="nav-text">ReRoPE</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">3.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yekun Chai"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">Yekun Chai</p>
  <div class="site-description" itemprop="description">Language is not just words.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yekun Chai</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2023/01/26/Position-Encoding-in-Transformers/";
    this.page.identifier = "2023/01/26/Position-Encoding-in-Transformers/";
    this.page.title = "Inductive Positions in Transformers";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
