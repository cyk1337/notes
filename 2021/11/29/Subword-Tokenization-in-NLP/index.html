<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/notes/images/dog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/notes/images/dog.png">
  <link rel="mask-icon" href="/notes/images/dog.png" color="#222">

<link rel="stylesheet" href="/notes/css/main.css">


<link rel="stylesheet" href="/notes/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cyk1337.github.io","root":"/notes/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Summary of word tokenization in natural language processing.">
<meta property="og:type" content="article">
<meta property="og:title" content="Subword Tokenization in Natural Language Processing">
<meta property="og:url" content="https://cyk1337.github.io/notes/2021/11/29/Subword-Tokenization-in-NLP/index.html">
<meta property="og:site_name" content="Yekun&#39;s Note">
<meta property="og:description" content="Summary of word tokenization in natural language processing.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/BPE-algorithm.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/BPE-vs-BBPE-Ja-En.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/BPE_encode_process.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/wordpiece.gif">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Unigram-LM-tokenization.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Unigram-vs-BPE-token-length-and-frequency.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Unigram-vs-BPE-evaluation.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Unigram-vs-BPE-in-segmentation.png">
<meta property="og:image" content="https://cyk1337.github.io/notes/images/Unigram-vs-BPE-finetuning-tasks.png">
<meta property="article:published_time" content="2021-11-29T03:59:00.000Z">
<meta property="article:modified_time" content="2024-08-01T10:19:59.983Z">
<meta property="article:author" content="Yekun Chai">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Pre-training">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Tokenization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cyk1337.github.io/notes/images/BPE-algorithm.png">

<link rel="canonical" href="https://cyk1337.github.io/notes/2021/11/29/Subword-Tokenization-in-NLP/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Subword Tokenization in Natural Language Processing | Yekun's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/notes/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Yekun's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Machine learning notes and writeup.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="https://cyk1337.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-blog">

    <a href="/notes/" rel="section"><i class="fa fa-rss-square fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/notes/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/notes/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <!-- chaiyekun added -->
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png?resize=149%2C149" alt="Fork me on GitHub"></a>
    <!-- 
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->

    <!-- (github fork span, top right)
    <a target="_blank" rel="noopener" href="https://github.com/cyk1337"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>
    -->
    <!-- (github fork span)
      <a target="_blank" rel="noopener" href="https://github.com/cyk1337" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    -->

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://cyk1337.github.io/notes/2021/11/29/Subword-Tokenization-in-NLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/notes/images/ernie.jpeg">
      <meta itemprop="name" content="Yekun Chai">
      <meta itemprop="description" content="Language is not just words.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yekun's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Subword Tokenization in Natural Language Processing
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-29 11:59:00" itemprop="dateCreated datePublished" datetime="2021-11-29T11:59:00+08:00">2021-11-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/notes/categories/LLM/Tokenization/" itemprop="url" rel="index"><span itemprop="name">Tokenization</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/notes/2021/11/29/Subword-Tokenization-in-NLP/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/11/29/Subword-Tokenization-in-NLP/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Summary of word tokenization in natural language processing.</p>
<span id="more"></span>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>The table summarizes the difference between various tokenization approaches.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Tokenization</th>
<th style="text-align:left">Methods</th>
<th style="text-align:left">Pros</th>
<th style="text-align:left">Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">word-based</td>
<td style="text-align:left">1.Space and punctuation tokenization;<br>2.Rule-based tokenization.</td>
<td style="text-align:left">Easy to use.</td>
<td style="text-align:left">1. Very large vocabularies.<br>2. OOV problem.<br>3. Loss of meanings across very similar words.</td>
</tr>
<tr>
<td style="text-align:center">char-based</td>
<td style="text-align:left">Splitting into chars</td>
<td style="text-align:left">1.Slimmer vocabularies.<br>2.Mostly open-vocabulary: fewer OOV words.</td>
<td style="text-align:left">1. Very long sequence.<br>2. Less meaningful individual tokens</td>
</tr>
<tr>
<td style="text-align:center">subword-based</td>
<td style="text-align:left">WordPiece <br>BPE<br>Unigram</td>
<td style="text-align:left">1.Good balance the vocabulary size and the sequence length;<br>2.Help identify similar syntactic or semantic situations in texts;<br>3.Can identify start of word tokens.</td>
<td style="text-align:left">Need training additional subword tokenizer.</td>
</tr>
</tbody>
</table>
</div>
<div class="note info">
            <p>Why subword?</p><ul><li><strong>Subword-based tokenization</strong> lies between character- and word-based tokenization, which arises from the idea that:<blockquote><p><strong>Frequently used words</strong> should not be split into smaller subwords;<br><strong>Rare words</strong> should be decomposed into meaningful subwords.</p></blockquote></li><li>Subwords help identify <em>similar syntactic or semantic situations</em> in texts, such as same prefix or sufix.</li><li>Subword tokenization can identify <em>start of word tokens</em>, such as “##” in WordPiece (BERT).</li></ul>
          </div>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><div class="note info">
            <p>It can be seen from the table that:</p><ul><li>OpenAI and Facebook favor BPE tokenization whereas Google prefers self-proposed WordPiece and Unigram methods. ;)</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">Tokenization</th><th style="text-align:center">#Vocab</th><th style="text-align:center">Corpus</th><th style="text-align:center">Org.</th></tr></thead><tbody><tr><td style="text-align:center">GPT</td><td style="text-align:center">BPE [Spacy/ftfy pre-tokenizer]</td><td style="text-align:center">40,478</td><td style="text-align:center">BooksCorpus</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-2</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">WebText (40GB)</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-3</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">Common Crawl, WebText2, Books1/2, Wikipedia</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-4</td><td style="text-align:center">BBPE</td><td style="text-align:center">100,256</td><td style="text-align:center">Public corpus, third-party data</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-4o</td><td style="text-align:center">BBPE</td><td style="text-align:center">200k</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">Gemini</td><td style="text-align:center">BPE</td><td style="text-align:center">256k</td><td style="text-align:center">A large sample of the entire training corpus.</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">code-davinci-001/002</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,281</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">text-davinci-003</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,281</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">gpt-3.5-turbo</td><td style="text-align:center">BBPE</td><td style="text-align:center">100,256</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">RoBERTa</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Facebook</td></tr><tr><td style="text-align:center">BART</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Facebook</td></tr><tr><td style="text-align:center">BERT</td><td style="text-align:center">WordPiece (30k)</td><td style="text-align:center">30k</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">T5</td><td style="text-align:center">WordPiece (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">C4</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">XLNet</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">BooksCorpus, enwiki, Giga5,  ClueWeb 201-B, Common Crawl</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">ELECTRA</td><td style="text-align:center">WordPiece</td><td style="text-align:center">30k</td><td style="text-align:center">base: same as BERT; large: same as XLNet</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">ALBERT</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">30k</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Gopher</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">MassiveText</td><td style="text-align:center">DeepMind</td></tr><tr><td style="text-align:center">Chinchilla</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">MassiveText</td><td style="text-align:center">DeepMind</td></tr><tr><td style="text-align:center">PaLM</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">256k</td><td style="text-align:center">dataset from LamDA, GLaM, and code</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">LaMDA</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">2.97B documents, 1.12B dialogs,  and 13.39B dialog utterances.</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Galactica</td><td style="text-align:center">BPE</td><td style="text-align:center">50k</td><td style="text-align:center">Papers, reference material,  encyclopedias and other scientific sources</td><td style="text-align:center">Meta</td></tr><tr><td style="text-align:center">Gemma</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">256k</td><td style="text-align:center">Same as Gemma 1 and Gemini</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Gemma 2</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">256k</td><td style="text-align:center">Same as Gemma 1 and Gemini</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Llama</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">CommonCrawl, C4, GitHub, Wiki,  Books, ArXiv, StackExchange</td><td style="text-align:center">Meta</td></tr><tr><td style="text-align:center">Llama 2</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">Same tokenizer as Llama 1</td><td style="text-align:center">Meta</td></tr><tr><td style="text-align:center">Llama 3</td><td style="text-align:center">BBPE (spm)</td><td style="text-align:center">128K</td><td style="text-align:center">GPT-4 100k tokenizer (“c100k_base”), with additional 28k non-English languages</td><td style="text-align:center">Meta</td></tr></tbody></table></div>
          </div>
<p>Following sections generally compares common subword methods in pre-trained models. Refer to following links<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Word Tokenization: How to Handle Out-Of-Vocabulary Words?](/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization)
">[13]</span></a></sup> for detailed tokenization process.<br><div class="note primary">
            <p><strong>Reading Recommendation</strong></p><ul><li><a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization">Word Tokenization: How to Handle Out-Of-Vocabulary Words?</a></li></ul>
          </div></p>
<p><strong>TL;DR</strong></p>
<ul>
<li><span class="label primary"> WordPiece $\Uparrow$</span> <strong>(probability-based)</strong> <em>merges</em> tokens based on <span class="label primary">bigram likelihood</span>. It uses a language model to evaluate the likelihood of subword pair mergence during each iteration, incrementally merging the neighbor unit pairs.</li>
<li><span class="label primary">Byte Pair Encoding (BPE) $\Uparrow$</span> <strong>(frequency-based)</strong> <em>merges</em> tokens based on <span class="label primary">bigram frequency</span>. It uses the subword pair co-occurrence to greedily merge neighbor pairs, which can effiectively balance the vocabulary size and the sequence length. It is based on the greedy <strong>longest-match-first algorithm</strong> (deterministic symbol replacement), which cannot generate multiple segmentations with probabilities.</li>
<li><span class="label info">Unigram Language Model $\Downarrow$</span> <strong>(subword regularization)</strong> <em>prunes</em> tokens based on <span class="label info">unigram LM perplexity</span>, which can be viewed as a probabilistic mixture of characters, subwords, and word segmentations, where the mixture probabiilty is computed using EM algorithm. It reduces the subword using a unigram LM with likelihood reduction.</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Tokenization</th>
<th style="text-align:center">#Vocab</th>
<th style="text-align:center">Update method</th>
<th style="text-align:center">New symbols</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">WordPiece</td>
<td style="text-align:center">&#8593;</td>
<td style="text-align:center">Bottom-up merge</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">BPE</td>
<td style="text-align:center">&#8593;</td>
<td style="text-align:center">Bottom-up merge</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Unigram</td>
<td style="text-align:center">&#8595;</td>
<td style="text-align:center">Prune</td>
<td style="text-align:center">✘</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte-Pair Encoding (BPE)"></a>Byte-Pair Encoding (BPE)</h2><p>Byte-Pair Encoding (BPE)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[BPE] Neural Machine Translation of Rare Words with Subword Units
Rico](http://www.aclweb.org/anthology/P16-1162.pdf)
">[8]</span></a></sup> firstly adopts a pre-tokenizer to split the text sequence into words, then curates a <span class="label warning">base vocabulary</span> consisting of all character symbol sets in the training data for <span class="label primary">frequency-based merge</span>.</p>
<p><strong>Pre-tokenization</strong>&nbsp; The pre-tokenization can be:</p>
<ul>
<li>Space tokenization, e.g. GPT-2, RoBERTa.</li>
<li>Rule-based tokenization (Moses), e.g. XLM.</li>
<li>Spacy and ftfy: GPT.</li>
</ul>
<p><strong>Frequency-based Merge</strong>&nbsp; Starting with the base vocabulary, BPE counts the frequency of each neighbor pair and selects the <span class="label warning">unit pair that occurs most frequently</span> to the base vocabulary. Then it searches for the next unit pair that occurs the most frequently.</p>
<p><img width="50%" data-src="/notes/images/BPE-algorithm.png" alt="BPE algorithm <small>[15]</small>" /></p>
<h3 id="Byte-level-BPE-BBPE"><a href="#Byte-level-BPE-BBPE" class="headerlink" title="Byte-level BPE (BBPE)"></a>Byte-level BPE (BBPE)</h3><h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><div class="note default">
            <p><a target="_blank" rel="noopener" href="https://www.unicode.org/versions/Unicode12.1.0/">Unicode</a> vs <a target="_blank" rel="noopener" href="https://web.cortland.edu/flteach/mm-course/characters.html">Byte</a>:</p><ul><li><strong>Unicode</strong>: Unicode is an encoding for textual characters which is able to represent characters from different languages. Each character is represented by a <strong>unicode code point</strong>. Unicode consists of a total of <strong>137,929 characters</strong>.</li><li><strong>Byte</strong>: 8 bits is called a byte. One byte character set can contain <strong>256 characters</strong>.</li></ul>
          </div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">u&#x27;Hi&#x27;</span>.encode(<span class="string">&#x27;ASCII&#x27;</span>)</span><br><span class="line"><span class="string">b&#x27;Hi&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48\x69&#x27;</span></span><br><span class="line"><span class="string">b&#x27;Hi&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48\x69&#x27;</span>.decode(<span class="string">&#x27;ASCII&#x27;</span>) <span class="comment"># hex (ascii) to str (unicode)</span></span><br><span class="line"><span class="string">&#x27;Hi&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">chr</span>(<span class="number">72</span>)</span><br><span class="line"><span class="string">&#x27;H&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48&#x27;</span></span><br><span class="line"><span class="string">b&#x27;H&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">ord</span>(<span class="string">b&#x27;\x48&#x27;</span>)</span><br><span class="line"><span class="number">72</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">hex</span>(<span class="number">72</span>)</span><br><span class="line"><span class="string">&#x27;0x48&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48&#x27;</span>.decode()</span><br><span class="line"><span class="string">&#x27;H&#x27;</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">BPE Tokenization</th>
<th style="text-align:center">#Code points</th>
<th style="text-align:center">Seq length</th>
<th style="text-align:center">OOV</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Unicode-level</td>
<td style="text-align:center">13k+</td>
<td style="text-align:center">L</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Byte-level</td>
<td style="text-align:center">256</td>
<td style="text-align:center">&#8804;4L</td>
<td style="text-align:center">✘</td>
</tr>
</tbody>
</table>
</div>
<p><em>Unicode code point</em> contains 130k+ points to cover the full space of textual characters, which can increase the base vocabulary size of BPE. Thus, <strong>Applying BPE to the byte sequence of language</strong> is a great idea proposed in GPT-2<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[GPT-2]Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)
">[14]</span></a></sup> to reduce the vocabulary size.  However, directly applying byte-level BPE can result in suboptimum because the greedy frequency-based heuristic in BPE tend to <span class="label danger"> merge common words into neighbors to generate overfit sub-tokens</span>, such as “-ing.”, “-ing!”, “-ing?”.</p>
<p>To avoid this, GPT-2<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[GPT-2]Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)
">[14]</span></a></sup> <span class="label success"> prevents BPE from merging across different character categories for any byte sequence except space</span>. With byte-level subwords, BBPE can represent any texts using moderate vocabulary size <strong>without out-of-vocabulary problem</strong>. Moreover, it will increase the byte sequence length to x4 maximum.</p>
<h4 id="BBPE"><a href="#BBPE" class="headerlink" title="BBPE"></a>BBPE</h4><p>The <span class="label warning">base vocabulary</span> contains all possible base characters in the training data. It can become large if all unicode characters are included. Thus, GPT-2<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[GPT-2]Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)
">[14]</span></a></sup> used Byte-level BPE (BBPE) by resorting to <em>byte</em> sequence of texts instead of unicode character strings for base vocabulary construction. It is also adopted by RoBERTa, BART, GPT-2, and GPT-3.</p>
<p><strong>Vocabulary Size</strong>&nbsp; The final vocabulary size is the size of base vocabulary plus the # of merges, where the # of merges is a hyperparameter. For instance,</p>
<ul>
<li>GPT (character-level BPE) has 40,478 vocabularies: 478 base vocabularies + 40k merges.</li>
<li>GPT-2 (byte-level BPE) has 50,257 vocabularies: 256 base vocabularies + 1 [EOS] token + 50k merges. </li>
</ul>
<p><img data-src="/notes/images/BPE-vs-BBPE-Ja-En.png" alt="Examples of Ja-En tokenization with various vocabulary sizes"></p>
<p>We plot the BPE encoding process as follows:</p>
<p><img data-src="/notes/images/BPE_encode_process.png" width="60%" alt="BPE merge process"/></p>
<h5 id="GPT-2-implementation"><a href="#GPT-2-implementation" class="headerlink" title="GPT-2 implementation"></a>GPT-2 implementation</h5><p>The implementation of GPT-2 &amp; RoBERTa.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">GPT-2 &amp; RoBERTa</span></span><br><span class="line"><span class="string">Byte pair encoding utilities from GPT-2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original source: https://github.com/openai/gpt-2/blob/master/src/encoder.py</span></span><br><span class="line"><span class="string">Original license: MIT</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bytes_to_unicode</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns list of utf-8 byte and a corresponding list of unicode strings.</span></span><br><span class="line"><span class="string">    The reversible bpe codes work on unicode strings.</span></span><br><span class="line"><span class="string">    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.</span></span><br><span class="line"><span class="string">    When you&#x27;re at something like a 10B token dataset you end up needing around 5K for decent coverage.</span></span><br><span class="line"><span class="string">    This is a signficant percentage of your normal, say, 32K bpe vocab.</span></span><br><span class="line"><span class="string">    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.</span></span><br><span class="line"><span class="string">    And avoids mapping to whitespace/control characters the bpe code barfs on.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bs = (</span><br><span class="line">        <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;!&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;~&quot;</span>) + <span class="number">1</span>))</span><br><span class="line">        + <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;¡&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;¬&quot;</span>) + <span class="number">1</span>))</span><br><span class="line">        + <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;®&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;ÿ&quot;</span>) + <span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">    cs = bs[:]</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span> ** <span class="number">8</span>):</span><br><span class="line">        <span class="keyword">if</span> b <span class="keyword">not</span> <span class="keyword">in</span> bs:</span><br><span class="line">            bs.append(b)</span><br><span class="line">            cs.append(<span class="number">2</span> ** <span class="number">8</span> + n)</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    cs = [<span class="built_in">chr</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> cs]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(<span class="built_in">zip</span>(bs, cs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pairs</span>(<span class="params">word</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return set of symbol pairs in a word.</span></span><br><span class="line"><span class="string">    Word is represented as tuple of symbols (symbols being variable-length strings).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pairs = <span class="built_in">set</span>()</span><br><span class="line">    prev_char = word[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> word[<span class="number">1</span>:]:</span><br><span class="line">        pairs.add((prev_char, char))</span><br><span class="line">        prev_char = char</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, bpe_merges, errors=<span class="string">&quot;replace&quot;</span></span>):</span></span><br><span class="line">        self.encoder = encoder <span class="comment"># bpe-vocab.json -&gt; &#123;subword:id&#125;</span></span><br><span class="line">        self.decoder = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.encoder.items()&#125; <span class="comment"># &#123;id: subword&#125;</span></span><br><span class="line">        self.errors = errors  <span class="comment"># how to handle errors in decoding</span></span><br><span class="line">        <span class="comment"># &#123;byte: unicode&#125;</span></span><br><span class="line">        self.byte_encoder = bytes_to_unicode()</span><br><span class="line">        self.byte_decoder = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.byte_encoder.items()&#125; <span class="comment"># &#123;unicode: byte&#125;</span></span><br><span class="line">        <span class="comment"># bpe-merges.txt -&gt; &#123;tuple: rank&#125;</span></span><br><span class="line">        self.bpe_ranks = <span class="built_in">dict</span>(<span class="built_in">zip</span>(bpe_merges, <span class="built_in">range</span>(<span class="built_in">len</span>(bpe_merges)))) </span><br><span class="line">        self.cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">            self.re = re</span><br><span class="line">        <span class="keyword">except</span> ImportError:</span><br><span class="line">            <span class="keyword">raise</span> ImportError(<span class="string">&quot;Please install regex with: pip install regex&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should have added re.IGNORECASE so BPE merges</span></span><br><span class="line">        <span class="comment"># can happen for capitalized versions of contractions</span></span><br><span class="line">        self.pat = self.re.<span class="built_in">compile</span>(</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bpe</span>(<span class="params">self, token</span>):</span></span><br><span class="line">        <span class="comment"># check if already processed</span></span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> self.cache:</span><br><span class="line">            <span class="keyword">return</span> self.cache[token]</span><br><span class="line">        word = <span class="built_in">tuple</span>(token)</span><br><span class="line">        pairs = get_pairs(word) <span class="comment"># count bigrams</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">            <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            bigram = <span class="built_in">min</span>(pairs, key=<span class="keyword">lambda</span> pair: self.bpe_ranks.get(pair, <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)))</span><br><span class="line">            <span class="keyword">if</span> bigram <span class="keyword">not</span> <span class="keyword">in</span> self.bpe_ranks:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            first, second = bigram</span><br><span class="line">            new_word = []</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="comment"># find all possible merges for a bigram</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    j = word.index(first, i)</span><br><span class="line">                    new_word.extend(word[i:j])</span><br><span class="line">                    i = j</span><br><span class="line">                <span class="keyword">except</span>: <span class="comment"># no further merge</span></span><br><span class="line">                    new_word.extend(word[i:])</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># bigram match &amp; satisfy length limit</span></span><br><span class="line">                <span class="keyword">if</span> word[i] == first <span class="keyword">and</span> i &lt; <span class="built_in">len</span>(word) - <span class="number">1</span> <span class="keyword">and</span> word[i + <span class="number">1</span>] == second: </span><br><span class="line">                    new_word.append(first + second)</span><br><span class="line">                    i += <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_word.append(word[i])</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            new_word = <span class="built_in">tuple</span>(new_word)</span><br><span class="line">            word = new_word <span class="comment"># update merged tokens</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(word) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pairs = get_pairs(word) <span class="comment"># new possible pairs</span></span><br><span class="line">        word = <span class="string">&quot; &quot;</span>.join(word)</span><br><span class="line">        self.cache[token] = word <span class="comment"># cache raw tokens</span></span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        bpe_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> self.re.findall(self.pat, text):</span><br><span class="line">            token = <span class="string">&quot;&quot;</span>.join(self.byte_encoder[b] <span class="keyword">for</span> b <span class="keyword">in</span> token.encode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">            bpe_tokens.extend(</span><br><span class="line">                self.encoder[bpe_token] <span class="keyword">for</span> bpe_token <span class="keyword">in</span> self.bpe(token).split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> bpe_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        text = <span class="string">&quot;&quot;</span>.join([self.decoder.get(token, token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line">        text = <span class="built_in">bytearray</span>([self.byte_decoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> text]).decode(</span><br><span class="line">            <span class="string">&quot;utf-8&quot;</span>, errors=self.errors</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encoder</span>(<span class="params">encoder_json_path:<span class="string">&quot;bpe-vocab.json&quot;</span>, vocab_bpe_path:<span class="string">&quot;bpe-merge.txt&quot;</span></span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(encoder_json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        encoder = json.load(f)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_bpe_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        bpe_data = f.read()</span><br><span class="line">    bpe_merges = [<span class="built_in">tuple</span>(merge_str.split()) <span class="keyword">for</span> merge_str <span class="keyword">in</span> bpe_data.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> Encoder(</span><br><span class="line">        encoder=encoder,</span><br><span class="line">        bpe_merges=bpe_merges,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># RoBERTa source code.</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> contextlib</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Helper script to encode raw text with the GPT-2 BPE using multiple processes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The encoder.json and vocab.bpe files can be obtained here:</span></span><br><span class="line"><span class="string">    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json</span></span><br><span class="line"><span class="string">    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--encoder-json&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;path to encoder.json&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--vocab-bpe&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;path to vocab.bpe&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--inputs&quot;</span>,</span><br><span class="line">        nargs=<span class="string">&quot;+&quot;</span>,</span><br><span class="line">        default=[<span class="string">&quot;-&quot;</span>],</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;input files to filter/encode&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--outputs&quot;</span>,</span><br><span class="line">        nargs=<span class="string">&quot;+&quot;</span>,</span><br><span class="line">        default=[<span class="string">&quot;-&quot;</span>],</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;path to save encoded outputs&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--keep-empty&quot;</span>,</span><br><span class="line">        action=<span class="string">&quot;store_true&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;keep empty lines&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--workers&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">20</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(args.inputs) == <span class="built_in">len</span>(</span><br><span class="line">        args.outputs</span><br><span class="line">    ), <span class="string">&quot;number of input and output paths should match&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> contextlib.ExitStack() <span class="keyword">as</span> stack:</span><br><span class="line">        inputs = [</span><br><span class="line">            stack.enter_context(<span class="built_in">open</span>(<span class="built_in">input</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">input</span> != <span class="string">&quot;-&quot;</span></span><br><span class="line">            <span class="keyword">else</span> sys.stdin</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> args.inputs</span><br><span class="line">        ]</span><br><span class="line">        outputs = [</span><br><span class="line">            stack.enter_context(<span class="built_in">open</span>(output, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">            <span class="keyword">if</span> output != <span class="string">&quot;-&quot;</span></span><br><span class="line">            <span class="keyword">else</span> sys.stdout</span><br><span class="line">            <span class="keyword">for</span> output <span class="keyword">in</span> args.outputs</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        encoder = MultiprocessingEncoder(args)</span><br><span class="line">        pool = Pool(args.workers, initializer=encoder.initializer)</span><br><span class="line">        encoded_lines = pool.imap(encoder.encode_lines, <span class="built_in">zip</span>(*inputs), <span class="number">100</span>)</span><br><span class="line">            </span><br><span class="line">        stats = Counter()</span><br><span class="line">        <span class="keyword">for</span> i, (filt, enc_lines) <span class="keyword">in</span> <span class="built_in">enumerate</span>(encoded_lines, start=<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> filt == <span class="string">&quot;PASS&quot;</span>:</span><br><span class="line">                <span class="keyword">for</span> enc_line, output_h <span class="keyword">in</span> <span class="built_in">zip</span>(enc_lines, outputs):</span><br><span class="line">                    <span class="built_in">print</span>(enc_line, file=output_h)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stats[<span class="string">&quot;num_filtered_&quot;</span> + filt] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;processed &#123;&#125; lines&quot;</span>.<span class="built_in">format</span>(i), file=sys.stderr)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> stats.most_common():</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[&#123;&#125;] filtered &#123;&#125; lines&quot;</span>.<span class="built_in">format</span>(k, v), file=sys.stderr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiprocessingEncoder</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initializer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> bpe</span><br><span class="line">        bpe = get_encoder(self.args.encoder_json, self.args.vocab_bpe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, line</span>):</span></span><br><span class="line">        <span class="keyword">global</span> bpe</span><br><span class="line">        ids = bpe.encode(line)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, ids))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="keyword">global</span> bpe</span><br><span class="line">        <span class="keyword">return</span> bpe.decode(tokens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode_lines</span>(<span class="params">self, lines</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Encode a set of lines. All lines will be encoded together.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_lines = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line) == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.args.keep_empty:</span><br><span class="line">                <span class="keyword">return</span> [<span class="string">&quot;EMPTY&quot;</span>, <span class="literal">None</span>]</span><br><span class="line">            tokens = self.encode(line)</span><br><span class="line">            enc_lines.append(<span class="string">&quot; &quot;</span>.join(tokens))</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;PASS&quot;</span>, enc_lines]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_lines</span>(<span class="params">self, lines</span>):</span></span><br><span class="line">        dec_lines = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            tokens = <span class="built_in">map</span>(<span class="built_in">int</span>, line.strip().split())</span><br><span class="line">            dec_lines.append(self.decode(tokens))</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;PASS&quot;</span>, dec_lines]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h5 id="tiktoken-implementation"><a href="#tiktoken-implementation" class="headerlink" title="tiktoken implementation"></a>tiktoken implementation</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; `tiktoken` BPE implementation (for educational purpose) &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> annotations</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> regex</span><br><span class="line"></span><br><span class="line"><span class="comment"># pip3 install tiktoken&gt;=0.4.0</span></span><br><span class="line"><span class="keyword">import</span> tiktoken </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleBytePairEncoding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *, pat_str: <span class="built_in">str</span>, mergeable_ranks: <span class="built_in">dict</span>[<span class="built_in">bytes</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Creates an Encoding object.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># A regex pattern string that is used to split the input text</span></span><br><span class="line">        self.pat_str = pat_str</span><br><span class="line">        <span class="comment"># A dictionary mapping token bytes to their ranks. The ranks correspond to merge priority</span></span><br><span class="line">        self.mergeable_ranks = mergeable_ranks</span><br><span class="line"></span><br><span class="line">        self._decoder = &#123;token: token_bytes <span class="keyword">for</span> token_bytes, token <span class="keyword">in</span> mergeable_ranks.items()&#125;</span><br><span class="line">        self._pat = regex.<span class="built_in">compile</span>(pat_str)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text: <span class="built_in">str</span>, visualise: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;colour&quot;</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Encodes a string into tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.encode(&quot;hello world&quot;)</span></span><br><span class="line"><span class="string">        [388, 372]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Use the regex to split the text into (approximately) words</span></span><br><span class="line">        words = self._pat.findall(text)</span><br><span class="line">        tokens = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="comment"># Turn each word into tokens, using the byte pair encoding algorithm</span></span><br><span class="line">            word_bytes = word.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            word_tokens = bpe_encode(self.mergeable_ranks, word_bytes, visualise=visualise)</span><br><span class="line">            tokens.extend(word_tokens)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_bytes</span>(<span class="params">self, tokens: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bytes</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes a list of tokens into bytes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.decode_bytes([388, 372])</span></span><br><span class="line"><span class="string">        b&#x27;hello world&#x27;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">b&quot;&quot;</span>.join(self._decoder[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, tokens: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes a list of tokens into a string.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Decoded bytes are not guaranteed to be valid UTF-8. In that case, we replace</span></span><br><span class="line"><span class="string">        the invalid bytes with the replacement character &quot;�&quot;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.decode([388, 372])</span></span><br><span class="line"><span class="string">        &#x27;hello world&#x27;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode_bytes(tokens).decode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;replace&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_tokens_bytes</span>(<span class="params">self, tokens: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">bytes</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes a list of tokens into a list of bytes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Useful for visualising how a string is tokenised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.decode_tokens_bytes([388, 372])</span></span><br><span class="line"><span class="string">        [b&#x27;hello&#x27;, b&#x27; world&#x27;]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> [self._decoder[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">training_data: <span class="built_in">str</span>, vocab_size: <span class="built_in">int</span>, pat_str: <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train a BPE tokeniser on some data!&quot;&quot;&quot;</span></span><br><span class="line">        mergeable_ranks = bpe_train(data=training_data, vocab_size=vocab_size, pat_str=pat_str)</span><br><span class="line">        <span class="keyword">return</span> SimpleBytePairEncoding(pat_str=pat_str, mergeable_ranks=mergeable_ranks)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_tiktoken</span>(<span class="params">encoding</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(encoding, <span class="built_in">str</span>):</span><br><span class="line">            encoding = tiktoken.get_encoding(encoding)</span><br><span class="line">        <span class="keyword">return</span> SimpleBytePairEncoding(</span><br><span class="line">            pat_str=encoding._pat_str, mergeable_ranks=encoding._mergeable_ranks</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bpe_encode</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    mergeable_ranks: <span class="built_in">dict</span>[<span class="built_in">bytes</span>, <span class="built_in">int</span>], <span class="built_in">input</span>: <span class="built_in">bytes</span>, visualise: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;colour&quot;</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">    parts = [<span class="built_in">bytes</span>([b]) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">input</span>]</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># See the intermediate merges play out!</span></span><br><span class="line">        <span class="keyword">if</span> visualise:</span><br><span class="line">            <span class="keyword">if</span> visualise <span class="keyword">in</span> [<span class="string">&quot;colour&quot;</span>, <span class="string">&quot;color&quot;</span>]:</span><br><span class="line">                visualise_tokens(parts)</span><br><span class="line">            <span class="keyword">elif</span> visualise == <span class="string">&quot;simple&quot;</span>:</span><br><span class="line">                <span class="built_in">print</span>(parts)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over all pairs and find the pair we want to merge the most</span></span><br><span class="line">        min_idx = <span class="literal">None</span></span><br><span class="line">        min_rank = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i, pair <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(parts[:-<span class="number">1</span>], parts[<span class="number">1</span>:])):</span><br><span class="line">            rank = mergeable_ranks.get(pair[<span class="number">0</span>] + pair[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (min_rank <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> rank &lt; min_rank):</span><br><span class="line">                min_idx = i</span><br><span class="line">                min_rank = rank</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If there were no pairs we could merge, we&#x27;re done!</span></span><br><span class="line">        <span class="keyword">if</span> min_rank <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">assert</span> min_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Otherwise, merge that pair and leave the rest unchanged. Then repeat.</span></span><br><span class="line">        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + <span class="number">1</span>]] + parts[min_idx + <span class="number">2</span> :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> visualise:</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    tokens = [mergeable_ranks[part] <span class="keyword">for</span> part <span class="keyword">in</span> parts]</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bpe_train</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    data: <span class="built_in">str</span>, vocab_size: <span class="built_in">int</span>, pat_str: <span class="built_in">str</span>, visualise: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;colour&quot;</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">bytes</span>, <span class="built_in">int</span>]:</span></span><br><span class="line">    <span class="comment"># First, add tokens for each individual byte value</span></span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; <span class="number">2</span>**<span class="number">8</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;vocab_size must be at least 256, so we can encode all bytes&quot;</span>)</span><br><span class="line">    ranks = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>**<span class="number">8</span>):</span><br><span class="line">        ranks[<span class="built_in">bytes</span>([i])] = i</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Splinter up our data into lists of bytes</span></span><br><span class="line">    <span class="comment"># data = &quot;Hello world&quot;</span></span><br><span class="line">    <span class="comment"># words = [</span></span><br><span class="line">    <span class="comment">#     [b&#x27;H&#x27;, b&#x27;e&#x27;, b&#x27;l&#x27;, b&#x27;l&#x27;, b&#x27;o&#x27;],</span></span><br><span class="line">    <span class="comment">#     [b&#x27; &#x27;, b&#x27;w&#x27;, b&#x27;o&#x27;, b&#x27;r&#x27;, b&#x27;l&#x27;, b&#x27;d&#x27;]</span></span><br><span class="line">    <span class="comment"># ]</span></span><br><span class="line">    words: <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">bytes</span>]] = [</span><br><span class="line">        [<span class="built_in">bytes</span>([b]) <span class="keyword">for</span> b <span class="keyword">in</span> word.encode(<span class="string">&quot;utf-8&quot;</span>)] <span class="keyword">for</span> word <span class="keyword">in</span> regex.findall(pat_str, data)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Now, use our data to figure out which merges we should make</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(ranks) &lt; vocab_size:</span><br><span class="line">        <span class="comment"># Find the most common pair. This will become our next token</span></span><br><span class="line">        stats = collections.Counter()</span><br><span class="line">        <span class="keyword">for</span> piece <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(piece[:-<span class="number">1</span>], piece[<span class="number">1</span>:]):</span><br><span class="line">                stats[pair] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        most_common_pair = <span class="built_in">max</span>(stats, key=<span class="keyword">lambda</span> x: stats[x])</span><br><span class="line">        token_bytes = most_common_pair[<span class="number">0</span>] + most_common_pair[<span class="number">1</span>]</span><br><span class="line">        token = <span class="built_in">len</span>(ranks)</span><br><span class="line">        <span class="comment"># Add the new token!</span></span><br><span class="line">        ranks[token_bytes] = token</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now merge that most common pair in all the words. That is, update our training data</span></span><br><span class="line">        <span class="comment"># to reflect our decision to make that pair into a new token.</span></span><br><span class="line">        new_words = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            new_word = []</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> (word[i], word[i + <span class="number">1</span>]) == most_common_pair:</span><br><span class="line">                    <span class="comment"># We found our pair! Merge it</span></span><br><span class="line">                    new_word.append(token_bytes)</span><br><span class="line">                    i += <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_word.append(word[i])</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(word) - <span class="number">1</span>:</span><br><span class="line">                new_word.append(word[i])</span><br><span class="line">            new_words.append(new_word)</span><br><span class="line">        words = new_words</span><br><span class="line"></span><br><span class="line">        <span class="comment"># See the intermediate merges play out!</span></span><br><span class="line">        <span class="keyword">if</span> visualise:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;The current most common pair is <span class="subst">&#123;most_common_pair[<span class="number">0</span>]&#125;</span> + <span class="subst">&#123;most_common_pair[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;So we made <span class="subst">&#123;token_bytes&#125;</span> our <span class="subst">&#123;<span class="built_in">len</span>(ranks)&#125;</span>th token&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> visualise <span class="keyword">in</span> [<span class="string">&quot;colour&quot;</span>, <span class="string">&quot;color&quot;</span>]:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Now the first fifty words in our training data look like:&quot;</span>)</span><br><span class="line">                visualise_tokens([token <span class="keyword">for</span> word <span class="keyword">in</span> words[:<span class="number">50</span>] <span class="keyword">for</span> token <span class="keyword">in</span> word])</span><br><span class="line">            <span class="keyword">elif</span> visualise == <span class="string">&quot;simple&quot;</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Now the first twenty words in our training data look like:&quot;</span>)</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> words[:<span class="number">20</span>]:</span><br><span class="line">                    <span class="built_in">print</span>(word)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ranks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualise_tokens</span>(<span class="params">token_values: <span class="built_in">list</span>[<span class="built_in">bytes</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    backgrounds = itertools.cycle(</span><br><span class="line">        [<span class="string">f&quot;\u001b[48;5;<span class="subst">&#123;i&#125;</span>m&quot;</span>.encode() <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">167</span>, <span class="number">179</span>, <span class="number">185</span>, <span class="number">77</span>, <span class="number">80</span>, <span class="number">68</span>, <span class="number">134</span>]]</span><br><span class="line">    )</span><br><span class="line">    interleaved = itertools.chain.from_iterable(<span class="built_in">zip</span>(backgrounds, token_values))</span><br><span class="line">    <span class="built_in">print</span>((<span class="string">b&quot;&quot;</span>.join(interleaved) + <span class="string">&quot;\u001b[0m&quot;</span>.encode()).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_simple_encoding</span>():</span></span><br><span class="line">    gpt2_pattern = (</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?[\p&#123;L&#125;]+| ?[\p&#123;N&#125;]+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(__file__, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.read()</span><br><span class="line"></span><br><span class="line">    enc = SimpleBytePairEncoding.train(data, vocab_size=<span class="number">600</span>, pat_str=gpt2_pattern)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;This is the sequence of merges performed in order to encode &#x27;hello world&#x27;:&quot;</span>)</span><br><span class="line">    tokens = enc.encode(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line">    <span class="keyword">assert</span> enc.decode(tokens) == <span class="string">&quot;hello world&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> enc.decode_bytes(tokens) == <span class="string">b&quot;hello world&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> enc.decode_tokens_bytes(tokens) == [<span class="string">b&quot;hello&quot;</span>, <span class="string">b&quot; world&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> enc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train a BPE tokeniser on a small amount of text</span></span><br><span class="line">enc = train_simple_encoding()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualise how the GPT-4 encoder encodes text</span></span><br><span class="line">enc = SimpleBytePairEncoding.from_tiktoken(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line">y = enc.encode(<span class="string">&quot;hello world aaaaaaaaaaaa&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><p>WordPiece<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
">[9]</span></a></sup><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)
">[10]</span></a></sup> can be viewed as a <strong>language-modeling based</strong> BPE variant. It trains with similar process to the BPE but uses disparate merge rule: WordPiece select the <span class="label warning">unit pair that maximizes the likelihood of training data at utmost</span>, rather than choose the most frequent pair. <strong>WordPiece chooses the subword pair that has the maximum mutual information value</strong>.</p>
<p>WordPiece scores the likelihood of possible pairs using an <em>n</em>-gram LM. <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
">[9]</span></a></sup> mentioned that training LMs for every possible merge is prohibit, they used aggressive heuristics to reduce the budget. However, the public training implementation is unavailable.</p>
<p>The BERT tokenization applies two tokenizers one after another:</p>
<ul>
<li><strong>BasicTokenizer</strong>:<ol>
<li>Convert text to unicode.</li>
<li>Clean text: invalid character removal and whitespace cleanup.</li>
<li>Use whitespace to seperate Chinese characters.</li>
<li>Whitespace tokenization.</li>
<li>Lowercase &amp; Strips accents.</li>
<li>Split punctuations.</li>
</ol>
</li>
<li><strong>WordpieceTokenizer</strong>:<ol>
<li>Convert texts to unicode.</li>
<li>Apply WordPiece, a greedy longest-match-first algorithm to perform tokenization given vocabulary.</li>
</ol>
</li>
</ul>
<p><img width="50%" data-src="/notes/images/wordpiece.gif" alt="WordPiece" /></p>
<div class="note info">
            <p>All Chinese inputs are split into characters as if no wordpiece applied. </p>
          </div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT Implementation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs end-to-end tokenziation.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_file, do_lower_case=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.vocab = load_vocab(vocab_file)</span><br><span class="line">    self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text):</span><br><span class="line">      <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">        split_tokens.append(sub_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span>(<span class="params">self, ids</span>):</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.inv_vocab, ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, do_lower_case=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      do_lower_case: Whether to lower case the input.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">    <span class="comment"># models. This is also applied to the English models now, but it doesn&#x27;t</span></span><br><span class="line">    <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">    <span class="comment"># and generally don&#x27;t have any Chinese data in them (there are Chinese</span></span><br><span class="line">    <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">    <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line"></span><br><span class="line">    orig_tokens = whitespace_tokenize(text)</span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">      <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line"></span><br><span class="line">    output_tokens = whitespace_tokenize(<span class="string">&quot; &quot;</span>.join(split_tokens))</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = unicodedata.normalize(<span class="string">&quot;NFD&quot;</span>, text)</span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">      cat = unicodedata.category(char)</span><br><span class="line">      <span class="keyword">if</span> cat == <span class="string">&quot;Mn&quot;</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      output.append(char)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    chars = <span class="built_in">list</span>(text)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    start_new_word = <span class="literal">True</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">      char = chars[i]</span><br><span class="line">      <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">        output.append([char])</span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> start_new_word:</span><br><span class="line">          output.append([])</span><br><span class="line">        start_new_word = <span class="literal">False</span></span><br><span class="line">        output[-<span class="number">1</span>].append(char)</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [<span class="string">&quot;&quot;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">      cp = <span class="built_in">ord</span>(char)</span><br><span class="line">      <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">        output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">        output.append(char)</span><br><span class="line">        output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output.append(char)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span>(<span class="params">self, cp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:</span></span><br><span class="line">    <span class="comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span></span><br><span class="line">    <span class="comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span></span><br><span class="line">    <span class="comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span></span><br><span class="line">    <span class="comment"># space-separated words, so they are not treated specially and handled</span></span><br><span class="line">    <span class="comment"># like the all of the other languages.</span></span><br><span class="line">    <span class="keyword">if</span> ((cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>) <span class="keyword">or</span></span><br><span class="line">        (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)):  <span class="comment">#</span></span><br><span class="line">      <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">      cp = <span class="built_in">ord</span>(char)</span><br><span class="line">      <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xfffd</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">        output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output.append(char)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs WordPiece tokenziation.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab, unk_token=<span class="string">&quot;[UNK]&quot;</span>, max_input_chars_per_word=<span class="number">200</span></span>):</span></span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">    using the given vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string">      input = &quot;unaffable&quot;</span></span><br><span class="line"><span class="string">      output = [&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">        already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">    output_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">      chars = <span class="built_in">list</span>(token)</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">len</span>(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      is_bad = <span class="literal">False</span></span><br><span class="line">      start = <span class="number">0</span></span><br><span class="line">      sub_tokens = []</span><br><span class="line">      <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">        end = <span class="built_in">len</span>(chars)</span><br><span class="line">        cur_substr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; end:</span><br><span class="line">          substr = <span class="string">&quot;&quot;</span>.join(chars[start:end])</span><br><span class="line">          <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            substr = <span class="string">&quot;##&quot;</span> + substr</span><br><span class="line">          <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          is_bad = <span class="literal">True</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_whitespace</span>(<span class="params">char</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Checks whether `chars` is a whitespace character.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># \t, \n, and \r are technically contorl characters but we treat them</span></span><br><span class="line">  <span class="comment"># as whitespace since they are generally considered as such.</span></span><br><span class="line">  <span class="keyword">if</span> char == <span class="string">&quot; &quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\t&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\n&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\r&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  cat = unicodedata.category(char)</span><br><span class="line">  <span class="keyword">if</span> cat == <span class="string">&quot;Zs&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_control</span>(<span class="params">char</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Checks whether `chars` is a control character.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># These are technically control characters but we count them as whitespace</span></span><br><span class="line">  <span class="comment"># characters.</span></span><br><span class="line">  <span class="keyword">if</span> char == <span class="string">&quot;\t&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\n&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\r&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  cat = unicodedata.category(char)</span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">in</span> (<span class="string">&quot;Cc&quot;</span>, <span class="string">&quot;Cf&quot;</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_punctuation</span>(<span class="params">char</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Checks whether `chars` is a punctuation character.&quot;&quot;&quot;</span></span><br><span class="line">  cp = <span class="built_in">ord</span>(char)</span><br><span class="line">  <span class="comment"># We treat all non-letter/number ASCII as punctuation.</span></span><br><span class="line">  <span class="comment"># Characters such as &quot;^&quot;, &quot;$&quot;, and &quot;`&quot; are not in the Unicode</span></span><br><span class="line">  <span class="comment"># Punctuation class but we treat them as punctuation anyways, for</span></span><br><span class="line">  <span class="comment"># consistency.</span></span><br><span class="line">  <span class="keyword">if</span> ((cp &gt;= <span class="number">33</span> <span class="keyword">and</span> cp &lt;= <span class="number">47</span>) <span class="keyword">or</span> (cp &gt;= <span class="number">58</span> <span class="keyword">and</span> cp &lt;= <span class="number">64</span>) <span class="keyword">or</span></span><br><span class="line">      (cp &gt;= <span class="number">91</span> <span class="keyword">and</span> cp &lt;= <span class="number">96</span>) <span class="keyword">or</span> (cp &gt;= <span class="number">123</span> <span class="keyword">and</span> cp &lt;= <span class="number">126</span>)):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  cat = unicodedata.category(char)</span><br><span class="line">  <span class="keyword">if</span> cat.startswith(<span class="string">&quot;P&quot;</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    vocab_file=<span class="string">&quot;./cased_L-12_H-768_A-12/vocab.txt&quot;</span></span><br><span class="line">    tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=<span class="literal">True</span>)</span><br><span class="line">    output_tokens = tokenizer.tokenize(<span class="string">&quot;&quot;&quot;This text is included to </span></span><br><span class="line"><span class="string">        make sure Unicode is handled properly: 力加勝北区ᴵᴺᵀᵃছজটডণত&quot;&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Unigram-Language-Model"><a href="#Unigram-Language-Model" class="headerlink" title="Unigram Language Model"></a>Unigram Language Model</h2><p>Unigram Language Model<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://www.aclweb.org/anthology/P18-1007.pdf)
">[11]</span></a></sup> initializes its base vocabulary with a large # of vocabulary and gradually removes a portion (e.g., 20%) of units according to the likelihood change. It use a unigram LM to evaluate the likelihood increase after subword removal, where the probability of each unit is computed using EM algorithm. The drop process will stop until reach the pre-defined vocabulary size.</p>
<p><img width="50%" data-src="/notes/images/Unigram-LM-tokenization.png" alt="Ungram LM algorithm <small>[11]</small>" /></p>
<p>Since unigram is not based on merge rules (in contrast to BPE and WordPiece), there has several ways of tokenizing new text after training. Therefore, unigram also saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. It simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their possibilities.</p>
<p>Assume that the set of all possible tokenizations for a word $x_i$ is defined as $S(x_i)$, the overall loss is defined as:</p>
<script type="math/tex; mode=display">
\mathcal{L} = - \sum_{i=1}^N \log \bigg( \sum_{x \in S(x_i)} p(x) \bigg)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> digamma</span><br><span class="line"></span><br><span class="line"><span class="comment"># To efficiently determine the next possible words</span></span><br><span class="line"><span class="comment"># We need a Trie data structure</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, word, value</span>):</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                node[ch] = &#123;&#125;</span><br><span class="line">            node = node[ch]</span><br><span class="line">        node[<span class="string">&#x27;&lt;END&gt;&#x27;</span>] = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            node = node[ch]</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;&lt;END&gt;&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> node[<span class="string">&#x27;&lt;END&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_value</span>(<span class="params">self, word, value</span>):</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;word not in trie&quot;</span>)</span><br><span class="line">            node = node[ch]</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;&lt;END&gt;&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;word not in trie&quot;</span>)</span><br><span class="line">        node[<span class="string">&#x27;&lt;END&gt;&#x27;</span>] = value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentencePieceTrainer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.trie = <span class="literal">None</span></span><br><span class="line">        self.maxlen = <span class="literal">None</span></span><br><span class="line">        self.vocab_size = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_trie</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        trie = Trie()</span><br><span class="line">        norm = <span class="built_in">sum</span>(<span class="built_in">list</span>(tokens.values()))</span><br><span class="line">        logsum = digamma(norm)</span><br><span class="line"></span><br><span class="line">        maxlen = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> tok, val <span class="keyword">in</span> tokens.items():</span><br><span class="line">            trie.add(tok, digamma(val)-logsum)</span><br><span class="line">            maxlen = <span class="built_in">max</span>(maxlen, <span class="built_in">len</span>(tok))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trie, maxlen</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_step</span>(<span class="params">self, text, trie</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># d[i] contains the maximum log_prob of any tokenization</span></span><br><span class="line">        <span class="comment"># of text[:i], initialized to 0 (i.e. log(0)=-infty)</span></span><br><span class="line">        d = [-np.inf]*(N+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># p[i] (stands for parent) contains the number of characters of</span></span><br><span class="line">        <span class="comment"># the final token in the most likely sequence that ends at index i</span></span><br><span class="line">        p = [<span class="literal">None</span>]*(N+<span class="number">1</span>)</span><br><span class="line">        d[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># find all possible final words. Have to look back</span></span><br><span class="line">            <span class="comment"># a distance set by the length of the longest token</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i-self.maxlen, <span class="number">0</span>), i):</span><br><span class="line"></span><br><span class="line">                final_token = text[j:i]</span><br><span class="line">                final_value = trie.get_value(final_token)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># if the current ending word has a higher log-probability,</span></span><br><span class="line">                <span class="comment"># save that value and store the word (i.e. # chars to backtrack)</span></span><br><span class="line">                <span class="keyword">if</span> final_value <span class="keyword">and</span> d[j]+final_value &gt; d[i]:</span><br><span class="line">                    d[i] = d[j]+final_value</span><br><span class="line">                    p[i] = <span class="built_in">len</span>(final_token)</span><br><span class="line">            <span class="keyword">if</span> p[i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Encountered unknown token &#x27;<span class="subst">&#123;text[i-<span class="number">1</span>]&#125;</span>&#x27;.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        loss = d[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> loss, p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward_step</span>(<span class="params">self, text, p</span>):</span></span><br><span class="line">        idx = <span class="built_in">len</span>(p)</span><br><span class="line">        tokenization = []</span><br><span class="line">        <span class="keyword">while</span> idx &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># move back the number of steps p tells you to</span></span><br><span class="line">            next_idx = idx-p[idx-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># extract the final token</span></span><br><span class="line">            tok = text[next_idx-<span class="number">1</span>:idx-<span class="number">1</span>]</span><br><span class="line">            tokenization.append(tok)</span><br><span class="line"></span><br><span class="line">            idx = next_idx</span><br><span class="line">        tokenization = <span class="built_in">list</span>(<span class="built_in">reversed</span>(tokenization))</span><br><span class="line">        <span class="keyword">return</span> tokenization</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">E_step</span>(<span class="params">self, tokenization, trie</span>):</span></span><br><span class="line">        <span class="comment"># get the new token counts based on updated tokenization</span></span><br><span class="line">        counts = collections.Counter(tokenization)</span><br><span class="line">        norm = <span class="built_in">sum</span>(<span class="built_in">list</span>(counts.values()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bayesianify them: https://cs.stanford.edu/~pliang/papers/tutorial-acl2007-talk.pdf</span></span><br><span class="line">        <span class="comment"># https://github.com/google/sentencepiece/blob/master/src/unigram_model_trainer.cc</span></span><br><span class="line">        <span class="comment"># we are returning the log probabilties here (alpha=0 prior)</span></span><br><span class="line">        logsum = digamma(norm)</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> counts.items():</span><br><span class="line">            counts[k] = digamma(v)-logsum</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> counts.items():</span><br><span class="line">            trie.set_value(k, v)</span><br><span class="line">        <span class="keyword">return</span> trie</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">M_step</span>(<span class="params">self, text, trie</span>):</span></span><br><span class="line">        loss, p = self.forward_step(text, trie)</span><br><span class="line">        tokenization = self.backward_step(text, p)</span><br><span class="line">        <span class="keyword">return</span> tokenization, loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">EM_step</span>(<span class="params">self, text, tokenization, trie</span>):</span></span><br><span class="line">        trie = self.E_step(tokenization, trie)</span><br><span class="line">        tokenization, loss = self.M_step(text, trie)</span><br><span class="line">        <span class="keyword">return</span> loss, tokenization, trie</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">EM_round</span>(<span class="params">self, text, tokens, delta=<span class="number">0.01</span>, max_iter=<span class="number">10</span></span>):</span></span><br><span class="line">        tokenization, old_loss = self.M_step(text, self.trie)</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;EM iter <span class="subst">&#123;step&#125;</span>: &quot;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            loss, tokenization, trie = self.EM_step(text, tokenization, self.trie)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Loss=<span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(old_loss-loss) &lt; delta:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            old_loss = loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_tokens</span>(<span class="params">self, tokens, characters, vocab_size, trim_frac=<span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Tokens are passed by reference and modified in place.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            True: to indicate to caller that more rounds are needed</span></span><br><span class="line"><span class="string">            False: to indicate we successfully hit the target vocab size</span></span><br><span class="line"><span class="string">            ValueError: if the vocab size cannot be reached.&quot;&quot;&quot;</span></span><br><span class="line">        sorted_tokens = tokens.most_common()</span><br><span class="line">        N = <span class="built_in">len</span>(sorted_tokens)</span><br><span class="line">        n_trim = <span class="built_in">int</span>(trim_frac*N)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(N)):</span><br><span class="line">            <span class="keyword">if</span> N &lt;= vocab_size:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> n_trim &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            tok = sorted_tokens[i][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> tok <span class="keyword">not</span> <span class="keyword">in</span> characters:</span><br><span class="line">                self.trie.set_value(tok, <span class="number">0</span>) <span class="comment"># we need to delete it from the trie (that sticks around)</span></span><br><span class="line">                tokens.pop(tok) <span class="comment"># also need to delete from tokens, so the next round doesn&#x27;t see it</span></span><br><span class="line">                n_trim -= <span class="number">1</span></span><br><span class="line">                N -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n_trim &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Could not reduce tokens further. Please increase vocab size&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, text, tokens, characters, vocab_size, delta=<span class="number">0.01</span>, max_iter=<span class="number">5</span>, max_rounds=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; To turn off pruning, just set max_rounds=1 &quot;&quot;&quot;</span></span><br><span class="line">        text = re.sub(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;_&#x27;</span>, text)</span><br><span class="line">        <span class="keyword">if</span> vocab_size &gt; <span class="built_in">len</span>(tokens):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Vocab size is larger than the availble number of tokens <span class="subst">&#123;<span class="built_in">len</span>(tokens)&#125;</span>.&quot;</span>)</span><br><span class="line">        self.trie, self.maxlen = self._initialize_trie(tokens)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_rounds+<span class="number">1</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;--- Round <span class="subst">&#123;i&#125;</span>. Vocab size: <span class="subst">&#123;<span class="built_in">len</span>(tokens)&#125;</span> ---&quot;</span>)</span><br><span class="line">            self.EM_round(text, tokens, delta, max_iter)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.prune_tokens(tokens, characters, vocab_size):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generalized_forward_step</span>(<span class="params">self, text, trie, nbest_size=<span class="number">1</span></span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(text)</span><br><span class="line">        d = [-np.inf]*(N+<span class="number">1</span>)</span><br><span class="line">        p = [<span class="literal">None</span>]*(N+<span class="number">1</span>)</span><br><span class="line">        d[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">            d_queue = []</span><br><span class="line">            p_queue = []</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i-self.maxlen, <span class="number">0</span>), i):</span><br><span class="line">                final_token = text[j:i]</span><br><span class="line">                final_value = trie.get_value(final_token)</span><br><span class="line">                <span class="keyword">if</span> final_value:</span><br><span class="line">                    curr_d = d[j]+final_value</span><br><span class="line">                    curr_p = <span class="built_in">len</span>(final_token)</span><br><span class="line">                    d[i] = <span class="built_in">max</span>(d[i], curr_d)</span><br><span class="line">                    d_queue.append(curr_d)</span><br><span class="line">                    p_queue.append(curr_p)</span><br><span class="line">            ids = np.argsort(d_queue)[-nbest_size:]</span><br><span class="line">            p[i] = [p_queue[z] <span class="keyword">for</span> z <span class="keyword">in</span> ids]</span><br><span class="line">        <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generalized_backward_step</span>(<span class="params">self, text, p</span>):</span></span><br><span class="line">        idx = <span class="built_in">len</span>(p)</span><br><span class="line">        tokenization = []</span><br><span class="line">        <span class="keyword">while</span> idx &gt; <span class="number">1</span>:</span><br><span class="line">            back_steps = np.random.choice(p[idx-<span class="number">1</span>])</span><br><span class="line">            next_idx = idx-back_steps</span><br><span class="line">            tok = text[next_idx-<span class="number">1</span>:idx-<span class="number">1</span>]</span><br><span class="line">            tokenization.append(tok)</span><br><span class="line">            idx = next_idx</span><br><span class="line">        tokenization = <span class="built_in">list</span>(<span class="built_in">reversed</span>(tokenization))</span><br><span class="line">        <span class="keyword">return</span> tokenization</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text, nbest_size=<span class="number">1</span></span>):</span></span><br><span class="line">        text = re.sub(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;_&#x27;</span>, text)</span><br><span class="line">        <span class="keyword">if</span> self.trie <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Trainer has not yet been fit. Cannot tokenize.&quot;</span>)</span><br><span class="line">        p = self.generalized_forward_step(text, self.trie, nbest_size)</span><br><span class="line">        tokenization = self.generalized_backward_step(text, p)</span><br><span class="line">        <span class="keyword">return</span> tokenization</span><br></pre></td></tr></table></figure>
<p>Refer to <sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)
">[18]</span></a></sup> for details.</p>
<h2 id="SentencePiece-Library"><a href="#SentencePiece-Library" class="headerlink" title="SentencePiece Library"></a>SentencePiece Library</h2><p>SentencePiece<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226)
">[12]</span></a></sup><sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SentencePiece](https://github.com/google/sentencepiece)
">[17]</span></a></sup> includes the space in the base vocabulary then use BPE or unigram algorithm to tokenize. XLNet, T5, ALBERT use SentencePiece for subword tokenization. It uses the unigram by default.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SentencePiece</span></span><br><span class="line">--byte_fallback: (<span class="built_in">type</span>: bool, default: <span class="literal">false</span>)</span><br><span class="line">    decompose unknown pieces into UTF-8 byte pieces.    </span><br><span class="line">    Note: need to <span class="built_in">set</span> --character_coverage less than 1.0, otherwise byte-fall-backed tokens may not appear <span class="keyword">in</span> the training data.</span><br><span class="line">--character_coverage: (<span class="built_in">type</span>: double; default:0.9995)</span><br><span class="line">    character coverage of determining the minimal symbols.</span><br><span class="line">    </span><br><span class="line"><span class="comment"># see: https://github.com/google/sentencepiece/blob/master/doc/options.md</span></span><br></pre></td></tr></table></figure>
<p>Pros：</p>
<ol>
<li>C++ implementations makes it blazingly fast to tokenize.</li>
<li>It is <strong>whitespace agnostic</strong>, supporting to train non-whitespace delineated languages, such as Chinese and Japanese with the same ease as English or French.<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)
">[18]</span></a></sup></li>
<li>It works at the byte level.</li>
</ol>
<h3 id="Basic-usage"><a href="#Basic-usage" class="headerlink" title="Basic usage"></a>Basic usage</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># env / data</span></span><br><span class="line">pip install sentencepiece</span><br><span class="line">wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line"><span class="comment"># train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`</span></span><br><span class="line"><span class="comment"># `m.vocab` is just a reference. not used in the segmentation.</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># makes segmenter instance and loads the model file (m.model)</span></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode: text =&gt; id</span></span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;This is a test&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_ids(<span class="string">&#x27;This is a test&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># decode: id =&gt; text</span></span><br><span class="line"><span class="built_in">print</span>(sp.decode_pieces([<span class="string">&#x27;▁This&#x27;</span>, <span class="string">&#x27;▁is&#x27;</span>, <span class="string">&#x27;▁a&#x27;</span>, <span class="string">&#x27;▁t&#x27;</span>, <span class="string">&#x27;est&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(sp.decode_ids([<span class="number">209</span>, <span class="number">31</span>, <span class="number">9</span>, <span class="number">375</span>, <span class="number">586</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns vocab size</span></span><br><span class="line"><span class="built_in">print</span>(sp.get_piece_size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># id &lt;=&gt; piece conversion</span></span><br><span class="line"><span class="built_in">print</span>(sp.id_to_piece(<span class="number">209</span>))</span><br><span class="line"><span class="built_in">print</span>(sp.piece_to_id(<span class="string">&#x27;▁This&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns 0 for unknown tokens (we can change the id for UNK)</span></span><br><span class="line"><span class="built_in">print</span>(sp.piece_to_id(<span class="string">&#x27;__MUST_BE_UNKNOWN__&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;unk&gt;, &lt;s&gt;, &lt;/s&gt; are defined by default. Their ids are (0, 1, 2)</span></span><br><span class="line"><span class="comment"># &lt;s&gt; and &lt;/s&gt; are defined as &#x27;control&#x27; symbol.</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">  <span class="built_in">print</span>(sp.id_to_piece(<span class="built_in">id</span>), sp.is_control(<span class="built_in">id</span>))</span><br></pre></td></tr></table></figure>
<h3 id="User-defined-and-control-symbols"><a href="#User-defined-and-control-symbols" class="headerlink" title="User defined and control symbols"></a>User defined and control symbols</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Example of user defined symbols</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_user --user_defined_symbols=&lt;sep&gt;,&lt;cls&gt; --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp_user = spm.SentencePieceProcessor()</span><br><span class="line">sp_user.load(<span class="string">&#x27;m_user.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ids are reserved in both mode.</span></span><br><span class="line"><span class="comment"># &lt;unk&gt;=0, &lt;s&gt;=1, &lt;/s&gt;=2, &lt;sep&gt;=3, &lt;cls&gt;=4</span></span><br><span class="line"><span class="comment"># user defined symbols allow these symbol to apper in the text.</span></span><br><span class="line"><span class="built_in">print</span>(sp_user.encode_as_pieces(<span class="string">&#x27;this is a test&lt;sep&gt; hello world&lt;cls&gt;&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sp_user.piece_to_id(<span class="string">&#x27;&lt;sep&gt;&#x27;</span>))  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(sp_user.piece_to_id(<span class="string">&#x27;&lt;cls&gt;&#x27;</span>))  <span class="comment"># 4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;3=&#x27;</span>, sp_user.decode_ids([<span class="number">3</span>]))  <span class="comment"># decoded to &lt;sep&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;4=&#x27;</span>, sp_user.decode_ids([<span class="number">4</span>]))  <span class="comment"># decoded to &lt;cls&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="Unigram-sampling-and-nbest-segmentation-for-subword-regularization"><a href="#Unigram-sampling-and-nbest-segmentation-for-subword-regularization" class="headerlink" title="Unigram: sampling and nbest segmentation for subword regularization"></a>Unigram: sampling and nbest segmentation for subword regularization</h3><p>When <code>--model_type=unigram</code> (default) is used, we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://www.aclweb.org/anthology/P18-1007.pdf)
">[11]</span></a></sup> for more detail. <code>nbest_size</code> is the number of highest-ranked groups of tokens to sample from at each time, where <strong>-1</strong> means all of the possibilities.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Can obtain different segmentations per request.</span></span><br><span class="line"><span class="comment"># There are two hyperparamenters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  <span class="built_in">print</span>(sp.sample_encode_as_pieces(<span class="string">&#x27;hello world&#x27;</span>, -<span class="number">1</span>, <span class="number">0.1</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  <span class="built_in">print</span>(sp.sample_encode_as_ids(<span class="string">&#x27;hello world&#x27;</span>, -<span class="number">1</span>, <span class="number">0.1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sample</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    result = sp.encode(<span class="string">&#x27;This is a test&#x27;</span>, out_type=<span class="built_in">str</span>, enable_sampling=<span class="literal">True</span>, alpha=<span class="number">0.1</span>, nbest_size=-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get 10 best</span></span><br><span class="line"><span class="built_in">print</span>(sp.nbest_encode_as_pieces(<span class="string">&#x27;hello world&#x27;</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(sp.nbest_encode_as_ids(<span class="string">&#x27;hello world&#x27;</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h3 id="BPE-model"><a href="#BPE-model" class="headerlink" title="BPE model"></a>BPE model</h3><p>Sentencepiece also supports BPE (byte pair encoding) model by setting <code>--model_type=bpe</code>. The BPE model does not support sampling and n-best segmentation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe&#x27;</span>)</span><br><span class="line">sp_bpe = spm.SentencePieceProcessor()</span><br><span class="line">sp_bpe.load(<span class="string">&#x27;m_bpe.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;*** BPE ***&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(sp_bpe.encode_as_pieces(<span class="string">&#x27;thisisatesthelloworld&#x27;</span>)) <span class="comment"># [&#x27;▁this&#x27;, &#x27;is&#x27;, &#x27;at&#x27;, &#x27;est&#x27;, &#x27;he&#x27;, &#x27;llow&#x27;, &#x27;or&#x27;, &#x27;ld&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(sp_bpe.nbest_encode_as_pieces(<span class="string">&#x27;hello world&#x27;</span>, <span class="number">5</span>))  <span class="comment"># [] (returns an empty list)</span></span><br></pre></td></tr></table></figure>
<h3 id="Character-and-word-model"><a href="#Character-and-word-model" class="headerlink" title="Character and word model"></a>Character and word model</h3><p>Sentencepiece supports character and word segmentation with <code>--model_type=char</code> and <code>--model_type=character</code> flags.<br>In <code>word</code> segmentation, sentencepiece just segments tokens with whitespaces, so the input text must be pre-tokenized. We can apply different segmentation algorithm transparently without changing pre/post processors.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># char model</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_char --model_type=char --vocab_size=400&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp_char = spm.SentencePieceProcessor()</span><br><span class="line">sp_char.load(<span class="string">&#x27;m_char.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sp_char.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sp_char.encode_as_ids(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word model</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_word --model_type=word --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp_word = spm.SentencePieceProcessor()</span><br><span class="line">sp_word.load(<span class="string">&#x27;m_word.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sp_word.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))  <span class="comment"># &#x27;.&#x27; will not be one token.</span></span><br><span class="line"><span class="built_in">print</span>(sp_word.encode_as_ids(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="Text-normalization"><a href="#Text-normalization" class="headerlink" title="Text normalization"></a>Text normalization</h3><p>Sentencepiece provides the following general pre-defined normalization rules. We can change the normalizer with <code>--normaliation_rule_name=&lt;NAME&gt;</code> flag.</p>
<ul>
<li><strong>nmt_nfkc</strong>: NFKC normalization with some additional normalization around spaces. (default)</li>
<li><strong>nfkc</strong>: original: NFKC normalization.</li>
<li><strong>nmt_nfkc_cf</strong>: nmt_nfkc + Unicode case folding (mostly lower casing)</li>
<li><strong>nfkc_cf</strong>: nfkc + Unicode case folding.</li>
<li><strong>identity</strong>: no normalization</li>
</ul>
<p>The TSV file is fed with <code>--normalization_rule_tsv=&lt;FILE&gt;</code> flag.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tocode</span>(<span class="params">s</span>):</span>                                                                               </span><br><span class="line">    out = []                                                                                 </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> s:                                                                              </span><br><span class="line">        out.append(<span class="built_in">str</span>(<span class="built_in">hex</span>(<span class="built_in">ord</span>(c))).replace(<span class="string">&#x27;0x&#x27;</span>, <span class="string">&#x27;U+&#x27;</span>))                                     </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(out)          </span><br><span class="line"></span><br><span class="line"><span class="comment"># TSV format:  source Unicode code points &lt;tab&gt; target code points</span></span><br><span class="line"><span class="comment"># normalize &quot;don&#x27;t =&gt; do not,  I&#x27;m =&gt; I am&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;normalization_rule.tsv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  f.write(tocode(<span class="string">&quot;I&#x27;m&quot;</span>) + <span class="string">&#x27;\t&#x27;</span> + tocode(<span class="string">&quot;I am&quot;</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">  f.write(tocode(<span class="string">&quot;don&#x27;t&quot;</span>) + <span class="string">&#x27;\t&#x27;</span> + tocode(<span class="string">&quot;do not&quot;</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">open</span>(<span class="string">&#x27;normalization_rule.tsv&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).read())</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_tsv=normalization_rule.tsv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line"><span class="comment"># m.model embeds the normalization rule compiled into an FST.</span></span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&quot;I&#x27;m busy&quot;</span>))  <span class="comment"># normalzied to `I am busy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&quot;I don&#x27;t know it.&quot;</span>))  <span class="comment"># normalized to &#x27;I do not know it.&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="Vocabulary-restriction"><a href="#Vocabulary-restriction" class="headerlink" title="Vocabulary restriction"></a>Vocabulary restriction</h3><p>We can encode the text only using the tokens specified with <code>set_vocabulary</code> method.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gets all tokens as Python list.</span></span><br><span class="line">vocabs = [sp.id_to_piece(<span class="built_in">id</span>) <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(sp.get_piece_size())]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Aggregates the frequency of each token in the training data.</span></span><br><span class="line">freq = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;botchan.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        line = line.rstrip()</span><br><span class="line">        <span class="keyword">for</span> piece <span class="keyword">in</span> sp.encode_as_pieces(line):</span><br><span class="line">            freq.setdefault(piece, <span class="number">0</span>)</span><br><span class="line">            freq[piece] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"><span class="comment"># only uses the token appearing more than 1000 times in the training data.</span></span><br><span class="line">vocabs = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x : x <span class="keyword">in</span> freq <span class="keyword">and</span> freq[x] &gt; <span class="number">1000</span>, vocabs))</span><br><span class="line">sp.set_vocabulary(vocabs)</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the restriction</span></span><br><span class="line">sp.reset_vocabulary()</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="Extracting-crossing-words-pieces"><a href="#Extracting-crossing-words-pieces" class="headerlink" title="Extracting crossing-words pieces"></a>Extracting crossing-words pieces</h3><p>Sentencepieces does not extract pieces crossing multiple <strong>words</strong> (here the <strong>word</strong> means the space delimited tokens). The piece will never contain the whitespace marker (_) in the middle.</p>
<p><code>--split_by_whtespace=false</code> disables this restriction and allows to extract pieces crossing multiple words. In CJK (Chinese/Japanese/Korean), this flag will not affect the final segmentation results so much as words are not tokenized with whitespaces in CJK.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000 --split_by_whitespace=false&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gets all tokens as Python list.</span></span><br><span class="line">vocabs = [sp.id_to_piece(<span class="built_in">id</span>) <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(sp.get_piece_size())]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> piece <span class="keyword">in</span> vocabs[<span class="number">0</span>:<span class="number">500</span>]:</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">&#x27;\w+▁\w+&#x27;</span>, piece):</span><br><span class="line">        <span class="built_in">print</span>(piece)</span><br></pre></td></tr></table></figure>
<h3 id="Getting-byte-offsets-of-tokens"><a href="#Getting-byte-offsets-of-tokens" class="headerlink" title="Getting byte offsets of tokens"></a>Getting byte offsets of tokens</h3><p>Sentencepiece keeps track of byte offset (span) of each token, which is useful for highlighting the token on top of unnormalized text.</p>
<p>We first need to install <strong>protobuf</strong> module and <strong>sentencepiece_pb2.py</strong> as the byte offsets and all other meta data for segementation are encoded in protocol buffer. <code>encode_as_serialized_proto</code> method resturns serialized SentencePieceText proto. You can get the deserialized object by calling ParseFromString method.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install protobuf</span><br><span class="line">wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece_pb2</span><br><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One best result</span></span><br><span class="line">spt = sentencepiece_pb2.SentencePieceText()</span><br><span class="line">spt.ParseFromString(sp.encode_as_serialized_proto(<span class="string">&#x27;ｈｅｌｌｏ&#x27;</span>)) <span class="comment"># Full width hello</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># begin/end (offsets) are pointing to the original input.</span></span><br><span class="line"><span class="built_in">print</span>(spt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nbest results</span></span><br><span class="line">nspt = sentencepiece_pb2.NBestSentencePieceText()</span><br><span class="line">nspt.ParseFromString(sp.nbest_encode_as_serialized_proto(<span class="string">&#x27;ｈｅｌｌｏ&#x27;</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># print(nspt)</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">text: &quot;\357\275\210\357\275\205\357\275\214\357\275\214\357\275\217&quot;</span></span><br><span class="line"><span class="string">pieces &#123;</span></span><br><span class="line"><span class="string">  piece: &quot;\342\226\201he&quot;</span></span><br><span class="line"><span class="string">  id: 28</span></span><br><span class="line"><span class="string">  surface: &quot;\357\275\210\357\275\205&quot;</span></span><br><span class="line"><span class="string">  begin: 0</span></span><br><span class="line"><span class="string">  end: 6</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">pieces &#123;</span></span><br><span class="line"><span class="string">  piece: &quot;ll&quot;</span></span><br><span class="line"><span class="string">  id: 98</span></span><br><span class="line"><span class="string">  surface: &quot;\357\275\214\357\275\214&quot;</span></span><br><span class="line"><span class="string">  begin: 6</span></span><br><span class="line"><span class="string">  end: 12</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">pieces &#123;</span></span><br><span class="line"><span class="string">  piece: &quot;o&quot;</span></span><br><span class="line"><span class="string">  id: 38</span></span><br><span class="line"><span class="string">  surface: &quot;\357\275\217&quot;</span></span><br><span class="line"><span class="string">  begin: 12</span></span><br><span class="line"><span class="string">  end: 15</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="Add-new-special-tokens"><a href="#Add-new-special-tokens" class="headerlink" title="Add new special tokens"></a>Add new special tokens</h3><p>For the need of expanding new special tokens to pre-trained sentencepiece model, such as <code>[MASK0-99]</code>, <code>[DOMAIN0-99]</code>, and so on.<br>Ref: <sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[sentencepiece/python/add_new_vocab.ipynb](https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb)
">[19]</span></a></sup><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Run this code in google/sentencepiece/python/</span></span><br><span class="line"><span class="comment"># Load pre-trained sentencepiece model</span></span><br><span class="line"><span class="keyword">import</span> sentencepiece_model_pb2 <span class="keyword">as</span> model</span><br><span class="line">m = model.ModelProto()</span><br><span class="line">m.ParseFromString(<span class="built_in">open</span>(<span class="string">&quot;old.model&quot;</span>, <span class="string">&quot;rb&quot;</span>).read())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the list of new tokens want to add</span></span><br><span class="line">special_tokens = <span class="built_in">open</span>(<span class="string">&quot;special_tokens.txt&quot;</span>, <span class="string">&quot;r&quot;</span>).read().split(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add new tokens to sentencepiece model</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> special_tokens:</span><br><span class="line">    new_token = model.ModelProto().SentencePiece()</span><br><span class="line">    new_token.piece = token</span><br><span class="line">    new_token.score = <span class="number">0</span></span><br><span class="line">    m.pieces.append(new_token)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Save new sentencepiece model</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;new.model&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(m.SerializeToString())</span><br></pre></td></tr></table></figure></p>
<h3 id="Handle-whitespaces-newlines"><a href="#Handle-whitespaces-newlines" class="headerlink" title="Handle whitespaces/newlines"></a>Handle whitespaces/newlines</h3><p><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece/issues/684">GitHub Issue</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--remove_extra_whitespaces=<span class="literal">false</span></span><br><span class="line"><span class="comment"># In addition, newlines are all normalized whitespaces internally by default. You can stop all normalizations with </span></span><br><span class="line">--normalization_rule_name=identity</span><br></pre></td></tr></table></figure></p>
<p>Ref:<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub issue: Manually modifying SentencePiece model?](https://github.com/google/sentencepiece/issues/121)
">[20]</span></a></sup>.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">% <span class="built_in">cd</span> src</span><br><span class="line">% protoc --python_out=. sentencepiece_model.proto</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; import sentencepiece_model_pb2 as model</span><br><span class="line">&gt;&gt;&gt; m = model.ModelProto()</span><br><span class="line">&gt;&gt;&gt; m.ParseFromString(open(<span class="string">&#x27;../python/test/test_ja_model.model&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>).<span class="built_in">read</span>())</span><br><span class="line">352301</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> p <span class="keyword">in</span> m.pieces:</span><br><span class="line">...     p.score += 10.0</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; with open(<span class="string">&#x27;new.model&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) as f:</span><br><span class="line">...     f.write(m.SerializeToString())</span><br></pre></td></tr></table></figure><br>Refer to <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=T7F349Sd2Bzg">Sentencepiece python module example</a></p>
<h2 id="Huggingface-tokenizers"><a href="#Huggingface-tokenizers" class="headerlink" title="Huggingface tokenizers"></a>Huggingface tokenizers</h2><h3 id="Add-special-tokens"><a href="#Add-special-tokens" class="headerlink" title="Add special tokens"></a>Add special tokens</h3><p><sup id="fnref:22"><a href="#fn:22" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[why does huggingface t5 tokenizer ignore some of the whitespaces?](https://stackoverflow.com/questions/72214408/why-does-huggingface-t5-tokenizer-ignore-some-of-the-whitespaces)
">[22]</span></a></sup><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> AddedToken</span><br><span class="line">tokenizer.add_special_tokens(&#123;<span class="string">&quot;additional_special_tokens&quot;</span>: [AddedToken(<span class="string">&quot;\n&quot;</span>)]&#125;)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.special_tokens_map)</span><br></pre></td></tr></table></figure></p>
<h3 id="Handle-non-space-separated-language"><a href="#Handle-non-space-separated-language" class="headerlink" title="Handle non-space-separated language"></a>Handle non-space-separated language</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/huggingface/tokenizers/issues/990</span></span><br><span class="line">from tokenizers import trainers, models, Tokenizer, pre_tokenizers</span><br><span class="line"></span><br><span class="line">pre_tokenizer = pre_tokenizers.Sequence(</span><br><span class="line">    [</span><br><span class="line">        pre_tokenizers.WhitespaceSplit(),</span><br><span class="line">        pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pre_tokenizer.pre_tokenize_str(<span class="string">&quot;私 は りんご が 好き です&quot;</span>))</span><br><span class="line"><span class="comment"># [(&#x27;ç§ģ&#x27;, (0, 1)), (&#x27;ãģ¯&#x27;, (2, 3)), (&#x27;ãĤĬãĤĵãģĶ&#x27;, (4, 7)), (&#x27;ãģĮ&#x27;, (8, 9)), (&#x27;å¥½ãģį&#x27;, (10, 12)), (&#x27;ãģ§ãģĻ&#x27;, (13, 15))]</span></span><br></pre></td></tr></table></figure>
<h2 id="OpenAI-tiktoken"><a href="#OpenAI-tiktoken" class="headerlink" title="OpenAI tiktoken"></a>OpenAI tiktoken</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># install</span></span><br><span class="line">pip install --upgrade tiktoken</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"></span><br><span class="line">enc = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line"></span><br><span class="line">toks = enc.encode(<span class="string">&quot;tiktoken tokenizer&quot;</span>)</span><br><span class="line"></span><br><span class="line">x = enc.decode([<span class="number">83</span>, <span class="number">1609</span>, <span class="number">5963</span>, <span class="number">374</span>, <span class="number">2294</span>, <span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Refer to <sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
">[23]</span></a></sup><sup id="fnref:24"><a href="#fn:24" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[OpenAI tiktoken](https://github.com/openai/tiktoken)
">[24]</span></a></sup><sup id="fnref:25"><a href="#fn:25" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[OpenAI API Tokenizer](https://platform.openai.com/tokenizer)">[25]</span></a></sup>.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>OpenAI models</strong></th>
<th><strong>Vocab</strong></th>
<th><strong>#Vocab</strong></th>
<th><strong>#Numeric<br> token</strong></th>
<th><strong>Task</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt2</td>
<td>gpt2</td>
<td>50257</td>
<td>1691</td>
<td>open source</td>
</tr>
<tr>
<td>davinci/curie/gabbage/ada<br>text-{name}-001 (name=davinci/curie/gabbage/ada)</td>
<td>r50k_base</td>
<td>50257</td>
<td>1691</td>
<td>old text</td>
</tr>
<tr>
<td>text-similarity-davinci-001<br>text-search-{name}-doc-001<br>(name=davinci/curie/gabbage/ada)<br>code-search-{name}-code-001<br>(name=gabbage/ada)</td>
<td>r50k_base</td>
<td>50257</td>
<td>1691</td>
<td>old embedding</td>
</tr>
<tr>
<td>text-davinci-003<br>text-davinci-002</td>
<td>p50k_base</td>
<td>50281<br>(add whitespace tokens of length 2-24)</td>
<td>1691</td>
<td>text</td>
</tr>
<tr>
<td>code-davinci-001/002<br>code-cushman-001/002<br>davinci/cushman-codex</td>
<td>p50k_base</td>
<td>50281<br>(add whitespace tokens of length 2-24)</td>
<td>1691</td>
<td>code</td>
</tr>
<tr>
<td>text/code-davinci-edit-001</td>
<td>p50k_edit</td>
<td>50284 (add 3 special tokens)</td>
<td>1691</td>
<td>edit</td>
</tr>
<tr>
<td>text-embedding-ada-002</td>
<td>cl100k_base</td>
<td>100256</td>
<td>1122</td>
<td>embedding</td>
</tr>
<tr>
<td>gpt-3.5-turbo<br>gpt-4</td>
<td>cl100k_base</td>
<td>100256</td>
<td>1122</td>
<td>chat</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Empirical-Analysis"><a href="#Empirical-Analysis" class="headerlink" title="Empirical Analysis"></a>Empirical Analysis</h2><h3 id="Unigram-vs-Char-based-BPE"><a href="#Unigram-vs-Char-based-BPE" class="headerlink" title="Unigram vs Char-based BPE"></a>Unigram vs Char-based BPE</h3><p><strong>Unigram aligns better than char-based BPE does in morphology.</strong> <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720)3720)
">[15]</span></a></sup> argued that Unigram LM tokenization can recover subword units that align with morphology much better than BPE do, using SentencePiece<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226)
">[12]</span></a></sup> implementation on English and Japanese Wikipedia. </p>
<p>It can be seen from the below figure that Unigram tends to produce longer subword units than BPE on average and have more tokens of moderate frequency.</p>
<p><img data-src="/notes/images/Unigram-vs-BPE-token-length-and-frequency.png" alt="English subword token vocabulary comparison between Unigram and BPE tokenization."></p>
<p>As shown in the table, BPE tokenization tends to merge common tokens, such as English inflectional suffixes and Japanese particles, into their neighbors even though resulting units are not semantically meaningful. This may be due to the greedy construction of BPE tokenization.</p>
<p><img data-src="/notes/images/Unigram-vs-BPE-evaluation.png" alt="Unigram vs char-based BPE tokenization &lt;small&gt;[11]&lt;/small&gt;"></p>
<p><sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720)3720)
">[15]</span></a></sup> found that segmentations produced by Unigram LM align more closely to the morphological references in both English and Japanese.<br><img data-src="/notes/images/Unigram-vs-BPE-in-segmentation.png" alt="Subword boundaries between tokenized subwords and morphological segmentations."></p>
<p><strong>Models using Unigram outperform counterparts using BPE in finetuning downstream tasks.</strong> <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720)3720)
">[15]</span></a></sup> claimed that fine-tuning models pretrained with unigram LM tokenization produces better performance than with BPE tokenization for experimented tasks.</p>
<p><img data-src="/notes/images/Unigram-vs-BPE-finetuning-tasks.png" alt="Unigram vs char-based BPE on finetuning downstream tasks"></p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;chai2021tokenization-PTMs,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Word Tokenization for Pre-trained Models&#125;&#125;,</span><br><span class="line">  year = &#123;2021&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2021/11/29/Subword-Tokenization-in-NLP/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization">Word Tokenization: How to Handle Out-Of-Vocabulary Words?</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c">A comprehensive guide to subword tokenisers</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://huggingface.co/blog/how-to-train">How to train a new language model from scratch using Transformers and Tokenizers</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb#scrollTo=tCvb9epa2iZV">Colab: train your tokenizer</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/google-research/bert">Github: Google BERT</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/python/latest/quicktour.html">Hugginface tokenizer</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/">How to Train BPE, WordPiece, and Unigram Tokenizers from Scratch using Hugging Face</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/P16-1162.pdf">[BPE] Neural Machine Translation of Rare Words with Subword Units
Rico</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.08144.pdf">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/P18-1007.pdf">Taku Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.06226">Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization">Word Tokenization: How to Handle Out-Of-Vocabulary Words?</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="http://www.persagen.com/files/misc/radford2019language.pdf">[GPT-2]Language models are unsupervised multitask learners</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.03720">Byte Pair Encoding is Suboptimal for Language Model Pretraining</a>3720)<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15">SentencePiece Tokenizer Demystified
</a><a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb">SentencePiece Python Module Example</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15">SentencePiece Tokenizer Demystified</a><a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb">sentencepiece/python/add_new_vocab.ipynb</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece/issues/121">GitHub issue: Manually modifying SentencePiece model?</a><a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://medium.com/geekculture/easy-sentencepiece-for-subword-tokenization-in-python-and-tensorflow-4361a1ed8e39">Easy SentencePiece for Subword Tokenization in Python and Tensorflow</a><a href="#fnref:21" rev="footnote"> ↩</a></span></li><li id="fn:22"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/72214408/why-does-huggingface-t5-tokenizer-ignore-some-of-the-whitespaces">why does huggingface t5 tokenizer ignore some of the whitespaces?</a><a href="#fnref:22" rev="footnote"> ↩</a></span></li><li id="fn:23"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">How to count tokens with tiktoken</a><a href="#fnref:23" rev="footnote"> ↩</a></span></li><li id="fn:24"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken">OpenAI tiktoken</a><a href="#fnref:24" rev="footnote"> ↩</a></span></li><li id="fn:25"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">25.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a target="_blank" rel="noopener" href="https://platform.openai.com/tokenizer">OpenAI API Tokenizer</a><a href="#fnref:25" rev="footnote"> ↩</a></span></li></ol></div></div>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/notes/tags/LLM/" rel="tag"># LLM</a>
              <a href="/notes/tags/Pre-training/" rel="tag"># Pre-training</a>
              <a href="/notes/tags/NLP/" rel="tag"># NLP</a>
              <a href="/notes/tags/Tokenization/" rel="tag"># Tokenization</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/notes/2021/10/09/PTMs/Scaling-Up-LLMs/" rel="prev" title="Scaling Up Large Language Models: A Summary">
      <i class="fa fa-chevron-left"></i> Scaling Up Large Language Models: A Summary
    </a></div>
      <div class="post-nav-item">
    <a href="/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/" rel="next" title="Mask Denoising Strategy for Pre-trained Language Models">
      Mask Denoising Strategy for Pre-trained Language Models <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Preliminaries"><span class="nav-number">1.</span> <span class="nav-text">Preliminaries</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">2.</span> <span class="nav-text">Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Byte-Pair-Encoding-BPE"><span class="nav-number">2.1.</span> <span class="nav-text">Byte-Pair Encoding (BPE)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Byte-level-BPE-BBPE"><span class="nav-number">2.1.1.</span> <span class="nav-text">Byte-level BPE (BBPE)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Background"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Background</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BBPE"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">BBPE</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GPT-2-implementation"><span class="nav-number">2.1.1.2.1.</span> <span class="nav-text">GPT-2 implementation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tiktoken-implementation"><span class="nav-number">2.1.1.2.2.</span> <span class="nav-text">tiktoken implementation</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WordPiece"><span class="nav-number">2.2.</span> <span class="nav-text">WordPiece</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unigram-Language-Model"><span class="nav-number">2.3.</span> <span class="nav-text">Unigram Language Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SentencePiece-Library"><span class="nav-number">2.4.</span> <span class="nav-text">SentencePiece Library</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic-usage"><span class="nav-number">2.4.1.</span> <span class="nav-text">Basic usage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#User-defined-and-control-symbols"><span class="nav-number">2.4.2.</span> <span class="nav-text">User defined and control symbols</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unigram-sampling-and-nbest-segmentation-for-subword-regularization"><span class="nav-number">2.4.3.</span> <span class="nav-text">Unigram: sampling and nbest segmentation for subword regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BPE-model"><span class="nav-number">2.4.4.</span> <span class="nav-text">BPE model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Character-and-word-model"><span class="nav-number">2.4.5.</span> <span class="nav-text">Character and word model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-normalization"><span class="nav-number">2.4.6.</span> <span class="nav-text">Text normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vocabulary-restriction"><span class="nav-number">2.4.7.</span> <span class="nav-text">Vocabulary restriction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Extracting-crossing-words-pieces"><span class="nav-number">2.4.8.</span> <span class="nav-text">Extracting crossing-words pieces</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-byte-offsets-of-tokens"><span class="nav-number">2.4.9.</span> <span class="nav-text">Getting byte offsets of tokens</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-new-special-tokens"><span class="nav-number">2.4.10.</span> <span class="nav-text">Add new special tokens</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Handle-whitespaces-newlines"><span class="nav-number">2.4.11.</span> <span class="nav-text">Handle whitespaces&#x2F;newlines</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Huggingface-tokenizers"><span class="nav-number">2.5.</span> <span class="nav-text">Huggingface tokenizers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-special-tokens"><span class="nav-number">2.5.1.</span> <span class="nav-text">Add special tokens</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Handle-non-space-separated-language"><span class="nav-number">2.5.2.</span> <span class="nav-text">Handle non-space-separated language</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OpenAI-tiktoken"><span class="nav-number">2.6.</span> <span class="nav-text">OpenAI tiktoken</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Empirical-Analysis"><span class="nav-number">2.7.</span> <span class="nav-text">Empirical Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unigram-vs-Char-based-BPE"><span class="nav-number">2.7.1.</span> <span class="nav-text">Unigram vs Char-based BPE</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">3.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yekun Chai"
      src="/notes/images/ernie.jpeg">
  <p class="site-author-name" itemprop="name">Yekun Chai</p>
  <div class="site-description" itemprop="description">Language is not just words.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/notes/archives">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/notes/categories/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/notes/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://cyk1337.github.io" title="Home → https://cyk1337.github.io"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cyk1337" title="GitHub → https://github.com/cyk1337" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaiyekun@gmail.com" title="E-Mail → mailto:chaiyekun@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/ychai1224" title="Twitter → https://twitter.com/ychai1224" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/9479335/cyk" title="StackOverflow → https://stackoverflow.com/users/9479335/cyk" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/notes/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yekun Chai</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/notes/lib/anime.min.js"></script>
  <script src="/notes/lib/pjax/pjax.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="//cdn.bootcdn.net/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="/notes/lib/velocity/velocity.min.js"></script>
  <script src="/notes/lib/velocity/velocity.ui.min.js"></script>

<script src="/notes/js/utils.js"></script>

<script src="/notes/js/motion.js"></script>


<script src="/notes/js/schemes/muse.js"></script>


<script src="/notes/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/notes/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  <!-- chaiyekun added  -->
   
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.3/moment.min.js"></script>
<script src="/notes/lib/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20180201,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "months");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "mins");
    ages = ages.replace(/seconds?/, "secs");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I'm here for ${ages}`;
  }
  // create if not exists ==> fix multiple footer bugs ;)
  if ($('#time').length > 0) {
    var prev = document.getElementById("time");
    prev.remove();
  } 
  var div = document.createElement("div");
  div.setAttribute("id", "time");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
 
  timer();
  setInterval("timer()",1000)
</script>

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://cyk0.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://cyk1337.github.io/notes/2021/11/29/Subword-Tokenization-in-NLP/";
    this.page.identifier = "2021/11/29/Subword-Tokenization-in-NLP/";
    this.page.title = "Subword Tokenization in Natural Language Processing";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://cyk0.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
<script src="/notes/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/notes/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":96,"height":160,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.1},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
