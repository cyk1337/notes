<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Diffusion Models: A Mathematical Note from Scratch</title>
    <url>/notes/2022/12/12/Diffusion-Models-Math-Guide/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A diffusion probabilistic model is a parameterized Markov chain trained to reverse a predefined forward process, closely related to both likelihood-based optimization and score matching. The forward diffusion process is a stochastic process constructed to gradually corrupt the original data into random nose.</p>
<span id="more"></span>
<!--## Summary-->
<h1 id="Gaussian-Diffusion-Continuous"><a href="#Gaussian-Diffusion-Continuous" class="headerlink" title="Gaussian Diffusion (Continuous)"></a>Gaussian Diffusion (Continuous)</h1><p>Diffusion models <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sohl-Dickstein, Jascha, et al. [Deep Unsupervised Learning Using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585). ICML 2015">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ho, Jonathan, et al. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). arXiv:2006.11239, arXiv, 16 Dec. 2020">[2]</span></a></sup> are latent variable models inspired by the non-equilibrium statistical physics ( thermodynamics) that gradually destroy structure in data distribution through an iterative forward diffusion process, and then learn a reversal process to recover the original data structure through iterative denoising. </p>
<div class="note info">
            <p>Diffusion models can be treated as a Markovian Hierarchical Variational Autoencoder with three restrictions:<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luo, C. (2022). [Understanding diffusion models: A unified perspective](https://arxiv.org/pdf/2208.11970). arXiv preprint arXiv:2208.11970.">[6]</span></a></sup></p><ol><li>The latent dimension is the same as the original data.</li><li>The encoder is not learned, instead uses a (pre-defined) linear Gaussian model.</li><li>The latent in final timestep $T$ is an isotropic Gaussian.</li></ol>
          </div>
<h2 id="Forward-Diffusion-process"><a href="#Forward-Diffusion-process" class="headerlink" title="Forward (Diffusion) process"></a>Forward (Diffusion) process</h2><p>Given a data point sampled from the data distribution $\mathbf{x}_0 \sim q(\mathbf{x})$. The forward diffusion process gradually applied a (<em>fixed</em>) <strong>linear Gaussian model</strong> at each timestep $t$ out of $T$ steps:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N} (\mathbf{x}_t; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
\end{align}</script><p>where the forward diffusion transitions produce a series of gradually noisy samples  $\mathbf{x}_1, \cdots, \mathbf{x}_T$. Each noisy sample has the exactly same dimension as the original data point $\mathbf{x}_0$. </p>
<p>Under the Markovian assumption, the Gaussian noise is gradually added to examples from previous timestep, with the variance schedule <script type="math/tex">\{\beta_t \in (0, 1) \}_{t=1}^T</script>. Given a large number of $T \rightarrow \infty$, $\mathbf{x}_T$ can ideally be an isotropic Gaussian noise.  </p>
<p><img data-src="/notes/images/diffusion_swiss_roll.png" alt="1st row: (fixed) forward process; 2nd row: (trained) reverse trajectory; Last row: drift term.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Sohl-Dickstein, Jascha, et al. [Deep Unsupervised Learning Using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585). ICML 2015&quot;&gt;[1]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt; "></p>
<p>Let $\alpha_t = 1 - \beta_t$, the linear Gaussian model in the forward process is rewritten as:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N} (\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, (1-\alpha_t) \mathbf{I})
\end{align}</script><p>Under the reparameterization trick, samples <script type="math/tex">\mathbf{x}_t \sim q (\mathbf{x}_t | \mathbf{x}_{t-1})</script> can be rewritten as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \pmb{\epsilon} \,\,\,\, \text{with } \,\,\,\, \pmb{\epsilon}\sim \mathcal{N} (\pmb{\epsilon}; \mathbf{0},\mathbf{I})
\end{align}</script><p>In similar vein, samples $\mathbf{x}_{t-1}$ can be rewritten as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{x}_{t-1} = \sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \pmb{\epsilon} \,\,\,\, \text{with } \,\,\,\, \pmb{\epsilon}\sim \mathcal{N} (\pmb{\epsilon}; \mathbf{0},\mathbf{I})
\end{align}</script><p>Let <script type="math/tex">\bar{\alpha}_t = \prod_{i=1}^t \alpha_i</script>. Usually, the update step gets larger as the timestep increases, <em>i.e.</em>, $\beta_1 &lt; \beta_2 &lt; \cdots &lt; \beta_T$ and thus $\bar{\alpha}_1 &gt; \bar{\alpha}_2 &gt; \cdots \bar{\alpha}_T$.</p>
<p>Suppose we have $2T$ random noise variables <script type="math/tex">\{ \pmb{\epsilon}_t, \bar{\pmb{\epsilon}}_t \}_{t=1}^T \overset{\text{i.i.d}}{\sim} \mathcal{N} (\pmb{\epsilon}; \mathbf{0},\mathbf{I})</script>. </p>
<p>For an arbitrary sample $\mathbf{x}_t \sim q(\mathbf{x}_t | \mathbf{x}_0)$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{x}_t &{}= \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \pmb{\epsilon}_{t-1}  \\
&{}= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \pmb{\epsilon}_{t-2}) + \sqrt{1-\alpha_{t}} \pmb{\epsilon}_{t-1} \nonumber \\
&{}= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t- \alpha_t \alpha_{t-1}} \pmb{\epsilon}_{t-2} + \sqrt{1-\alpha_{t}} \pmb{\epsilon}_{t-1} \nonumber \\
&{}= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\sqrt{\alpha_t- \alpha_t \alpha_{t-1}}^2 + \sqrt{1-\alpha_{t}}^2 } \bar{\pmb{\epsilon}}_{t-2})  \nonumber \\
&{}= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\pmb{\epsilon}}_{t-2})\\
&{}= \cdots \nonumber \\
&{}= \sqrt{\prod_{i=1}^t \alpha_i \mathbf{x}_0} + \sqrt{1 - \prod_{i=1}^t \alpha_i \pmb{\epsilon}_0} \\
&{}= \color{blue}{\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \bar{\pmb{\epsilon}}_0} \label{forward_add_noise} \\
&{} \sim \mathcal{N} (\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I}) \label{noise_process}
\end{align}</script><p>Therefore, the linear Gaussian form is derived as: <script type="math/tex">q(\mathbf{x}_t | \mathbf{x}_0) \sim \mathcal{N} (\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})</script>.</p>
<h2 id="Reverse-process"><a href="#Reverse-process" class="headerlink" title="Reverse process"></a>Reverse process</h2><p>The reverse diffusion process, with the form <script type="math/tex">p_\theta(\mathbf{x}_0) := \int p_\theta (\mathbf{x}_{0:T} d \mathbf{x}_{1:T})</script>, learns the reversal of diffusion process by gradually denoising from timestep T to 1. The reverse process is defined as a Markov chain with learned Gaussian transitions starting at $p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})$:</p>
<script type="math/tex; mode=display">
\begin{align}
p_\theta (\mathbf{x}_{0:T}) &{}:= p(\mathbf{x}_T) \prod_{t=1}^T  p_\theta (\mathbf{x}_{t-1}\vert \mathbf{x}_t) \\
p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_{t}) &{} := \mathcal{N} (\mathbf{x}_{t-1}; \pmb{\mu}_\theta (\mathbf{x}_{t},t), \pmb{\Sigma}_\theta (\mathbf{x}_{t}, t))
\end{align}</script><p><img data-src="/notes/images/Diffusion-process.png" alt="Diffusion process.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Ho, Jonathan, et al. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). arXiv:2006.11239, arXiv, 16 Dec. 2020&quot;&gt;[2]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<p>Therefore, we can derive the Gussian form of both <script type="math/tex">q(\mathbf{x}_t | \mathbf{x}_0)</script> and <script type="math/tex">q(\mathbf{x}_{t-1} | \mathbf{x}_0)</script>. Using Bayes rule, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) &{}= \frac{q(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{x}_0) \cdot q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x}_t | \mathbf{x}_0)} \\
&{}= \frac{\mathcal{N} (\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_0, (1-\alpha_t) \mathbf{I}) \cdot \mathcal{N} (\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0, (1-\bar{\alpha}_{t-1}) \mathbf{I})}{\mathcal{N} (\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})} \\
&{}\propto \exp \big\{ -\frac{1}{2} ( \frac{(\mathbf{x}_t - \sqrt{\alpha} \mathbf{x}_{t-1})^2}{1-\alpha_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} + \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} ) \big\} \\
&{}= \exp \big\{ -\frac{1}{2} (\frac{-2\sqrt{\alpha_t} \mathbf{x}_t \mathbf{x}_{t-1} + \alpha_t \mathbf{x}_{t-1}^2 }{ 1 - \alpha_t} ) +  \frac{\mathbf{x}_{t-1}^2 - 2 \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_{t-1}\mathbf{x}_0}{1-\bar{\alpha}_{t-1}}   + C(\mathbf{x}_t, \mathbf{x}_0) \big\} \\
&{}\propto \exp\Big\{ -\frac{1}{2} \big( (\frac{\alpha_t}{1-\alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) \mathbf{x}_{t-1}^2 - 2(\frac{\sqrt{\alpha_t}}{1-\alpha_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0) \mathbf{x}_{t-1}   \big) \Big\} \\
&{}= \exp\Big\{ -\frac{1}{2} (\frac{1}{\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}})
  \Big[ \mathbf{x}_{t-1}^2 - 2\frac{\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\mathbf{x}_0 }{1-\bar{\alpha}_t} \mathbf{x}_{t-1}\Big] \Big\} \\
&{}\propto \mathcal{N}\Big(\mathbf{x}_{t-1}; \underbrace{\frac{\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\mathbf{x}_0 }{1-\bar{\alpha}_t}}_{\color{blue}{\pmb{\mu}(\mathbf{x}_t, \mathbf{x}_0)}}, \underbrace{\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}}_{\color{green}{\pmb{\Sigma}_q(t)}} \Big) 
\label{mu}
\end{align}</script><p>In each timestep, <script type="math/tex">\mathbf{x}_{t-1} \sim q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)</script> follows the Gaussian distribution. The mean <script type="math/tex">\pmb{\mu}(\mathbf{x}_t, \mathbf{x}_0)</script> is a function of <script type="math/tex">\mathbf{x}_t</script> and <script type="math/tex">\mathbf{x}_0</script>, and <script type="math/tex">\pmb{\Sigma}_q(t)</script> is a function of $\alpha$ coefficient (either as hyperparameter or learned with neural networks). The variance can be formulated as: <script type="math/tex">\pmb{\Sigma}_q (t) = \sigma^2_q (t) \mathbf{I}</script>, where <script type="math/tex">\sigma^2=\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}</script>.</p>
<p>Since <script type="math/tex">p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t)</script> does not condition on <script type="math/tex">\mathbf{x}_0</script>, we thus optimize the KL divergence between the means of two Gaussians:</p>
<script type="math/tex; mode=display">
\begin{align}
&{}\mathop{\arg\min}_\theta \; \mathbb{KL} \Big( q(\mathbf{x}_{t-1}\vert \mathbf{x}_t, \mathbf{x}_0) \Vert p_\theta (\mathbf{x}_{t-1} \Vert \mathbf{x}_t) \Big) \\
= &{}\mathop{\arg\min}_\theta \; \mathbb{KL} \Big( \mathcal{N} \big( \mathbf{x}_{t-1}; \pmb{\mu}_q, \pmb{\Sigma}_q(t) \big) \Vert \mathcal{N} \big( \mathbf{x}_{t-1}; \pmb{\mu}_\theta, \pmb{\Sigma}_q(t) \big)
\Big) \\
= &{}\mathop{\arg\min}_\theta \; \frac{1}{2} \Big[ \log \frac{|\pmb{\Sigma}_q (t)|}{| \pmb{\Sigma}_q (t)|} -d + \text{tr} (\pmb{\Sigma}_q (t)^{-1} \Sigma_q (t)) + (\pmb{\mu}_\theta - \pmb{\mu}_q)^T \pmb{\Sigma}_q (t)^{-1} (\pmb{\mu}_\theta - \pmb{\mu}_q) \Big] \\
= &{}\mathop{\arg\min}_\theta \; \frac{1}{2} \Big[ \log 1 -d + d + (\pmb{\mu}_\theta - \pmb{\mu}_q)^T \pmb{\Sigma}_q (t)^{-1} (\pmb{\mu}_\theta - \pmb{\mu}_q) \Big] \\
= &{}\mathop{\arg\min}_\theta \;\frac{1}{2} \Big[ (\pmb{\mu}_\theta - \pmb{\mu}_q)^T \Sigma_q (t)^{-1} (\pmb{\mu}_\theta - \pmb{\mu}_q) \Big] \\
= &{}\mathop{\arg\min}_\theta \;\frac{1}{2} \Big[ (\pmb{\mu}_\theta - \pmb{\mu}_q)^T (\sigma_q^2 (t)\mathbf{I})^{-1} (\pmb{\mu}_\theta - \pmb{\mu}_q) \Big] \\
= &{}\mathop{\arg\min}_\theta \; \frac{1}{2 \sigma_q^2 (t)} \Vert \pmb{\mu}_\theta - \pmb{\mu}_q \Vert_2^2 \label{kl}
\end{align}</script><p>Given Eq.$\eqref{mu}$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\pmb{\mu}_q (\mathbf{x}_t, \mathbf{x}_0) &{}= \frac{\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) \color{green}{\mathbf{x}_0} }{1-\bar{\alpha}_t} \label{mu_q}  \\
\pmb{\mu}_\theta (\mathbf{x}_t, t) &{}= \frac{\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) \color{blue}{\hat{\mathbf{x}}_\theta (\mathbf{x}_t, t)} }{1-\bar{\alpha}_t}  \label{mu_theta} \\
\end{align}</script><p>Therefore, Eq.$\eqref{kl}$ can be rewritten as:</p>
<script type="math/tex; mode=display">
\begin{align}
&{}\mathop{\arg\min}_\theta \; \mathbb{KL} \Big( q(\mathbf{x}_{t-1}\vert \mathbf{x}_t, \mathbf{x}_0) \Vert p_\theta (\mathbf{x}_{t-1} \Vert \mathbf{x}_t) \Big) \\
= &{}\mathop{\arg\min}_\theta \; \mathbb{KL} \Big( \mathcal{N} \big( \mathbf{x}_{t-1}; \pmb{\mu}_q, \pmb{\Sigma}_q(t) \big) \Vert \mathcal{N} \big( \mathbf{x}_{t-1}; \pmb{\mu}_\theta, \pmb{\Sigma}_q(t) \big)
\Big) \\
= &{}\mathop{\arg\min}_\theta \; \frac{1}{2\sigma_q^2 (t)} \big\Vert \frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) \hat{\mathbf{x}}_\theta (\mathbf{x}_t, t) }{1-\bar{\alpha}_t} - \frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) \mathbf{x}_0 }{1-\bar{\alpha}_t}    \big\Vert_2^2 \\
= &{}\mathop{\arg\min}_\theta \; \frac{1}{2\sigma_q^2 (t)} \big\Vert \frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) }{1-\bar{\alpha}_t} (\hat{\mathbf{x}}_\theta (\mathbf{x}_t, t) - \mathbf{x}_0)  \big\Vert_2^2 \\
= &{}\mathop{\arg\min}_\theta \; \frac{1}{2\sigma_q^2 (t)}  \frac{\bar{\alpha}_{t-1}(1-\alpha_t)^2 }{(1-\bar{\alpha}_t)^2} \big\Vert (\hat{\mathbf{x}}_\theta (\mathbf{x}_t, t) - \mathbf{x}_0)  \big\Vert_2^2  \label{loss_mu}
\end{align}</script><div class="note info">
            <p>Intuitive understanding towards the diffusion process<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sohl-Dickstein, Jascha, et al. [Deep Unsupervised Learning Using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585). ICML 2015">[1]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luo, C. (2022). [Understanding diffusion models: A unified perspective](https://arxiv.org/pdf/2208.11970). arXiv preprint arXiv:2208.11970.">[6]</span></a></sup>.</p><script type="math/tex; mode=display">\begin{align}\log p(\mathbf{x}) &{}=  \log \int p(\mathbf{x}_{0:T}) d \mathbf{x}_{1:T} \\&{}= \log \int \frac{ p(\mathbf{x}_{0:T}) q(\mathbf{x}_{1:T} | \mathbf{x}_0)}{q(\mathbf{x}_{1:T} | \mathbf{x}_0)} d \mathbf{x}_{1:T} \\&{}= \log \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \frac{p(\mathbf{x}_{0:T}) }{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \Big] \\&{} \geq \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log \frac{p(\mathbf{x}_{0:T}) }{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \Big] \\&{} =\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log \frac{p(\mathbf{x}_{T}) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) }{\prod_{t=1}^T q(\mathbf{x}_t |\mathbf{x}_{t-1})} \Big] \\&{} = \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log \frac{p(\mathbf{x}_{T}) p_\theta( \mathbf{x}_0 | \mathbf{x}_1) \prod_{t=2}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) }{q(\mathbf{x}_T |\mathbf{x}_{T-1}) \prod_{t=1}^{T-1} q(\mathbf{x}_t |\mathbf{x}_{t-1})} \Big] \\&{} = \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log \frac{p(\mathbf{x}_{T}) p_\theta ( \mathbf{x}_0 | \mathbf{x}_1) \prod_{t=1}^{T-1} p_\theta(\mathbf{x}_{t}|\mathbf{x}_{t+1}) }{q(\mathbf{x}_T |\mathbf{x}_{T-1}) \prod_{t=1}^{T-1} q(\mathbf{x}_t |\mathbf{x}_{t-1})} \Big] \\&{} = \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log p_\theta( \mathbf{x}_0 | \mathbf{x}_1) \Big] + \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log \frac{\log p(\mathbf{x}_T)}{q(\mathbf{x}_T |\mathbf{x}_{T-1})} \Big] + \nonumber \\ &{} \qquad\qquad\qquad \quad\quad    \sum_{t=1}^{T-1} \mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }\Big[ \log \frac{p_\theta (\mathbf{x}_t \vert \mathbf{x}_{t+1}) }{ q (\mathbf{x}_t \vert \mathbf{x}_{t-1}) } \Big]\\&{} = \mathbb{E}_{q(\mathbf{x}_{1}|\mathbf{x}_0) }\Big[ \log p_\theta( \mathbf{x}_0 | \mathbf{x}_1) \Big] + \mathbb{E}_{q(\mathbf{x}_{T-1}, \mathbf{x}_{T}|\mathbf{x}_0) }\Big[ \log \frac{\log p(\mathbf{x}_T)}{q(\mathbf{x}_T |\mathbf{x}_{T-1})} \Big] + \nonumber \\ &{} \qquad\qquad\qquad \quad\quad    \sum_{t=1}^{T-1} \mathbb{E}_{q(\mathbf{x}_{t-1}, \mathbf{x}_{t}, \mathbf{x}_{t+1}|\mathbf{x}_0) }\Big[ \log \frac{p_\theta (\mathbf{x}_t \vert \mathbf{x}_{t+1}) }{ q (\mathbf{x}_t \vert \mathbf{x}_{t-1}) } \Big] \\&{} = \underbrace{\mathbb{E}_{q(\mathbf{x}_{1}|\mathbf{x}_0) }\Big[ \log p_\theta( \mathbf{x}_0 | \mathbf{x}_1) \Big]}_{\text{reconstruction}} + \underbrace{\mathbb{E}_{q(\mathbf{x}_{T-1} |\mathbf{x}_0) } \Big[ \mathbb{KL}(q(\mathbf{x}_T |\mathbf{x}_{T-1}) \vert \log p(\mathbf{x}_T)) \Big]}_{\text{prior matching} \rightarrow 0} + \nonumber \\ &{} \qquad\qquad\qquad \quad\quad    \underbrace{\sum_{t=1}^{T-1} \mathbb{E}_{q(\mathbf{x}_{t-1}, \mathbf{x}_{t}, \mathbf{x}_{t+1}|\mathbf{x}_0) }\Big[ \log \frac{p_\theta (\mathbf{x}_t \vert \mathbf{x}_{t+1}) }{ q (\mathbf{x}_t \vert \mathbf{x}_{t-1}) } \Big]}_{\text{consistency}}\end{align}</script><ol><li>The reconstruction term corresponds to the first-step optimization.</li><li>The prior matching term does not contain trainable parameters, requiring no optimization.</li><li>The consistency term makes the denoising process at timestep $t$ match the corresponding diffusion step from a cleaner input.</li></ol><p>The ELBO objective is thus approximated across all noise levels over the expection of all timesteps.</p>
          </div>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>The ELBO objective can be derived as <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).">[7]</span></a></sup></p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_\text{VLB}  &{}= - \log p_\theta(\mathbf{x}_0) \\
&\leq - \log p_\theta(\mathbf{x}_0) + D_\text{KL}(q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) \| p_\theta(\mathbf{x}_{1:T}\vert\mathbf{x}_0) ) \\
&= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}) / p_\theta(\mathbf{x}_0)} \Big] \\
&= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&= \mathbb{E}_q \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\
&= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big]\\
&= \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\
&= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]
\end{align}</script><div class="note info">
            <p>The training of diffusion process can be implemented by learning a neural network to predict either of following three formats (given a arbitrary noised version <script type="math/tex">\mathbf{x}_t</script>):</p><ol><li>The original natural input <script type="math/tex">\mathbf{x}_0</script>. See Eq.$\eqref{loss_mu}$.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luo, C. (2022). [Understanding diffusion models: A unified perspective](https://arxiv.org/pdf/2208.11970). arXiv preprint arXiv:2208.11970.">[6]</span></a></sup> empirically finds it leads to worse sampling quality early.</li><li>The source noise $\pmb{\epsilon}_0$ ($\pmb{\epsilon}$-prediction parameterization). <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ho, Jonathan, et al. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). arXiv:2006.11239, arXiv, 16 Dec. 2020">[2]</span></a></sup></li><li>The score of input at an arbitrary noise level <script type="math/tex">\nabla \log p(\mathbf{x}_t)</script>. <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang Song & Stefano Ermon. [Generative modeling by estimating gradients of the data distribution](https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf). NeurIPS 2019.">[8]</span></a></sup></li><li>The velocity of diffusion latents <script type="math/tex">\mathbf{x}_t</script>.  <sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Salimans, Tim and Jonathan Ho. [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/pdf/2202.00512.pdf). ICLR 2022.">[19]</span></a></sup></li></ol>
          </div>
<h3 id="pmb-epsilon-prediction-parameterization"><a href="#pmb-epsilon-prediction-parameterization" class="headerlink" title="$\pmb{\epsilon}$-prediction parameterization"></a>$\pmb{\epsilon}$-prediction parameterization</h3><p>We arrange the Eq.$\eqref{noise_process}$ as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\pmb{\epsilon}_t}{\sqrt{\bar{\alpha}_t}} \label{denoise}
\end{align}</script><p>Plugging Eq.$\eqref{denoise}$ into the denoising transition mean in Eq.$\eqref{mu_q}$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\pmb{\mu}_q (\mathbf{x}_t, \mathbf{x}_0) &{}= \frac{\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) \color{green}{\mathbf{x}_0} }{1-\bar{\alpha}_t} \\
&{}= \frac{\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t) \color{grey}{\frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\pmb{\epsilon}_t}{\sqrt{\bar{\alpha}_t}}} }{1-\bar{\alpha}_t} \\
&{}= \frac{1}{\sqrt{\alpha_t}} \mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\pmb{\epsilon}_t \label{eps_q}
\end{align}</script><p>Similarly, the approximate denoising transition mean <script type="math/tex">\hat{\pmb{\epsilon}}_\theta (\mathbf{x}_t, t)</script> is:</p>
<script type="math/tex; mode=display">
\begin{align}
\pmb{\mu}_\theta (\mathbf{x}_t, t) 
&{}= \frac{1}{\sqrt{\alpha_t}} \mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\hat{\pmb{\epsilon}}_t(\mathbf{x}_t, t) \label{eps_theta}
\end{align}</script><p>Plugging the Eq.$\eqref{eps_q}$ and $\eqref{eps_theta}$ into Eq.$\eqref{kl}$, we can write:</p>
<script type="math/tex; mode=display">
\begin{align}
&{}\mathop{\arg\min}_\theta \; \mathbb{KL} \Big( q(\mathbf{x}_{t-1}\vert \mathbf{x}_t, \mathbf{x}_0) \Vert p_\theta (\mathbf{x}_{t-1} \Vert \mathbf{x}_t) \Big) \\
= &{}\mathop{\arg\min}_\theta \; \mathbb{KL} \Big( \mathcal{N} \big( \mathbf{x}_{t-1}; \pmb{\mu}_q, \pmb{\Sigma}_q(t) \big) \Vert \mathcal{N} \big( \mathbf{x}_{t-1}; \pmb{\mu}_\theta, \pmb{\Sigma}_q(t) \big)
\Big) \\
=&{}\mathop{\arg\min}_\theta \; \frac{1}{2 \sigma_q^2 (t)} \Vert \pmb{\mu}_\theta - \pmb{\mu}_q \Vert_2^2 \\
=&{}\mathop{\arg\min}_\theta \;  \frac{1}{2 \sigma_q^2 (t)} \Vert \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\pmb{\epsilon}_t  -  \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\hat{\pmb{\epsilon}}_t(\mathbf{x}_t, t)  \Vert_2^2 \\
=&{}\mathop{\arg\min}_\theta \;  \frac{1}{2 \sigma_q^2 (t)}  \frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t} \Vert \pmb{\epsilon}_t  -  \hat{\pmb{\epsilon}}_t(\mathbf{x}_t, t) \Vert_2^2 \\
=&{}\mathop{\arg\min}_\theta \;  \frac{1}{2 \sigma_q^2 (t)}  \frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t} \Vert \pmb{\epsilon}_t  -  \pmb{\epsilon}_\theta ( \underbrace{\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}_t}_{\text{Plugging Eq.\eqref{forward_add_noise}}} , t) \Vert_2^2 \label{loss_noise}
\end{align}</script><p><strong>Simplified objective</strong>: <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ho, Jonathan, et al. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). arXiv:2006.11239, arXiv, 16 Dec. 2020">[2]</span></a></sup> empirically find it better to remove the weighting term in Eq.$\eqref{loss_noise}$:</p>
<script type="math/tex; mode=display">
\begin{align}
\color{blue}{\mathcal{L}_\text{simple}} &{}=  \Vert \pmb{\epsilon}_t  -  \hat{\pmb{\epsilon}}_t(\mathbf{x}_t, t) \Vert_2^2 \\
&{}= \Vert \pmb{\epsilon}_t  -  \pmb{\epsilon}_\theta ( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}_t , t) \Vert_2^2
\end{align}</script><p>The training objective resembles denoising score matching over multiple noise scales indexed by $t$. It can be treated as using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.</p>
<p>The overall DDPM training algorithm is:</p>
<p><img data-src="/notes/images/DDpM-training.png" alt="DDPM training process.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Ho, Jonathan, et al. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). arXiv:2006.11239, arXiv, 16 Dec. 2020&quot;&gt;[2]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<p>The sampling process resembles Langevin dynamics with $\pmb{\epsilon}_\theta$ as a learned gradient of the data density.  </p>
<p><img data-src="/notes/images/DDPM sampling process.png" alt="DDPM sampling process.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Ho, Jonathan, et al. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239). arXiv:2006.11239, arXiv, 16 Dec. 2020&quot;&gt;[2]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<h3 id="Velocity-prediction"><a href="#Velocity-prediction" class="headerlink" title="Velocity prediction"></a>Velocity prediction</h3><p><sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Salimans, Tim and Jonathan Ho. [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/pdf/2202.00512.pdf). ICLR 2022.">[19]</span></a></sup> propose to parameterize the diffusion velocity by predicting the velocity of diffusion latents, by predicting <script type="math/tex">\mathbf{v} \equiv \alpha_t \epsilon - \sigma_t \mathbf{x}</script>, which gives <script type="math/tex">\hat{\mathbf{x}} = \alpha_t \mathbf{z}_t - \sigma_t \hat{\mathbf{x}}_\theta (\mathbf{z}_t)</script>.</p>
<p>Let <script type="math/tex">\phi_t = \arctan (\sigma_t / \alpha_t)</script>, assumming a variance preserving diffusion process, we have <script type="math/tex">\alpha_\phi = \cos (\phi), \sigma_\phi = \sin (\phi)</script>, and hence <script type="math/tex">\mathbf{z}_\phi = \cos (\phi) \mathbf{x} + \sin (\phi) \epsilon</script>.</p>
<p><sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Salimans, Tim and Jonathan Ho. [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/pdf/2202.00512.pdf). ICLR 2022.">[19]</span></a></sup> thus define the velocity of <script type="math/tex">\mathbf{z}_\phi</script> as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{v}_\phi &{}\equiv \frac{d \mathbf{z}_\phi}{d \phi} \\
&{}= \frac{d \cos (\phi)}{d \phi} \mathbf{x} + \frac{d \sin (\phi)}{d \phi} \epsilon \\
&{}= \cos (\phi) \epsilon - \sin (\phi) \mathbf{x}
\end{align}</script><p>By rearranging the $\epsilon$, $\mathbf{x}$, $\mathbf{v}$, we then get:</p>
<script type="math/tex; mode=display">
\begin{align}
\sin (\phi) \mathbf{x} &{}= \cos(\phi) \epsilon - \mathbf{v}_\phi \\
&{}= \frac{\cos(\phi)}{\sin(\phi)} (\mathbf{z} - \cos(\phi) \mathbf{x}) - \mathbf{v}_\phi \\
\sin^2(\phi) \mathbf{x} &{}= \cos(\phi) \mathbf{z} - \cos^2(\phi)\mathbf{x} - \sin (\phi) \mathbf{v}_\phi \\
(\sin^2(\phi) + \cos^2(\phi))\mathbf{x} &{}= \mathbf{x} = \cos(\phi) \mathbf{z} - \sin (\phi) \mathbf{v}_\phi
\end{align}</script><p>We also get <script type="math/tex">\epsilon = \sin (\phi) \mathbf{z}_\phi + \cos (\phi)\mathbf{v}_\phi</script>.</p>
<p>The predicted velocity is defined as:</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{v}_\theta (\mathbf{z}_\phi) \equiv \cos(\phi) \hat{\epsilon}_\theta (\mathbf{z}_\phi) - \sin (\phi) \hat{\mathbf{x}}_\theta (\mathbf{z}_\phi) 
\end{align}</script><p>where <script type="math/tex">\hat{\epsilon}_\theta (\mathbf{z}_\phi) = (\mathbf{z}_\phi - \cos(\phi)\hat{\mathbf{x}}_\theta (\mathbf{z}_\phi) ) / \sin(\phi)</script>.</p>
<p><img data-src="/notes/images/v-objective-diffusion-vis.png" alt="The visualization of reparameterization in terms of $\phi$ and $\mathbf{v}_\phi$"></p>
<p>Following algorithm illustrates the complete training process:</p>
<p><img data-src="/notes/images/diffusion-v-objective-training-alg.png" alt="Training algorithm in &lt;sup id=&quot;fnref:19&quot;&gt;&lt;a href=&quot;#fn:19&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Salimans, Tim and Jonathan Ho. [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/pdf/2202.00512.pdf). ICLR 2022.&quot;&gt;[19]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<h2 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h2><p>For conditional generation, it includes classifier-guided or classifier-free methods. The  distinct difference is the existence of an extra classifier for condition guidance.</p>
<h3 id="Classifier-Guidance"><a href="#Classifier-Guidance" class="headerlink" title="Classifier Guidance"></a>Classifier Guidance</h3><p><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dhariwal, Prafulla, and Alex Nichol. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233). arXiv:2105.05233, arXiv, 1 June 2021">[4]</span></a></sup> utilized a trained classifier <script type="math/tex">f_\phi (y \vert \mathbf{x}_t,t)</script> on noisy image <script type="math/tex">\mathbf{x}_t</script> to obtain the gradients towards input <script type="math/tex">\nabla_\mathbf{x} \log f_\phi (y \vert \mathbf{x}_t)</script> to guide the sampling process using the condition $y$, such as the target class label. </p>
<div class="note info">
            <p>Given a Gaussian <script type="math/tex">\mathbf{x} \sim \mathcal{N}(\pmb{\mu}, \pmb{\sigma}^2\mathbf{I})</script>, the log derivative of the density function<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang Song & Stefano Ermon. [Generative modeling by estimating gradients of the data distribution](https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf). NeurIPS 2019.">[8]</span></a></sup> is:</p><script type="math/tex; mode=display">\begin{align}\nabla_\mathbf{x} \log p(\mathbf{x}) &{}= \nabla_\mathbf{x} \Big( - \frac{1}{s\sigma^2} (\mathbf{x} - \pmb{\mu})^2 \Big) \\&{}= -\frac{\mathbf{x} - \pmb{\mu}}{\pmb{\sigma}^2} \\&{}= -\frac{\pmb{\epsilon}}{\pmb{\sigma}} \qquad \qquad\qquad \text{with}\qquad\pmb{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{1})\end{align}</script><p>Given Eq.$\eqref{noise_process}$, we have:</p><script type="math/tex; mode=display">\begin{align}\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) &{}= \mathbb{E}_{q(\mathbf{x}_0)} \Big[ \nabla_{\mathbf{x}_t}  q(\mathbf{x}_t \vert \mathbf{x}_0) \Big] \\&{}= \mathbb{E}_{q(\mathbf{x}_0)} \Big[ -\frac{\pmb{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1-\bar{\alpha}_t}} \Big] \\&{}= -\frac{\pmb{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1-\bar{\alpha}_t}}\end{align}</script>
          </div>
<p>The score function for the joint distribution <script type="math/tex">q (\mathbf{x}_t, y)</script> is:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_{\mathbf{x}_t} \log q (\mathbf{x}_t, y) &{}= \nabla_{\mathbf{x}_t} \log q (\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \log q (y \vert \mathbf{x}_t) \\
&{}\approx - \frac{1}{\sqrt{1-\bar{\alpha}_t}} \pmb{\epsilon} (\mathbf{x}_t, t) + \nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t) \\
&{}= - \frac{1}{\sqrt{1-\bar{\alpha}_t}} \big( \pmb{\epsilon}_\theta (\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t) \big)
\end{align}</script><p>The classifier-guided predictor <script type="math/tex">\bar{\pmb{\epsilon}}_\theta</script> thus obtains a truncation-like effect by sampling in the direction of the gradient of image classifier to perform conditional generation:</p>
<script type="math/tex; mode=display">
\begin{align}
\bar{\pmb{\epsilon}}_\theta (\mathbf{x}_t, t) = \pmb{\epsilon}_\theta (\mathbf{x}_t, t) - \sqrt{1-\bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t)
\end{align}</script><p>Classifier guided prediction <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dhariwal, Prafulla, and Alex Nichol. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233). arXiv:2105.05233, arXiv, 1 June 2021">[4]</span></a></sup> uses a weight factor $w$ to contrail the shifted gradient:</p>
<script type="math/tex; mode=display">
\begin{align}
\bar{\pmb{\epsilon}}_\theta (\mathbf{x}_t, t) = \pmb{\epsilon}_\theta (\mathbf{x}_t, t) - \sqrt{1-\bar{\alpha}_t} \nabla_{\mathbf{x}_t} {\color{red} w}  \log f_\phi (y \vert \mathbf{x}_t) \label{classifier_guidance}
\end{align}</script><p><img data-src="/notes/images/classifier-guided-diffusion.png" alt="Conditional generation with DDPM and DDIM&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Dhariwal, Prafulla, and Alex Nichol. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233). arXiv:2105.05233, arXiv, 1 June 2021&quot;&gt;[4]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<h3 id="Classifier-Free-Guidance"><a href="#Classifier-Free-Guidance" class="headerlink" title="Classifier-Free Guidance"></a>Classifier-Free Guidance</h3><p>Classifier guidiance introduces an auxiliary classifier and thus complicates the training process. It is naturally to think about the approach of conditional generation without any explicit classifier <script type="math/tex">f_\phi</script> entirely. Instead of sampling in the direction of the gradient of image classifier, <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ho, Jonathan, and Tim Salimans. [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598). 2021. openreview.net">[5]</span></a></sup> proposes to combine the score estimates of a conditional diffusion model <script type="math/tex">p_\theta (\mathbf{x}|y)</script> and a jointly trained unconditional model <script type="math/tex">p_\theta (\mathbf{x})</script> via a single model. </p>
<p>Specifically, when training conditional diffusion <script type="math/tex">p_\theta (\mathbf{x}|y)</script> parameterized by the score estimator <script type="math/tex">\pmb{\epsilon}_\theta (\mathbf{x}_t, t, y)</script>, <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ho, Jonathan, and Tim Salimans. [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598). 2021. openreview.net">[5]</span></a></sup> randomly gets rid of the conditions by setting $y=\emptyset$, that is <script type="math/tex">\pmb{\epsilon}_\theta (\mathbf{x}_t, t) = \pmb{\epsilon}_\theta (\mathbf{x}_t, t, \emptyset)</script></p>
<p>The gradient of an implicit classifier can be formulated with the difference between conditional and unconditional classifiers:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t) &{} = \nabla_{\mathbf{x}_t} \log p (\mathbf{x}_t \vert y) - \nabla_{\mathbf{x}_t} \log p (\mathbf{x}_t) \\
&{}= - \frac{1}{\sqrt{1-\bar{\alpha}_t}} \big( \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y) - \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y=\emptyset) \big)
\end{align}</script><p>Plugging into the Eq.$\eqref{classifier_guidance}$, the score estimator will be:</p>
<script type="math/tex; mode=display">
\begin{align}
\bar{\pmb{\epsilon}}_\theta (\mathbf{x}_t, t) &{} = \pmb{\epsilon}_\theta (\mathbf{x}_t, t) - \sqrt{1-\bar{\alpha}_t} \nabla_{\mathbf{x}_t} w  \log f_\phi (y \vert \mathbf{x}_t) \\
&{}= \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y) -  w \Big( \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y) - \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y=\emptyset) \Big)\\
&{}= (w+1) \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y) - w \cdot \pmb{\epsilon}_\theta (\mathbf{x}_t, t, y=\emptyset)
\end{align}</script><hr>
<h1 id="Categorical-Diffusion-Discrete"><a href="#Categorical-Diffusion-Discrete" class="headerlink" title="Categorical Diffusion (Discrete)"></a>Categorical Diffusion (Discrete)</h1><p>Gaussian diffusion process focuses on continuous state space, such as real-valued image and waveform data. There has been research trials by applying the Gaussian diffusion into categorical data, which requires relaxing or embedding discrete data into continuous spaces. A more natural way is to use categorical diffusion that corrupts the categorical data such as language in discrete state spaces.</p>
<p><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sohl-Dickstein, Jascha, et al. [Deep Unsupervised Learning Using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585). ICML 2015">[1]</span></a></sup> firstly introduces the diffusion models with discrete state spaces over <em>binary</em> random variables. <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P. and Welling, M. [Argmax flows and multinomial diffusion: Learning categorical distributions](https://proceedings.neurips.cc/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf). NeurIPS 2021.">[9]</span></a></sup> extended the model class to <em>categorical</em> random variables with transition matrices characterized by uniform transition probabilities. <sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Austin, J., Johnson, D.D., Ho, J., Tarlow, D. and van den Berg, R., 2021. [Structured denoising diffusion models in discrete state-spaces](https://proceedings.neurips.cc/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf). NeurIPS 2021.">[10]</span></a></sup> introduces discrete denoising diffusion probabilistic models (D3PM) by more generally extending the state corruption process.</p>
<p><img data-src="/notes/images/D3PM.png" alt="Quantizedd swiss roll. Each dot represents a 2D categorical variable. &lt;br&gt;Top: Diffused samples from the uniform, discretized Gaussian, and absorbing state, with transition matrices $\mathbf{Q}$. &lt;br&gt; Bottom: Learned discretized Gaussian reverse process."></p>
<h2 id="Discrete-Diffusion-D3PM"><a href="#Discrete-Diffusion-D3PM" class="headerlink" title="Discrete Diffusion (D3PM)"></a>Discrete Diffusion (D3PM)</h2><p>For scalar discrete random variables with $K$ categories <script type="math/tex">x_t, x_{t-1} \in 1,\cdots, K</script>, the forward transition probability can be represented by matrices: <script type="math/tex">[\mathbf{Q}_t]_{ij} = q (x_t = j \vert x_{t-1}=i)</script>. </p>
<p>Denoting the one hot version of $x$ with the <u>row vector</u> <script type="math/tex">\mathbf{x}</script> , a categorical distribution $\text{Cat} (\mathbf{x}, \mathbf{p})$ over the one-hot row vector $\mathbf{x}$ with probabilities given by the row vector $\mathbf{p}$, we can write:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \text{Cat} (\mathbf{x}_t; \mathbf{p}=\mathbf{x}_{t-1}\mathbf{Q}_t)
\end{align}</script><p>The term <script type="math/tex">\mathbf{x}_{t-1}\mathbf{Q}_t</script> can be understood as a row vector-matrix product. $\mathbf{Q}$ is assumed to apply to each image pixel or sequence token independently. $q$ factorizes over the higher dimensions. Thus we write <script type="math/tex">q(\mathbf{x}_t \vert \mathbf{x}_{t-1})</script> w.r.t a single element. </p>
<h2 id="Discrete-state-spaces"><a href="#Discrete-state-spaces" class="headerlink" title="Discrete state spaces"></a>Discrete state spaces</h2><p>Starting from <script type="math/tex">\mathbf{x}_0</script>, the $t$-step marginal at time $t-1$:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{x}_t \vert \mathbf{x}_0) = \text{Cat} (\mathbf{x}_t; \mathbf{p}=\mathbf{x}_0 \mathbf{\overline{Q}}_t) \quad \quad \text{with} \quad\quad \mathbf{\overline{Q}}_t= \prod_{i=1}^t\mathbf{Q}_i
\end{align}</script><p>The posterior is:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) &{}= \frac{q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) q(\mathbf{x}_{t-1}\vert \mathbf{x}_0)}{q(\mathbf{x}_{t}\vert \mathbf{x}_0)} \qquad\qquad\qquad \text{Markov property}\\
&{}= \frac{q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1}\vert \mathbf{x}_0)}{q(\mathbf{x}_{t}\vert \mathbf{x}_0)} \\
&{}=\text{Cat} (\mathbf{x}_t; \mathbf{p}= \frac{\mathbf{x}_t \mathbf{Q}_t^\top \odot \mathbf{x}_0 \mathbf{\overline{Q}}_{t-1}}{\mathbf{x}_0 \mathbf{\overline{Q}}_{t} \mathbf{x}_t^\top} )
\end{align}</script><p>Assuming that the reverse process <script type="math/tex">p_\theta (\mathbf{x}_t \vert \mathbf{x}_{t-1})</script> is factorized as conditionally independent over all the elements, the KL divergence between $q$ and $p_\theta$ is summing over all values of each random variable.</p>
<h3 id="Forward-Markov-transition-matrices"><a href="#Forward-Markov-transition-matrices" class="headerlink" title="Forward Markov transition matrices"></a>Forward Markov transition matrices</h3><ol>
<li>Uniform<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P. and Welling, M. [Argmax flows and multinomial diffusion: Learning categorical distributions](https://proceedings.neurips.cc/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf). NeurIPS 2021.">[9]</span></a></sup>. Given $\beta_t \in [0,1]$, the transition matrix <script type="math/tex">\mathbf{Q}_t = (1-\beta_t)\mathbf{I} + \frac{\beta_t}{K} \mathbb{1}\mathbb{1}^\top</script>.</li>
<li>Absorbing state. Define transition matrix with an absorbing state (called [MASK]), such that each token either stays the same or transitions to [MASK] with some probability $\beta_t$. This is motivated by BERT. For images, it reuses the grey pixels as the [MASK] absorbing token.</li>
<li>Discretized Gaussian. <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P. and Welling, M. [Argmax flows and multinomial diffusion: Learning categorical distributions](https://proceedings.neurips.cc/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf). NeurIPS 2021.">[9]</span></a></sup> uses a discretized, truncated Gaussian distribution for ordinal data such as images.</li>
<li>Token embedding distance. <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P. and Welling, M. [Argmax flows and multinomial diffusion: Learning categorical distributions](https://proceedings.neurips.cc/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf). NeurIPS 2021.">[9]</span></a></sup>  uses similarity in an embedding space to guide the forward process, so that the transitions become more frequently between tokens that have simialr embeddings, , while maintaining a uniform stationary distribution.</li>
</ol>
<h3 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h3><script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_\lambda = \mathcal{L}_{\text{vlb}} + \lambda \mathbb{E}_{q(\mathbf{x}_0}\mathbb{E}_{q(\mathbf{x}_t \vert \mathbf{x}_0)} [- \log \tilde{p}_\theta (\mathbf{x}_0 \vert \mathbf{x}_t)]
\end{align}</script><div class="note info">
            <p><strong>BERT is a one-step diffusion model</strong>. For a one-step diffusion process in which <script type="math/tex">q(\mathbf{x}_1 \vert \mathbf{x}_0)</script> replaces 10% of tokens with [MASK] and 5% uniformly at random. We have:</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}_\text{vlb} - \mathcal{L}_\text{T} &{}= - \mathbb{E}_{q(\mathbf{x}_1 \vert \mathbf{x}_0)} [\log p_\theta (\mathbf{x}_0 \vert \mathbf{x}_1)] \\&{}= \mathcal{L}_\text{BERT}\end{align}</script><hr><p><strong>Autoregressive models are (discrete) diffusion models</strong>. Consider a diffusion process taht deterministically masks tokens one-by-one in a sequence of length $T$: </p><script type="math/tex; mode=display">\begin{align}q([\textbf{x}_t]_i | \textbf{x}_0) =\left\{                \begin{array}{ll}                  [\textbf{x}_0]_i \qquad \text{if}\quad i<T-t\\                  \text{[MASK]} \quad\text{otherwise}                \end{array}    \right.\end{align}</script><p>For the position $i \neq T-t$, the KL divergence </p><script type="math/tex; mode=display">\begin{align}\mathbb{KL}(q([\mathbf{x}_{t-1}]_i \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta([\mathbf{x}_{t-1}]_i \vert\mathbf{x}_t)) \rightarrow 0\end{align}</script><p>Therefore, the KL divergence is computed over the tokens at position $i$, which is exactly the standard cross entropy loss for an autoregressive model.</p><script type="math/tex; mode=display">\begin{align}\mathbb{KL}(q([\mathbf{x}_{t-1}]_i \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta([\mathbf{x}_{t-1}]_i \vert\mathbf{x}_t)) &= q([\mathbf{x}_{t-1}]_i \vert \mathbf{x}_t, \mathbf{x}_0) \cdot \log \frac{q([\mathbf{x}_{t-1}]_i \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta([\mathbf{x}_{t-1}]_i \vert\mathbf{x}_t)} \\&=-p_\theta([\mathbf{x}_0]_i \vert\mathbf{x}_t) \\&= -p_\theta(x_{t-1}\vert x_{>t})\end{align}</script><hr><p><strong>(Generative) Maskde Language-Models are diffusion models</strong>. Generated MLMs<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-Predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, April 2019.">[15]</span></a></sup><sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov random field language model. arXiv preprint arXiv:1902.04094, February 2019.">[16]</span></a></sup> are generative models that generate text from a sequence of [MASK] tokens.</p>
          </div>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sohl-Dickstein, Jascha, et al. <a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning Using Nonequilibrium Thermodynamics</a>. ICML 2015<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ho, Jonathan, et al. <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a>. arXiv:2006.11239, arXiv, 16 Dec. 2020<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Nichol, Alex, and Prafulla Dhariwal. <a href="https://arxiv.org/abs/2102.09672">Improved Denoising Diffusion Probabilistic Models</a>. arXiv:2102.09672, arXiv, 18 Feb. 2021<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dhariwal, Prafulla, and Alex Nichol. <a href="https://arxiv.org/abs/2105.05233">Diffusion Models Beat GANs on Image Synthesis</a>. arXiv:2105.05233, arXiv, 1 June 2021<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ho, Jonathan, and Tim Salimans. <a href="https://arxiv.org/abs/2207.12598">Classifier-Free Diffusion Guidance</a>. 2021. openreview.net<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Luo, C. (2022). <a href="https://arxiv.org/pdf/2208.11970">Understanding diffusion models: A unified perspective</a>. arXiv preprint arXiv:2208.11970.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a>.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yang Song &amp; Stefano Ermon. <a href="https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf">Generative modeling by estimating gradients of the data distribution</a>. NeurIPS 2019.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P. and Welling, M. <a href="https://proceedings.neurips.cc/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf">Argmax flows and multinomial diffusion: Learning categorical distributions</a>. NeurIPS 2021.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Austin, J., Johnson, D.D., Ho, J., Tarlow, D. and van den Berg, R., 2021. <a href="https://proceedings.neurips.cc/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf">Structured denoising diffusion models in discrete state-spaces</a>. NeurIPS 2021.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Li, X.L., Thickstun, J., Gulrajani, I., Liang, P. and Hashimoto, T.B., 2022. <a href="https://arxiv.org/pdf/2205.14217">Diffusion-LM Improves Controllable Text Generation</a>. arXiv preprint arXiv:2205.14217.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Gong, S., Li, M., Feng, J., Wu, Z. and Kong, L., 2022. <a href="https://arxiv.org/pdf/2210.08933">Diffuseq: Sequence to sequence text generation with diffusion models</a>. arXiv preprint arXiv:2210.08933.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lin, Z., Gong, Y., Shen, Y., Wu, T., Fan, Z., Lin, C., Chen, W. and Duan, N., 2022. <a href="https://arxiv.org/pdf/2212.11685">GENIE: Large Scale Pre-training for Text Generation with Diffusion Model</a>. arXiv preprint arXiv:2212.11685.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">He, Z., Sun, T., Wang, K., Huang, X. and Qiu, X., 2022. <a href="https://arxiv.org/pdf/2211.15029">DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models</a>. arXiv preprint arXiv:2211.15029.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-Predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, April 2019.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov random field language model. arXiv preprint arXiv:1902.04094, February 2019.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sergios Karagiannakos,Nikolas Adaloglou. <a href="https://theaisummer.com/diffusion-models/">How diffusion models work: the math from scratch</a>. AI Summer. September 2022.<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</a>. Huggingface Blog.  June 2022.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Salimans, Tim and Jonathan Ho. <a href="https://arxiv.org/pdf/2202.00512.pdf">Progressive Distillation for Fast Sampling of Diffusion Models</a>. ICLR 2022.<a href="#fnref:19" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Diffusion Models</category>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Diffusion Models</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Efficient Large-Scale Distributed Training</title>
    <url>/notes/2022/04/17/Efficient-Large-Scale-Distributed-Training/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A note of distributed training methods for large neural models.<br><span id="more"></span></p>
<h2 id="Training-Parallelism"><a href="#Training-Parallelism" class="headerlink" title="Training Parallelism"></a>Training Parallelism</h2><h3 id="Data-Parallelism"><a href="#Data-Parallelism" class="headerlink" title="Data Parallelism"></a>Data Parallelism</h3><p>Data parallelism (DP) is a technique where we replicate the entire model’s parameters across multiple devices. During training, the mini-batch of data is partitioned evenly across all participating devices. This means that each device, or DP process, operates on a distinct subset of the data samples.</p>
<p>The training process in data parallelism involves each device executing its own forward and backward propagation. This computes the gradients based on the subset of data it has been assigned. Once the gradients are computed, they are averaged across all devices to ensure a consistent update to the model parameters.</p>
<h3 id="Model-Parallelism"><a href="#Model-Parallelism" class="headerlink" title="Model Parallelism"></a>Model Parallelism</h3><p>Model Parallelism (MP)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019). [Megatron-lm: Training multi-billion parameter language models using model parallelism](https://deepsense.ai/wp-content/uploads/2023/04/1909.08053.pdf). arXiv preprint arXiv:1909.08053.">[1]</span></a></sup> offers a way to scale neural network training beyond the memory limitations of a single device by distributing the model’s computation across multiple processes. This strategy is particularly useful for large transformer models that would otherwise be too large to fit on a single GPU.</p>
<p>Tensor-level Model Parallelism (MP) divides the model’s computation vertically among different devices or processes. </p>
<h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h4><p>To illustrate how MP works, let’s consider the standard Multilayer Perceptron (MLP) block within a transformer model, which is represented by the following equations:</p>
<p>\begin{equation}<br>    Y=\mathrm{GeLU}(XA)<br>\end{equation}</p>
<p>\begin{equation}<br>    Z=\mathrm{Dropout}(YB)<br>\end{equation}</p>
<p><img data-src="/notes/images/model_parallelism.png" alt="Megatron-1: MP"></p>
<p>For the MLP block, tensor-level MP splits the weight matrix<br>$A$ into columns $A = [A_1, A_2]$. By partitioning<br>$A$, the GeLU activation function can be applied independently to the outputs of each partitioned matrix multiplication (GEMM):</p>
<p>\begin{equation}<br>    [Y_1,Y_2]=[\text{GeLU}(XA_1),\text{GeLU}(XA_2)]<br>\end{equation}</p>
<p>The subsequent GEMM, represented by matrix $B$, is split along its rows. This enables direct input from the GeLU activations without the need for inter-process communication, as depicted in the figure above.</p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>The self-attention mechanism is a cornerstone of transformer models, described by the following equation:</p>
<p>\begin{equation}<br>    \text{Attention}(X,Q,K,V)=\text{softmax}(\frac{(XQ)(XK)^\top}{\sqrt{d_k}})XV<br>\end{equation}</p>
<p>For self-attention black, MP partitions the GEMMs for key ($K$), query ($Q$), and value ($V$) matrices along their columns. This allows for the matrix multiplication of each attention head to be distributed across individual GPUs. The output linear layer’s GEMM is then split along its rows, facilitating an efficient transformer layer that requires only two all-reduce operations in both the forward and backward passes.</p>
<p><img data-src="/notes/images/mp-communication.png" alt="Communication op for MP."></p>
<div class="note info">
            <p>When it comes to components like <strong>dropout, layer normalization, and residual connections</strong>, MP adopts a different approach. Instead of splitting these operations, MP replicates their computations across GPUs. This ensures that the output of the MP region can seamlessly integrate with these operations without additional device communication.</p><p>To achieve this, MP maintains duplicate copies of the layer normalization parameters on each GPU. As a result, each GPU can perform dropout and residual connection operations independently, taking the output from the MP region and processing it locally.</p>
          </div>
<p>Model Parallelism, by partitioning the model’s computation across multiple devices, effectively enables training of large-scale transformer models that would otherwise exceed the memory capabilities of a single device. Careful consideration of how operations like dropout and layer normalization are handled ensures that MP remains efficient without compromising the integrity of the model’s training.</p>
<h3 id="Pipeline-Parallelism"><a href="#Pipeline-Parallelism" class="headerlink" title="Pipeline Parallelism"></a>Pipeline Parallelism</h3><p>Pipeline Parallelism (PP)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Huang, Yanping, et al. "GPipe: Easy Scaling with Micro-Batch Pipel ine Parallelism." Computer Vision and Pattern Recognition (2019).">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Harlap, Aaron, et al. "Pipedream: Fast and efficient pipeline parallel dnn training." arXiv preprint arXiv:1806.03377 (2018).">[6]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Narayanan, Deepak, et al. "[Efficient large-scale language model training on gpu clusters using megatron-lm](https://arxiv.org/pdf/2104.04473)." Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.">[2]</span></a></sup> </p>
<h4 id="GPipe"><a href="#GPipe" class="headerlink" title="GPipe"></a>GPipe</h4><p>GPipe<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Huang, Yanping, et al. "GPipe: Easy Scaling with Micro-Batch Pipel ine Parallelism." Computer Vision and Pattern Recognition (2019).">[5]</span></a></sup> pipelines different sub-sequences of layers on separate accelerators, where consecutive groups of layers can be partitioned into cells. GPipe divides the input mini-batch into smaller micro-batches, enabling different accelerators to work on different micro-batches simutaneously.</p>
<p><img data-src="/notes/images/Gpipe.png" alt="GPipe"></p>
<p><strong>Pipeline Bubble</strong> (bubble size): Given the PP statges $p$ (PP degree), the sequence of $L$ layers can be partitioned into $p$ composite layers, or cells. The numbder of micro-batches in a batch as $m$. The PP bubble consists of $p-1$ forward passes at the start of a batch, and $p-1$ backward passes at the end. Thus, the pipeline bubble size (bubble time fraction) is defined as:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \text{Bubble time fraction (bubble size)}=1- \frac{md}{(m+d-1)d}=\frac{d-1}{m+d-1}.
\end{equation}</script><p><img data-src="/notes/images/GPipe-2.png" alt="GPipe"></p>
<p>When $m &gt; 4d$, the bubble overhead is negligible.</p>
<h4 id="PipeDream"><a href="#PipeDream" class="headerlink" title="PipeDream"></a>PipeDream</h4><p>PipeDream<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Harlap, Aaron, et al. "Pipedream: Fast and efficient pipeline parallel dnn training." arXiv preprint arXiv:1806.03377 (2018).">[6]</span></a></sup> revolutionizes the efficiency of pipeline parallelism in deep learning with its one-forward-one-backward (1F1B) strategy. This approach guarantees that no GPU remains idle during the steady state, thereby ensuring continuous forward progress with each minibatch. It achieves this by immediately initiating the backward pass for a minibatch as soon as its forward pass is completed.</p>
<p><img data-src="/notes/images/PipeDream-1F1B.png" alt="PipeDream-1F1B"> </p>
<p>The 1F1B strategy interleaves the forward and backward computations at the minibatch level. This tight coupling of passes optimizes the use of GPU resources and accelerates the learning process, as each minibatch benefits from immediate backward propagation, leading to quicker gradient updates and model improvements.</p>
<p><img data-src="/notes/images/PipeDream.png" alt="PipeDream-1F1B"></p>
<p>For interleaved schedules in PipeDream, it allows each device to handle multiple subsets of layers, referred to as model chunks, rather than being restricted to a single, contiguous block of layers. As a result, each device in the pipeline is responsible for multiple pipeline stages, dramatically increasing the efficiency of the computation distribution.</p>
<p>The figure above illustrates that if each device manages $v$ stages, or model chunks, the time required for processing a minibatch through both the forward and backward passes is reduced to $\frac{1}{v}$  of the time it would have previously taken. Consequently, this reduction in processing time diminishes the pipeline bubble—the period when some GPUs might otherwise be idle—resulting in a more streamlined and efficient training process. The bubble size can be quantified as follows:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \text{Bubble time fraction (bubble size)}=\frac{1}{v} \cdot \frac{d-1}{m+d-1}.
\end{equation}</script><p>Here, $d$ represents the degree of pipeline parallelism (number of devices), and $m$ is the number of microbatches in a batch. </p>
<!--
PipeDream-flush
PipeDream-2BW <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Narayanan, Deepak, et al. "Memory-efficient pipeline-parallel dnn training." International Conference on Machine Learning. PMLR, 2021.">[7]</span></a></sup>
-->
<h3 id="Sequence-Parallelism"><a href="#Sequence-Parallelism" class="headerlink" title="Sequence Parallelism"></a>Sequence Parallelism</h3><p>Model parallelism (MP) retains critical components like layer normalization, dropout, and residual connections across the MP group intact. A key insight presented in <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Korthikanti, Vijay Anand, et al. "[Reducing activation recomputation in large transformer models](https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf)." Proceedings of Machine Learning and Systems 5 (2023): 341-353">[3]</span></a></sup> is that in certain regions of transformer blocks, operations are independent along the sequence dimension. SP <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Korthikanti, Vijay Anand, et al. "[Reducing activation recomputation in large transformer models](https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf)." Proceedings of Machine Learning and Systems 5 (2023): 341-353">[3]</span></a></sup> partitions these regions along the sequence dimension for enhanced parallel processing.</p>
<p><img data-src="/notes/images/Sequence Parallelism.png" alt="Sequence parallesim (Megatron-3)"></p>
<p>Consider the following standard non-parallel block within a transformer layer:</p>
<script type="math/tex; mode=display">
\begin{align}
Y& =\mathrm{LayerNorm}(X) \\
Z& =\mathrm{GeLU}(YA), \\
W& =ZB, \\
V &=\mathrm{Dropout}(W), 
\end{align}</script><p>Sequence parallelism splits the input to the layer normalization along the sequence dimension: <script type="math/tex">X=[X_{1}^{s},X_{2}^{s}]</script>. Consequently, the output of the layer normalization is also parallel along the sequence dimension: <script type="math/tex">Y=[Y_{1}^{s},Y_{2}^{s}]</script>. The subsequent linear layer with GeLU activations requires the complete input $Y$, necessitating an all-gather operation. The matrices <script type="math/tex">A=[A_{1}^{c},A_{2}^{c}]</script> and <script type="math/tex">B=[B_{1}^{r},B_{2}^{t}]</script> are partitioned along their columns and rows, respectively. This partitioning strategy helps to minimize communication overhead and allows us to compute $W_1$ and $W_2$ independently. Afterwards, $W=W_1+W_2$ is combined and passed through the dropout layer using reduce-scatter to maintain parallelism along the sequence dimension.</p>
<p><img data-src="/notes/images/TP-megatron3.png" alt="Tensor Parallesim (Megatron-3)"></p>
<p>Putting it all together, we articulate the SP processing steps as follows:</p>
<script type="math/tex; mode=display">
\begin{align}
[Y_{1}^{s},Y_{2}^{s}]& =\mathrm{LayerNorm}([X_{1}^{s},X_{2}^{s}]), \\
\text{Y}& =g(Y_1^s,Y_2^s), \\
[Z_1^h,Z_2^h]& =[\mathrm{GeLU}(YA_{1}^{c}), \mathrm{GeLU}(YA_{2}^{c})], \\
W_{1}& =Z_{1}^{h}B_{1}^{r} \quad
W_{2}=Z_{2}^{h}B_{2}^{r}, \\
[W_{1}^{s},W_{2}^{s}]& =\bar{g}(W_1,W_2), \\
[V_{1}^{s},V_{2}^{s}]& =[\mathrm{Dropout}(W_{1}^{s}), \mathrm{Dropout}(W_{2}^{s})]. 
\end{align}</script><p>SP divides and conquers the workload along the sequence dimension without compromising the integrity of the underlying operations.</p>
<!--
### Megatron 1/2/3
4D parallel
-->
<h3 id="Memory-Efficient-Optimizer-ZeRO"><a href="#Memory-Efficient-Optimizer-ZeRO" class="headerlink" title="Memory-Efficient Optimizer (ZeRO)"></a>Memory-Efficient Optimizer (ZeRO)</h3><p>Zero Redundancy Optimizer (ZeRO)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. "Zero: Memory optimizations toward training trillion parameter models." In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.">[4]</span></a></sup> optimizes the memory by removing the memory state redundancies across DP processes by partitioning the model states instead of replicating them. ZeRO-DP has three main statges, corresponding to the partitioning of optimizer states, gradients, and parameters.</p>
<ol>
<li>Optimizer state partitioning ($P_\text{os}$);</li>
<li>Add gradient partitioning ($P_\text{os+g}$));</li>
<li>Add parameter partitioning ($P_\text{os+g+p}$));</li>
</ol>
<p><img data-src="/notes/images/ZeRO.png" alt="ZeRO-DP"></p>
<p>Details refer to <a href="https://www.deepspeed.ai/tutorials/zero/">https://www.deepspeed.ai/tutorials/zero/</a>.</p>
<h3 id="Pytorch-FSDP"><a href="#Pytorch-FSDP" class="headerlink" title="Pytorch FSDP"></a>Pytorch FSDP</h3><p>PyTorch Fully Sharded Data Parallel (FSDP)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhao, Yanli, et al. "Pytorch fsdp: experiences on scaling fully sharded data parallel." arXiv preprint arXiv:2304.11277 (2023).">[8]</span></a></sup> is designed to accommodate extremely large models that exceed the memory capacity of a single GPU. By decomposing a model instance into smaller fragments, FSDP manages each fragment independently. During the forward and backward computations, FSDP strategically materializes only the unsharded parameters and gradients for one fragment at a time, while keeping the rest of the parameters and gradients in their sharded state.</p>
<p>This resource management means that FSDP only fully materializes the parameters and gradients for a single fragment at any given time, allowing the remaining fragments to remain sharded and thus minimizing memory usage.</p>
<p><img data-src="/notes/images/FSDP.png" alt="Pytorch FSDP"></p>
<p>FFSDP employs a sharding factor $F$ ato determine the number of ranks over which the parameters are distributed:</p>
<ol>
<li>When$F=1$, FSDP replicates the entire model across all devices, reducing to the conventional Data Parallel (DP) approach, which relies on all-reduce operations for gradient synchronization.</li>
<li>For $F=W$,, where $W$ is the global world size, FSDP fully shards the model so that each device maintains only $\frac{1}{W}$ of the total model parameters. </li>
<li>When $F \in (1, W)$, FSDP enables hybrid sharding, balancing between replication and full sharding.</li>
</ol>
<p><img data-src="/notes/images/FSDP-full-sharding.png" alt="FSDP full sharding"></p>
<p><strong>Sharding strategy: flatten-concat-chunk algorithm</strong>. FSDP uses a sharding strategy known as the flatten-concat-chunk algorithm. This technique entails organizing all parameters within an FSDP unit into a single contiguous <code>FlatParameter</code>. This <code>FlatParameter</code>, which is a one-dimensional tensor, is created by concatenating and flattening the individual parameters, with padding added as necessary to ensure the size is divisible by the sharding factor $F$. The <code>FlatParameter</code> is then divided into equal-sized chunks, with the number of chunks corresponding to the sharding factor, and each chunk is assigned to a different rank.</p>
<p>By leveraging this strategy, FSDP streamlines communication between the parameters and ensures an even distribution of the model across the ranks. This allows for efficient scaling of model training across multiple GPUs, making it possible to train models that were previously too large to fit in the memory of a single device.</p>
<p><img data-src="/notes/images/FDSP hybrid sharding.png" alt="FDSP hybrid sharding"></p>
<h2 id="Mixed-Precision-Training"><a href="#Mixed-Precision-Training" class="headerlink" title="Mixed Precision Training"></a>Mixed Precision Training</h2><p>Mixed precision methods<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="NVIDIA. [Train with mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)">[9]</span></a></sup><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Micikevicius, Paulius, et al. "Mixed precision training." arXiv preprint arXiv:1710.03740 (2017).">[10]</span></a></sup> utilize different numerical formats within a single computational workload, optimizing operations by executing them in half-precision (FP16) format. This approach not only accelerates training but also reduces memory usage, allowing for larger models or batch sizes.</p>
<p>During mixed precision training, weights, activations, and gradients are predominantly stored as FP16 to benefit from the reduced precision’s efficiency. However, to maintain the training stability and model quality, an FP32 master copy of weights is kept. This master copy is updated with weight gradients during the optimization step. For each iteration, an FP16 copy of the master weights is used for the forward and backward passes.</p>
<p><img data-src="/notes/images/mixed-precision-trend.png" alt="Mixed precision training"></p>
<p>Mixed precision methods<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="NVIDIA. [Train with mixed Precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)">[9]</span></a></sup><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Micikevicius, Paulius, et al. "Mixed precision training." arXiv preprint arXiv:1710.03740 (2017).">[10]</span></a></sup> combine the use of different numerical formats in one computational workload.<br><img data-src="/notes/images/Mixed Precision Training.png" alt="Mixed Precision Training"></p>
<p>The training procedure for mixed precision training can be summarized as follows:</p>
<div class="note info">
            <p><strong>Mixed precision training:</strong></p><ol><li>Keep a master copy of weights in full precision (FP32).</li><li>For each iteration:<br> a. Make an FP16 copy of the weights;<br> b. Forward propagation (FP16 weights and activations).<br> c. Multiply the results loss with the scaling factor $S$.<br> d. Backward propagation (FP16 weights, activations, and their gradients).<br> e. Multiply (scaling down) the weight gradient with 1/S.<br> f. Updating the master weights in FP32, applying necessary adjustments like gradient clipping.</li></ol>
          </div>
<p><strong>Dynamic loss scaling</strong>: SThis technique involves starting with a large scaling factor and adjusting it dynamically throughout the training process. If no numerical overflow occurs for a predefined number of iterations $N$, the scaling factor $S$ is increased. Conversely, if an overflow is detected, the current weight update is skipped, and $S$ is decreased to prevent future overflows.</p>
<div class="note info">
            <p><strong>Mixed precision training:</strong></p><ol><li>Maintain a primary copy of weights in FP32.</li><li>Initialize $S$ to a large value.</li><li>For each iteration:<br> a. Make an FP16 copy of the weights;<br> b. Forward propagation (FP16 weights and activations).<br> c. Multiply the results loss with the scaling factor $S$.<br> d. Backward propagation (FP16 weights, activations, and their gradients).<br> e. If there is an <code>inf</code> or <code>nan</code> in weights gradients:<pre><code> (1) Reduce S. (2) Skip the weight update and move to the next iteration.</code></pre> f. Multiply the weight gradient with 1/S.<br> g. Complete the weight update (including gradient clipping, etc.)<br> h. If there has not been an <code>inf</code> or <code>nan</code> in the last $N$ iterations, increase $S$.</li></ol>
          </div>
<p>Implementation:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Creates model and optimizer in default precision</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a GradScaler once at the beginning of training.</span></span><br><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Runs the forward pass with autocasting.</span></span><br><span class="line">        <span class="keyword">with</span> autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class="line">        <span class="comment"># Backward passes under autocast are not recommended.</span></span><br><span class="line">        <span class="comment"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.</span></span><br><span class="line">        <span class="comment"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span><br><span class="line">        <span class="comment"># otherwise, optimizer.step() is skipped.</span></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Updates the scale for next iteration.</span></span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure></p>
<p><strong>Precision</strong>: fp16/bf16/fp8</p>
<p><img data-src="/notes/images/floating-point.png" alt=""></p>
<h2 id="Memory-Efficient-Methods"><a href="#Memory-Efficient-Methods" class="headerlink" title="Memory-Efficient Methods"></a>Memory-Efficient Methods</h2><h3 id="CPU-Offload"><a href="#CPU-Offload" class="headerlink" title="CPU Offload"></a>CPU Offload</h3><p><strong>CPU offload</strong>: Offloading model states to GPU memory. When GPU memory reaches its capacity, a potential solution is to transfer data that is not immediately required to the CPU, retrieving it when necessary at a later stage</p>
<h3 id="Activation-Recomputation"><a href="#Activation-Recomputation" class="headerlink" title="Activation Recomputation"></a>Activation Recomputation</h3><p><strong>Activation recomputation / Selective recomputation</strong>: Only selected activations are stored for backpropagation while most activations are discarded as they can be recomputed again during the backpropagation. This strategy involves selectively preserving only a subset of activations for use in the backpropagation process. The majority of activations, deemed less critical, are not stored; instead, they are dynamically recalculated as needed during the backpropagation phase.</p>
<!--
## Flash Attention
Flash Attention v1/2/3
## Decoding
vLLM / Mooncake
-->
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., &amp; Catanzaro, B. (2019). <a href="https://deepsense.ai/wp-content/uploads/2023/04/1909.08053.pdf">Megatron-lm: Training multi-billion parameter language models using model parallelism</a>. arXiv preprint arXiv:1909.08053.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Narayanan, Deepak, et al. &quot;<a href="https://arxiv.org/pdf/2104.04473">Efficient large-scale language model training on gpu clusters using megatron-lm</a>.&quot; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Korthikanti, Vijay Anand, et al. &quot;<a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf">Reducing activation recomputation in large transformer models</a>.&quot; Proceedings of Machine Learning and Systems 5 (2023): 341-353<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. &quot;Zero: Memory optimizations toward training trillion parameter models.&quot; In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Huang, Yanping, et al. &quot;GPipe: Easy Scaling with Micro-Batch Pipel ine Parallelism.&quot; Computer Vision and Pattern Recognition (2019).<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Harlap, Aaron, et al. &quot;Pipedream: Fast and efficient pipeline parallel dnn training.&quot; arXiv preprint arXiv:1806.03377 (2018).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Narayanan, Deepak, et al. &quot;Memory-efficient pipeline-parallel dnn training.&quot; International Conference on Machine Learning. PMLR, 2021.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhao, Yanli, et al. &quot;Pytorch fsdp: experiences on scaling fully sharded data parallel.&quot; arXiv preprint arXiv:2304.11277 (2023).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">NVIDIA. <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">Train with mixed Precision</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Micikevicius, Paulius, et al. &quot;Mixed precision training.&quot; arXiv preprint arXiv:1710.03740 (2017).<a href="#fnref:10" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Pre-training</category>
        <category>LLM</category>
        <category>Distributed Training</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Pre-training</tag>
        <tag>Distributed Training</tag>
      </tags>
  </entry>
  <entry>
    <title>Large Language Models for Programming Languages</title>
    <url>/notes/2022/05/13/Large-Language-Models-for-Programming-Languages/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A note of code pre-trained language models (PLMs).<br><span id="more"></span></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>Source</th>
<th>#params</th>
<th>L2R LM</th>
<th>Mask LM</th>
<th>seq2seq LM</th>
<th>Code  structure</th>
<th>Warmup</th>
<th>tokenizer</th>
<th>Model</th>
<th>#PLs</th>
<th>Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>CuBERT</td>
<td>ICML’20 (Google)</td>
<td>345M</td>
<td></td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>python tokenizer</td>
<td>BERT-large</td>
<td>1</td>
<td>7.4M Python files</td>
</tr>
<tr>
<td>CodeBERT</td>
<td>EMNLP’20 findings (MSRA)</td>
<td>125M</td>
<td></td>
<td>✔️</td>
<td></td>
<td></td>
<td>✔️</td>
<td>BBPE</td>
<td>fine-tuned RoBERTa</td>
<td>6</td>
<td>CodeSearchNet (6 PL languages)</td>
</tr>
<tr>
<td>GPT-C</td>
<td>CSEC/FSE’20 (MS)</td>
<td>366M</td>
<td>✔️</td>
<td></td>
<td></td>
<td></td>
<td>-</td>
<td>BBPE</td>
<td>GPT-2 variant</td>
<td>1/multi</td>
<td>monolingual/multilingual PLs</td>
</tr>
<tr>
<td>CodeGPT</td>
<td>(MSRA)</td>
<td>124M</td>
<td>✔️</td>
<td></td>
<td></td>
<td></td>
<td>both</td>
<td>BBPE</td>
<td>GPT-2 variant</td>
<td>1</td>
<td>from GitHub</td>
</tr>
<tr>
<td>PLBART</td>
<td>NAACL’21</td>
<td>406M</td>
<td></td>
<td></td>
<td>✔️</td>
<td></td>
<td>-</td>
<td>SentencePiece</td>
<td>BART-base</td>
<td>2</td>
<td>470M Java, 219M Python, NL 47M.</td>
</tr>
<tr>
<td>CodeT5</td>
<td>EMNLP’21 (Salesforce/NTU)</td>
<td>220M (T5 base) 60M (T5 small)</td>
<td></td>
<td></td>
<td>✔️</td>
<td>identifier</td>
<td>-</td>
<td>BBPE</td>
<td>T5-base</td>
<td>6+2</td>
<td>8.35M instances  (CodeSearchNet/Collected)</td>
</tr>
<tr>
<td>UniXcoder</td>
<td>ACL’22 (MSRA)</td>
<td>~110M (BERT-base)</td>
<td></td>
<td></td>
<td>✔️</td>
<td>✔️</td>
<td>-</td>
<td>BBPE</td>
<td>BERT-base</td>
<td>6</td>
<td>CodeSearchNet</td>
</tr>
<tr>
<td>DOBF</td>
<td>NeurIPS’21  (FAIR France)</td>
<td>base</td>
<td></td>
<td></td>
<td>✔️</td>
<td>code obfuscation</td>
<td>both</td>
<td>BBPE</td>
<td>CodeBERT init/ from scratch</td>
<td>2</td>
<td>Python/Java files in GitHub  repo from Google BigQuery</td>
</tr>
<tr>
<td>GraphCodeBERT</td>
<td>ICLR’21 (MSRA)</td>
<td>125M</td>
<td></td>
<td>✔️</td>
<td></td>
<td>data flow</td>
<td>✔️</td>
<td>BBPE</td>
<td>CodeBERT init.</td>
<td>6</td>
<td>CodeSearchNet (6 PLs)</td>
</tr>
<tr>
<td>SynCoBERT</td>
<td>AAAI-22</td>
<td>125M</td>
<td></td>
<td>✔️</td>
<td></td>
<td>✔️</td>
<td>✔️</td>
<td>BBPE</td>
<td>CodeBERT init.</td>
<td>6</td>
<td>CodeSearchNet</td>
</tr>
<tr>
<td>CodeParrot</td>
<td>huggingface</td>
<td>1.5B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>GPT-2</td>
<td>1</td>
<td>20M files Python files from  Google BigQuery Github database</td>
</tr>
<tr>
<td>GPT-Neo</td>
<td>EleutherAI</td>
<td>2.7B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>Transformer Decoder</td>
<td>-</td>
<td>Mix</td>
</tr>
<tr>
<td>GPT-NeoX</td>
<td>EleutherAI</td>
<td>20B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>GPT-NeoX</td>
<td>-</td>
<td>Mix</td>
</tr>
<tr>
<td>GPT-J</td>
<td>(open source)</td>
<td>6B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>GPT</td>
<td>-</td>
<td>Mix</td>
</tr>
<tr>
<td>PolyCoder</td>
<td>CMU</td>
<td>2.7B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>GPT-2</td>
<td>12</td>
<td>24M files (12 PLs).</td>
</tr>
<tr>
<td>Codex</td>
<td>OpenAI</td>
<td>12B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>✔️</td>
<td>BBPE</td>
<td>fine-tuned GPT-3</td>
<td>1</td>
<td>159GB python files after filtering.</td>
</tr>
<tr>
<td>AlphaCode</td>
<td>DeepMind</td>
<td>41B / 9B</td>
<td></td>
<td></td>
<td>✔️</td>
<td>-</td>
<td>-</td>
<td>SentencePiece</td>
<td>enc-dec</td>
<td>12</td>
<td>715.1 GB after filtering.</td>
</tr>
<tr>
<td>Google’s (Austin 2021)</td>
<td>Google Resarch</td>
<td>137B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>SentencePiece</td>
<td>Decoder</td>
<td>-</td>
<td>mixed (2.97B documents)</td>
</tr>
<tr>
<td>InCoder</td>
<td>Meta AI</td>
<td>6.7B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>Decoder</td>
<td>28</td>
<td>1TB -&gt; 250GB.  GitHub and GitLab via API.</td>
</tr>
<tr>
<td>CodeGen</td>
<td>Salesforce</td>
<td>16.1B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BBPE</td>
<td>Decoder</td>
<td>Multi</td>
<td>GitHub</td>
</tr>
<tr>
<td>PaLM-Coder</td>
<td>Google Research</td>
<td>540B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>SentencePiece</td>
<td>Decoder</td>
<td>Multi</td>
<td>Mixed</td>
</tr>
<tr>
<td>StarCoder</td>
<td>BigCode project</td>
<td>15.5B</td>
<td>✔️</td>
<td></td>
<td></td>
<td>-</td>
<td>-</td>
<td>BPE</td>
<td>Decoder</td>
<td>86</td>
<td>The Stack (GitHub)</td>
</tr>
</tbody>
</table>
</div>
<!--<span class="label success">xxx</span>-->
<h1 id="Evaluation-task"><a href="#Evaluation-task" class="headerlink" title="Evaluation task"></a>Evaluation task</h1><div class="note info">
            <ul><li>Program understanding: code search, program repair, bug detection and localization,.</li><li>Program generation: code completion, program synthesis, code summarization, source code to pseudo-code mapping, API-sequece prediction, natural language to code mapping, document generation.</li></ul>
          </div>
<p><strong>Code token types</strong>: local variables, methods or APIs, arguments, punctuation, language keywords, delimiters.</p>
<p><img data-src="/notes/images/CodeXGLUE.png" alt=""></p>
<p>CodeXGLUE<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu and Shujie Liu. “[CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.](https://arxiv.org/pdf/2102.04664.pdf)” ArXiv abs/2102.04664 (2021).
">[5]</span></a></sup> includes 14 datasets, consisting of 10 diversified PL understanding and generation tasks.</p>
<ul>
<li><strong>code-code</strong>: <ol>
<li>Clone detection: Measure the semantic similarity between codes. It includes two subtasks: binary classification between a pair of codes and code retrieval, where the goal is to find semantically similar codes. </li>
<li>Defect detection: The object is to identify whether a body of source code contains defects that may be used to attract software systems, such as resource leaks, use-after-free vulnerabilities, and DoS attack.</li>
<li>Cloze test: predict the masked token of a code and includes two subtasks: (1) to measure the accuracy of predicting the masked token from the whole vocabulary (2) to test the semantic reasoning ability by distinguishing between “max” and “min”.</li>
<li>Code completion: Predict following tokens based on a code context. Two subtasks: (1) token-level completion: check whether the next token has been predicted correctly; (2) line-level completion: test the goodness of the generated line.</li>
<li>Code repair: to refine the code by fixing the bugs automatically.</li>
<li>Code-to-code translation: translating from one PL to another one.</li>
</ol>
</li>
<li><strong>text-code</strong>: <ol>
<li>NL code search: It measures the semantic relatedness between texts and codes. Two subtasks: (1) Given an NL query, find the most relevant code in a collection of codes; (2) Given a query-code pair, predict whether the code answers the query or not.</li>
<li>Text-to-code generation: generate a code via a NL description.</li>
</ol>
</li>
<li><strong>code-text</strong>: <ol>
<li>Code summarization: generate the NL comment for a code.</li>
</ol>
</li>
<li><strong>text-text</strong>: <ol>
<li>Documentation translation: translate code documentation from one NL to another one.    </li>
</ol>
</li>
</ul>
<h1 id="Code-PLMs"><a href="#Code-PLMs" class="headerlink" title="Code PLMs"></a>Code PLMs</h1><h2 id="CuBERT"><a href="#CuBERT" class="headerlink" title="CuBERT"></a>CuBERT</h2><p><span class="label warning">Background</span>: <strong>There is no attempt yet to obtain the high-quality contextual embeddings of source code</strong>, and evaluate it on multiple program-understanding tasks simultaneously. That is the gap that CuBERT aims to mitigate.</p>
<p>CuBERT<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kanade, Aditya, Petros Maniatis, Gogul Balakrishnan and Kensen Shi. “[Learning and Evaluating Contextual Embedding of Source Code.](http://proceedings.mlr.press/v119/kanade20a/kanade20a.pdf)” ICML (2020).
">[1]</span></a></sup> (code understanding BERT) presents <span class="label success">the first attempt at code pre-training</span> on (python) source code.</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><ul>
<li><strong>Pre-training data</strong>: <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kanade, Aditya, Petros Maniatis, Gogul Balakrishnan and Kensen Shi. “[Learning and Evaluating Contextual Embedding of Source Code.](http://proceedings.mlr.press/v119/kanade20a/kanade20a.pdf)” ICML (2020).
">[1]</span></a></sup> curated 7.4 million python files with a total of 9.3 billion tokens (1.6 billion unique).</li>
<li><strong>Tokenization</strong>: first tokenize the python program using the standard Python tokenizer (<em>tokenize</em> package);; then greedily compress them into a subword vocabulary using the <em>SubwordTextEncoder</em> in the Tensor2Tensor project, resulting in ~50k tokens.</li>
<li><strong>Vocabulary size</strong>: ~50K.</li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li><strong>Model config</strong>: BERT-large models.</li>
<li>Training details: Linear warm up 10% of examples.</li>
<li><strong>Pre-training task</strong>: masked language model (MLM); next sentence prediction (NSP).</li>
<li><a href="https://github.com/google-research/google-research/tree/master/cubert">Models and datasets</a></li>
</ul>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p> It shows that CuBERT can use only 33% labeled data with only 2 epoch to match the baselines trained with full data and many more epochs.<br><img data-src="/notes/images/CuBERT-results.png" alt=""></p>
<h2 id="CodeBERT"><a href="#CodeBERT" class="headerlink" title="CodeBERT"></a>CodeBERT</h2><p>Background: the success of PLMs drive the surge of multi-modal pre-training, which are learned from <strong>bi-modality</strong>.</p>
<p>CodeBERT<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Feng, Zhangyin, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang and Ming Zhou. “[CodeBERT: A Pre-Trained Model for Programming and Natural Languages.](https://aclanthology.org/2020.findings-emnlp.139.pdf)” Findings of EMNLP (2020).
">[2]</span></a></sup> is a bimodal PLM for natural language (NL) and programming language (PL).<br>It is <span class="label success">the first large NL-PL PLM</span> for multiple PLs. </p>
<h3 id="Data-1"><a href="#Data-1" class="headerlink" title="Data"></a>Data</h3><ul>
<li><strong>Pre-training data</strong>: CodeSearchNet<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Husain, Hamel, Hongqi Wu, Tiferet Gazit, Miltiadis Allamanis and Marc Brockschmidt. “[CodeSearchNet Challenge: Evaluating the State of Semantic Code Search.](https://arxiv.org/pdf/1909.09436.pdf)” *ArXiv* abs/1909.09436 (2019).
">[3]</span></a></sup> (1) <em>bimodal</em> data of NL-PL pairs: 2.1M datapoints; (2) large amount of <em>unimodal</em> code data without paired documents: 6.4M codes across six PLs (Python, Java, JavaScript, PHP, Ruby, and Go).</li>
</ul>
<p><img data-src="/notes/images/CodeSearchNet-statistics.png" width="60%"></p>
<ul>
<li>Tokenization: RoBERTa BBPE.</li>
</ul>
<h3 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h3><ul>
<li>Model config: RoBERTa_base. (125M params)</li>
<li><strong>Pre-training task</strong>:<br>(1) MLM;<br>(2) Replaced token detection (RTD). Different from ELECTRA, it uses <em>n</em>-gram LMs as PL and NL generator as shown in the figure.<br><img data-src="/notes/images/CodeBERT.png" alt="RTD"></li>
<li><strong>Finetune settings</strong>: (1) For NL code search, use the CodeBERT for pre-training; (2) For code-to-text generation, it uses an encoder-decoder model, and initializes the encoder with CoderBERT.</li>
</ul>
<h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><ul>
<li><strong>NL code search</strong>: “Init=S” vs. “Init=R” $\rightarrow$ RoBERTa warmup confers performance gain. This observation is <strong>different from OpenAI Codex!!</strong><br><img data-src="/notes/images/NL-code-search.png" alt=""></li>
<li>NL-PL probing (zero-shot): construct dataset to fill in a keyword from {max, maximize, min, minimize, less, greater}.</li>
<li>Code documentation generation: <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Feng, Zhangyin, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang and Ming Zhou. “[CodeBERT: A Pre-Trained Model for Programming and Natural Languages.](https://aclanthology.org/2020.findings-emnlp.139.pdf)” Findings of EMNLP (2020).
">[2]</span></a></sup> initializes the encoder of encoder-decoder framework with CoderBERT and evaluates the results by means of the smoothed BLEU score.</li>
<li>Test on C# that is unseen before. The performance is worse than code2seq which uses the compositional paths of its abstract syntax tree (AST). The AST experiments of CodeBERT fail.</li>
</ul>
<h2 id="CodeGPT"><a href="#CodeGPT" class="headerlink" title="CodeGPT"></a>CodeGPT</h2><p>CodeGPT<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu and Shujie Liu. “[CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.](https://arxiv.org/pdf/2102.04664.pdf)” ArXiv abs/2102.04664 (2021).
">[5]</span></a></sup> is a variant of GPT-2 (L12/A12/H768). <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu and Shujie Liu. “[CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.](https://arxiv.org/pdf/2102.04664.pdf)” ArXiv abs/2102.04664 (2021).
">[5]</span></a></sup> trained both from scratch with newly obtained vocabularies and from GPT-2 initialization with original vocabularies (termed <strong>CodeGPT-adapted</strong>).</p>
<ul>
<li>Pre-training data: monolingual data on Python and Java in the CodeSearchNet dataset, including 1.1M Python functions and 1.6M Java methods.</li>
<li>Huggingface model: <a href="https://huggingface.co/microsoft/CodeGPT-small-java">CodeGPT-small</a>, <a href="https://huggingface.co/microsoft/CodeGPT-small-java-adaptedGPT2">CodeGPT-small-java-adapted</a>.</li>
</ul>
<p><img data-src="/notes/images/CodeXGLUE-baseline-models.png" alt=""></p>
<h2 id="GPT-C"><a href="#GPT-C" class="headerlink" title="GPT-C"></a>GPT-C</h2><ul>
<li>Background: Majority of argument completion in code completion systems only work when the name of the method or API call is already typed in, thus leaving the task of completing the method calls to software developers. </li>
<li><strong>Cons</strong>: Previously existing code completion tools have focused on specific token types or features, often failing to have a holistic view of the surrounding context.</li>
<li><strong>Motivating Example</strong>: The example below shows an method completion and an argument completion in C Sharp PL served by the Intellicode extension in Visual Studio IDE, and the whole-line of code completion generated by IntelliCode Compose. </li>
</ul>
<p><img data-src="/notes/images/GPT-C-example.png" alt=""></p>
<p><strong>GPT-C</strong> <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> (<em>i.e.</em>, IntelliCode Compose), a variant of GPT-2, can generate syntactically correct code in multiple PLs, capable of <strong><span class="label success">completing an entire line of code in a couple of key strokes</span></strong>. It is able to learn to infer <strong>types of PL identifiers</strong> and <strong>long-range code semantics</strong> without inputs extracted by means of a static analyzer explicitly passed to the model as features.</p>
<h3 id="Data-2"><a href="#Data-2" class="headerlink" title="Data"></a>Data</h3><p><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> collects 52k top-starred (non-fork) project in GitHub, containing over 4.7M source code files, comprising over 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript PLs. </p>
<ul>
<li>Data split: It splits the data into 70/30 as dev/test on the repository level. The dev set is then split into 80/20 as training/validation set. The final deployed model is re-trained using the entire dataset.</li>
</ul>
<p><img data-src="/notes/images/GPT-C-training-data.png" alt=""></p>
<p>Tokenization (see below figure example):</p>
<ol>
<li>BPE tokenization. It uses <strong>sentencepiece</strong> tokenizer with special tokens for control flow and code structure representations. For control flow tokens &lt;BOF&gt; and &lt;EOF&gt; to mark the beginning and ending of a file in order to disambiguate similar identifier names in different files, and &lt;EOL&gt; to mark the ending of a line. Since python uses white-spaces and indentation to demarcate code scope, <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> introduces &lt;INDENT&gt; and &lt;DEDENT&gt; tokens to represent those scope delimiters. </li>
<li>Splitting PL identifiers using casing conventions. $\rightarrow$ work for PL, not for NL.</li>
</ol>
<p><img data-src="/notes/images/GPT-C-tokenization.png" alt=""></p>
<p><strong>Exposing sensitive data through code suggestions</strong>.  The figure shows an example completion served by the <a href="https://www.tabnine.com/blog/deep/"><em>TabNine</em></a> system exposing irrelevant and potentially sensitive data.<br><img data-src="/notes/images/TabNine-hash-value-completion.png" alt=""></p>
<p>To address this problem, the training should be shielded from  inadvertently gaining access to secrets or personally identifiable data. For this reason, <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> identifies and normalizes numeric literals, string literals and comments, including docstrings, to &lt;NUM_LIT&gt;, &lt;STR_LIT&gt;, and &lt;COMMENT&gt; special tokens, respectively.</p>
<h3 id="Model-2"><a href="#Model-2" class="headerlink" title="Model"></a>Model</h3><ul>
<li>Model config: GPT-2. <ul>
<li>Best monolingual model: L24, H16, #vocab 50k.</li>
<li>Best multilingual model: L26, H16, #vocab 60k.</li>
</ul>
</li>
<li>Training details: training from scratch; weight tying; </li>
<li>Decoding: beam search.</li>
</ul>
<h3 id="Code-completion-system"><a href="#Code-completion-system" class="headerlink" title="Code completion system"></a>Code completion system</h3><p>For user experience, if a response time under 100ms is necessary to avoid any feeling of delay or lag. To achieve it in a cloud-based model deployment setting, <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> presents caching on the client side. When typing a non-alphanumeric character, suggestions are queried from the server. Those suggestions, each as a list of tokens along with their scores, are stored in a trie placed in the cache. This allows to prune the tree efficiently at a character-level as the user continues typing. <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> simply traverse this tree greedily by always branching to the node with the highest score.</p>
<p>To preserve accuracy, <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> terminates the completion-tree traversal if none of the child nodes has a score that is equal to or larger than the score of its parent multiplied by a ratio $R$, defined as:</p>
<script type="math/tex; mode=display">
R = \frac{\alpha}{1 + e^{-L / \kappa}}</script><p>where $L$ is the position of the root node of the trie, $\alpha$ is the relaxation factor, and $\kappa$ is the curvature factor. $\alpha$ is used to adjust the values of $R$ for very small or very large values of $L$. A lower value of $\alpha$ would relax the policy producing longer completion suggestions, while a value closer to 1.0 would tighten the policy producing shorter suggestions. $\kappa$ controls the rate of increase of the $R$: A smaller $\kappa$ would give a steeper curve for smaller values of $L$, producing shorter suggestions, while a larger value of $\kappa$ would yield a flatter curve resulting in longer completion suggestion. <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> selects $\alpha=0.8$ and $\kappa=10$ to gain a balance between suggestion length and  relevance.</p>
<p><img data-src="/notes/images/GPT-C-completion-caching.png" alt=""></p>
<h3 id="Multilingual-model"><a href="#Multilingual-model" class="headerlink" title="Multilingual model"></a>Multilingual model</h3><ul>
<li>Tokenization: BPE tokenizer.</li>
<li>Evaluation metric: perplexity, ROUGE-L, Levenshtein similarity. ROUGE-L variant is based on the longest common subsequence (LCS) statistics, which takes into count structure similarity and identifies longest co-occurring <em>n</em>-grams. Levenshtein distance measures how many single-character edits - including insertion, substitution, or deletion - does it take to transform one sequence of tokens to another.</li>
<li>Online evaluation metric: <strong>surfacing rate (SR)</strong> and <strong>click-through rate (CTR)</strong>.<br>(1) <strong>SR</strong> is the total number of completions displayed divided by the total number of times a completion could potentially be shown, which is after every character typed into a code document. The SR is not only dependent on the accuracy of the model but also on the typing speed of a user and their network reliability.<br>(2) The <strong>CTR</strong> is defined as the fraction of accepted completions over the total number of completions displayed. The low CTR can be partially attributed to the momentum in typing.</li>
</ul>
<p>Baseline:</p>
<ol>
<li>Language-agnostic baseline.</li>
<li>Language type-embedding. Add language type embedding with the token and position embedding matrices.</li>
<li>Language-specific control codes. Insert a sequence of tokens in the beginning of each training sample code: “lang %s remaining token sequence”, where lang $\in$ {Python, C#, JavaScript, TypeScript}.</li>
<li>Add a PL classification pre-training task to detect programming languages besides language modeling.</li>
</ol>
<p><img data-src="/notes/images/MultiGPT-C-results.png" alt=""></p>
<h2 id="GraphCodeBERT"><a href="#GraphCodeBERT" class="headerlink" title="GraphCodeBERT"></a>GraphCodeBERT</h2><ul>
<li>Background: Existing code PLMs regard the code snippets as a sequence of tokens, while ignoring the inherent structure of code that contains semantics.</li>
</ul>
<p>GraphCodeBERT<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Guo, Daya, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang and M. Zhou. “[GraphCodeBERT: Pre-training Code Representations with Data Flow.](https://arxiv.org/pdf/2009.08366)” ICLR (2021).
">[6]</span></a></sup> is the first code PLM that uses <strong>semantic structure of code</strong> to learn code representation. It presents two structure-aware pre-training tasks: (1) data flow edge prediction (to learn from code structure); (2) variable alignment across source code and data flow (to align source code and code structure).</p>
<ul>
<li>Data: CodeSearchNet (6 PLs)</li>
</ul>
<p>GraphCodeBERT<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Guo, Daya, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang and M. Zhou. “[GraphCodeBERT: Pre-training Code Representations with Data Flow.](https://arxiv.org/pdf/2009.08366)” ICLR (2021).
">[6]</span></a></sup> incorporates the code structure of data flow into pre-training. <strong>Data flow</strong> is a graph that represents dependency relation between variables, in which nodes represent variables and edges represent where the value of each variable comes from. </p>
<h3 id="Data-flow"><a href="#Data-flow" class="headerlink" title="Data flow"></a>Data flow</h3><p>Given a source code, <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> firstly parses the code into an abstract syntax tree (AST) which includes syntax structure of code. The terminal nodes (leaves) are used to identify the variable sequence $V$. We take each variable as a node of graph and a direct edge $\epsilon = \langle v_i, v_j \rangle$ from $v_i$ to $v_j$ refers the value of $j$-th variable comes form $i$-th variable. The set of directed edges as $E = { \epsilon_1, \epsilon_2, \cdots, \epsilon_l }$ and graph $\mathcal{G}(C) = (V, E)$ is data flow that represents dependency relation between variables in the source code.</p>
<p><img data-src="/notes/images/Data-flow.png" alt=""></p>
<h3 id="Model-3"><a href="#Model-3" class="headerlink" title="Model"></a>Model</h3><p><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “[IntelliCode compose: code generation using transformer.](https://arxiv.org/pdf/2005.08025.pdf)” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).
">[4]</span></a></sup> concats the comment, source code, and variables as the sequence input. It uses a graph-guided masked attention that represents the relation between source code tokens and nodes of the data flow. Given $\langle v_i, c_i \rangle / \langle c_j, v_i \rangle \in E’$, if the variable $v_i$ is identified from the source code token $c_j$, it allows the node and code attend to each other if and only if $\langle v_i, c_i \rangle / \langle c_j, v_i \rangle \in E’ $ . </p>
<p>The graph-guided masked attention matrix $M$ is as follows:</p>
<script type="math/tex; mode=display">
M_{ij} = \left\{
                \begin{array}{ll}
                  -\infty & \textrm{only mask the data-flow variable matrix } \\
                  0 & \textrm{otherwise}
                \end{array}
    \right.</script><p><img data-src="/notes/images/GraphCodeBERT.png" alt=""></p>
<h3 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h3><ul>
<li><strong>Edge prediction</strong>: randomly mask 20% nodes by adding infinite values in the mask, then predict these masked edges. The probability of the edge is the dot-product following a sigmoid function using representations of two nodes.</li>
<li><strong>Node alignment</strong>: randomly sample 20% nodes, mask edges between code tokens and sampled nodes, then predict masked edges.</li>
</ul>
<p><img data-src="/notes/images/GraphCodeBERT-node-alignment.png" alt=""></p>
<h3 id="Results-2"><a href="#Results-2" class="headerlink" title="Results"></a>Results</h3><p>The table reports the Mean Reciprocal Rank (MRR) on the CodeSearchNet.<br><img data-src="/notes/images/GraphCodeBERT-results-on-CodeSearchNet.png" alt=""></p>
<ul>
<li>Case study<br>After a small change, GraphCodeBERT w/ data flow can also makes the correct prediction while that w/o data flow can not.<br><img data-src="/notes/images/GraphCodeBERT-case.png" alt=""></li>
</ul>
<h2 id="TransCoder"><a href="#TransCoder" class="headerlink" title="TransCoder"></a>TransCoder</h2><p>TransCoder<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> uses a transformer encoder-decoder model to perform monolingual PL translation, in which the encoder is initialized as XLM, and the decoder is randomly initialized.</p>
<p><sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> instantiates the pre-training with following settings of unsupervised transcompilation:</p>
<ol>
<li><p><strong>Cross PL model pre-training</strong>. Thr cross-lingual nature comes from the significant number of common tokens (anchor points) that exist across languages. In the context of English-French translation, the anchor points consists essentially of digits and city and people names. In PL, these anchor points come form common keyworks (<em>e.g.</em>, for , while, if, try), and also digits, mathematical operators, and English strings that appear in the source code. <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> treats the PL. </p>
</li>
<li><p><strong>Cross PL model pre-training</strong>. Thr cross-lingual nature comes from the significant number of common tokens (anchor points) that exist across languages. In the context of English-French translation, the anchor points consists essentially of digits and city and people names. In PL, these anchor points come form common keyworks (<em>e.g.</em>, for , while, if, try), and also digits, mathematical operators, and English strings that appear in the source code. <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> applies the masked language modeling (MLM) pre-training on source code sequences.</p>
</li>
<li>Denoising auto-encoding (DAE). <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> predict a sequence of code tokens given a corrupted version of that sequence, that is, randomly mask, remove and shuffle input tokens.</li>
<li>Back-translation (BT). The translation quality will tend to be low if the model is never trained to do what is expected to do at test time, <em>i.e.,</em> to translate functions from one language to another. <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> applies back-translation, one of the most effective methods to leverage monolingual data in a wearkly-supervised scenario.</li>
</ol>
<p><img data-src="/notes/images/TransCoder.png" alt=""></p>
<h3 id="Data-3"><a href="#Data-3" class="headerlink" title="Data"></a>Data</h3><ul>
<li><a href="https://console.cloud.google.com/marketplace/details/github/github-repos">GitHub public dataset on Google BigQuery</a> contains more than 2.8 million open source GitHub repositories.</li>
<li>Tokenization: <em><a href="https://github.com/c2nes/javalang">javalang</a></em> tokenizer for java, <a href="https://docs.python.org/3/library/tokenize.html">tokenizer of the standard library</a> for Python, <em><a href="https://docs.python.org/3/library/tokenize.html">clang</a></em> for C++. These tokenizers ensure that meaningless modeifications (<em>e.g.</em>, add extra new lines or spaces) in the code do not have any impact on the tokenized sequences. The <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “[Unsupervised Translation of Programming Languages.](https://arxiv.org/pdf/2006.03511.pdf)” NeurIPS (2020).
">[11]</span></a></sup> learns BPE codes using <a href="https://github.com/glample/fastBPE">FastBPE</a> on extracted tokens, and split tokens into subword units.</li>
<li>TransCoder train the DAE and BT objectives on <strong>functions only</strong>. <strong>Keeping comments in the source code increases the number of achorpoints</strong> across language, which results in a better overall performance.<h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3></li>
<li>Evaluation:<br>(1) BLEU.<br>(2) Reference match: the percentage of translations that perfectly match the ground truth reference.<br>(3) Computational accuracy.: whether the hypothesis function generates the same outputs as the reference when given the same inputs.</li>
<li>Decoding: beam search</li>
</ul>
<h2 id="DOBF"><a href="#DOBF" class="headerlink" title="DOBF"></a>DOBF</h2><ul>
<li>Background: Previous PL pre-training uses masked language model objectives, which was initially designed for NL and does not leverage the particular structure of source code. <strong>PL is more structured than NL</strong>, which makes predicting masked tokens much easier for PLs.</li>
</ul>
<p><strong>Deobfuscation (DOBF)</strong> <sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rozière, Baptiste, Marie-Anne Lachaux, Marc Szafraniec and Guillaume Lample. “[DOBF: A Deobfuscation Pre-Training Objective for Programming Languages.](https://proceedings.neurips.cc/paper/2021/file/7d6548bdc0082aacc950ed35e91fcccb-Paper.pdf)” NeurIPS (2021).
">[10]</span></a></sup> proposes a new objective based on the <strong>deobfuscation of identifier names</strong> in source code. It leverages the particular structure of PLs. Although it does not require any parallel copora of source code aligned to NL, DOBF outperform GraphCodeBERT, CodeBERT and MLM pre-training on multiple downstream tasks.</p>
<h3 id="Deobfuscation-objective"><a href="#Deobfuscation-objective" class="headerlink" title="Deobfuscation objective"></a>Deobfuscation objective</h3><p>DOBF obfuscates code snippets by replacing class, function and variable names with special tokens, and train a model to recover the original names. When an identifier is selected, all of tis instances in the code are replaced by the same special token. This differs from MLM when the name of a variable can appear multiple times while being masked a single time. As a result, the feaction of meaningful tokens masked by the objective is language independent: for more verbose languages (<em>e.g.,</em> Java), the less informative syntax-related tokens will not be masked out by the DOBF objective.</p>
<p>Each identifier is replaced with probability <script type="math/tex">p_\textrm{obf} \in [0, 1]</script>. We ensure that the original input is modefied: if no identifier is replaced, we draw a random one to obfuscate. When <script type="math/tex">p_\textrm{obf} = 0</script>, only one random identifier in the input is obfuscated. When <script type="math/tex">p_\textrm{obf}=1</script>, all the identifiers defined in the file will be obfuscated. The model needs to recover a dictionary mapping special tokens to their initial values.</p>
<p><img data-src="/notes/images/DOBF.png" alt=""></p>
<h4 id="Pre-training-1"><a href="#Pre-training-1" class="headerlink" title="Pre-training"></a>Pre-training</h4><ul>
<li>Pre-training data: Python/Java files within GitHub public repos avilable on Google BigQuery.</li>
<li>Model: Encoder-decoder.</li>
<li>Tokenizer: BBPE (same as CodeBERT).</li>
</ul>
<h3 id="CodeXGLUE-results"><a href="#CodeXGLUE-results" class="headerlink" title="CodeXGLUE results"></a>CodeXGLUE results</h3><ol>
<li>DOBF beats COdeBERT by a wide margin on NL code search and code summarization, showing that PL data aligned with NL is unnecessary to train an effective model on those tasks.</li>
<li>Objectives such as MLM and DAE that provide unstructured noise are complementary to DOBF.<br><img data-src="/notes/images/DOBF-results.png" alt=""></li>
</ol>
<h2 id="PLBART"><a href="#PLBART" class="headerlink" title="PLBART"></a>PLBART</h2><p>PLBART (Program and Language BART)<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ahmad, Wasi Uddin, Saikat Chakraborty, Baishakhi Ray and Kai-Wei Chang. “[Unified Pre-training for Program Understanding and Generation.](https://aclanthology.org/2021.naacl-main.211.pdf)” NAACL (2021).
">[7]</span></a></sup> is a bidirectional and autoregressive transformer pre-trained on unlabeled data across PL and NL to learn multilingual representations applicable to a broad spectrum of program and language understanding and generation applications.</p>
<h3 id="Data-4"><a href="#Data-4" class="headerlink" title="Data"></a>Data</h3><ul>
<li>Data: Java and Python repo on Google BigQuery; StackOverflow posts (including both questions and answers, excluing code snipeets) by downloading the data dump (7th Sep 2020) from stackexchange.</li>
<li>Tokenizer: sentencepiece.</li>
<li>Vocabulary: (newly trained) #50k subwords.</li>
</ul>
<p>One key challenge to aggregate data from differnt modalities is that some modalities may have more data, such as we have 14 times more data in PL than NL. Thus, it mixes and up/down samples the data following XLM-R<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Conneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. “[Unsupervised Cross-lingual Representation Learning at Scale.](https://aclanthology.org/2020.acl-main.747.pdf)” ACL (2020).
">[9]</span></a></sup> to alleviate the bias towards PL. It samples instances for pre-training according to multinomial distribution with probabilities ($q_1, q_2, \cdots, q_N$):</p>
<script type="math/tex; mode=display">
q_i = \frac{1}{p_i} \cdot \frac{p_i^\alpha}{\sum_{j=1}^N p_j^\alpha}</script><p>where <script type="math/tex">p_i = \frac{n_i}{\sum_{j=1}^N n_j}</script>. $N$ is the total number of languages and $n_i$ is the total number of instances in language $i$. The smoothing parameter $\alpha=0.3$.</p>
<h3 id="Denoising-pre-training"><a href="#Denoising-pre-training" class="headerlink" title="Denoising pre-training"></a>Denoising pre-training</h3><ul>
<li>Config: BART-base (L6 encoder, L6 decoder, H768, A12) ~140M params.</li>
<li>Pre-training tasks: mask 35% of the tokens in each instance.<ol>
<li>token masking.</li>
<li>token deletion.</li>
<li>token infilling: sample text spans and replace them with a single mask token.</li>
</ol>
</li>
<li>Input/output format: A language id symbol (e.g., &lt;java&gt;, &lt;python&gt;) is appended / prepended to the encoder/decoder inputs, respectively.<br><img data-src="/notes/images/PLBART.png" alt=""></li>
</ul>
<h3 id="Results-3"><a href="#Results-3" class="headerlink" title="Results"></a>Results</h3><ul>
<li>Evaluation metrics: <ol>
<li>BLEU for generation, except smoothed BLEU for code summarization;</li>
<li>CodeBLEU: considers grammatical and logical correctness based on the AST and data-flow structure.</li>
<li>Exact match (EM): evaluates if generated sequence exactly matches the reference.</li>
</ol>
</li>
</ul>
<p>It shows that PLBART learns better generic program semantics. It achieves the highest improvement in Ruby, however, PLBART is not pre-trained on Ruby.<br><img data-src="/notes/images/PLBART-results.png" alt=""></p>
<h2 id="CodeT5"><a href="#CodeT5" class="headerlink" title="CodeT5"></a>CodeT5</h2><ul>
<li>Background: Previous work reply on the encoder- or decoder- only models, <em>i.e.</em>, BERT or GPT, which is suboptimal for generation and understanding tasks, respectively. Initializing the encoder with CoderBERT and decoder with random initialization cannot benefit from pre-trianing. Also, most works regard the PL as a sequence of tokens like NL, ignoring the rich structural information in the code, which is vital for comprehending the code sementics.</li>
</ul>
<p>CodeT5<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Yue, Weishi Wang, Shafiq R. Joty and Steven C. H. Hoi. “[CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.](https://aclanthology.org/2021.emnlp-main.685.pdf)” EMNLP (2021).
">[8]</span></a></sup> is a unified encoder-decoder model, which considers the token type information in the source code. It proposes an identifier-aware pre-training objective.</p>
<p><img data-src="/notes/images/CodeT5.png" alt=""></p>
<h3 id="Data-5"><a href="#Data-5" class="headerlink" title="Data"></a>Data</h3><ul>
<li>Data: CodeSearchNet, collected C/C# from BigQuery. ~8.35M instances for pre-training (8 PLs).</li>
<li>Tokenizer: (newly trained) BBPE. It largely reduces the length of tokenized code sequence by 30%-45% on downstream tasks.</li>
<li>Vocabulary: #32k, plus [PAD], [CLS], [SEP], [MASK0-99].</li>
</ul>
<p><img data-src="/notes/images/CodeT5-data.png" alt=""></p>
<h3 id="Model-4"><a href="#Model-4" class="headerlink" title="Model"></a>Model</h3><ul>
<li>Config: CodeT5-small (60M); CodeT5-base (220M).</li>
</ul>
<h3 id="Pre-training-2"><a href="#Pre-training-2" class="headerlink" title="Pre-training"></a>Pre-training</h3><ul>
<li>Encoding NL/PL: CodeT5 converts the PL segment into an Abstact Syntac Tree (AST) and extract the node types for each code token. Then, it constructs a sequence of binary labels <script type="math/tex">\mathbf{y} \in \{0,1\}^m</script> for the PL segment, were each <script type="math/tex">y_i \in \{0,1\}</script> represents whether the code token is an identifier or not.</li>
</ul>
<ol>
<li>Masked span prediction: the same corrupted rate (15%) as T5 and average span length to be 3 by uniformly sampling spans from 1 to 5 tokens. It also employ whole word masking as in ERNIE.</li>
<li><strong>Identifier tagging</strong>: use the CodeT5 encoder to predict whether the token is an identifier or not (binary classification). </li>
<li><strong>Masked identifier prediction</strong>: mask all identifiers in the PL and use a unique sentinel token for all occurrences of one specific identifier. It is called <strong> <span class="label danger">obfuscation</span> </strong> where changing identifier names does not impact the code semantics. See the (c) subfigure below.</li>
<li>Bimodal dual generation. Train NL $\rightarrow$ PL and PL $\rightarrow$ NL generation simultaneously, which can be seen as a special case of T5’s (full) span masking.<br><img data-src="/notes/images/CodeT5-task.png" alt=""></li>
</ol>
<h3 id="Results-4"><a href="#Results-4" class="headerlink" title="Results"></a>Results</h3><p>In code summarization tasks, CodeT5 outperforms previous SOTA with smaller model parameters (50M vs 140M).<br><img data-src="/notes/images/CodeT5-code-summ.png" alt=""></p>
<p><strong>Ablation test</strong>:</p>
<ol>
<li>Masked span prediction (MSP)</li>
<li>Identifier tagging (IT)</li>
<li>Masked identifier prediction (MIP)</li>
</ol>
<p>It is observed that removing MSP can largely reduce the generation task performance but instead increase the defect detection performance, indicating that MSP can capture syntactic information for generation tasks.</p>
<p>Removing MIP would hurt the defect detection task the most, indicating that it might focus more on code semantic understanding.<br><img data-src="/notes/images/Ablation-test.png" alt=""></p>
<h2 id="UniXCoder"><a href="#UniXCoder" class="headerlink" title="UniXCoder"></a>UniXCoder</h2><ul>
<li>Background: The cons of previous work:<br>(1) Encoder-only models is inferior to generation tasks, which requires an additional decoder for generation.<br>(2) Decoder-only models underperform in understanding tasks.<br>(3) Encoder-decoder models (PLBART, CodeT5) are sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference.</li>
</ul>
<p>UniXCoder<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Guo, Daya, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou and Jian Yin. “[UniXcoder: Unified Cross-Modal Pre-training for Code Representation.](https://www.aclanthology.org/2022.acl-long.499.pdf)” ACL (2022).
">[12]</span></a></sup> uses a UniLM structure for code pre-training. It uses three objectives:</p>
<ol>
<li>MLM</li>
<li>Unidirectional LM</li>
<li>Denoising objective (similar to T5 for enc-dec mode): first split the input sequence into  <script type="math/tex">\max(\lfloor \rfloor, 1)</script> chunks and then randmly mask a span of from 1 to $2l-1$ tokens for each chunk, $n$ is the length of the input, $r=15%$ is the corruption rate and $l=5$ is the average length of masked spans.</li>
</ol>
<p><img data-src="/notes/images/UniXCoder.png" alt=""></p>
<h2 id="InCoder"><a href="#InCoder" class="headerlink" title="InCoder"></a>InCoder</h2><ul>
<li>Background: Code is seldom written in a single left-to-right pass and is instead repeatly edited and refined.</li>
</ul>
<p>InCoder<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fried, Daniel, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer and Mike Lewis. “[InCoder: A Generative Model for Code Infilling and Synthesis.](https://arxiv.org/pdf/2204.05999.pdf)” ArXiv abs/2204.05999 (2022).
">[13]</span></a></sup>, a unified generative model that can perform <strong>program synthesis</strong> (via left-to-right generation) as well as <strong>editing</strong> (via infilling), is the first large generative code model (6.7B) that is able to infill arbitrary regions.</p>
<p>It learns to infill by randomly replacing spans of code with a sentinel token and moving them to the end of the sequence. The model is trained to predict all tokens in the complete sequence in this permuted ordering. During inference, it can edit code by replacing spans with sentinel tokens, prompting the model with the new sequence, and having it generate new tokens to replace the masked spans.</p>
<p><img data-src="/notes/images/InCoder.png" alt=""></p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>It samples the number of spans from a Poisson distribution with a mean of one, truncated to the support [1, 256], so that there are typically a small number of spans. The length of each span is sampled unifromly from the length of the document and the set of sampled spans is rejected and resampled if any spans overlap.</p>
<p>Once spans are sampled, each span $k$ is replaced with a special masked sentinel token &lt;MASK:k&gt;. The sequence of tokens in the span is then moved to the end of document. Let “Left” be the left context, and “Right” be the right context, “Span” be the sampled span between left and right contexts, then it maximizes the log probability of the masked document: log P([Left; &lt;MASK:0&gt; Right; &lt;MASK:0&gt; Span; &lt;EOM&gt;]).</p>
<p><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fried, Daniel, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer and Mike Lewis. “[InCoder: A Generative Model for Code Infilling and Synthesis.](https://arxiv.org/pdf/2204.05999.pdf)” ArXiv abs/2204.05999 (2022).
">[13]</span></a></sup> computes the probability of the sequence auto-regressively and train the model using <strong>cross-entropy loss on all tokens except the mask sentinel tokens &lt;MASK:k&gt;</strong>, so that the model does not generate these tokens during inference.</p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>During inference, <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fried, Daniel, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer and Mike Lewis. “[InCoder: A Generative Model for Code Infilling and Synthesis.](https://arxiv.org/pdf/2204.05999.pdf)” ArXiv abs/2204.05999 (2022).
">[13]</span></a></sup> samples the target spans autoregressively from the distribution: P([Left; &lt;MASK:0&gt; Right; &lt;MASK:0&gt; Span; &lt;EOM&gt;]).</p>
<h3 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h3><p>It uses (1) public code with permissive, non-copyleft, open-source licenses and (2) StackOverflow questions, answers, and comments.</p>
<p><img data-src="/notes/images/InCoder-data.png" alt=""></p>
<h4 id="Code-data"><a href="#Code-data" class="headerlink" title="Code data"></a>Code data</h4><p><strong>Sources</strong>:<br>(1) Code files and repo metadata from GitHub and GitLab via public APIs. ~670M public non-fork repos, including all code from a list of 28 PLs (determined by file extention).<br>(2) include all other Python and Jupyter files obtainable through the GitHub archive on BigQUery that cannot already obtain from GitHub directly.<br>(3) All text and code (with markdown formatiing removed from text cells) in Jupyter notebooks.</p>
<p><strong>Deduplication</strong>:<br>(1) Remove code files using exact match on the sequence of alphanumereic tokens in the file.<br>(2) Use regular expressions to replace email address with dummy address “remove@example.com”</p>
<p><strong>Decontamination</strong>:</p>
<ul>
<li>Remove overlap between training data and the evaluation set. Remove any repos contained in the validation and test set of CodeSearchNet.</li>
</ul>
<p><strong>Filtering</strong>:<br>Remove that contain </p>
<ul>
<li>any line longer than 3000 tokens</li>
<li>an average line length greater than 100 tokens</li>
<li>less than 40% of their chars being alphanumetric or underscores</li>
<li>appear to be automatically generated, using substring match.</li>
</ul>
<h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><p>It trains a new BBPE, allowing tokens to extend across whitespace (excluding newline characters) so that common code idioms (e.g., <em>import numpy as np</em>) are single tokens in the vocabulary. It reduces the total number of tokens required to encode the training corpus by 45% relative to the BBPE tokenizer and vocabulary of GPT-2.</p>
<h3 id="Results-5"><a href="#Results-5" class="headerlink" title="Results"></a>Results</h3><p>The table compares the generative code models on the HumanEval and MBPP becnmarks, which requires models to condition on NL descriptions (docstrings) to produce Python programs (typically a single function), and evaluates overall functional accuracy (pass rates).</p>
<p>InCoder achieves comparable performance on the HumanEval metrics to CodeGen-Multi<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Nijkamp, Erik, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haiquan Wang, Yingbo Zhou, Silvio Savarese and Caiming Xiong. “[A Conversational Paradigm for Program Synthesis.](https://arxiv.org/pdf/2203.13474.pdf)” ArXiv abs/2203.13474 (2022).
">[14]</span></a></sup>.<br><img data-src="/notes/images/InCoder-results.png" alt=""></p>
<h2 id="Codex"><a href="#Codex" class="headerlink" title="Codex"></a>Codex</h2><p>Codex<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Mark et al. “[Evaluating Large Language Models Trained on Code.](https://arxiv.org/pdf/2107.03374.pdf)” ArXiv abs/2107.03374 (2021).
">[15]</span></a></sup>, a finetuned variant of GPT-3 created by OpenAI, has powered the <a href="https://copilot.github.com/">GitHub Copilot</a> and exceled at a variety of codeing tasks.</p>
<h3 id="Pre-training-Setup"><a href="#Pre-training-Setup" class="headerlink" title="Pre-training Setup"></a>Pre-training Setup</h3><ul>
<li>Model: 175B GPT-3 (Transformer decoder).</li>
</ul>
<h3 id="Data-6"><a href="#Data-6" class="headerlink" title="Data"></a>Data</h3><p>Collect 179GB unique Python files under 1MB.<br>Filter out files:</p>
<ul>
<li>which were likely auto-generated</li>
<li>had average line length greater than 100</li>
<li>had maximum line length geater than 1000</li>
<li>contained small percentage of alphanumeric chacters.</li>
</ul>
<p>After filtering, it has 159GB.</p>
<p><strong>Tokenizer</strong>: GPT-3 tokenizer plus additional set of tokens for whitespace runs of different lengths (<strong>multi-whitespace tokens</strong>), allowing to reducing approximately 30% fewer tokens.</p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><strong>Pass@k</strong> metric: First generate n ≥ k samples per task, count the number of correct samples c ≤ n which pass unit tests, and calculate the unbiased estimator.</p>
<script type="math/tex; mode=display">
\textrm{pass@k} := \mathbb{E}_\textrm{problems} \bigg[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \bigg]</script><p>The numpy script for the unbiased estimate of pass@k.</p>
<p><img data-src="/notes/images/pass-at-k-code.png" width="60%"></p>
<p>Results</p>
<p><img data-src="/notes/images/Codex-pass-rate-vs-model-size.png" width="60%"></p>
<h2 id="AlphaCode"><a href="#AlphaCode" class="headerlink" title="AlphaCode"></a>AlphaCode</h2><p>AlphaCode<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Li, Yujia et al. “[Competition-Level Code Generation with AlphaCode.](https://arxiv.org/pdf/2203.07814.pdf)” ArXiv abs/2203.07814 (2022): n. pag.
">[16]</span></a></sup> is an encoder-decoder transformer model developed by DeepMind, achieving on average top 54.3% with more than 5,000 human participants on Codeforces.</p>
<h3 id="Pre-training-Setup-1"><a href="#Pre-training-Setup-1" class="headerlink" title="Pre-training Setup"></a>Pre-training Setup</h3><ul>
<li>Tokenizer: Sentencepiece</li>
<li>Vocabulary size: 8k, trained on a mix of GitHub and CodeContests data.]</li>
<li>Data: GitHub repos including several popular languages. It follows Codex to filter out all files larger than 1MB or with lines longer than 1000 characters, to exclude automatically generated code. It also remove duplicates of the same file, ignoring whitespace in comparisons. It has 715.1GB code intotal.</li>
</ul>
<p><img data-src="/notes/images/AlphaCode-data.png" width="60%"><br><img data-src="/notes/images/AlphaCode.png" alt=""></p>
<h2 id="PolyCoder"><a href="#PolyCoder" class="headerlink" title="PolyCoder"></a>PolyCoder</h2><p>PolyCoder<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Xu, Frank F., Uri Alon, Graham Neubig and Vincent J. Hellendoorn. “[A Systematic Evaluation of Large Language Models of Code.](https://arxiv.org/pdf/2202.13169.pdf)” DL4C @ ICLR 2022 (2022).
">[17]</span></a></sup> is a 2.7B code language model trained on 12 different PLs, achieving the new SOTA in C langauge.</p>
<ul>
<li>Model: GPT-2.</li>
<li>Tokenizer: BBPE.</li>
<li>Data: at least 50 stars of 12 PLs from GitHub (stopping at 15k per language).</li>
</ul>
<p><img data-src="/notes/images/Preprocessing-comp.png" alt=""></p>
<p><img data-src="/notes/images/PolyCoder-data.png" alt=""></p>
<h2 id="CodeGen"><a href="#CodeGen" class="headerlink" title="CodeGen"></a>CodeGen</h2><p>CodeGen<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Nijkamp, Erik, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haiquan Wang, Yingbo Zhou, Silvio Savarese and Caiming Xiong. “[A Conversational Paradigm for Program Synthesis.](https://arxiv.org/pdf/2203.13474.pdf)” ArXiv abs/2203.13474 (2022).
">[14]</span></a></sup> is a 16.1B causal language model pre-trained on code created by Salesforce, outperforming OpenAI Codex on HumanEval.</p>
<h2 id="PaLM-Coder"><a href="#PaLM-Coder" class="headerlink" title="PaLM-Coder"></a>PaLM-Coder</h2><p>PaLM-Coder<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chowdhery, Aakanksha et al. “[PaLM: Scaling Language Modeling with Pathways.](https://arxiv.org/pdf/2204.02311.pdf)” ArXiv abs/2204.02311 (2022).
">[18]</span></a></sup> is a fine-tuned 540B PaLM with decoder-only setup, training on GitHub repositories.</p>
<h2 id="StarCoder"><a href="#StarCoder" class="headerlink" title="StarCoder"></a>StarCoder</h2><p>The BigCode community proposes StarCoder<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Li, R., Allal, L.B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T.Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M., Umapathi, L.K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J., Patel, S.S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C.J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C.M., Hughes, S.M., Wolf, T., Guha, A., Werra, L.V., & Vries, H.D. (2023). [StarCoder: may the source be with you!](https://arxiv.org/pdf/2305.06161.pdf) ArXiv, abs/2305.06161.">[19]</span></a></sup>, a 15.5B causal LLM with 8k context length, which was trained towards Fill-in-the-Middle (FIM) objective on 1T tokens of 86 programming languages from The Stack, an open-source code corpora from GitHub. It uses multi-query attention (for faster inference) and learned absolute positional embeddings. StarCoder finetuned on Python outperforms OpenAI code-cushman-001 on HumanEval.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kanade, Aditya, Petros Maniatis, Gogul Balakrishnan and Kensen Shi. “<a href="http://proceedings.mlr.press/v119/kanade20a/kanade20a.pdf">Learning and Evaluating Contextual Embedding of Source Code.</a>” ICML (2020).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Feng, Zhangyin, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang and Ming Zhou. “<a href="https://aclanthology.org/2020.findings-emnlp.139.pdf">CodeBERT: A Pre-Trained Model for Programming and Natural Languages.</a>” Findings of EMNLP (2020).<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Husain, Hamel, Hongqi Wu, Tiferet Gazit, Miltiadis Allamanis and Marc Brockschmidt. “<a href="https://arxiv.org/pdf/1909.09436.pdf">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search.</a>” <em>ArXiv</em> abs/1909.09436 (2019).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Svyatkovskiy, Alexey, Shao Kun Deng, Shengyu Fu and Neel Sundaresan. “<a href="https://arxiv.org/pdf/2005.08025.pdf">IntelliCode compose: code generation using transformer.</a>” Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2020).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu and Shujie Liu. “<a href="https://arxiv.org/pdf/2102.04664.pdf">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.</a>” ArXiv abs/2102.04664 (2021).<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Guo, Daya, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang and M. Zhou. “<a href="https://arxiv.org/pdf/2009.08366">GraphCodeBERT: Pre-training Code Representations with Data Flow.</a>” ICLR (2021).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ahmad, Wasi Uddin, Saikat Chakraborty, Baishakhi Ray and Kai-Wei Chang. “<a href="https://aclanthology.org/2021.naacl-main.211.pdf">Unified Pre-training for Program Understanding and Generation.</a>” NAACL (2021).<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, Yue, Weishi Wang, Shafiq R. Joty and Steven C. H. Hoi. “<a href="https://aclanthology.org/2021.emnlp-main.685.pdf">CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.</a>” EMNLP (2021).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Conneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. “<a href="https://aclanthology.org/2020.acl-main.747.pdf">Unsupervised Cross-lingual Representation Learning at Scale.</a>” ACL (2020).<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rozière, Baptiste, Marie-Anne Lachaux, Marc Szafraniec and Guillaume Lample. “<a href="https://proceedings.neurips.cc/paper/2021/file/7d6548bdc0082aacc950ed35e91fcccb-Paper.pdf">DOBF: A Deobfuscation Pre-Training Objective for Programming Languages.</a>” NeurIPS (2021).<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lachaux, Marie-Anne, Baptiste Rozière, Lowik Chanussot and Guillaume Lample. “<a href="https://arxiv.org/pdf/2006.03511.pdf">Unsupervised Translation of Programming Languages.</a>” NeurIPS (2020).<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Guo, Daya, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou and Jian Yin. “<a href="https://www.aclanthology.org/2022.acl-long.499.pdf">UniXcoder: Unified Cross-Modal Pre-training for Code Representation.</a>” ACL (2022).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Fried, Daniel, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer and Mike Lewis. “<a href="https://arxiv.org/pdf/2204.05999.pdf">InCoder: A Generative Model for Code Infilling and Synthesis.</a>” ArXiv abs/2204.05999 (2022).<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Nijkamp, Erik, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haiquan Wang, Yingbo Zhou, Silvio Savarese and Caiming Xiong. “<a href="https://arxiv.org/pdf/2203.13474.pdf">A Conversational Paradigm for Program Synthesis.</a>” ArXiv abs/2203.13474 (2022).<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, Mark et al. “<a href="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code.</a>” ArXiv abs/2107.03374 (2021).<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Li, Yujia et al. “<a href="https://arxiv.org/pdf/2203.07814.pdf">Competition-Level Code Generation with AlphaCode.</a>” ArXiv abs/2203.07814 (2022): n. pag.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Xu, Frank F., Uri Alon, Graham Neubig and Vincent J. Hellendoorn. “<a href="https://arxiv.org/pdf/2202.13169.pdf">A Systematic Evaluation of Large Language Models of Code.</a>” DL4C @ ICLR 2022 (2022).<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chowdhery, Aakanksha et al. “<a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Scaling Language Modeling with Pathways.</a>” ArXiv abs/2204.02311 (2022).<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Li, R., Allal, L.B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T.Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M., Umapathi, L.K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J., Patel, S.S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C.J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C.M., Hughes, S.M., Wolf, T., Guha, A., Werra, L.V., &amp; Vries, H.D. (2023). <a href="https://arxiv.org/pdf/2305.06161.pdf">StarCoder: may the source be with you!</a> ArXiv, abs/2305.06161.<a href="#fnref:19" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
  </entry>
  <entry>
    <title>Mask Denoising Strategy for Pre-trained Language Models</title>
    <url>/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Mask modeling is a crucial role in pre-training language models. This note provides a short summary.</p>
<span id="more"></span>
<h2 id="BERT-RoBERTa-Mask"><a href="#BERT-RoBERTa-Mask" class="headerlink" title="BERT/RoBERTa Mask"></a>BERT/RoBERTa Mask</h2><p>BERT<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: Google BERT](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/create_pretraining_data.py#L342)
">[1]</span></a></sup> applies <strong>masked language modeling (MLM)</strong> on the sequence of text segments. Specifically, BERT uses a uniform masking rate of 15% after <strong>WordPiece tokenization</strong>, where it replace the masked tokens with<br>1) <strong>[MASK]</strong> 80% of time time,<br>2) with a random word 10% of the time, and<br>3) 10% unchanged, to bias the representation towards the actual observed word.</p>
<p>The random replacement only occurs for 15% of all tokens (<em>i.e.</em>, 10% of 15%), this does not seem to harm the model’s language understanding capacity.</p>
<div class="note info">
            <p><strong>BERT</strong> applies <strong>static masking</strong> for multiple runs ahead of time and keeps unchanged afterwards; while <strong>RoBERTa</strong> adopts <strong>dynamic masking</strong> in an on-the-fly manner during training.</p>
          </div>
<h3 id="Google-BERT-Implementation"><a href="#Google-BERT-Implementation" class="headerlink" title="Google BERT Implementation"></a>Google BERT Implementation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. Google BERT implementation. (w/ wwm)</span></span><br><span class="line">MaskedLmInstance = collections.namedtuple(<span class="string">&quot;MaskedLmInstance&quot;</span>,</span><br><span class="line">                                          [<span class="string">&quot;index&quot;</span>, <span class="string">&quot;label&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span>(<span class="params">tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 max_predictions_per_seq, vocab_words, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates the predictions for the masked LM objective.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&quot;[CLS]&quot;</span> <span class="keyword">or</span> token == <span class="string">&quot;[SEP]&quot;</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> <span class="built_in">len</span>(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">&quot;##&quot;</span>)):</span><br><span class="line">      cand_indexes[-<span class="number">1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line"></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  output_tokens = <span class="built_in">list</span>(tokens)</span><br><span class="line"></span><br><span class="line">  num_to_predict = <span class="built_in">min</span>(max_predictions_per_seq,</span><br><span class="line">                       <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">int</span>(<span class="built_in">round</span>(<span class="built_in">len</span>(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = <span class="built_in">set</span>()</span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) + <span class="built_in">len</span>(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">&quot;[MASK]&quot;</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> <span class="built_in">len</span>(masked_lms) &lt;= num_to_predict</span><br><span class="line">  masked_lms = <span class="built_in">sorted</span>(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure>
<h3 id="Huggingface-Implementation"><a href="#Huggingface-Implementation" class="headerlink" title="Huggingface Implementation"></a>Huggingface Implementation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Huggingface implementation: https://github.com/huggingface/transformers/blob/d72343d2b804d0304d93bac1c1b58e0dafd5e820/src/transformers/data/data_collator.py#L606</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForLanguageModeling</span>(<span class="params">DataCollatorMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they</span></span><br><span class="line"><span class="string">    are not all of the same length.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):</span></span><br><span class="line"><span class="string">            The tokenizer used for encoding the data.</span></span><br><span class="line"><span class="string">        mlm (`bool`, *optional*, defaults to `True`):</span></span><br><span class="line"><span class="string">            Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs</span></span><br><span class="line"><span class="string">            with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked</span></span><br><span class="line"><span class="string">            tokens and the value to predict for the masked token.</span></span><br><span class="line"><span class="string">        mlm_probability (`float`, *optional*, defaults to 0.15):</span></span><br><span class="line"><span class="string">            The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.</span></span><br><span class="line"><span class="string">        pad_to_multiple_of (`int`, *optional*):</span></span><br><span class="line"><span class="string">            If set will pad the sequence to a multiple of the provided value.</span></span><br><span class="line"><span class="string">        return_tensors (`str`):</span></span><br><span class="line"><span class="string">            The type of Tensor to return. Allowable values are &quot;np&quot;, &quot;pt&quot; and &quot;tf&quot;.</span></span><br><span class="line"><span class="string">    &lt;Tip&gt;</span></span><br><span class="line"><span class="string">    For best performance, this data collator should be used with a dataset having items that are dictionaries or</span></span><br><span class="line"><span class="string">    BatchEncoding, with the `&quot;special_tokens_mask&quot;` key, as returned by a [`PreTrainedTokenizer`] or a</span></span><br><span class="line"><span class="string">    [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.</span></span><br><span class="line"><span class="string">    &lt;/Tip&gt;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: PreTrainedTokenizerBase</span><br><span class="line">    mlm: <span class="built_in">bool</span> = <span class="literal">True</span></span><br><span class="line">    mlm_probability: <span class="built_in">float</span> = <span class="number">0.15</span></span><br><span class="line">    pad_to_multiple_of: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span><br><span class="line">    tf_experimental_compile: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line">    return_tensors: <span class="built_in">str</span> = <span class="string">&quot;pt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__post_init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.mlm <span class="keyword">and</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. &quot;</span></span><br><span class="line">                <span class="string">&quot;You should pass `mlm=False` to train on causal language modeling instead.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">if</span> self.tf_experimental_compile:</span><br><span class="line">            <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">            self.tf_mask_tokens = tf.function(self.tf_mask_tokens, jit_compile=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_bernoulli</span>(<span class="params">shape, probability</span>):</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        prob_matrix = tf.fill(shape, probability)</span><br><span class="line">        <span class="keyword">return</span> tf.cast(prob_matrix - tf.random.uniform(shape, <span class="number">0</span>, <span class="number">1</span>) &gt;= <span class="number">0</span>, tf.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_mask_tokens</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, inputs: <span class="type">Any</span>, vocab_size, mask_token_id, special_tokens_mask: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        input_shape = tf.shape(inputs)</span><br><span class="line">        <span class="comment"># 1 for a special token, 0 for a normal token in the special tokens mask</span></span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)</span></span><br><span class="line">        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability) &amp; ~special_tokens_mask</span><br><span class="line">        <span class="comment"># Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens</span></span><br><span class="line">        labels = tf.where(masked_indices, inputs, -<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = self.tf_bernoulli(input_shape, <span class="number">0.8</span>) &amp; masked_indices</span><br><span class="line"></span><br><span class="line">        inputs = tf.where(indices_replaced, mask_token_id, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = self.tf_bernoulli(input_shape, <span class="number">0.1</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=tf.int64)</span><br><span class="line">        inputs = tf.where(indices_random, random_words, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Handle dict or lists with proper padding and conversion to tensor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            batch = self.tokenizer.pad(examples, return_tensors=<span class="string">&quot;tf&quot;</span>, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = &#123;</span><br><span class="line">                <span class="string">&quot;input_ids&quot;</span>: _tf_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If special token mask has been preprocessed, pop it from the dict.</span></span><br><span class="line">        special_tokens_mask = batch.pop(<span class="string">&quot;special_tokens_mask&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> self.mlm:</span><br><span class="line">            <span class="keyword">if</span> special_tokens_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                special_tokens_mask = [</span><br><span class="line">                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>)</span><br><span class="line">                    <span class="keyword">for</span> val <span class="keyword">in</span> batch[<span class="string">&quot;input_ids&quot;</span>].numpy().tolist()</span><br><span class="line">                ]</span><br><span class="line">                <span class="comment"># Cannot directly create as bool</span></span><br><span class="line">                special_tokens_mask = tf.cast(tf.convert_to_tensor(special_tokens_mask, dtype=tf.int64), tf.<span class="built_in">bool</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                special_tokens_mask = tf.cast(special_tokens_mask, tf.<span class="built_in">bool</span>)</span><br><span class="line">            batch[<span class="string">&quot;input_ids&quot;</span>], batch[<span class="string">&quot;labels&quot;</span>] = self.tf_mask_tokens(</span><br><span class="line">                tf.cast(batch[<span class="string">&quot;input_ids&quot;</span>], tf.int64),</span><br><span class="line">                special_tokens_mask=special_tokens_mask,</span><br><span class="line">                mask_token_id=self.tokenizer.mask_token_id,</span><br><span class="line">                vocab_size=<span class="built_in">len</span>(self.tokenizer),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = batch[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">            <span class="keyword">if</span> self.tokenizer.pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># Replace self.tokenizer.pad_token_id with -100</span></span><br><span class="line">                labels = tf.where(labels == self.tokenizer.pad_token_id, -<span class="number">100</span>, labels)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels = tf.identity(labels)  <span class="comment"># Makes a copy, just in case</span></span><br><span class="line">            batch[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="comment"># Handle dict or lists with proper padding and conversion to tensor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            batch = self.tokenizer.pad(examples, return_tensors=<span class="string">&quot;pt&quot;</span>, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = &#123;</span><br><span class="line">                <span class="string">&quot;input_ids&quot;</span>: _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If special token mask has been preprocessed, pop it from the dict.</span></span><br><span class="line">        special_tokens_mask = batch.pop(<span class="string">&quot;special_tokens_mask&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> self.mlm:</span><br><span class="line">            batch[<span class="string">&quot;input_ids&quot;</span>], batch[<span class="string">&quot;labels&quot;</span>] = self.torch_mask_tokens(</span><br><span class="line">                batch[<span class="string">&quot;input_ids&quot;</span>], special_tokens_mask=special_tokens_mask</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = batch[<span class="string">&quot;input_ids&quot;</span>].clone()</span><br><span class="line">            <span class="keyword">if</span> self.tokenizer.pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                labels[labels == self.tokenizer.pad_token_id] = -<span class="number">100</span></span><br><span class="line">            batch[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, special_tokens_mask: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">        labels = inputs.clone()</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)</span></span><br><span class="line">        probability_matrix = torch.full(labels.shape, self.mlm_probability)</span><br><span class="line">        <span class="keyword">if</span> special_tokens_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            special_tokens_mask = [</span><br><span class="line">                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">            ]</span><br><span class="line">            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            special_tokens_mask = special_tokens_mask.<span class="built_in">bool</span>()</span><br><span class="line"></span><br><span class="line">        probability_matrix.masked_fill_(special_tokens_mask, value=<span class="number">0.0</span>)</span><br><span class="line">        masked_indices = torch.bernoulli(probability_matrix).<span class="built_in">bool</span>()</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = torch.bernoulli(torch.full(labels.shape, <span class="number">0.8</span>)).<span class="built_in">bool</span>() &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = torch.bernoulli(torch.full(labels.shape, <span class="number">0.5</span>)).<span class="built_in">bool</span>() &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = torch.randint(<span class="built_in">len</span>(self.tokenizer), labels.shape, dtype=torch.long)</span><br><span class="line">        inputs[indices_random] = random_words[indices_random]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Handle dict or lists with proper padding and conversion to tensor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            batch = self.tokenizer.pad(examples, return_tensors=<span class="string">&quot;np&quot;</span>, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch = &#123;</span><br><span class="line">                <span class="string">&quot;input_ids&quot;</span>: _numpy_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If special token mask has been preprocessed, pop it from the dict.</span></span><br><span class="line">        special_tokens_mask = batch.pop(<span class="string">&quot;special_tokens_mask&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> self.mlm:</span><br><span class="line">            batch[<span class="string">&quot;input_ids&quot;</span>], batch[<span class="string">&quot;labels&quot;</span>] = self.numpy_mask_tokens(</span><br><span class="line">                batch[<span class="string">&quot;input_ids&quot;</span>], special_tokens_mask=special_tokens_mask</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = np.copy(batch[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">            <span class="keyword">if</span> self.tokenizer.pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                labels[labels == self.tokenizer.pad_token_id] = -<span class="number">100</span></span><br><span class="line">            batch[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, special_tokens_mask: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        labels = np.copy(inputs)</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)</span></span><br><span class="line">        probability_matrix = np.full(labels.shape, self.mlm_probability)</span><br><span class="line">        <span class="keyword">if</span> special_tokens_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            special_tokens_mask = [</span><br><span class="line">                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">            ]</span><br><span class="line">            special_tokens_mask = np.array(special_tokens_mask, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            special_tokens_mask = special_tokens_mask.astype(np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">        probability_matrix[special_tokens_mask] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># Numpy doesn&#x27;t have bernoulli, so we use a binomial with 1 trial</span></span><br><span class="line">        masked_indices = np.random.binomial(<span class="number">1</span>, probability_matrix, size=probability_matrix.shape).astype(np.<span class="built_in">bool</span>)</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = np.random.binomial(<span class="number">1</span>, <span class="number">0.8</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.mask_token_id</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        <span class="comment"># indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced</span></span><br><span class="line">        indices_random = (</span><br><span class="line">            np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        )</span><br><span class="line">        random_words = np.random.randint(</span><br><span class="line">            low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64</span><br><span class="line">        )</span><br><span class="line">        inputs[indices_random] = random_words</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># w/ wwm</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForWholeWordMask</span>(<span class="params">DataCollatorForLanguageModeling</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Data collator used for language modeling that masks entire words.</span></span><br><span class="line"><span class="string">    - collates batches of tensors, honoring their tokenizer&#x27;s pad_token</span></span><br><span class="line"><span class="string">    - preprocesses batches for masked language modeling</span></span><br><span class="line"><span class="string">    &lt;Tip&gt;</span></span><br><span class="line"><span class="string">    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically</span></span><br><span class="line"><span class="string">    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will</span></span><br><span class="line"><span class="string">    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].</span></span><br><span class="line"><span class="string">    &lt;/Tip&gt;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            input_ids = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_ids = examples</span><br><span class="line">            examples = [&#123;<span class="string">&quot;input_ids&quot;</span>: e&#125; <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line"></span><br><span class="line">        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line"></span><br><span class="line">        mask_labels = []</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> examples:</span><br><span class="line">            ref_tokens = []</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> tolist(e[<span class="string">&quot;input_ids&quot;</span>]):</span><br><span class="line">                token = self.tokenizer._convert_id_to_token(<span class="built_in">id</span>)</span><br><span class="line">                ref_tokens.append(token)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-&gt; [喜，##欢]</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;chinese_ref&quot;</span> <span class="keyword">in</span> e:</span><br><span class="line">                ref_pos = tolist(e[<span class="string">&quot;chinese_ref&quot;</span>])</span><br><span class="line">                len_seq = <span class="built_in">len</span>(e[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_seq):</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> ref_pos:</span><br><span class="line">                        ref_tokens[i] = <span class="string">&quot;##&quot;</span> + ref_tokens[i]</span><br><span class="line">            mask_labels.append(self._whole_word_mask(ref_tokens))</span><br><span class="line">        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            input_ids = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_ids = examples</span><br><span class="line">            examples = [&#123;<span class="string">&quot;input_ids&quot;</span>: e&#125; <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line"></span><br><span class="line">        batch_input = _tf_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line"></span><br><span class="line">        mask_labels = []</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> examples:</span><br><span class="line">            ref_tokens = []</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> tolist(e[<span class="string">&quot;input_ids&quot;</span>]):</span><br><span class="line">                token = self.tokenizer._convert_id_to_token(<span class="built_in">id</span>)</span><br><span class="line">                ref_tokens.append(token)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-&gt; [喜，##欢]</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;chinese_ref&quot;</span> <span class="keyword">in</span> e:</span><br><span class="line">                ref_pos = tolist(e[<span class="string">&quot;chinese_ref&quot;</span>])</span><br><span class="line">                len_seq = <span class="built_in">len</span>(e[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_seq):</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> ref_pos:</span><br><span class="line">                        ref_tokens[i] = <span class="string">&quot;##&quot;</span> + ref_tokens[i]</span><br><span class="line">            mask_labels.append(self._whole_word_mask(ref_tokens))</span><br><span class="line">        batch_mask = _tf_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        inputs, labels = self.tf_mask_tokens(batch_input, batch_mask)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            input_ids = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_ids = examples</span><br><span class="line">            examples = [&#123;<span class="string">&quot;input_ids&quot;</span>: e&#125; <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line"></span><br><span class="line">        batch_input = _numpy_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line"></span><br><span class="line">        mask_labels = []</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> examples:</span><br><span class="line">            ref_tokens = []</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> tolist(e[<span class="string">&quot;input_ids&quot;</span>]):</span><br><span class="line">                token = self.tokenizer._convert_id_to_token(<span class="built_in">id</span>)</span><br><span class="line">                ref_tokens.append(token)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-&gt; [喜，##欢]</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;chinese_ref&quot;</span> <span class="keyword">in</span> e:</span><br><span class="line">                ref_pos = tolist(e[<span class="string">&quot;chinese_ref&quot;</span>])</span><br><span class="line">                len_seq = <span class="built_in">len</span>(e[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_seq):</span><br><span class="line">                    <span class="keyword">if</span> i <span class="keyword">in</span> ref_pos:</span><br><span class="line">                        ref_tokens[i] = <span class="string">&quot;##&quot;</span> + ref_tokens[i]</span><br><span class="line">            mask_labels.append(self._whole_word_mask(ref_tokens))</span><br><span class="line">        batch_mask = _numpy_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)</span><br><span class="line">        inputs, labels = self.numpy_mask_tokens(batch_input, batch_mask)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_whole_word_mask</span>(<span class="params">self, input_tokens: <span class="type">List</span>[<span class="built_in">str</span>], max_predictions=<span class="number">512</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Get 0/1 labels for masked tokens with whole word mask proxy</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.tokenizer, (BertTokenizer, BertTokenizerFast)):</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. &quot;</span></span><br><span class="line">                <span class="string">&quot;Please refer to the documentation for more information.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        cand_indexes = []</span><br><span class="line">        <span class="keyword">for</span> (i, token) <span class="keyword">in</span> <span class="built_in">enumerate</span>(input_tokens):</span><br><span class="line">            <span class="keyword">if</span> token == <span class="string">&quot;[CLS]&quot;</span> <span class="keyword">or</span> token == <span class="string">&quot;[SEP]&quot;</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span> token.startswith(<span class="string">&quot;##&quot;</span>):</span><br><span class="line">                cand_indexes[-<span class="number">1</span>].append(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cand_indexes.append([i])</span><br><span class="line"></span><br><span class="line">        random.shuffle(cand_indexes)</span><br><span class="line">        num_to_predict = <span class="built_in">min</span>(max_predictions, <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">int</span>(<span class="built_in">round</span>(<span class="built_in">len</span>(input_tokens) * self.mlm_probability))))</span><br><span class="line">        masked_lms = []</span><br><span class="line">        covered_indexes = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) &gt;= num_to_predict:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">            <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) + <span class="built_in">len</span>(index_set) &gt; num_to_predict:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            is_any_index_covered = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">                <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">                    is_any_index_covered = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">                covered_indexes.add(index)</span><br><span class="line">                masked_lms.append(index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(covered_indexes) != <span class="built_in">len</span>(masked_lms):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Length of covered_indexes is not equal to length of masked_lms.&quot;</span>)</span><br><span class="line">        mask_labels = [<span class="number">1</span> <span class="keyword">if</span> i <span class="keyword">in</span> covered_indexes <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(input_tokens))]</span><br><span class="line">        <span class="keyword">return</span> mask_labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, mask_labels: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set</span></span><br><span class="line"><span class="string">        &#x27;mask_labels&#x27; means we use whole word mask (wwm), we directly mask idxs according to it&#x27;s ref.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        labels = inputs.clone()</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)</span></span><br><span class="line"></span><br><span class="line">        probability_matrix = mask_labels</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = [</span><br><span class="line">            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">        ]</span><br><span class="line">        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.<span class="built_in">bool</span>), value=<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels.eq(self.tokenizer.pad_token_id)</span><br><span class="line">            probability_matrix.masked_fill_(padding_mask, value=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        masked_indices = probability_matrix.<span class="built_in">bool</span>()</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = torch.bernoulli(torch.full(labels.shape, <span class="number">0.8</span>)).<span class="built_in">bool</span>() &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = torch.bernoulli(torch.full(labels.shape, <span class="number">0.5</span>)).<span class="built_in">bool</span>() &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = torch.randint(<span class="built_in">len</span>(self.tokenizer), labels.shape, dtype=torch.long)</span><br><span class="line">        inputs[indices_random] = random_words[indices_random]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, mask_labels: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set</span></span><br><span class="line"><span class="string">        &#x27;mask_labels&#x27; means we use whole word mask (wwm), we directly mask idxs according to it&#x27;s ref.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        input_shape = tf.shape(inputs)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        labels = tf.identity(inputs)</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)</span></span><br><span class="line"></span><br><span class="line">        masked_indices = tf.cast(mask_labels, tf.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = [</span><br><span class="line">            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels</span><br><span class="line">        ]</span><br><span class="line">        masked_indices = masked_indices &amp; ~tf.cast(special_tokens_mask, dtype=tf.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = inputs == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices = masked_indices &amp; ~padding_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens</span></span><br><span class="line">        labels = tf.where(masked_indices, inputs, -<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = self.tf_bernoulli(input_shape, <span class="number">0.8</span>) &amp; masked_indices</span><br><span class="line"></span><br><span class="line">        inputs = tf.where(indices_replaced, self.tokenizer.mask_token_id, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        indices_random = self.tf_bernoulli(input_shape, <span class="number">0.1</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        random_words = tf.random.uniform(input_shape, maxval=<span class="built_in">len</span>(self.tokenizer), dtype=tf.int64)</span><br><span class="line">        inputs = tf.where(indices_random, random_words, inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span>, mask_labels: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set</span></span><br><span class="line"><span class="string">        &#x27;mask_labels&#x27; means we use whole word mask (wwm), we directly mask idxs according to it&#x27;s ref.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        labels = np.copy(inputs)</span><br><span class="line">        <span class="comment"># We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)</span></span><br><span class="line"></span><br><span class="line">        masked_indices = mask_labels.astype(np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = [</span><br><span class="line">            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()</span><br><span class="line">        ]</span><br><span class="line">        masked_indices[np.array(special_tokens_mask, dtype=np.<span class="built_in">bool</span>)] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices[padding_mask] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])</span></span><br><span class="line">        indices_replaced = np.random.binomial(<span class="number">1</span>, <span class="number">0.8</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices</span><br><span class="line">        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 10% of the time, we replace masked input tokens with random word</span></span><br><span class="line">        <span class="comment"># indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced</span></span><br><span class="line">        indices_random = (</span><br><span class="line">            np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=labels.shape).astype(np.<span class="built_in">bool</span>) &amp; masked_indices &amp; ~indices_replaced</span><br><span class="line">        )</span><br><span class="line">        random_words = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.tokenizer), size=labels.shape, dtype=np.int64)</span><br><span class="line">        inputs[indices_random] = random_words[indices_random]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The rest of the time (10% of the time) we keep the masked input tokens unchanged</span></span><br><span class="line">        <span class="keyword">return</span> inputs, labels</span><br></pre></td></tr></table></figure>
<h3 id="SpanBERT-Implementation"><a href="#SpanBERT-Implementation" class="headerlink" title="SpanBERT Implementation"></a>SpanBERT Implementation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3. SpanBERT implementation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertRandomMaskingScheme</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args, tokens, pad, mask_id</span>):</span></span><br><span class="line">        self.args = args</span><br><span class="line">        self.mask_ratio = <span class="built_in">getattr</span>(self.args, <span class="string">&#x27;mask_ratio&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.pad = pad</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.mask_id = mask_id</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mask</span>(<span class="params">self, sentence, tagmap=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;mask tokens for masked language model training</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sentence: 1d tensor, token list to be masked</span></span><br><span class="line"><span class="string">            mask_ratio: ratio of tokens to be masked in the sentence</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            masked_sent: masked sentence</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">        mask_num = math.ceil(sent_length * self.mask_ratio)</span><br><span class="line">        mask = np.random.choice(sent_length, mask_num, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> bert_masking(sentence, mask, self.tokens, self.pad, self.mask_id)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bert_masking</span>(<span class="params">sentence, mask, tokens, pad, mask_id</span>):</span></span><br><span class="line">    sentence = np.copy(sentence)</span><br><span class="line">    sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">    target = np.copy(sentence)</span><br><span class="line">    mask = <span class="built_in">set</span>(mask)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sent_length):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mask:</span><br><span class="line">            rand = np.random.random()</span><br><span class="line">            <span class="keyword">if</span> rand &lt; <span class="number">0.8</span>:</span><br><span class="line">                sentence[i] = mask_id</span><br><span class="line">            <span class="keyword">elif</span> rand &lt; <span class="number">0.9</span>:</span><br><span class="line">                <span class="comment"># sample random token according to input distribution</span></span><br><span class="line">                sentence[i] = np.random.choice(tokens)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target[i] = pad</span><br><span class="line">    <span class="keyword">return</span> sentence, target, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="Span-Mask"><a href="#Span-Mask" class="headerlink" title="Span Mask"></a>Span Mask</h2><div class="note info">
            <p>Span masking consists of random masking, named entity masking, etc. </p><ol><li>ERNIE<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Yu et al. [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf). ArXiv abs/1904.09223 (2019)">[6]</span></a></sup> applies knowledge masking on the input sequence including <strong>entity-</strong> and <strong>phrase-</strong> level masking to inject knowledge composition.</li><li>SpanBERT<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: SpanBERT](https://github.com/facebookresearch/SpanBERT/blob/0670d8b6a38f6714b85ea7a033f16bd8cc162676/pretraining/fairseq/data/masking.py)">[2]</span></a></sup> employs random span masking under a clamped geometric distribution.</li><li>BERT-WWM<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cui, Yiming et al. [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/pdf/1906.08101.pdf)">[7]</span></a></sup> uses whole word masking (for Chinese BERT) rather than randomly masking subword pieces to retain the whole meaning of a word.</li></ol>
          </div>
<p>SpanBERT<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: SpanBERT](https://github.com/facebookresearch/SpanBERT/blob/0670d8b6a38f6714b85ea7a033f16bd8cc162676/pretraining/fairseq/data/masking.py)
">[2]</span></a></sup> iteratively samples span’s length under a (clamped) geometric distribution <script type="math/tex">\mathcal{l} \sim \textrm{Geo}(p)</script>, <em>i.e.</em>, </p>
<script type="math/tex; mode=display">P(x=p)=(1-p)^k p</script><p>which is skewed towards shorter spans ($p=0.2$). It also clips $\mathcal{l}$ with <script type="math/tex">\mathcal{l} = \min (\mathcal{l}, 10)</script>, yielding a mean span length of $\bar{\mathcal{l}}=3.8$. SpanBERT measures span length in complete words, not subword tokens, making the masked spans even longer.</p>
<p>The masking strategies are the same as BERT: masking 15% in total, where replacing 80% of tokens with [MASK], 10% with random tokens, and 10% unchanged.</p>
<p><img data-src="/notes/images/SpanBERT-span-length.png" width="40%"></p>
<p><img data-src="/notes/images/SpanBERT.png" alt="SpanBERT"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SpanBERT implementation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PairWithSpanMaskingScheme</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args, tokens, pad, mask_id, paragraph_info</span>):</span> </span><br><span class="line">        self.args = args</span><br><span class="line">        self.mask_ratio = <span class="built_in">getattr</span>(self.args, <span class="string">&#x27;mask_ratio&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.args = args</span><br><span class="line">        self.max_pair_targets = args.max_pair_targets</span><br><span class="line">        self.lower = args.span_lower</span><br><span class="line">        self.upper = args.span_upper</span><br><span class="line">        self.pad = pad</span><br><span class="line">        self.mask_id = mask_id</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.paragraph_info = paragraph_info</span><br><span class="line">        self.lens = <span class="built_in">list</span>(<span class="built_in">range</span>(self.lower, self.upper + <span class="number">1</span>))</span><br><span class="line">        self.p = args.geometric_p</span><br><span class="line">        self.len_distrib = [self.p * (<span class="number">1</span>-self.p)**(i - self.lower) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.lower, self.upper + <span class="number">1</span>)] <span class="keyword">if</span> self.p &gt;= <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.len_distrib = [x / (<span class="built_in">sum</span>(self.len_distrib)) <span class="keyword">for</span> x <span class="keyword">in</span> self.len_distrib]</span><br><span class="line">        <span class="built_in">print</span>(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mask</span>(<span class="params">self, sentence, tagmap=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;mask tokens for masked language model training</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sentence: 1d tensor, token list to be masked</span></span><br><span class="line"><span class="string">            mask_ratio: ratio of tokens to be masked in the sentence</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            masked_sent: masked sentence</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">        mask_num = math.ceil(sent_length * self.mask_ratio)</span><br><span class="line">        mask = <span class="built_in">set</span>()</span><br><span class="line">        word_piece_map = self.paragraph_info.get_word_piece_map(sentence)</span><br><span class="line">        spans = []</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(mask) &lt; mask_num:</span><br><span class="line">            span_len = np.random.choice(self.lens, p=self.len_distrib)</span><br><span class="line">            tagged_indices = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> tagmap <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                tagged_indices = [<span class="built_in">max</span>(<span class="number">0</span>, i - np.random.randint(<span class="number">0</span>, span_len)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tagmap.length()) <span class="keyword">if</span> tagmap[i]]</span><br><span class="line">                tagged_indices += [np.random.choice(sent_length)] * <span class="built_in">int</span>(<span class="built_in">len</span>(tagged_indices) == <span class="number">0</span>)</span><br><span class="line">            anchor  = np.random.choice(sent_length) <span class="keyword">if</span> np.random.rand() &gt;= self.args.tagged_anchor_prob <span class="keyword">else</span> np.random.choice(tagged_indices)</span><br><span class="line">            <span class="keyword">if</span> anchor <span class="keyword">in</span> mask:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># find word start, end</span></span><br><span class="line">            left1, right1 = self.paragraph_info.get_word_start(sentence, anchor, word_piece_map), self.paragraph_info.get_word_end(sentence, anchor, word_piece_map)</span><br><span class="line">            spans.append([left1, left1])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left1, right1):</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(mask) &gt;= mask_num:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                mask.add(i)</span><br><span class="line">                spans[-<span class="number">1</span>][-<span class="number">1</span>] = i</span><br><span class="line">            num_words = <span class="number">1</span></span><br><span class="line">            right2 = right1</span><br><span class="line">            <span class="keyword">while</span> num_words &lt; span_len <span class="keyword">and</span> right2 &lt; <span class="built_in">len</span>(sentence) <span class="keyword">and</span> <span class="built_in">len</span>(mask) &lt; mask_num:</span><br><span class="line">                <span class="comment"># complete current word</span></span><br><span class="line">                left2 = right2</span><br><span class="line">                right2 = self.paragraph_info.get_word_end(sentence, right2, word_piece_map)</span><br><span class="line">                num_words += <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left2, right2):</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(mask) &gt;= mask_num:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    mask.add(i)</span><br><span class="line">                    spans[-<span class="number">1</span>][-<span class="number">1</span>] = i</span><br><span class="line">        sentence, target, pair_targets = span_masking(sentence, spans, self.tokens, self.pad, self.mask_id, self.max_pair_targets, mask, replacement=self.args.replacement_method, endpoints=self.args.endpoints)</span><br><span class="line">        <span class="keyword">if</span> self.args.return_only_spans:</span><br><span class="line">            pair_targets = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> sentence, target, pair_targets</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParagraphInfo</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dictionary</span>):</span></span><br><span class="line">        self.dictionary = dictionary</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_piece_map</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [self.dictionary.is_start_word(i) <span class="keyword">for</span> i <span class="keyword">in</span> sentence]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_at_k</span>(<span class="params">self, sentence, left, right, k, word_piece_map=<span class="literal">None</span></span>):</span></span><br><span class="line">        num_words = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> num_words &lt; k <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(sentence):</span><br><span class="line">            <span class="comment"># complete current word</span></span><br><span class="line">            left = right</span><br><span class="line">            right = self.get_word_end(sentence, right, word_piece_map)</span><br><span class="line">            num_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left, right</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_start</span>(<span class="params">self, sentence, anchor, word_piece_map=<span class="literal">None</span></span>):</span></span><br><span class="line">        word_piece_map = word_piece_map <span class="keyword">if</span> word_piece_map <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.get_word_piece_map(sentence)</span><br><span class="line">        left  = anchor</span><br><span class="line">        <span class="keyword">while</span> left &gt; <span class="number">0</span> <span class="keyword">and</span> word_piece_map[left] == <span class="literal">False</span>:</span><br><span class="line">            left -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br><span class="line">    <span class="comment"># word end is next word start</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_end</span>(<span class="params">self, sentence, anchor, word_piece_map=<span class="literal">None</span></span>):</span></span><br><span class="line">        word_piece_map = word_piece_map <span class="keyword">if</span> word_piece_map <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.get_word_piece_map(sentence)</span><br><span class="line">        right = anchor + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> right &lt; <span class="built_in">len</span>(sentence) <span class="keyword">and</span> word_piece_map[right] == <span class="literal">False</span>:</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> right</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">span_masking</span>(<span class="params">sentence, spans, tokens, pad, mask_id, pad_len, mask, replacement=<span class="string">&#x27;word_piece&#x27;</span>, endpoints=<span class="string">&#x27;external&#x27;</span></span>):</span></span><br><span class="line">    sentence = np.copy(sentence)</span><br><span class="line">    sent_length = <span class="built_in">len</span>(sentence)</span><br><span class="line">    target = np.full(sent_length, pad)</span><br><span class="line">    pair_targets = []</span><br><span class="line">    spans = merge_intervals(spans)</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(mask) == <span class="built_in">sum</span>([e - s + <span class="number">1</span> <span class="keyword">for</span> s,e <span class="keyword">in</span> spans])</span><br><span class="line">    <span class="comment"># print(list(enumerate(sentence)))</span></span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> spans:</span><br><span class="line">        lower_limit = <span class="number">0</span> <span class="keyword">if</span> endpoints == <span class="string">&#x27;external&#x27;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        upper_limit = sent_length - <span class="number">1</span> <span class="keyword">if</span> endpoints == <span class="string">&#x27;external&#x27;</span> <span class="keyword">else</span> sent_length</span><br><span class="line">        <span class="keyword">if</span> start &gt; lower_limit <span class="keyword">and</span> end &lt; upper_limit:</span><br><span class="line">            <span class="keyword">if</span> endpoints == <span class="string">&#x27;external&#x27;</span>:</span><br><span class="line">                pair_targets += [[start - <span class="number">1</span>, end + <span class="number">1</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pair_targets += [[start, end]]</span><br><span class="line">            pair_targets[-<span class="number">1</span>] += [sentence[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>)]</span><br><span class="line">        rand = np.random.random()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">assert</span> i <span class="keyword">in</span> mask</span><br><span class="line">            target[i] = sentence[i]</span><br><span class="line">            <span class="keyword">if</span> replacement == <span class="string">&#x27;word_piece&#x27;</span>:</span><br><span class="line">                rand = np.random.random()</span><br><span class="line">            <span class="keyword">if</span> rand &lt; <span class="number">0.8</span>:</span><br><span class="line">                sentence[i] = mask_id</span><br><span class="line">            <span class="keyword">elif</span> rand &lt; <span class="number">0.9</span>:</span><br><span class="line">                <span class="comment"># sample random token according to input distribution</span></span><br><span class="line">                sentence[i] = np.random.choice(tokens)</span><br><span class="line">    pair_targets = pad_to_len(pair_targets, pad, pad_len + <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># if pair_targets is None:</span></span><br><span class="line">    <span class="keyword">return</span> sentence, target, pair_targets</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_intervals</span>(<span class="params">intervals</span>):</span></span><br><span class="line">    intervals = <span class="built_in">sorted</span>(intervals, key=<span class="keyword">lambda</span> x : x[<span class="number">0</span>])</span><br><span class="line">    merged = []</span><br><span class="line">    <span class="keyword">for</span> interval <span class="keyword">in</span> intervals:</span><br><span class="line">        <span class="comment"># if the list of merged intervals is empty or if the current</span></span><br><span class="line">        <span class="comment"># interval does not overlap with the previous, simply append it.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> merged <span class="keyword">or</span> merged[-<span class="number">1</span>][<span class="number">1</span>] + <span class="number">1</span> &lt; interval[<span class="number">0</span>]:</span><br><span class="line">            merged.append(interval)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># otherwise, there is overlap, so we merge the current and previous</span></span><br><span class="line">        <span class="comment"># intervals.</span></span><br><span class="line">            merged[-<span class="number">1</span>][<span class="number">1</span>] = <span class="built_in">max</span>(merged[-<span class="number">1</span>][<span class="number">1</span>], interval[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> merged</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_to_len</span>(<span class="params">pair_targets, pad, max_pair_target_len</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(pair_targets)):</span><br><span class="line">        pair_targets[i] = pair_targets[i][:max_pair_target_len]</span><br><span class="line">        this_len = <span class="built_in">len</span>(pair_targets[i])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(max_pair_target_len - this_len):</span><br><span class="line">            pair_targets[i].append(pad)</span><br><span class="line">    <span class="keyword">return</span> pair_targets</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/SpanBERT-masking-scheme-comparison.png" alt="Results of SpanBERT mask scheme."></p>
<p>It can be seen from the table that with the exception of coreference resolution, masking random spans is preferable to other strategies. Although <strong>linguistic masking schemes (named entities and noun phrases)</strong> are often competitive with random spans, their performance is not consistent. For <strong>coreference resolution</strong>, <strong>masking random subword toekns</strong> is preferable to any form of span masking.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NER mask</span></span><br><span class="line">class NERSpanMaskingScheme(object):</span><br><span class="line">    def __init__(self, args, tokens, pad, mask_id, paragraph_info): </span><br><span class="line">        self.args = args</span><br><span class="line">        self.mask_ratio = getattr(self.args, <span class="string">&#x27;mask_ratio&#x27;</span>, None)</span><br><span class="line">        self.max_pair_targets = args.max_pair_targets</span><br><span class="line">        self.lower = args.span_lower</span><br><span class="line">        self.upper = args.span_upper</span><br><span class="line">        self.pad = pad</span><br><span class="line">        self.mask_id = mask_id</span><br><span class="line">        self.tokens = tokens</span><br><span class="line">        self.paragraph_info = paragraph_info</span><br><span class="line">        self.lens = list(range(self.lower, self.upper + 1))</span><br><span class="line">        self.p = args.geometric_p</span><br><span class="line">        self.len_distrib = [self.p * (1-self.p)**(i - self.lower) <span class="keyword">for</span> i <span class="keyword">in</span> range(self.lower, self.upper + 1)] <span class="keyword">if</span> self.p &gt;= 0 <span class="keyword">else</span> None</span><br><span class="line">        self.len_distrib = [x / (sum(self.len_distrib)) <span class="keyword">for</span> x <span class="keyword">in</span> self.len_distrib]</span><br><span class="line">        <span class="built_in">print</span>(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line">    def mask_random_span(self, sentence, mask_num, word_piece_map, spans, mask, span_len, anchor):</span><br><span class="line">        <span class="comment"># find word start, end</span></span><br><span class="line">        left1, right1 = self.paragraph_info.get_word_start(sentence, anchor, word_piece_map), self.paragraph_info.get_word_end(sentence, anchor, word_piece_map)</span><br><span class="line">        spans.append([left1, left1])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(left1, right1):</span><br><span class="line">            <span class="keyword">if</span> len(mask) &gt;= mask_num:</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            mask.add(i)</span><br><span class="line">            spans[-1][-1] = i</span><br><span class="line">        num_words = 1</span><br><span class="line">        right2 = right1</span><br><span class="line">        <span class="keyword">while</span> num_words &lt; span_len and right2 &lt; len(sentence) and len(mask) &lt; mask_num:</span><br><span class="line">            <span class="comment"># complete current word</span></span><br><span class="line">            left2 = right2</span><br><span class="line">            right2 = self.paragraph_info.get_word_end(sentence, right2, word_piece_map)</span><br><span class="line">            num_words += 1</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(left2, right2):</span><br><span class="line">                <span class="keyword">if</span> len(mask) &gt;= mask_num:</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line">                mask.add(i)</span><br><span class="line">                spans[-1][-1] = i</span><br><span class="line"></span><br><span class="line">    def mask_entity(self, sentence, mask_num, word_piece_map, spans, mask, entity_spans):</span><br><span class="line">        <span class="keyword">if</span> len(entity_spans) &gt; 0:</span><br><span class="line">            entity_span = entity_spans[np.random.choice(range(len(entity_spans)))]</span><br><span class="line">            spans.append([entity_span[0], entity_span[0]])</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(entity_span[0], entity_span[1] + 1):</span><br><span class="line">                <span class="keyword">if</span> len(mask) &gt;= mask_num:</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line">                spans[-1][-1] = idx</span><br><span class="line">                mask.add(idx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def mask(self, sentence, entity_map=None):</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;mask tokens for masked language model training</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sentence: 1d tensor, token list to be masked</span></span><br><span class="line"><span class="string">            mask_ratio: ratio of tokens to be masked in the sentence</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            masked_sent: masked sentence</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        sent_length = len(sentence)</span><br><span class="line">        mask_num = math.ceil(sent_length * self.mask_ratio)</span><br><span class="line">        mask = <span class="built_in">set</span>()</span><br><span class="line">        word_piece_map = self.paragraph_info.get_word_piece_map(sentence)</span><br><span class="line">        <span class="comment"># get entity spans</span></span><br><span class="line">        entity_spans, spans = [], []</span><br><span class="line">        new_entity = True</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(entity_map.length()):</span><br><span class="line">            <span class="keyword">if</span> entity_map[i] and new_entity:</span><br><span class="line">                entity_spans.append([i, i])</span><br><span class="line">                new_entity = False</span><br><span class="line">            <span class="keyword">elif</span> entity_map[i] and not new_entity:</span><br><span class="line">                entity_spans[-1][-1] = i</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_entity = True</span><br><span class="line">        <span class="keyword">while</span> len(mask) &lt; mask_num:</span><br><span class="line">            <span class="keyword">if</span> np.random.random() &lt;= self.args.ner_masking_prob:</span><br><span class="line">                self.mask_entity(sentence, mask_num, word_piece_map, spans, mask, entity_spans)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                span_len = np.random.choice(self.lens, p=self.len_distrib)</span><br><span class="line">                anchor  = np.random.choice(sent_length)</span><br><span class="line">                <span class="keyword">if</span> anchor <span class="keyword">in</span> mask:</span><br><span class="line">                    <span class="built_in">continue</span></span><br><span class="line">                self.mask_random_span(sentence, mask_num, word_piece_map, spans, mask, span_len, anchor)</span><br><span class="line">        sentence, target, pair_targets = span_masking(sentence, spans, self.tokens, self.pad, self.mask_id, self.max_pair_targets, mask, replacement=self.args.replacement_method, endpoints=self.args.endpoints)</span><br><span class="line">        <span class="keyword">if</span> self.args.return_only_spans:</span><br><span class="line">            pair_targets = None</span><br><span class="line">        <span class="built_in">return</span> sentence, target, pair_targets</span><br></pre></td></tr></table></figure>
<h2 id="MASS-Mask"><a href="#MASS-Mask" class="headerlink" title="MASS Mask"></a>MASS Mask</h2><p>MASS<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Song, Kaitao et al. [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/pdf/1905.02450.pdf). ICML (2019).
">[3]</span></a></sup> encoder replaces each masked token by a special [MASK] token, leading to unchanged length overall. Then the decoder predicts the masked tokens autoregressively.</p>
<p><img data-src="/notes/images/MASS Mask.png" alt="MASS Mask"></p>
<h2 id="BART-Mask"><a href="#BART-Mask" class="headerlink" title="BART Mask"></a>BART Mask</h2><p>BART<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lewis, Mike et al. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://aclanthology.org/2020.acl-main.703.pdf). ACL 2020.
">[4]</span></a></sup> replaces corrputed continuous spans of the encoder input as single [MASK], and trains the decoder in an autogressive way using a transformer encoder-decoder architecture.</p>
<p><img data-src="/notes/images/BART-Mask.png" alt="BART masking"></p>
<p>BART allows any type of document corruption, including:</p>
<ul>
<li><strong>Token Masking</strong>: BERT masking.</li>
<li><strong>Token Deletion</strong>: random tokens are deleted from the input.</li>
<li><strong>Text Infilling</strong>: amounts of text spans are corrupted, with span length drawn from a Poission distribution ($\lambda=3$). Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of [MASK] tokens.</li>
<li><strong>Sentence Permutation</strong>: Divide a document into peices of sentences based on full stops, and randomly shuffle them.</li>
<li><strong>Document Rotation</strong>: uniformly chose a token at random to rotate the document.</li>
</ul>
<h2 id="T5-Span-Mask"><a href="#T5-Span-Mask" class="headerlink" title="T5 Span Mask"></a>T5 Span Mask</h2><p>T5<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Raffel, Colin et al. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf). JMLR 2020.
">[5]</span></a></sup> replaces <span class="label success">with unique sentinel</span> <span class="label warning"> the corrupted spans</span> in the input sequence, and predicts the <span class="label success">concatenation of corrupted spans prefixed by the sentinal token</span> used in the input. Specifically, T5 first <strong>replaces the entirety of each consecutive span of corrupted tokens with a unique mask token</strong>. Then, the target sequence becomes the <strong>concatenation of the corrupted spans, each prefixed by the mask token</strong> used to replace it in the input.</p>
<p><img data-src="/notes/images/T5-mask.png" width="60%"></p>
<p><img data-src="/notes/images/T5-results-on-objectives.png" alt=""></p>
<p>As shown in the table, BERT-syle objective simply replaces 15% of the input tokens without the original random token swapping step, and reconstruct the original uncorrupted sequence.</p>
<p>The <span class="label primary">first two rows</span> (<em>i.e.,</em> BERT-style and MASS-style objectives) predict the entire uncorrupted text span which <span class="label danger">requires self-attention over long sequences in the decoder</span>. To avoid this, T5 applies the strategies in the last two rows. The <span class="label primary">last row</span>(<em>i.e.</em>, Drop corrupted tokens) simply <span class="label primary">drops the corrupted tokens from the input sequence completely</span> and task the model with reconstructing the dropped tokens in order. </p>
<p>It can be seen from the table that <span class="label success">"dropping corrupted spans"</span> completely produced a small improvement in the GLUE score thanks to the significatly higher score on CoLA.<br>The <span class="label primary">first two rows</span> (<em>i.e.,</em> BERT-style and MASS-style objectives) predict the entire uncorrupted text span which <span class="label danger">requires self-attention over long sequences in the decoder</span>. To avoid this, T5 applies the strategies in the last two rows. The <span class="label primary">last row</span>(<em>i.e.</em>, Drop corrupted tokens) simply <span class="label primary">drops the corrupted tokens from the input sequence completely</span> and task the model with reconstructing the dropped tokens in order. (60.45 vs avg. baseline 53.84). However, dropping tokens completely performed worse than replacing with sentinel tokens on SuperGLUE. The last two rows’ variants <span class="label success">make the target sequence shorter and consequently make training faster</span>. </p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2022mask-PTMs,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Mask Strategy for Pre-trained Models&#125;&#125;,</span><br><span class="line">  year = &#123;2022&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2022/01/10/Mask-Denoising-Strategy-for-Pre-trained-Models/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/create_pretraining_data.py#L342">GitHub: Google BERT</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/facebookresearch/SpanBERT/blob/0670d8b6a38f6714b85ea7a033f16bd8cc162676/pretraining/fairseq/data/masking.py">GitHub: SpanBERT</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Song, Kaitao et al. <a href="https://arxiv.org/pdf/1905.02450.pdf">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>. ICML (2019).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lewis, Mike et al. <a href="https://aclanthology.org/2020.acl-main.703.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>. ACL 2020.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raffel, Colin et al. <a href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. JMLR 2020.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Yu et al. <a href="https://arxiv.org/pdf/1904.09223.pdf">ERNIE: Enhanced Representation through Knowledge Integration</a>. ArXiv abs/1904.09223 (2019)<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cui, Yiming et al. <a href="https://arxiv.org/pdf/1906.08101.pdf">Pre-Training with Whole Word Masking for Chinese BERT</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>LLM</category>
        <category>Pre-training</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Pre-training</tag>
      </tags>
  </entry>
  <entry>
    <title>Inductive Positions in Transformers</title>
    <url>/notes/2023/01/26/Position-Encoding-in-Transformers/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>We summarize the positional encoding approaches in transformers.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>PE</strong></th>
<th style="text-align:center"><strong>Relative</strong></th>
<th style="text-align:center"><strong>Trainable</strong></th>
<th style="text-align:center"><strong>Each Layer</strong></th>
<th style="text-align:center"><strong>Extrapolation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Sinusoidal</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
</tr>
<tr>
<td style="text-align:center">T5 bias</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">RoPE</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
</tr>
<tr>
<td style="text-align:center">ALiBi</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">KERPLE</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Sandwich</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">xPos</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><h3 id="Sinusoidal-Position-Embeddings"><a href="#Sinusoidal-Position-Embeddings" class="headerlink" title="Sinusoidal Position Embeddings"></a>Sinusoidal Position Embeddings</h3><p>Sinusoidal position embeddings<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “[Attention is All you Need](https://arxiv.org/pdf/1706.03762.pdf).” NeurIPS (2017).
">[1]</span></a></sup> are constantly encoded vectors to be added on token embeddings of the first transformer layer. </p>
<script type="math/tex; mode=display">\text{PE}_{(\text{pos}, 2i)} = \sin(\frac{\text{pos}}{10000^{2i/d_\text{model}}})</script><script type="math/tex; mode=display">\text{PE}_{(\text{pos}, 2i+1)} = \cos(\frac{\text{pos}}{10000^{2i/d_\text{model}}})</script><p>where $\text{pos}$ is the position in the sentence and $i$ is the order along the  embedding vector dimension. Assume this allows to learn to attend by relative positions, since for and fixed offset $k$, <script type="math/tex">\text{PE}_{\text{pos}+k}</script> can be represented as the linear function of <script type="math/tex">\text{PE}_{\text{pos}}</script> .</p>
<p><img data-src="/notes/images/Transformer-positional-encoding.png" alt="Position encoding"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Positional encoding layer in PyTorch</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0.</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># generate with maximum length</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>, d_model, <span class="number">2</span>) * - (math.log(<span class="number">1e4</span>) / d_model))</span><br><span class="line">        pe[:, ::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        seq_len = x.size(<span class="number">1</span>) <span class="comment"># take the sequence length</span></span><br><span class="line">        x = x + Variable(self.pe[:, :seq_len], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li>Refer to <a href="/notes/2019/01/22/NLP/Attention-in-a-nutshell/#Absolute-Positional-Encoding">Attention in a Nutshell</a>.</li><li>Relative position in Transformer-XL: refer to <a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/#Relative-positional-encoding">Transformer Variants: A Peek</a></li></ul>
          </div>
<h3 id="Rotary-Position-Embedding-RoPE"><a href="#Rotary-Position-Embedding-RoPE" class="headerlink" title="Rotary Position Embedding (RoPE)"></a>Rotary Position Embedding (RoPE)</h3><p>Rotary Position Embedding (RoPE) <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Su, Jianlin, Yu Lu, Shengfeng Pan, Bo Wen and Yunfeng Liu. “[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf).” ArXiv abs/2104.09864 (2021).
">[3]</span></a></sup><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Rotary Embeddings: A Relative Revolution](https://blog.eleuther.ai/rotary-embeddings/)
">[4]</span></a></sup> proposes to use complex numbers as the base field of encoding space. Instead of working in $\mathbb{R}^d$, it uses consecutive pairs of elements of the query and key vectors in $\mathbb{C}^{d/2}$ to be  a single complex number.</p>
<p>Specifically, instead of viewing <script type="math/tex">\mathbf{q}=(q_1,q_2,q_3,q_4,\ldots,q_{d})</script> as a $d$-dimensional real vector, RoPE views it as <script type="math/tex">\mathbf{q}=(q_1+iq_2, q_3+iq_4,\ldots q_{d-1} + iq_{d})\in\mathbb{C}^{d/2}</script>. If $d$ is odd, RoPE pads it with a dummy coordinate to ensure things line up correctly. Alternatives, it simply increases $d$ by one.</p>
<h4 id="Derivation"><a href="#Derivation" class="headerlink" title="Derivation"></a>Derivation</h4><p>The complex number format of RoPE is written as:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{q}, m) = R_f(\mathbf{q}, m)e^{i\Theta_f(\mathbf{q}, m)}=\mathbf{q}e^{i(\Theta(\mathbf{q})+m\mathbf{\theta})} = \sum_{j=1}^{d/2} q_je^{im\theta_j} \vec{e_j}
\end{align}</script><p>It is convenient to convert into matrix equation:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{q}, m) =
\begin{pmatrix}
M_1 & & & \\
& M_2 & & \\
& & \ddots & \\
& & & M_{d/2}
\end{pmatrix}
\begin{pmatrix}
q_1\\
q_2\\
\vdots\\
q_d
\end{pmatrix} = \mathbf{\Theta_m Q_m} = \mathbf{\Theta_m W_q X_m}
\end{align}</script><p>where <script type="math/tex">M_j=\begin{pmatrix}\cos m\theta_j & -\sin m\theta_j \\sin m\theta_j & \cos m\theta_j\end{pmatrix}</script> , $\mathbf{\Theta_m}$ is the block diagonal rotation matrix, $\mathbf{W_q}$ is learned query weights, and $\mathbf{X_m}$ is the embedding of the $m$-th token.</p>
<p>Due to the high computation cost of sparse matrix, it is implemented as:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{q}, m) = \begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1} 
\end{pmatrix}\odot\begin{pmatrix}\cos m\theta_0 \\ \cos m\theta_0 \\ \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2-1} \\ \cos m\theta_{d/2-1} 
\end{pmatrix} + \begin{pmatrix}-q_1 \\ q_0 \\ -q_3 \\ q_2 \\ \vdots \\ -q_{d-1} \\ q_{d-2} 
\end{pmatrix}\odot\begin{pmatrix}\sin m\theta_0 \\ \sin m\theta_0 \\ \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2-1} \\ \sin m\theta_{d/2-1} 
\end{pmatrix}
\end{align}</script><p>where $\odot$ denotes the element-wise product (*).</p>
<p><img data-src="/notes/images/RoPE.png" alt="RoPE"></p>
<ul>
<li>Extension to Multiple Dimensions</li>
</ul>
<script type="math/tex; mode=display">\begin{align*} \langle f(\mathbf{q}, m, i),f(\mathbf{k}, n, j) \rangle &= \langle f_1(\mathbf{q}{:d/2}, m),f_1(\mathbf{k}{:d/2}, n) \rangle + \langle f_2(\mathbf{q}{d/2:}, i),f_2(\mathbf{k}{d/2:}, j) \rangle \ &= g_1(\mathbf{q}{:d/2}, \mathbf{k}{:d/2}, m - n) + g_2(\mathbf{q}{d/2:}, \mathbf{k}{d/2:}, i - j) \ &= g(\mathbf{q}, \mathbf{k}, m - n, i - j) \end{align*}</script><div class="note info">
            <p><strong>Difference from sinusoidal embedding</strong><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Rotary Embeddings: A Relative Revolution](https://blog.eleuther.ai/rotary-embeddings/)">[4]</span></a></sup></p><ol><li>Sinusoidal embeddings apply to each coordinate individually, while RoPE mixes pairs of coordinates.</li><li>Sinusoidal embeddings add a $\cos(m\theta)$ or $\sin(m\theta)$ term, while RoPE uses a multiplicative factor.</li></ol>
          </div>
<h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># source: https://huggingface.co/transformers/v4.8.0/_modules/transformers/models/roformer/modeling_roformer.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian-&gt;RoFormer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoFormerSinusoidalPositionalEmbedding</span>(<span class="params">nn.Embedding</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;This module produces sinusoidal positional embeddings of any length.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_positions: <span class="built_in">int</span>, embedding_dim: <span class="built_in">int</span>, padding_idx: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(num_positions, embedding_dim)</span><br><span class="line">        self.weight = self._init_weight(self.weight)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weight</span>(<span class="params">out: nn.Parameter</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in</span></span><br><span class="line"><span class="string">        the 2nd half of the vector. [dim // 2:]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n_pos, dim = out.shape</span><br><span class="line">        position_enc = np.array(</span><br><span class="line">            [[pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * (j // <span class="number">2</span>) / dim) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(dim)] <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(n_pos)]</span><br><span class="line">        )</span><br><span class="line">        out.requires_grad = <span class="literal">False</span>  <span class="comment"># set early to avoid an error in pytorch-1.8+</span></span><br><span class="line">        sentinel = dim // <span class="number">2</span> <span class="keyword">if</span> dim % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> (dim // <span class="number">2</span>) + <span class="number">1</span></span><br><span class="line">        out[:, <span class="number">0</span>:sentinel] = torch.FloatTensor(np.sin(position_enc[:, <span class="number">0</span>::<span class="number">2</span>]))</span><br><span class="line">        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, <span class="number">1</span>::<span class="number">2</span>]))</span><br><span class="line">        out.detach_()</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids_shape: torch.Size, past_key_values_length: <span class="built_in">int</span> = <span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;`input_ids_shape` is expected to be [bsz x seqlen].&quot;&quot;&quot;</span></span><br><span class="line">        bsz, seq_len = input_ids_shape[:<span class="number">2</span>]</span><br><span class="line">        positions = torch.arange(</span><br><span class="line">            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().forward(positions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rotary_position_embeddings</span>(<span class="params">sinusoidal_pos, query_layer, key_layer, value_layer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># https://kexue.fm/archives/8265</span></span><br><span class="line">    <span class="comment"># sin [batch_size, num_heads, sequence_length, embed_size_per_head//2]</span></span><br><span class="line">    <span class="comment"># cos [batch_size, num_heads, sequence_length, embed_size_per_head//2]</span></span><br><span class="line">    sin, cos = sinusoidal_pos.chunk(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># sin [θ0,θ1,θ2......θd/2-1] -&gt; sin_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]</span></span><br><span class="line">    sin_pos = torch.repeat_interleave(sin, <span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># cos [θ0,θ1,θ2......θd/2-1] -&gt; cos_pos [θ0,θ0,θ1,θ1,θ2,θ2......θd/2-1,θd/2-1]</span></span><br><span class="line">    cos_pos = torch.repeat_interleave(cos, <span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># rotate_half_query_layer [-q1,q0,-q3,q2......,-qd-1,qd-2]</span></span><br><span class="line">    rotate_half_query_layer = torch.stack([-query_layer[..., <span class="number">1</span>::<span class="number">2</span>], query_layer[..., ::<span class="number">2</span>]], dim=-<span class="number">1</span>).reshape_as(</span><br><span class="line">        query_layer</span><br><span class="line">    )</span><br><span class="line">    query_layer = query_layer * cos_pos + rotate_half_query_layer * sin_pos</span><br><span class="line">    <span class="comment"># rotate_half_key_layer [-k1,k0,-k3,k2......,-kd-1,kd-2]</span></span><br><span class="line">    rotate_half_key_layer = torch.stack([-key_layer[..., <span class="number">1</span>::<span class="number">2</span>], key_layer[..., ::<span class="number">2</span>]], dim=-<span class="number">1</span>).reshape_as(key_layer)</span><br><span class="line">    key_layer = key_layer * cos_pos + rotate_half_key_layer * sin_pos</span><br><span class="line">    <span class="keyword">if</span> value_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># rotate_half_value_layer [-v1,v0,-v3,v2......,-vd-1,vd-2]</span></span><br><span class="line">        rotate_half_value_layer = torch.stack([-value_layer[..., <span class="number">1</span>::<span class="number">2</span>], value_layer[..., ::<span class="number">2</span>]], dim=-<span class="number">1</span>).reshape_as(</span><br><span class="line">            value_layer</span><br><span class="line">        )</span><br><span class="line">        value_layer = value_layer * cos_pos + rotate_half_value_layer * sin_pos</span><br><span class="line">        <span class="keyword">return</span> query_layer, key_layer, value_layer</span><br><span class="line">    <span class="keyword">return</span> query_layer, key_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sinusoidal_pos = RoFormerSinusoidalPositionalEmbedding(</span><br><span class="line">    max_position_embeddings, </span><br><span class="line">    config.hidden_size // config.num_attention_heads</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">q, k = apply_rotary_position_embeddings(sinusoidal_pos, q, k)</span><br></pre></td></tr></table></figure>
<p>GPT-NeoX (PyTorch) implementation.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rotary</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, base=<span class="number">10000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>() / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">        self.seq_len_cached = <span class="literal">None</span></span><br><span class="line">        self.cos_cached = <span class="literal">None</span></span><br><span class="line">        self.sin_cached = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_dim=<span class="number">1</span></span>):</span></span><br><span class="line">        seq_len = x.shape[seq_dim]</span><br><span class="line">        <span class="keyword">if</span> seq_len != self.seq_len_cached:</span><br><span class="line">            self.seq_len_cached = seq_len</span><br><span class="line">            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.cos_cached = emb.cos()[:, <span class="literal">None</span>, <span class="literal">None</span>, :]</span><br><span class="line">            self.sin_cached = emb.sin()[:, <span class="literal">None</span>, <span class="literal">None</span>, :]</span><br><span class="line">        <span class="keyword">return</span> self.cos_cached, self.sin_cached</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rotary pos emb helpers:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_half</span>(<span class="params">x</span>):</span></span><br><span class="line">    x1, x2 = x[..., : x.shape[-<span class="number">1</span>] // <span class="number">2</span>], x[..., x.shape[-<span class="number">1</span>] // <span class="number">2</span> :]</span><br><span class="line">    <span class="keyword">return</span> torch.cat(</span><br><span class="line">        (-x2, x1), dim=x1.ndim - <span class="number">1</span></span><br><span class="line">    )  <span class="comment"># dim=-1 triggers a bug in torch &lt; 1.8.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.jit.script</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rotary_pos_emb</span>(<span class="params">q, k, cos, sin</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)</span><br></pre></td></tr></table></figure></p>
<h4 id="RoPE-with-Bias"><a href="#RoPE-with-Bias" class="headerlink" title="RoPE with Bias"></a>RoPE with Bias</h4><p><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Blog- Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)
">[6]</span></a></sup> finds that RoPE w/ Bias can increase the capability of length extrapolation.</p>
<script type="math/tex; mode=display">
\begin{equation}\boldsymbol{q}_m^{\top}\boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n\boldsymbol{k}_n \quad\to\quad (\boldsymbol{q}_m + \boldsymbol{a})^{\top}\boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n(\boldsymbol{k}_n + \boldsymbol{b})\end{equation}</script><p>where <script type="math/tex">\boldsymbol{\mathcal{R}}_m, \boldsymbol{\mathcal{R}}_n</script> are rotation matrix, $\boldsymbol{a}, \boldsymbol{b}$ are learnable bias.</p>
<div class="note success">
            <p><strong>NB</strong>: Pure self-attention softmax gets equivalent results with or without bias term, since it can be cancelled by the softmax normalization.</p><script type="math/tex; mode=display">\begin{equation}\frac{e^{\boldsymbol{q}\cdot(\boldsymbol{k}_n + \boldsymbol{b})}}{\sum\limits_n e^{\boldsymbol{q}\cdot(\boldsymbol{k}_n + \boldsymbol{b})}} = \frac{e^{\boldsymbol{q}\cdot\boldsymbol{k}_n}e^{\boldsymbol{q}\cdot\boldsymbol{b}}}{\sum\limits_n e^{\boldsymbol{q}\cdot\boldsymbol{k}_n} e^{\boldsymbol{q}\cdot\boldsymbol{b}}}= \frac{e^{\boldsymbol{q}\cdot\boldsymbol{k}_n}}{\sum\limits_n e^{\boldsymbol{q}\cdot\boldsymbol{k}_n}}\end{equation}</script><p>But reducing the bias term for self-attention with RoPE cannot obtain the same results.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Blog- Bias项的神奇作用：RoPE + Bias = 更好的长度外推性](https://kexue.fm/archives/9577)">[6]</span></a></sup></p>
          </div>
<h3 id="T5-Bias"><a href="#T5-Bias" class="headerlink" title="T5 Bias"></a>T5 Bias</h3><p>T5<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P.J. (2020). [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/volume21/20-074/20-074.pdf). JMLR.
">[7]</span></a></sup> adds no position encoding to word embeddings. Instead, it add a learned, shared bias to each query-key self-attention score that is dependent on just the distance between the query and key. In which multiple different distances share the same learned bias, which might be beneficial for length interpolation. Specifically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.</p>
<p>T5 uses a bucket of 32 learnable parameters and assign the relative position bias with a <strong>log-binning</strong> strategy:</p>
<script type="math/tex; mode=display">
\begin{align}
b_{m-n} = \left\{
                \begin{array}{ll}
                  \text{bucket}[0] &{} \text{if } m-n<0\\
                  \text{bucket}[m-n] &{} \text{if } 0 \leq m-n < 16\\
                  \text{bucket}[\min(31, \lfloor \frac{\log \frac{m-n}{16}}{\frac{128}{16}} \cdot 16 \rfloor)] &{} \text{if }  m-n \geq 16
                \end{array}
    \right.
\end{align}</script><p><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., Smith, N.A., & Lewis, M. (2022). [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). ICLR.
">[8]</span></a></sup> finds that T5 bias enables length extrapolation.</p>
<div class="note info">
            <p>T5 uses 32 embeddings for all models with ranges that increase in size logarithmically up to an offset of 128 beyond which it assigns all relative positions to the same embedding. All position embeddings are shared across all layers in T5, though within a given layer each attention head uses a different learned position embedding.</p>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">T5Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config: T5Config, has_relative_attention_bias=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line">        self.has_relative_attention_bias = has_relative_attention_bias</span><br><span class="line">        self.relative_attention_num_buckets = config.relative_attention_num_buckets</span><br><span class="line">        self.relative_attention_max_distance = config.relative_attention_max_distance</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.has_relative_attention_bias:</span><br><span class="line">            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_relative_position_bucket</span>(<span class="params">relative_position, bidirectional=<span class="literal">True</span>, num_buckets=<span class="number">32</span>, max_distance=<span class="number">128</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Adapted from Mesh Tensorflow:</span></span><br><span class="line"><span class="string">        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Translate relative position to a bucket number for relative attention. The relative position is defined as</span></span><br><span class="line"><span class="string">        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to</span></span><br><span class="line"><span class="string">        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for</span></span><br><span class="line"><span class="string">        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative</span></span><br><span class="line"><span class="string">        positions &gt;=max_distance map to the same bucket. All relative positions &lt;=-max_distance map to the same bucket.</span></span><br><span class="line"><span class="string">        This should allow for more graceful generalization to longer sequences than the model has been trained on</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            relative_position: an int32 Tensor</span></span><br><span class="line"><span class="string">            bidirectional: a boolean - whether the attention is bidirectional</span></span><br><span class="line"><span class="string">            num_buckets: an integer</span></span><br><span class="line"><span class="string">            max_distance: an integer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        relative_buckets = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> bidirectional:</span><br><span class="line">            num_buckets //= <span class="number">2</span></span><br><span class="line">            relative_buckets += (relative_position &gt; <span class="number">0</span>).to(torch.long) * num_buckets</span><br><span class="line">            relative_position = torch.<span class="built_in">abs</span>(relative_position)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            relative_position = -torch.<span class="built_in">min</span>(relative_position, torch.zeros_like(relative_position))</span><br><span class="line">        <span class="comment"># now relative_position is in the range [0, inf)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># half of the buckets are for exact increments in positions</span></span><br><span class="line">        max_exact = num_buckets // <span class="number">2</span></span><br><span class="line">        is_small = relative_position &lt; max_exact</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The other half of the buckets are for logarithmically bigger bins in positions up to max_distance</span></span><br><span class="line">        relative_position_if_large = max_exact + (</span><br><span class="line">            torch.log(relative_position.<span class="built_in">float</span>() / max_exact)</span><br><span class="line">            / math.log(max_distance / max_exact)</span><br><span class="line">            * (num_buckets - max_exact)</span><br><span class="line">        ).to(torch.long)</span><br><span class="line">        relative_position_if_large = torch.<span class="built_in">min</span>(</span><br><span class="line">            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)</span><br><span class="line">        <span class="keyword">return</span> relative_buckets</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_bias</span>(<span class="params">self, query_length, key_length, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute binned relative position bias&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            device = self.relative_attention_bias.weight.device</span><br><span class="line">        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, <span class="literal">None</span>]</span><br><span class="line">        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[<span class="literal">None</span>, :]</span><br><span class="line">        relative_position = memory_position - context_position  <span class="comment"># shape (query_length, key_length)</span></span><br><span class="line">        relative_position_bucket = self._relative_position_bucket(</span><br><span class="line">            relative_position,  <span class="comment"># shape (query_length, key_length)</span></span><br><span class="line">            bidirectional=(<span class="keyword">not</span> self.is_decoder),</span><br><span class="line">            num_buckets=self.relative_attention_num_buckets,</span><br><span class="line">            max_distance=self.relative_attention_max_distance,</span><br><span class="line">        )</span><br><span class="line">        values = self.relative_attention_bias(relative_position_bucket)  <span class="comment"># shape (query_length, key_length, num_heads)</span></span><br><span class="line">        values = values.permute([<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]).unsqueeze(<span class="number">0</span>)  <span class="comment"># shape (1, num_heads, query_length, key_length)</span></span><br><span class="line">        <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure>
<h3 id="ALiBi-Attention-with-Linear-Biases"><a href="#ALiBi-Attention-with-Linear-Biases" class="headerlink" title="ALiBi (Attention with Linear Biases)"></a>ALiBi (Attention with Linear Biases)</h3><div class="note warning">
            <p>Length extrapolation allows transformers training on short sequences while testing on substantially long sequences, by means of relative positional encoding. </p>
          </div>
<p>ALiBi<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., Smith, N.A., & Lewis, M. (2022). [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). ICLR.
">[8]</span></a></sup> adds a static, non-learnable bias to the query-key dot product. As is done in T5 bias and RoPE, it adds position information to keys and querys at each layer.</p>
<script type="math/tex; mode=display">
\mathbf{q}_m^T \mathbf{k}_n + \alpha \vert m-n \vert</script><p>where $\alpha$ is a head-specific slope (fixed). For $i$-th heads, the value of slope takes <script type="math/tex">\alpha_i = 2^{\frac{-8}{i}}</script>.</p>
<p><img data-src="/notes/images/ALiBi.png" alt="ALiBi"></p>
<div class="note info">
            <p>ALiBi bias is not multiplied by the <script type="math/tex">\sqrt{d_k}</script> scaling factor as in the original transformer.</p>
          </div>
<p><img data-src="/notes/images/ALiBI_perf.png" alt="Extrapolation: (validation-set’s) input sequence length (x-axis), versus perplexity (y-axis, lower is better)."></p>
<p>It is observed that ALiBi and T5 bias show length extrapolation ability<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., Smith, N.A., & Lewis, M. (2022). [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). ICLR.
">[8]</span></a></sup>, while RoPE and sinusoidal position do not have. </p>
<h3 id="KERPLE"><a href="#KERPLE" class="headerlink" title="KERPLE"></a>KERPLE</h3><p>KErnelize Relative Positional Embedding for Length Extrapolation (KERPLE)<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chi, T., Fan, T., Ramadge, P.J., & Rudnicky, A.I. (2022). [KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation](https://arxiv.org/pdf/2205.09921.pdf). NeurIPS.
">[9]</span></a></sup> proposes kernelized positional embeddings as follows:</p>
<script type="math/tex; mode=display">
\begin{align}
a_{m,n} = \frac{\exp(\frac{\mathbf{q}_m^T \mathbf{k}_n + \tilde{k}_{r_1,\cdots, r_{\mathcal{l}}(m, n)}}{\sqrt{d}})}{\sum_{i=1}^L \exp(\frac{\mathbf{q}_m^T \mathbf{k}_i + \tilde{k}_{r_1,\cdots, r_{\mathcal{l}}(m, i)}}{\sqrt{d}})}
\end{align}</script><p>where <script type="math/tex">r_1,\cdots, r_{\mathcal{l}}</script> are learnable parameters.</p>
<script type="math/tex; mode=display">
\begin{align}
a_{m,n} = \left\{
    \begin{array}{ll}
    \text{(power)}&{}
\mathbf{q}_m^{\top}\mathbf{k}_n - r_1|m - n|^{r_2} ,&{} r_1 >0, 0 < r_2 \leq 2\\ 
\text{(logarithmic)}&{} \mathbf{q}_m^{\top}\mathbf{k}_n - r_1\log(1+r_2|m - n|),&{} r_1, r_2 > 0 
\end{array}
\right.\label{eq:kerple}\end{align}</script><div class="note info">
            <p>Triangle kernel: <script type="math/tex">c-|m-n|</script>. It reduces to ALiBi.</p>
          </div>
<h3 id="xPos"><a href="#xPos" class="headerlink" title="xPos"></a>xPos</h3><p>Extrapolatable Position Embedding (XPOS)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., & Wei, F. (2022). [A Length-Extrapolatable Transformer](https://arxiv.org/pdf/2212.10554.pdf). ArXiv, abs/2212.10554.
">[10]</span></a></sup> proposes:</p>
<script type="math/tex; mode=display">
\begin{align}
f_q (q,n) &{}= A_q qe^{\lambda n}  &{} \text{let }\lambda = \xi+i\theta \in \mathbb{C}^{d/2} \\&{}:= qe^{\xi n + i\theta n} &{} \text{Remove linear factor $A_q$} \\ \nonumber \\

f_k (k,n) &{}= A_k ke^{-\lambda n}  &{} \text{let }\lambda = \xi+i\theta \in \mathbb{C}^{d/2} \\&{}:= ke^{\xi n + i\theta n} &{} \text{Remove linear factor $A_k$}
\end{align}</script><p><img data-src="/notes/images/XPOS.png" alt="xPos"></p>
<h3 id="Sandwich"><a href="#Sandwich" class="headerlink" title="Sandwich"></a>Sandwich</h3><p>The self-attention is calculated as:</p>
<script type="math/tex; mode=display">
\begin{align}
(\mathbf{W}_q(\mathbf{e}_m + \mathbf{p}_m))^T (\mathbf{W}_k(\mathbf{e}_n + \mathbf{p}_n)) &{} \approx \underbrace{\mathbf{e}_m^T\mathbf{W}_q^T\mathbf{W}_k\mathbf{e}_n^T}_{\text{semantic}} + \underbrace{\mathbf{p}_m^T\mathbf{p}_n}_{\text{position}}
\end{align}</script><p>The temporal bias terms is:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{p}_m^T\mathbf{p}_n =&{} \sum_{i=1}^{\bar{d}/2} \sin \big( \frac{m}{10000^{2i/\bar{d}}} \big) \sin \big( \frac{n}{10000^{2i/\bar{d}}} \big) + \cos \big( \frac{m}{10000^{2i/\bar{d}}} \big) \cos \big( \frac{n}{10000^{2i/\bar{d}}} \big)\\
=&{} \sum_{i=1}^{\bar{d}/2} \cos \big( \frac{m-n}{10000^{2i/\bar{d}}} \big)
\end{align}</script><p>Sandwich<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chi, T., Fan, T., Rudnicky, A., & Ramadge, P.J. (2022). [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://arxiv.org/pdf/2212.10356.pdf).
">[11]</span></a></sup> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">base = <span class="number">1e4</span> </span><br><span class="line">heads = <span class="number">12</span></span><br><span class="line">seq_len = <span class="number">8192</span></span><br><span class="line">positions = np.arange(seq_len)[..., <span class="literal">None</span>] </span><br><span class="line">bar_d = <span class="number">128</span> <span class="comment"># This is the hyperparameter of Sandwich </span></span><br><span class="line">i = np.arange(bar_d // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">pos_embs = np.concatenate([np.sin(positions / base ** (<span class="number">2</span> * i / bar_d)), </span><br><span class="line">                           np.cos(positions / base ** (<span class="number">2</span> * i / bar_d))], </span><br><span class="line">                           axis=-<span class="number">1</span>)</span><br><span class="line">sandwich = np.matmul(pos_embs, pos_embs.T) </span><br><span class="line">compression_ratio = np.arange(<span class="number">1</span>, heads + <span class="number">1</span>) * <span class="number">8</span> / heads </span><br><span class="line">multi_head_sandwich = sandwich[<span class="literal">None</span>, ...] / compression_ratio[..., <span class="literal">None</span>, <span class="literal">None</span>]</span><br></pre></td></tr></table></figure>
<h3 id="Randomized-Position"><a href="#Randomized-Position" class="headerlink" title="Randomized Position"></a>Randomized Position</h3><p><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ruoss, A., Del'etang, G., Genewein, T., Grau-Moya, J., Csordás, R., Abbana Bennani, M., Legg, S., & Veness, J. (2023). [Randomized Positional Encodings Boost Length Generalization of Transformers](https://arxiv.org/pdf/2305.16843.pdf). ACL.
">[13]</span></a></sup> introduce randomized position encoding that simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length.</p>
<p><img data-src="/notes/images/RandomPos.png" alt="Randomized Position"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">source code: https://github.com/deepmind/randomized_positional_encodings/blob/main/models/positional_encodings.py<span class="comment">#L160</span></span><br></pre></td></tr></table></figure>
<p>It allows transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average) across 15 algorithmic reasoning tasks.</p>
<p><img data-src="/notes/images/RandomPosResults.png" alt="Randomized position results."></p>
<h3 id="No-Position"><a href="#No-Position" class="headerlink" title="No Position"></a>No Position</h3><p><sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Haviv, Adi et al. “[Transformer Language Models without Positional Encodings Still Learn Positional Information.](https://arxiv.org/pdf/2203.16634.pdf)” Conference on Empirical Methods in Natural Language Processing (2022).
">[12]</span></a></sup> observe that LMs without any explicit position encoding (NoPos) are still competitive with standard transformers across datasets, model sizes, and sequence length. It shows that causal LMs might derive positional awareness not only from the explicit positioning mechanism, but also from the causal mask effects.</p>
<p><img data-src="/notes/images/NoPos.png" alt="No position"></p>
<div class="note success">
            <p>Causal transformer LM can achieve competitive results with original LMs, while the bidirectional masked LMs fail to converge. This may be because that causal LMs can learn positions from the autoregressive nature (left-to-right) but masked LMs are order-invariant.</p>
          </div>
<p><img data-src="/notes/images/NoPosResults.png" alt="Comparison results across different parameter sizes."></p>
<div class="note warning">
            <p><strong>NB</strong>: LMs without explicit positional encodings (NoPos) are always slightly worse, suggesting the importance of inductive positional bias.</p>
          </div>
<h3 id="Position-Interpolation"><a href="#Position-Interpolation" class="headerlink" title="Position Interpolation"></a>Position Interpolation</h3><p>Instead of extrapolation, <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Shouyuan, Sherman Wong, Liangjian Chen and Yuandong Tian. [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf). ArXiv abs/2306.15595 (2023).
">[15]</span></a></sup><sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Github discussion: Position Interpolation](https://github.com/ggerganov/llama.cpp/discussions/1965)
">[16]</span></a></sup><sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Reddit: A simple way to "Extending Context to 8K"](https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/)
">[17]</span></a></sup><sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Things I’m Learning While Training SuperHOT](https://kaiokendev.github.io/til#extending-context-to-8k)
">[18]</span></a></sup> presents position interpolation (PI) that directly downscales the non-integer position indices (RoPE-based) so that the maximum position index matches the previous context window limit in pre-training.</p>
<p><img data-src="/notes/images/Position Interpolation.png" alt="Position Interpolation"></p>
<p>It simply adds two lines of code.<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Things I’m Learning While Training SuperHOT](https://kaiokendev.github.io/til#extending-context-to-8k)
">[18]</span></a></sup><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">        </span><br><span class="line">        max_position_embeddings = <span class="number">8192</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(</span><br><span class="line">            self.max_seq_len_cached,</span><br><span class="line">            device=self.inv_freq.device,</span><br><span class="line">            dtype=self.inv_freq.dtype,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># These two lines:</span></span><br><span class="line">        self.scale = <span class="number">1</span> / <span class="number">4</span></span><br><span class="line">        t *= self.scale</span><br></pre></td></tr></table></figure></p>
<p>SuperHOT-13B<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Reddit: A simple way to "Extending Context to 8K"](https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/)
">[17]</span></a></sup> uptrained on scaling factor of 0.25, compared to base LLaMa 13B and a test LoRA trained on 6K sequence length with no scaling.</p>
<p><img data-src="/notes/images/PI-comp.png" alt="PPL evaluation"></p>
<div class="note warning">
            <p><strong>Note</strong> that PI requires further fine-tuning to take effects for length extrapolation.</p>
          </div>
<h3 id="NTK-Aware-Scaled-RoPE"><a href="#NTK-Aware-Scaled-RoPE" class="headerlink" title="NTK-Aware Scaled RoPE"></a>NTK-Aware Scaled RoPE</h3><div class="note danger">
            <p><strong>Background</strong></p><ol><li>“Simply interpolating the RoPE’s fourier space “linearly” is very sub-optimal, as it prevents the network to distinguish the order and positions of tokens that are very close by.”<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-Aware Scaled RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)">[19]</span></a></sup></li><li>“Scaling down the fourier features too much will eventually even prevent succesful finetunes (this is corroborated by the recent paper by Meta<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Shouyuan, Sherman Wong, Liangjian Chen and Yuandong Tian. [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf). ArXiv abs/2306.15595 (2023).">[15]</span></a></sup> that suggests an upper bound of ~600x)”<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-Aware Scaled RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)">[19]</span></a></sup></li></ol>
          </div>
<p>NTK-Aware Scaled RoPE<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-Aware Scaled RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
">[19]</span></a></sup><sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[RoPE is a β-ary encoding (Chinese)](https://spaces.ac.cn/archives/9675)
">[20]</span></a></sup> designs a nonlinear interpolation scheme using Neural Tangent Kernel (NTK) theory. It changes the base of the RoPE instead of the scale, which intuitively changes the “spinning” speed from which each of the RoPE’s dimension vectors shifts to the next.</p>
<p>Implementation <sup id="fnref:21"><a href="#fn:21" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NTK-aware RoPE colab](https://colab.research.google.com/drive/1VI2nhlyKvd5cw4-zHvAIk00cAVj2lCCC#scrollTo=b80b3f37)
">[21]</span></a></sup><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"></span><br><span class="line">old_init = transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.__init__</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ntk_scaled_init</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#The method is just these three lines</span></span><br><span class="line">    max_position_embeddings = <span class="number">16384</span></span><br><span class="line">    a = <span class="number">8</span> <span class="comment"># Alpha value</span></span><br><span class="line">    base = base * a ** (dim / (dim-<span class="number">2</span>)) <span class="comment">#Base change formula</span></span><br><span class="line"></span><br><span class="line">    old_init(self, dim, max_position_embeddings, base, device)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># apply ntk-sclaed init patch</span></span><br><span class="line">transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.__init__ = ntk_scaled_init</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>Average perplexity of LLaMA-7B on a set of 40 very long prompts (12k+ context size).</p>
<p><img data-src="/notes/images/ntk-rope-trend.png" alt="PI-comparison" width="70%"/></p>
<h3 id="Dynamic-Linear-RoPE"><a href="#Dynamic-Linear-RoPE" class="headerlink" title="Dynamic Linear RoPE"></a>Dynamic Linear RoPE</h3><p><em>Dynamic linear RoPE</em>: set the <code>scale</code> to <code>max_seq_len/current position length</code>, which can slowly increase the scale.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaLinearScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, scale=<span class="number">1</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)</span><br><span class="line">        t /= self.scale</span><br><span class="line">        freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">        <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">        dtype = torch.get_default_dtype()</span><br><span class="line">        self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_len=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># x: [bs, num_attention_heads, seq_len, head_size]</span></span><br><span class="line">        <span class="comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; self.max_seq_len_cached:</span><br><span class="line">            self.max_seq_len_cached = seq_len</span><br><span class="line">            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">            t /= self.scale</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Dynamic-NTK-Aware-Scaled-RoPE"><a href="#Dynamic-NTK-Aware-Scaled-RoPE" class="headerlink" title="Dynamic NTK-Aware Scaled RoPE"></a>Dynamic NTK-Aware Scaled RoPE</h3><div class="note warning">
            <p><em>Cons</em>: </p><ul><li>Compared to dynamic linear scaling, NTK-Aware has higher perplexity for shorter sequences, but better perplexity at the tail end of the sequence lengths. </li><li>NTK-aware RoPE suffers from catastrophic perplexity blowup, like regular RoPE and static linear scaling.</li></ul>
          </div>
<p><sup id="fnref:22"><a href="#fn:22" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Dynamic NTK-aware RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)
">[22]</span></a></sup> introduces dynamic NTK-aware scaling. The scaling of $\alpha$ is set to:</p>
<script type="math/tex; mode=display">(\alpha * \text{current sequence length} / \text{original model context length} ) - (\alpha -1)</script><p>This dynamically scales the $\alpha$ as the sequence length increases.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaDynamicScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, ntk=<span class="literal">False</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ntk = ntk</span><br><span class="line">        self.base = base</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)</span><br><span class="line">        freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">        <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">        dtype = torch.get_default_dtype()</span><br><span class="line">        self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_len=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># x: [bs, num_attention_heads, seq_len, head_size]</span></span><br><span class="line">        <span class="comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; self.max_seq_len_cached:</span><br><span class="line">            self.max_seq_len_cached = seq_len</span><br><span class="line">            <span class="keyword">if</span> self.ntk: <span class="comment">### dynamic NTK</span></span><br><span class="line">                base = self.base * ((self.ntk * seq_len / self.max_position_embeddings) - (self.ntk - <span class="number">1</span>)) ** (self.dim / (self.dim-<span class="number">2</span>))</span><br><span class="line">                inv_freq = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, self.dim, <span class="number">2</span>).<span class="built_in">float</span>().to(x.device) / self.dim))</span><br><span class="line">                self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.ntk:</span><br><span class="line">                t *= self.max_position_embeddings / seq_len</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/Dynamic-NTK-RoPE.png" alt="Dynamic NTK RoPE&lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;[Dynamic NTK-aware RoPE](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)
&quot;&gt;[22]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<h4 id="Partial-NTK-Scaled-RoPE"><a href="#Partial-NTK-Scaled-RoPE" class="headerlink" title="Partial NTK Scaled RoPE"></a>Partial NTK Scaled RoPE</h4><p>Combine RoPE, Linear, NTK.<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: Dynamic RoPE.](https://github.com/jquesnelle/scaled-rope/tree/master/scaled_rope)
">[23]</span></a></sup></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_correction_factor</span>(<span class="params">num_rotations, dim, base=<span class="number">10000</span>, max_position_embeddings=<span class="number">2048</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> (dim * math.log(max_position_embeddings/(num_rotations * <span class="number">2</span> * math.pi)))/(<span class="number">2</span> * math.log(base)) <span class="comment">#Inverse dim formula to find number of rotations</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_correction_range</span>(<span class="params">low_rot, high_rot, dim, base=<span class="number">10000</span>, max_position_embeddings=<span class="number">2048</span></span>):</span></span><br><span class="line">    low = math.floor(find_correction_factor(low_rot, dim, base, max_position_embeddings))</span><br><span class="line">    high = math.ceil(find_correction_factor(high_rot, dim, base, max_position_embeddings))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(low, <span class="number">0</span>), <span class="built_in">min</span>(high, dim-<span class="number">1</span>) <span class="comment">#Clamp values just in case</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_ramp_mask</span>(<span class="params"><span class="built_in">min</span>, <span class="built_in">max</span>, dim</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">min</span> == <span class="built_in">max</span>:</span><br><span class="line">        <span class="built_in">max</span> += <span class="number">0.001</span> <span class="comment">#Prevent singularity</span></span><br><span class="line"></span><br><span class="line">    linear_func = (torch.arange(dim, dtype=torch.float32) - <span class="built_in">min</span>) / (<span class="built_in">max</span> - <span class="built_in">min</span>)</span><br><span class="line">    ramp_func = torch.clamp(linear_func, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ramp_func</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_newbase_ntk</span>(<span class="params">dim, base=<span class="number">10000</span>, scale=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> base * scale ** (dim / (dim-<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LlamaPartNTKScaledRotaryEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, max_position_embeddings=<span class="number">2048</span>, base=<span class="number">10000</span>, scale=<span class="number">1</span>, ntk_factor=<span class="number">1</span>, extrapolation_factor=<span class="number">1</span>, original_max_position_embeddings=<span class="number">2048</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#Interpolation constants found experimentally for LLaMA (might not be totally optimal though)</span></span><br><span class="line">        <span class="comment">#Do not change unless there is a good reason for doing so!</span></span><br><span class="line">        beta_0 = <span class="number">1.25</span></span><br><span class="line">        beta_1 = <span class="number">0.75</span></span><br><span class="line">        gamma_0 = <span class="number">16</span></span><br><span class="line">        gamma_1 = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#Three RoPE extrapolation/interpolation methods</span></span><br><span class="line">        inv_freq_base = <span class="number">1.0</span> / (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line">        inv_freq_linear = <span class="number">1.0</span> / (scale * (base ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim)))</span><br><span class="line">        inv_freq_ntk = <span class="number">1.0</span> / (find_newbase_ntk(dim, base, scale) ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>).<span class="built_in">float</span>().to(device) / dim))</span><br><span class="line"></span><br><span class="line">        current_dtype = inv_freq_ntk.dtype</span><br><span class="line">        current_device = inv_freq_ntk.device</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#Combine NTK and Linear</span></span><br><span class="line">        low, high = find_correction_range(beta_0, beta_1, dim, base, original_max_position_embeddings)</span><br><span class="line">        inv_freq_mask = (<span class="number">1</span> - linear_ramp_mask(low, high, dim // <span class="number">2</span>).<span class="built_in">type</span>(current_dtype).to(current_device)) * ntk_factor</span><br><span class="line">        inv_freq = inv_freq_linear * (<span class="number">1</span> - inv_freq_mask) + inv_freq_ntk * inv_freq_mask</span><br><span class="line">    </span><br><span class="line">        <span class="comment">#Combine Extrapolation and NTK and Linear</span></span><br><span class="line">        low, high = find_correction_range(gamma_0, gamma_1, dim, base, original_max_position_embeddings)</span><br><span class="line">        inv_freq_mask = (<span class="number">1</span> - linear_ramp_mask(low, high, dim // <span class="number">2</span>).<span class="built_in">type</span>(current_dtype).to(current_device)) * extrapolation_factor</span><br><span class="line">        inv_freq = inv_freq * (<span class="number">1</span> - inv_freq_mask) + inv_freq_base * inv_freq_mask</span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build here to make `torch.jit.trace` work.</span></span><br><span class="line">        self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)</span><br><span class="line">        freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">        <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">        emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>)</span><br><span class="line">        dtype = torch.get_default_dtype()</span><br><span class="line">        self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(dtype), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, seq_len=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># x: [bs, num_attention_heads, seq_len, head_size]</span></span><br><span class="line">        <span class="comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &gt; self.max_seq_len_cached:</span><br><span class="line">            self.max_seq_len_cached = seq_len</span><br><span class="line">            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">            freqs = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, t, self.inv_freq)</span><br><span class="line">            <span class="comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span></span><br><span class="line">            emb = torch.cat((freqs, freqs), dim=-<span class="number">1</span>).to(x.device)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;cos_cached&quot;</span>, emb.cos()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">            self.register_buffer(<span class="string">&quot;sin_cached&quot;</span>, emb.sin()[<span class="literal">None</span>, <span class="literal">None</span>, :, :].to(x.dtype), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="beta-ary-RoPE"><a href="#beta-ary-RoPE" class="headerlink" title="$\beta$-ary RoPE"></a>$\beta$-ary RoPE</h4><p><sup id="fnref:24"><a href="#fn:24" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[β-ary RoPE (in Chinese)](https://kexue.fm/archives/9706)
">[24]</span></a></sup></p>
<h4 id="ReRoPE"><a href="#ReRoPE" class="headerlink" title="ReRoPE"></a>ReRoPE</h4><p><sup id="fnref:25"><a href="#fn:25" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ReROPE (in Chinese)](https://kexue.fm/archives/9708)">[25]</span></a></sup></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you Need</a>.” NeurIPS (2017).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, Ofir, Noah A. Smith and Mike Lewis. “<a href="https://arxiv.org/pdf/2108.12409.pdf">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.</a>” ICLR 2022.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Su, Jianlin, Yu Lu, Shengfeng Pan, Bo Wen and Yunfeng Liu. “<a href="https://arxiv.org/pdf/2104.09864.pdf">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>.” ArXiv abs/2104.09864 (2021).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.eleuther.ai/rotary-embeddings/">Rotary Embeddings: A Relative Revolution</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kexue.fm/archives/8265">RoFormer blog (Chinese)- Transformer升级之路：2、博采众长的旋转式位置编码</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kexue.fm/archives/9577">Blog- Bias项的神奇作用：RoPE + Bias = 更好的长度外推性</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raffel, C., Shazeer, N.M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P.J. (2020). <a href="https://jmlr.org/papers/volume21/20-074/20-074.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. JMLR.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, O., Smith, N.A., &amp; Lewis, M. (2022). <a href="https://arxiv.org/pdf/2108.12409.pdf">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>. ICLR.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chi, T., Fan, T., Ramadge, P.J., &amp; Rudnicky, A.I. (2022). <a href="https://arxiv.org/pdf/2205.09921.pdf">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation</a>. NeurIPS.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., &amp; Wei, F. (2022). <a href="https://arxiv.org/pdf/2212.10554.pdf">A Length-Extrapolatable Transformer</a>. ArXiv, abs/2212.10554.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chi, T., Fan, T., Rudnicky, A., &amp; Ramadge, P.J. (2022). <a href="https://arxiv.org/pdf/2212.10356.pdf">Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis</a>.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Haviv, Adi et al. “<a href="https://arxiv.org/pdf/2203.16634.pdf">Transformer Language Models without Positional Encodings Still Learn Positional Information.</a>” Conference on Empirical Methods in Natural Language Processing (2022).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ruoss, A., Del'etang, G., Genewein, T., Grau-Moya, J., Csordás, R., Abbana Bennani, M., Legg, S., &amp; Veness, J. (2023). <a href="https://arxiv.org/pdf/2305.16843.pdf">Randomized Positional Encodings Boost Length Generalization of Transformers</a>. ACL.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kexue.fm/archives/9444">Blog: Transformer升级之路：8、长度外推性与位置鲁棒性</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, Shouyuan, Sherman Wong, Liangjian Chen and Yuandong Tian. <a href="https://arxiv.org/pdf/2306.15595.pdf">Extending Context Window of Large Language Models via Positional Interpolation</a>. ArXiv abs/2306.15595 (2023).<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/ggerganov/llama.cpp/discussions/1965">Github discussion: Position Interpolation</a><a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k/">Reddit: A simple way to &quot;Extending Context to 8K&quot;</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kaiokendev.github.io/til#extending-context-to-8k">Things I’m Learning While Training SuperHOT</a><a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-Aware Scaled RoPE</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://spaces.ac.cn/archives/9675">RoPE is a β-ary encoding (Chinese)</a><a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://colab.research.google.com/drive/1VI2nhlyKvd5cw4-zHvAIk00cAVj2lCCC#scrollTo=b80b3f37">NTK-aware RoPE colab</a><a href="#fnref:21" rev="footnote"> ↩</a></span></li><li id="fn:22"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/">Dynamic NTK-aware RoPE</a><a href="#fnref:22" rev="footnote"> ↩</a></span></li><li id="fn:23"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/jquesnelle/scaled-rope/tree/master/scaled_rope">GitHub: Dynamic RoPE.</a><a href="#fnref:23" rev="footnote"> ↩</a></span></li><li id="fn:24"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kexue.fm/archives/9706">β-ary RoPE (in Chinese)</a><a href="#fnref:24" rev="footnote"> ↩</a></span></li><li id="fn:25"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">25.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kexue.fm/archives/9708">ReROPE (in Chinese)</a><a href="#fnref:25" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell Command Notes</title>
    <url>/notes/2020/05/12/Shell-Command-Notes/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A summary of helpful bash command sheets.<br><span id="more"></span></p>
<h1 id="Check"><a href="#Check" class="headerlink" title="Check"></a>Check</h1><ol>
<li>Check shell choices<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /etc/shells</span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/bin/zsh</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure></li>
<li>Check current shell path<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="variable">$SHELL</span></span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Assign values</span></span><br><span class="line">variable=`<span class="built_in">command</span>`</span><br><span class="line">variable=$(<span class="built_in">command</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># readonly</span></span><br><span class="line">myUrl=<span class="string">&quot;http://xxx&quot;</span></span><br><span class="line"><span class="built_in">readonly</span> myUrl</span><br><span class="line">myUrl=<span class="string">&quot;http://xxx&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete variable</span></span><br><span class="line"><span class="built_in">unset</span> variable_name</span><br></pre></td></tr></table></figure>
<h2 id="Special-Variable"><a href="#Special-Variable" class="headerlink" title="Special Variable"></a>Special Variable</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check pid</span></span><br><span class="line">$ <span class="built_in">echo</span> $$</span><br><span class="line">216240 </span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>Variable</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>$0</td>
<td>script name</td>
</tr>
<tr>
<td>$n</td>
<td>the n-th parameter to scripts or functions</td>
</tr>
<tr>
<td>$#</td>
<td># of parameters</td>
</tr>
<tr>
<td>$*</td>
<td>all parameters</td>
</tr>
<tr>
<td>$@</td>
<td>all parameters</td>
</tr>
<tr>
<td>$?</td>
<td>Exit status of previous command (success 0, fail 1)</td>
</tr>
<tr>
<td>$$</td>
<td>Current process id.</td>
</tr>
</tbody>
</table>
</div>
<div class="note info">
            <ul><li>“$*”: “$1 $2 … $n” (together)</li><li>“$@”: “$1” “$2” … “$n” (seperate)</li></ul>
          </div>
<h2 id="Shell-replacement"><a href="#Shell-replacement" class="headerlink" title="Shell replacement"></a>Shell replacement</h2><ol>
<li><p>Escape character</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -e <span class="string">&quot;Value of a is <span class="variable">$a</span> \n&quot;</span></span><br><span class="line">Value of a is 10\n</span><br></pre></td></tr></table></figure>
</li>
<li><p>Command replacement<br>First run command, cache the result, and output.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">`<span class="built_in">command</span>`</span><br><span class="line"></span><br><span class="line">$ USERS=`who | wc -l`</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;Logged in user are <span class="variable">$USERS</span>&quot;</span></span><br><span class="line">Logged <span class="keyword">in</span> user are 1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Variable-replacement"><a href="#Variable-replacement" class="headerlink" title="Variable replacement"></a>Variable replacement</h2><div class="table-container">
<table>
<thead>
<tr>
<th>format</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>${var}</td>
<td>variable value</td>
</tr>
<tr>
<td>${var:-word}</td>
<td>if var is empty/unset, return word (donot change var)</td>
</tr>
<tr>
<td>${var:=word}</td>
<td>if var is empty/unset, return word (set var to word)</td>
</tr>
<tr>
<td>${var:?message}</td>
<td>if var is empty/unset, return message to stderr. Used to check whether variable can be assigned values (Exit script)</td>
</tr>
<tr>
<td>${var:+word}</td>
<td>if var is defined, return word (donot change var)</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h1><p>Math<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">val=`expr 2 + 2` <span class="comment"># -&gt; 4</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Comp-op"><a href="#Comp-op" class="headerlink" title="Comp op"></a>Comp op</h2><p>Only support numbers, not string. Except that the string is number.</p>
<ul>
<li><code>-eq</code>: equal to</li>
<li><code>-ne</code>: not equal to</li>
<li><code>-gt</code>: greater than</li>
<li><code>-lt</code>: less than</li>
<li><code>-ge</code>: greater or equal to</li>
<li><code>-le</code>: less or equal to</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Op</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>-eq</td>
<td>if ==, return true</td>
<td>[ $a -eq $b ]</td>
</tr>
<tr>
<td>-ne</td>
<td>if !=, return true</td>
<td>[ $a -ne $b ]</td>
</tr>
<tr>
<td>-gt</td>
<td>if &gt;, return true</td>
<td>[ $a -gt $b ]</td>
</tr>
<tr>
<td>-lt</td>
<td>if &lt;, return true</td>
<td>[ $a -lt $b ]</td>
</tr>
<tr>
<td>-ge</td>
<td>if &gt;=, return true</td>
<td>[ $a -ge $b ]</td>
</tr>
<tr>
<td>-le</td>
<td>if &lt;=, return true</td>
<td>[ $a -le $b ]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> -eq <span class="variable">$b</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -eq <span class="variable">$b</span> : a is equal to b&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$a</span> -eq <span class="variable">$b</span>: a is not equal to b&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<h2 id="Bool-op"><a href="#Bool-op" class="headerlink" title="Bool op"></a>Bool op</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Op</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>!</td>
<td>not</td>
<td>[ !false], return true</td>
</tr>
<tr>
<td>-o</td>
<td>or</td>
<td>[ $a -lt 20 -o $b -gt 100 ]</td>
</tr>
<tr>
<td>-a</td>
<td>and</td>
<td>[ $a -lt 20 -a $b -gt 100 ]</td>
</tr>
</tbody>
</table>
</div>
<h2 id="String-op"><a href="#String-op" class="headerlink" title="String op"></a>String op</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Op</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>=</td>
<td>string equal</td>
<td>[ $a = $b ], return false</td>
</tr>
<tr>
<td>!=</td>
<td>string inequal</td>
<td>[ $a != $b ], return true</td>
</tr>
<tr>
<td>-z</td>
<td>len(str)!=0</td>
<td>[ -z $a ]</td>
</tr>
<tr>
<td>str</td>
<td>check if empty</td>
<td>[$a] check if empty</td>
</tr>
</tbody>
</table>
</div>
<h2 id="File-test-op"><a href="#File-test-op" class="headerlink" title="File test op"></a>File test op</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Op</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>-b file</td>
<td>check if it is a block device file</td>
<td>[-b $file]</td>
</tr>
<tr>
<td>-c file</td>
<td>check if it is a char device file</td>
<td>[-c $file]</td>
</tr>
<tr>
<td>-d file</td>
<td>check if it is dir</td>
<td>[-d $file]</td>
</tr>
<tr>
<td>-r file</td>
<td>check if it is readable</td>
<td>[-r $file]</td>
</tr>
<tr>
<td>-w file</td>
<td>check if it is writable</td>
<td>[-w $file]</td>
</tr>
<tr>
<td>-x file</td>
<td>check if it is executable</td>
<td>[-x $file]</td>
</tr>
<tr>
<td>-s file</td>
<td>check if file is empty</td>
<td>[-s $file]</td>
</tr>
<tr>
<td>-e file</td>
<td>check if file(incl. dir) exist</td>
<td>[-e $file]</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">file=<span class="string">&quot;/var/www/tutorialspoint/unix/test.sh&quot;</span></span><br><span class="line"><span class="keyword">if</span> [ -r <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File has read access&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File does not have read access&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -w <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File has write permission&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File does not have write permission&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -x <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File has execute permission&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File does not have execute permission&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File is an ordinary file&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;This is sepcial file&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -d <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File is a directory&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;This is not a directory&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -s <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File size is zero&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File size is not zero&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -e <span class="variable">$file</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File exists&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="string">&quot;File does not exist&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<h1 id="String"><a href="#String" class="headerlink" title="String"></a>String</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. get string length</span></span><br><span class="line">string=<span class="string">&quot;abcd&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#string&#125;</span> <span class="comment">#输出 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. get substring</span></span><br><span class="line">string=<span class="string">&quot;alibaba is a great company&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;string:1:4&#125;</span> <span class="comment">#输出liba</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. search substring</span></span><br><span class="line">string=<span class="string">&quot;alibaba is a great company&quot;</span></span><br><span class="line"><span class="built_in">echo</span> `expr index <span class="string">&quot;<span class="variable">$string</span>&quot;</span> is`</span><br></pre></td></tr></table></figure>
<h1 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h1><ol>
<li><p>Define array</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">array_name=(value0 value1 value2 value3)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">array_name[0]=value0</span><br><span class="line">array_name[1]=value1</span><br><span class="line">array_name[2]=value2</span><br></pre></td></tr></table></figure>
</li>
<li><p>Read array</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$&#123;arrray_name[index]&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get all elements in the array</span></span><br><span class="line"><span class="variable">$&#123;array_name[*]&#125;</span></span><br><span class="line"><span class="variable">$&#123;array_name[@]&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Get array length</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get the # of elements</span></span><br><span class="line">length=<span class="variable">$&#123;#array_name[@]&#125;</span></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">length=<span class="variable">$&#123;#array_name[*]&#125;</span></span><br><span class="line"><span class="comment"># get the length of single element</span></span><br><span class="line">length_n=<span class="variable">$&#123;#array_name[n]&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Condition"><a href="#Condition" class="headerlink" title="Condition"></a>Condition</h1><ol>
<li><p>If else</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ `expression` ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   <span class="comment"># Statement(s) to be executed if expression is true</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>If .. elseif … fi</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ expression ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> expression is <span class="literal">true</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> expression is not <span class="literal">true</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>If … elseif … fi</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ expression 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> expression 1 is <span class="literal">true</span></span><br><span class="line"><span class="keyword">elif</span> [ expression 2 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> expression 2 is <span class="literal">true</span></span><br><span class="line"><span class="keyword">elif</span> [ expression 3 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> expression 3 is <span class="literal">true</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> no expression is <span class="literal">true</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>In one line</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test is used too check the condition is true or not, silimar to ([])</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> $[2*3] -eq $[1+5]; <span class="keyword">then</span> <span class="built_in">echo</span> <span class="string">&#x27;The two numbers are equal!&#x27;</span>; <span class="keyword">fi</span>;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Case"><a href="#Case" class="headerlink" title="Case"></a>Case</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> 值 <span class="keyword">in</span></span><br><span class="line">模式1)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    command3</span><br><span class="line">    ;; <span class="comment"># break</span></span><br><span class="line">模式2）</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    command3</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    command3</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>
<p>Example:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">option=<span class="string">&quot;<span class="variable">$&#123;1&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$&#123;option&#125;</span> <span class="keyword">in</span></span><br><span class="line">   -f) FILE=<span class="string">&quot;<span class="variable">$&#123;2&#125;</span>&quot;</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;File name is <span class="variable">$FILE</span>&quot;</span></span><br><span class="line">      ;;</span><br><span class="line">   -d) DIR=<span class="string">&quot;<span class="variable">$&#123;2&#125;</span>&quot;</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;Dir name is <span class="variable">$DIR</span>&quot;</span></span><br><span class="line">      ;;</span><br><span class="line">   *) </span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;`basename <span class="variable">$&#123;0&#125;</span>`:usage: [-f file] | [-d directory]&quot;</span></span><br><span class="line">      <span class="built_in">exit</span> 1 <span class="comment"># Command to come out of the program with status 1</span></span><br><span class="line">      ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure></p>
<p>Result:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$./test.sh</span><br><span class="line">test.sh: usage: [ -f filename ] | [ -d directory ]</span><br><span class="line">$ ./test.sh -f index.htm</span><br><span class="line">$ vi test.sh</span><br><span class="line">$ ./test.sh -f index.htm</span><br><span class="line">File name is index.htm</span><br><span class="line">$ ./test.sh -d unix</span><br><span class="line">Dir name is unix</span><br><span class="line">$</span><br></pre></td></tr></table></figure></p>
<h1 id="For-loop"><a href="#For-loop" class="headerlink" title="For loop"></a>For loop</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> 变量 <span class="keyword">in</span> 列表</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h1 id="While-loop"><a href="#While-loop" class="headerlink" title="While loop"></a>While loop</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="built_in">command</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">   Statement(s) to be executed <span class="keyword">if</span> <span class="built_in">command</span> is <span class="literal">true</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h1 id="Until-loop"><a href="#Until-loop" class="headerlink" title="Until loop"></a>Until loop</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">until <span class="built_in">command</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">   Statement(s) to be executed until <span class="built_in">command</span> is <span class="literal">true</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h1 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">function_name</span></span> () &#123;</span><br><span class="line">    list of commands</span><br><span class="line">    <span class="variable">$1</span> <span class="comment"># the first param</span></span><br><span class="line">    <span class="variable">$2</span> <span class="comment"># the second param</span></span><br><span class="line">    [ <span class="built_in">return</span> value ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Output-redirection"><a href="#Output-redirection" class="headerlink" title="Output redirection"></a>Output redirection</h1><ul>
<li><code>stdin</code>: 0, default read from stdin.</li>
<li><code>stdout</code>: 1, default output to stdout.</li>
<li><code>stderr</code>: 2, default write error message to stderr.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># output to file</span></span><br><span class="line"><span class="built_in">command</span> &gt; file</span><br><span class="line"></span><br><span class="line"><span class="comment"># error redirect to file</span></span><br><span class="line"><span class="built_in">command</span>  2 &gt; file</span><br><span class="line"></span><br><span class="line"><span class="comment"># redirect stdout and stdout</span></span><br><span class="line"><span class="built_in">command</span> &gt; file 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="comment"># donot display stdout / stderr</span></span><br><span class="line"><span class="built_in">command</span> &gt; /dev/null</span><br></pre></td></tr></table></figure>
<h1 id="File-Operation"><a href="#File-Operation" class="headerlink" title="File Operation"></a>File Operation</h1><h2 id="Compress"><a href="#Compress" class="headerlink" title="Compress"></a>Compress</h2><h3 id="Extract-compressed-data"><a href="#Extract-compressed-data" class="headerlink" title="Extract compressed data"></a>Extract compressed data</h3><h4 id="unzip"><a href="#unzip" class="headerlink" title="unzip"></a>unzip</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">unzip &lt;file&gt;.zip</span><br><span class="line">    -d &lt;exdir-path&gt; <span class="comment"># extract files into exdir, (default: current dir if not specified -d.)</span></span><br><span class="line">    -l <span class="comment"># list files</span></span><br><span class="line">    -P <span class="comment"># &quot;extract passwd&quot;</span></span><br><span class="line">    -t <span class="comment"># test compressed archive data</span></span><br><span class="line">    -v <span class="comment"># list verbosely / show version info</span></span><br><span class="line">    -o <span class="comment"># overwrite WITHOUT prompting</span></span><br><span class="line">    -n <span class="comment"># never overwrite existing files</span></span><br></pre></td></tr></table></figure>
<h1 id="Set-envrionment"><a href="#Set-envrionment" class="headerlink" title="Set envrionment"></a>Set envrionment</h1><p>Set command is used to set and unset certain flags or settings within the sell environment.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> [option]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>-a</code>: It is used to mark variables that are modified or created for export.</li>
<li><code>-b</code>: It is used to notify of job termination immediately.</li>
<li><code>-e</code>: It is used to exit immediately if a command exits with a non-zero status.</li>
<li><code>-f</code>: It is used to disable the file name generation (globbing).</li>
<li><code>-h</code>: It is used to save the location of commands where they looked up.</li>
<li><code>-k</code>: It is used to place all assignment arguments in the environment variable of a command, except those that precede the command name.</li>
<li><code>-m</code>: It is used to enable Job control.</li>
<li><code>-n</code>: It is used to read commands.</li>
<li><code>-o</code>: It is used for option-name.</li>
<li><code>-p</code>: It is used to disable the processing of the ‘$ENV’ file and import shell functions. It is turned on whenever the real and effective user ids do not match. Turning off this option may cause the working uid and gid to be set as the authorized uid and gid.</li>
<li><code>-t</code>: It is used to exit from the command after executing one command.</li>
<li><code>-u</code>: It is used to treat unset variables as an error when substituting.</li>
<li><code>-v</code>: It is used to print shell input lines.</li>
<li><code>-x</code>: It is used to print commands and their arguments in a sequential way (as they are executed).</li>
<li><code>-B</code>: It is used to perform brace expansion by the Shell.</li>
<li><code>-C</code>: It is used to disallow existing regular files to be overwritten by redirection of output.</li>
<li><code>-E</code>: It is used if the ERR trap is inherited by the shell functions.</li>
<li><code>-H</code>: It is used to enable style history substitution. By default, it is on when the shell is interactive.</li>
<li><code>-P</code>: It is used if we do not want to follow symbolic links when executing commands.</li>
<li><code>-T</code>: If this flag is set, the DEBUG trap is inherited by the shell functions.</li>
</ul>
<h3 id="Compress-1"><a href="#Compress-1" class="headerlink" title="Compress"></a>Compress</h3><h2 id="File-info"><a href="#File-info" class="headerlink" title="File info"></a>File info</h2><p>Check the size of current directories:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># output all file and directories in the current dir, and sort</span></span><br><span class="line">du -h -d 1 | sort -h</span><br><span class="line"></span><br><span class="line"><span class="comment"># show the size of current subfiles</span></span><br><span class="line">du -h -d 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># show the total size of current dir</span></span><br><span class="line">du -sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># show the size of all subdirectoies (recursive to the max depth)</span></span><br><span class="line">du -h</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Subword Tokenization in Natural Language Processing</title>
    <url>/notes/2021/11/29/Subword-Tokenization-in-NLP/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Summary of word tokenization in natural language processing.</p>
<span id="more"></span>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>The table summarizes the difference between various tokenization approaches.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Tokenization</th>
<th style="text-align:left">Methods</th>
<th style="text-align:left">Pros</th>
<th style="text-align:left">Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">word-based</td>
<td style="text-align:left">1.Space and punctuation tokenization;<br>2.Rule-based tokenization.</td>
<td style="text-align:left">Easy to use.</td>
<td style="text-align:left">1. Very large vocabularies.<br>2. OOV problem.<br>3. Loss of meanings across very similar words.</td>
</tr>
<tr>
<td style="text-align:center">char-based</td>
<td style="text-align:left">Splitting into chars</td>
<td style="text-align:left">1.Slimmer vocabularies.<br>2.Mostly open-vocabulary: fewer OOV words.</td>
<td style="text-align:left">1. Very long sequence.<br>2. Less meaningful individual tokens</td>
</tr>
<tr>
<td style="text-align:center">subword-based</td>
<td style="text-align:left">WordPiece <br>BPE<br>Unigram</td>
<td style="text-align:left">1.Good balance the vocabulary size and the sequence length;<br>2.Help identify similar syntactic or semantic situations in texts;<br>3.Can identify start of word tokens.</td>
<td style="text-align:left">Need training additional subword tokenizer.</td>
</tr>
</tbody>
</table>
</div>
<div class="note info">
            <p>Why subword?</p><ul><li><strong>Subword-based tokenization</strong> lies between character- and word-based tokenization, which arises from the idea that:<blockquote><p><strong>Frequently used words</strong> should not be split into smaller subwords;<br><strong>Rare words</strong> should be decomposed into meaningful subwords.</p></blockquote></li><li>Subwords help identify <em>similar syntactic or semantic situations</em> in texts, such as same prefix or sufix.</li><li>Subword tokenization can identify <em>start of word tokens</em>, such as “##” in WordPiece (BERT).</li></ul>
          </div>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><div class="note info">
            <p>It can be seen from the table that:</p><ul><li>OpenAI and Facebook favor BPE tokenization whereas Google prefers self-proposed WordPiece and Unigram methods. ;)</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">Tokenization</th><th style="text-align:center">#Vocab</th><th style="text-align:center">Corpus</th><th style="text-align:center">Org.</th></tr></thead><tbody><tr><td style="text-align:center">GPT</td><td style="text-align:center">BPE [Spacy/ftfy pre-tokenizer]</td><td style="text-align:center">40,478</td><td style="text-align:center">BooksCorpus</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-2</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">WebText (40GB)</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-3</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">Common Crawl, WebText2, Books1/2, Wikipedia</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-4</td><td style="text-align:center">BBPE</td><td style="text-align:center">100,256</td><td style="text-align:center">Public corpus, third-party data</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">GPT-4o</td><td style="text-align:center">BBPE</td><td style="text-align:center">200k</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">Gemini</td><td style="text-align:center">BPE</td><td style="text-align:center">256k</td><td style="text-align:center">A large sample of the entire training corpus.</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">code-davinci-001/002</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,281</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">text-davinci-003</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,281</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">gpt-3.5-turbo</td><td style="text-align:center">BBPE</td><td style="text-align:center">100,256</td><td style="text-align:center">-</td><td style="text-align:center">OpenAI</td></tr><tr><td style="text-align:center">RoBERTa</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Facebook</td></tr><tr><td style="text-align:center">BART</td><td style="text-align:center">BBPE</td><td style="text-align:center">50,257</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Facebook</td></tr><tr><td style="text-align:center">BERT</td><td style="text-align:center">WordPiece (30k)</td><td style="text-align:center">30k</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">T5</td><td style="text-align:center">WordPiece (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">C4</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">XLNet</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">BooksCorpus, enwiki, Giga5,  ClueWeb 201-B, Common Crawl</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">ELECTRA</td><td style="text-align:center">WordPiece</td><td style="text-align:center">30k</td><td style="text-align:center">base: same as BERT; large: same as XLNet</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">ALBERT</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">30k</td><td style="text-align:center">BooksCorpus, enwiki</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Gopher</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">MassiveText</td><td style="text-align:center">DeepMind</td></tr><tr><td style="text-align:center">Chinchilla</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">MassiveText</td><td style="text-align:center">DeepMind</td></tr><tr><td style="text-align:center">PaLM</td><td style="text-align:center">Unigram (spm)</td><td style="text-align:center">256k</td><td style="text-align:center">dataset from LamDA, GLaM, and code</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">LaMDA</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">2.97B documents, 1.12B dialogs,  and 13.39B dialog utterances.</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Galactica</td><td style="text-align:center">BPE</td><td style="text-align:center">50k</td><td style="text-align:center">Papers, reference material,  encyclopedias and other scientific sources</td><td style="text-align:center">Meta</td></tr><tr><td style="text-align:center">Gemma</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">256k</td><td style="text-align:center">Same as Gemma 1 and Gemini</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Gemma 2</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">256k</td><td style="text-align:center">Same as Gemma 1 and Gemini</td><td style="text-align:center">Google</td></tr><tr><td style="text-align:center">Llama</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">CommonCrawl, C4, GitHub, Wiki,  Books, ArXiv, StackExchange</td><td style="text-align:center">Meta</td></tr><tr><td style="text-align:center">Llama 2</td><td style="text-align:center">BPE (spm)</td><td style="text-align:center">32k</td><td style="text-align:center">Same tokenizer as Llama 1</td><td style="text-align:center">Meta</td></tr><tr><td style="text-align:center">Llama 3</td><td style="text-align:center">BBPE (spm)</td><td style="text-align:center">128K</td><td style="text-align:center">GPT-4 100k tokenizer (“c100k_base”), with additional 28k non-English languages</td><td style="text-align:center">Meta</td></tr></tbody></table></div>
          </div>
<p>Following sections generally compares common subword methods in pre-trained models. Refer to following links<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Word Tokenization: How to Handle Out-Of-Vocabulary Words?](/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization)
">[13]</span></a></sup> for detailed tokenization process.<br><div class="note primary">
            <p><strong>Reading Recommendation</strong></p><ul><li><a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization">Word Tokenization: How to Handle Out-Of-Vocabulary Words?</a></li></ul>
          </div></p>
<p><strong>TL;DR</strong></p>
<ul>
<li><span class="label primary"> WordPiece $\Uparrow$</span> <strong>(probability-based)</strong> <em>merges</em> tokens based on <span class="label primary">bigram likelihood</span>. It uses a language model to evaluate the likelihood of subword pair mergence during each iteration, incrementally merging the neighbor unit pairs.</li>
<li><span class="label primary">Byte Pair Encoding (BPE) $\Uparrow$</span> <strong>(frequency-based)</strong> <em>merges</em> tokens based on <span class="label primary">bigram frequency</span>. It uses the subword pair co-occurrence to greedily merge neighbor pairs, which can effiectively balance the vocabulary size and the sequence length. It is based on the greedy <strong>longest-match-first algorithm</strong> (deterministic symbol replacement), which cannot generate multiple segmentations with probabilities.</li>
<li><span class="label info">Unigram Language Model $\Downarrow$</span> <strong>(subword regularization)</strong> <em>prunes</em> tokens based on <span class="label info">unigram LM perplexity</span>, which can be viewed as a probabilistic mixture of characters, subwords, and word segmentations, where the mixture probabiilty is computed using EM algorithm. It reduces the subword using a unigram LM with likelihood reduction.</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Tokenization</th>
<th style="text-align:center">#Vocab</th>
<th style="text-align:center">Update method</th>
<th style="text-align:center">New symbols</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">WordPiece</td>
<td style="text-align:center">&#8593;</td>
<td style="text-align:center">Bottom-up merge</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">BPE</td>
<td style="text-align:center">&#8593;</td>
<td style="text-align:center">Bottom-up merge</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Unigram</td>
<td style="text-align:center">&#8595;</td>
<td style="text-align:center">Prune</td>
<td style="text-align:center">✘</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte-Pair Encoding (BPE)"></a>Byte-Pair Encoding (BPE)</h2><p>Byte-Pair Encoding (BPE)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[BPE] Neural Machine Translation of Rare Words with Subword Units
Rico](http://www.aclweb.org/anthology/P16-1162.pdf)
">[8]</span></a></sup> firstly adopts a pre-tokenizer to split the text sequence into words, then curates a <span class="label warning">base vocabulary</span> consisting of all character symbol sets in the training data for <span class="label primary">frequency-based merge</span>.</p>
<p><strong>Pre-tokenization</strong>&nbsp; The pre-tokenization can be:</p>
<ul>
<li>Space tokenization, e.g. GPT-2, RoBERTa.</li>
<li>Rule-based tokenization (Moses), e.g. XLM.</li>
<li>Spacy and ftfy: GPT.</li>
</ul>
<p><strong>Frequency-based Merge</strong>&nbsp; Starting with the base vocabulary, BPE counts the frequency of each neighbor pair and selects the <span class="label warning">unit pair that occurs most frequently</span> to the base vocabulary. Then it searches for the next unit pair that occurs the most frequently.</p>
<p><img width="50%" data-src="/notes/images/BPE-algorithm.png" alt="BPE algorithm <small>[15]</small>" /></p>
<h3 id="Byte-level-BPE-BBPE"><a href="#Byte-level-BPE-BBPE" class="headerlink" title="Byte-level BPE (BBPE)"></a>Byte-level BPE (BBPE)</h3><h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><div class="note default">
            <p><a href="https://www.unicode.org/versions/Unicode12.1.0/">Unicode</a> vs <a href="https://web.cortland.edu/flteach/mm-course/characters.html">Byte</a>:</p><ul><li><strong>Unicode</strong>: Unicode is an encoding for textual characters which is able to represent characters from different languages. Each character is represented by a <strong>unicode code point</strong>. Unicode consists of a total of <strong>137,929 characters</strong>.</li><li><strong>Byte</strong>: 8 bits is called a byte. One byte character set can contain <strong>256 characters</strong>.</li></ul>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">u&#x27;Hi&#x27;</span>.encode(<span class="string">&#x27;ASCII&#x27;</span>)</span><br><span class="line"><span class="string">b&#x27;Hi&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48\x69&#x27;</span></span><br><span class="line"><span class="string">b&#x27;Hi&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48\x69&#x27;</span>.decode(<span class="string">&#x27;ASCII&#x27;</span>) <span class="comment"># hex (ascii) to str (unicode)</span></span><br><span class="line"><span class="string">&#x27;Hi&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">chr</span>(<span class="number">72</span>)</span><br><span class="line"><span class="string">&#x27;H&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48&#x27;</span></span><br><span class="line"><span class="string">b&#x27;H&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">ord</span>(<span class="string">b&#x27;\x48&#x27;</span>)</span><br><span class="line"><span class="number">72</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">hex</span>(<span class="number">72</span>)</span><br><span class="line"><span class="string">&#x27;0x48&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27;\x48&#x27;</span>.decode()</span><br><span class="line"><span class="string">&#x27;H&#x27;</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">BPE Tokenization</th>
<th style="text-align:center">#Code points</th>
<th style="text-align:center">Seq length</th>
<th style="text-align:center">OOV</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Unicode-level</td>
<td style="text-align:center">13k+</td>
<td style="text-align:center">L</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Byte-level</td>
<td style="text-align:center">256</td>
<td style="text-align:center">&#8804;4L</td>
<td style="text-align:center">✘</td>
</tr>
</tbody>
</table>
</div>
<p><em>Unicode code point</em> contains 130k+ points to cover the full space of textual characters, which can increase the base vocabulary size of BPE. Thus, <strong>Applying BPE to the byte sequence of language</strong> is a great idea proposed in GPT-2<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[GPT-2]Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)
">[14]</span></a></sup> to reduce the vocabulary size.  However, directly applying byte-level BPE can result in suboptimum because the greedy frequency-based heuristic in BPE tend to <span class="label danger"> merge common words into neighbors to generate overfit sub-tokens</span>, such as “-ing.”, “-ing!”, “-ing?”.</p>
<p>To avoid this, GPT-2<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[GPT-2]Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)
">[14]</span></a></sup> <span class="label success"> prevents BPE from merging across different character categories for any byte sequence except space</span>. With byte-level subwords, BBPE can represent any texts using moderate vocabulary size <strong>without out-of-vocabulary problem</strong>. Moreover, it will increase the byte sequence length to x4 maximum.</p>
<h4 id="BBPE"><a href="#BBPE" class="headerlink" title="BBPE"></a>BBPE</h4><p>The <span class="label warning">base vocabulary</span> contains all possible base characters in the training data. It can become large if all unicode characters are included. Thus, GPT-2<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[GPT-2]Language models are unsupervised multitask learners](http://www.persagen.com/files/misc/radford2019language.pdf)
">[14]</span></a></sup> used Byte-level BPE (BBPE) by resorting to <em>byte</em> sequence of texts instead of unicode character strings for base vocabulary construction. It is also adopted by RoBERTa, BART, GPT-2, and GPT-3.</p>
<p><strong>Vocabulary Size</strong>&nbsp; The final vocabulary size is the size of base vocabulary plus the # of merges, where the # of merges is a hyperparameter. For instance,</p>
<ul>
<li>GPT (character-level BPE) has 40,478 vocabularies: 478 base vocabularies + 40k merges.</li>
<li>GPT-2 (byte-level BPE) has 50,257 vocabularies: 256 base vocabularies + 1 [EOS] token + 50k merges. </li>
</ul>
<p><img data-src="/notes/images/BPE-vs-BBPE-Ja-En.png" alt="Examples of Ja-En tokenization with various vocabulary sizes"></p>
<p>We plot the BPE encoding process as follows:</p>
<p><img data-src="/notes/images/BPE_encode_process.png" width="60%" alt="BPE merge process"/></p>
<h5 id="GPT-2-implementation"><a href="#GPT-2-implementation" class="headerlink" title="GPT-2 implementation"></a>GPT-2 implementation</h5><p>The implementation of GPT-2 &amp; RoBERTa.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">GPT-2 &amp; RoBERTa</span></span><br><span class="line"><span class="string">Byte pair encoding utilities from GPT-2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original source: https://github.com/openai/gpt-2/blob/master/src/encoder.py</span></span><br><span class="line"><span class="string">Original license: MIT</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bytes_to_unicode</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns list of utf-8 byte and a corresponding list of unicode strings.</span></span><br><span class="line"><span class="string">    The reversible bpe codes work on unicode strings.</span></span><br><span class="line"><span class="string">    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.</span></span><br><span class="line"><span class="string">    When you&#x27;re at something like a 10B token dataset you end up needing around 5K for decent coverage.</span></span><br><span class="line"><span class="string">    This is a signficant percentage of your normal, say, 32K bpe vocab.</span></span><br><span class="line"><span class="string">    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.</span></span><br><span class="line"><span class="string">    And avoids mapping to whitespace/control characters the bpe code barfs on.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bs = (</span><br><span class="line">        <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;!&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;~&quot;</span>) + <span class="number">1</span>))</span><br><span class="line">        + <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;¡&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;¬&quot;</span>) + <span class="number">1</span>))</span><br><span class="line">        + <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;®&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;ÿ&quot;</span>) + <span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">    cs = bs[:]</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span> ** <span class="number">8</span>):</span><br><span class="line">        <span class="keyword">if</span> b <span class="keyword">not</span> <span class="keyword">in</span> bs:</span><br><span class="line">            bs.append(b)</span><br><span class="line">            cs.append(<span class="number">2</span> ** <span class="number">8</span> + n)</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    cs = [<span class="built_in">chr</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> cs]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(<span class="built_in">zip</span>(bs, cs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pairs</span>(<span class="params">word</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return set of symbol pairs in a word.</span></span><br><span class="line"><span class="string">    Word is represented as tuple of symbols (symbols being variable-length strings).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pairs = <span class="built_in">set</span>()</span><br><span class="line">    prev_char = word[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> word[<span class="number">1</span>:]:</span><br><span class="line">        pairs.add((prev_char, char))</span><br><span class="line">        prev_char = char</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, bpe_merges, errors=<span class="string">&quot;replace&quot;</span></span>):</span></span><br><span class="line">        self.encoder = encoder <span class="comment"># bpe-vocab.json -&gt; &#123;subword:id&#125;</span></span><br><span class="line">        self.decoder = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.encoder.items()&#125; <span class="comment"># &#123;id: subword&#125;</span></span><br><span class="line">        self.errors = errors  <span class="comment"># how to handle errors in decoding</span></span><br><span class="line">        <span class="comment"># &#123;byte: unicode&#125;</span></span><br><span class="line">        self.byte_encoder = bytes_to_unicode()</span><br><span class="line">        self.byte_decoder = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.byte_encoder.items()&#125; <span class="comment"># &#123;unicode: byte&#125;</span></span><br><span class="line">        <span class="comment"># bpe-merges.txt -&gt; &#123;tuple: rank&#125;</span></span><br><span class="line">        self.bpe_ranks = <span class="built_in">dict</span>(<span class="built_in">zip</span>(bpe_merges, <span class="built_in">range</span>(<span class="built_in">len</span>(bpe_merges)))) </span><br><span class="line">        self.cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">            self.re = re</span><br><span class="line">        <span class="keyword">except</span> ImportError:</span><br><span class="line">            <span class="keyword">raise</span> ImportError(<span class="string">&quot;Please install regex with: pip install regex&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should have added re.IGNORECASE so BPE merges</span></span><br><span class="line">        <span class="comment"># can happen for capitalized versions of contractions</span></span><br><span class="line">        self.pat = self.re.<span class="built_in">compile</span>(</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bpe</span>(<span class="params">self, token</span>):</span></span><br><span class="line">        <span class="comment"># check if already processed</span></span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> self.cache:</span><br><span class="line">            <span class="keyword">return</span> self.cache[token]</span><br><span class="line">        word = <span class="built_in">tuple</span>(token)</span><br><span class="line">        pairs = get_pairs(word) <span class="comment"># count bigrams</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">            <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            bigram = <span class="built_in">min</span>(pairs, key=<span class="keyword">lambda</span> pair: self.bpe_ranks.get(pair, <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)))</span><br><span class="line">            <span class="keyword">if</span> bigram <span class="keyword">not</span> <span class="keyword">in</span> self.bpe_ranks:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            first, second = bigram</span><br><span class="line">            new_word = []</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="comment"># find all possible merges for a bigram</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    j = word.index(first, i)</span><br><span class="line">                    new_word.extend(word[i:j])</span><br><span class="line">                    i = j</span><br><span class="line">                <span class="keyword">except</span>: <span class="comment"># no further merge</span></span><br><span class="line">                    new_word.extend(word[i:])</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># bigram match &amp; satisfy length limit</span></span><br><span class="line">                <span class="keyword">if</span> word[i] == first <span class="keyword">and</span> i &lt; <span class="built_in">len</span>(word) - <span class="number">1</span> <span class="keyword">and</span> word[i + <span class="number">1</span>] == second: </span><br><span class="line">                    new_word.append(first + second)</span><br><span class="line">                    i += <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_word.append(word[i])</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            new_word = <span class="built_in">tuple</span>(new_word)</span><br><span class="line">            word = new_word <span class="comment"># update merged tokens</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(word) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pairs = get_pairs(word) <span class="comment"># new possible pairs</span></span><br><span class="line">        word = <span class="string">&quot; &quot;</span>.join(word)</span><br><span class="line">        self.cache[token] = word <span class="comment"># cache raw tokens</span></span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        bpe_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> self.re.findall(self.pat, text):</span><br><span class="line">            token = <span class="string">&quot;&quot;</span>.join(self.byte_encoder[b] <span class="keyword">for</span> b <span class="keyword">in</span> token.encode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">            bpe_tokens.extend(</span><br><span class="line">                self.encoder[bpe_token] <span class="keyword">for</span> bpe_token <span class="keyword">in</span> self.bpe(token).split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> bpe_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        text = <span class="string">&quot;&quot;</span>.join([self.decoder.get(token, token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line">        text = <span class="built_in">bytearray</span>([self.byte_decoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> text]).decode(</span><br><span class="line">            <span class="string">&quot;utf-8&quot;</span>, errors=self.errors</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encoder</span>(<span class="params">encoder_json_path:<span class="string">&quot;bpe-vocab.json&quot;</span>, vocab_bpe_path:<span class="string">&quot;bpe-merge.txt&quot;</span></span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(encoder_json_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        encoder = json.load(f)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_bpe_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        bpe_data = f.read()</span><br><span class="line">    bpe_merges = [<span class="built_in">tuple</span>(merge_str.split()) <span class="keyword">for</span> merge_str <span class="keyword">in</span> bpe_data.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> Encoder(</span><br><span class="line">        encoder=encoder,</span><br><span class="line">        bpe_merges=bpe_merges,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># RoBERTa source code.</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> contextlib</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Helper script to encode raw text with the GPT-2 BPE using multiple processes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The encoder.json and vocab.bpe files can be obtained here:</span></span><br><span class="line"><span class="string">    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json</span></span><br><span class="line"><span class="string">    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--encoder-json&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;path to encoder.json&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--vocab-bpe&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;path to vocab.bpe&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--inputs&quot;</span>,</span><br><span class="line">        nargs=<span class="string">&quot;+&quot;</span>,</span><br><span class="line">        default=[<span class="string">&quot;-&quot;</span>],</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;input files to filter/encode&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--outputs&quot;</span>,</span><br><span class="line">        nargs=<span class="string">&quot;+&quot;</span>,</span><br><span class="line">        default=[<span class="string">&quot;-&quot;</span>],</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;path to save encoded outputs&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--keep-empty&quot;</span>,</span><br><span class="line">        action=<span class="string">&quot;store_true&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;keep empty lines&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--workers&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">20</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(args.inputs) == <span class="built_in">len</span>(</span><br><span class="line">        args.outputs</span><br><span class="line">    ), <span class="string">&quot;number of input and output paths should match&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> contextlib.ExitStack() <span class="keyword">as</span> stack:</span><br><span class="line">        inputs = [</span><br><span class="line">            stack.enter_context(<span class="built_in">open</span>(<span class="built_in">input</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">input</span> != <span class="string">&quot;-&quot;</span></span><br><span class="line">            <span class="keyword">else</span> sys.stdin</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> args.inputs</span><br><span class="line">        ]</span><br><span class="line">        outputs = [</span><br><span class="line">            stack.enter_context(<span class="built_in">open</span>(output, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">            <span class="keyword">if</span> output != <span class="string">&quot;-&quot;</span></span><br><span class="line">            <span class="keyword">else</span> sys.stdout</span><br><span class="line">            <span class="keyword">for</span> output <span class="keyword">in</span> args.outputs</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        encoder = MultiprocessingEncoder(args)</span><br><span class="line">        pool = Pool(args.workers, initializer=encoder.initializer)</span><br><span class="line">        encoded_lines = pool.imap(encoder.encode_lines, <span class="built_in">zip</span>(*inputs), <span class="number">100</span>)</span><br><span class="line">            </span><br><span class="line">        stats = Counter()</span><br><span class="line">        <span class="keyword">for</span> i, (filt, enc_lines) <span class="keyword">in</span> <span class="built_in">enumerate</span>(encoded_lines, start=<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> filt == <span class="string">&quot;PASS&quot;</span>:</span><br><span class="line">                <span class="keyword">for</span> enc_line, output_h <span class="keyword">in</span> <span class="built_in">zip</span>(enc_lines, outputs):</span><br><span class="line">                    <span class="built_in">print</span>(enc_line, file=output_h)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stats[<span class="string">&quot;num_filtered_&quot;</span> + filt] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;processed &#123;&#125; lines&quot;</span>.<span class="built_in">format</span>(i), file=sys.stderr)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> stats.most_common():</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[&#123;&#125;] filtered &#123;&#125; lines&quot;</span>.<span class="built_in">format</span>(k, v), file=sys.stderr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiprocessingEncoder</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initializer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> bpe</span><br><span class="line">        bpe = get_encoder(self.args.encoder_json, self.args.vocab_bpe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, line</span>):</span></span><br><span class="line">        <span class="keyword">global</span> bpe</span><br><span class="line">        ids = bpe.encode(line)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, ids))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="keyword">global</span> bpe</span><br><span class="line">        <span class="keyword">return</span> bpe.decode(tokens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode_lines</span>(<span class="params">self, lines</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Encode a set of lines. All lines will be encoded together.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_lines = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(line) == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.args.keep_empty:</span><br><span class="line">                <span class="keyword">return</span> [<span class="string">&quot;EMPTY&quot;</span>, <span class="literal">None</span>]</span><br><span class="line">            tokens = self.encode(line)</span><br><span class="line">            enc_lines.append(<span class="string">&quot; &quot;</span>.join(tokens))</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;PASS&quot;</span>, enc_lines]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_lines</span>(<span class="params">self, lines</span>):</span></span><br><span class="line">        dec_lines = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            tokens = <span class="built_in">map</span>(<span class="built_in">int</span>, line.strip().split())</span><br><span class="line">            dec_lines.append(self.decode(tokens))</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;PASS&quot;</span>, dec_lines]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h5 id="tiktoken-implementation"><a href="#tiktoken-implementation" class="headerlink" title="tiktoken implementation"></a>tiktoken implementation</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; `tiktoken` BPE implementation (for educational purpose) &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> annotations</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> regex</span><br><span class="line"></span><br><span class="line"><span class="comment"># pip3 install tiktoken&gt;=0.4.0</span></span><br><span class="line"><span class="keyword">import</span> tiktoken </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleBytePairEncoding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *, pat_str: <span class="built_in">str</span>, mergeable_ranks: <span class="built_in">dict</span>[<span class="built_in">bytes</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Creates an Encoding object.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># A regex pattern string that is used to split the input text</span></span><br><span class="line">        self.pat_str = pat_str</span><br><span class="line">        <span class="comment"># A dictionary mapping token bytes to their ranks. The ranks correspond to merge priority</span></span><br><span class="line">        self.mergeable_ranks = mergeable_ranks</span><br><span class="line"></span><br><span class="line">        self._decoder = &#123;token: token_bytes <span class="keyword">for</span> token_bytes, token <span class="keyword">in</span> mergeable_ranks.items()&#125;</span><br><span class="line">        self._pat = regex.<span class="built_in">compile</span>(pat_str)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text: <span class="built_in">str</span>, visualise: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;colour&quot;</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Encodes a string into tokens.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.encode(&quot;hello world&quot;)</span></span><br><span class="line"><span class="string">        [388, 372]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Use the regex to split the text into (approximately) words</span></span><br><span class="line">        words = self._pat.findall(text)</span><br><span class="line">        tokens = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="comment"># Turn each word into tokens, using the byte pair encoding algorithm</span></span><br><span class="line">            word_bytes = word.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            word_tokens = bpe_encode(self.mergeable_ranks, word_bytes, visualise=visualise)</span><br><span class="line">            tokens.extend(word_tokens)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_bytes</span>(<span class="params">self, tokens: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bytes</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes a list of tokens into bytes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.decode_bytes([388, 372])</span></span><br><span class="line"><span class="string">        b&#x27;hello world&#x27;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">b&quot;&quot;</span>.join(self._decoder[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, tokens: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes a list of tokens into a string.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Decoded bytes are not guaranteed to be valid UTF-8. In that case, we replace</span></span><br><span class="line"><span class="string">        the invalid bytes with the replacement character &quot;�&quot;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.decode([388, 372])</span></span><br><span class="line"><span class="string">        &#x27;hello world&#x27;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode_bytes(tokens).decode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&quot;replace&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_tokens_bytes</span>(<span class="params">self, tokens: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">bytes</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes a list of tokens into a list of bytes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Useful for visualising how a string is tokenised.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; enc.decode_tokens_bytes([388, 372])</span></span><br><span class="line"><span class="string">        [b&#x27;hello&#x27;, b&#x27; world&#x27;]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> [self._decoder[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">training_data: <span class="built_in">str</span>, vocab_size: <span class="built_in">int</span>, pat_str: <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train a BPE tokeniser on some data!&quot;&quot;&quot;</span></span><br><span class="line">        mergeable_ranks = bpe_train(data=training_data, vocab_size=vocab_size, pat_str=pat_str)</span><br><span class="line">        <span class="keyword">return</span> SimpleBytePairEncoding(pat_str=pat_str, mergeable_ranks=mergeable_ranks)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_tiktoken</span>(<span class="params">encoding</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(encoding, <span class="built_in">str</span>):</span><br><span class="line">            encoding = tiktoken.get_encoding(encoding)</span><br><span class="line">        <span class="keyword">return</span> SimpleBytePairEncoding(</span><br><span class="line">            pat_str=encoding._pat_str, mergeable_ranks=encoding._mergeable_ranks</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bpe_encode</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    mergeable_ranks: <span class="built_in">dict</span>[<span class="built_in">bytes</span>, <span class="built_in">int</span>], <span class="built_in">input</span>: <span class="built_in">bytes</span>, visualise: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;colour&quot;</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">    parts = [<span class="built_in">bytes</span>([b]) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">input</span>]</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># See the intermediate merges play out!</span></span><br><span class="line">        <span class="keyword">if</span> visualise:</span><br><span class="line">            <span class="keyword">if</span> visualise <span class="keyword">in</span> [<span class="string">&quot;colour&quot;</span>, <span class="string">&quot;color&quot;</span>]:</span><br><span class="line">                visualise_tokens(parts)</span><br><span class="line">            <span class="keyword">elif</span> visualise == <span class="string">&quot;simple&quot;</span>:</span><br><span class="line">                <span class="built_in">print</span>(parts)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate over all pairs and find the pair we want to merge the most</span></span><br><span class="line">        min_idx = <span class="literal">None</span></span><br><span class="line">        min_rank = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i, pair <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(parts[:-<span class="number">1</span>], parts[<span class="number">1</span>:])):</span><br><span class="line">            rank = mergeable_ranks.get(pair[<span class="number">0</span>] + pair[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> rank <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (min_rank <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> rank &lt; min_rank):</span><br><span class="line">                min_idx = i</span><br><span class="line">                min_rank = rank</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If there were no pairs we could merge, we&#x27;re done!</span></span><br><span class="line">        <span class="keyword">if</span> min_rank <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">assert</span> min_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Otherwise, merge that pair and leave the rest unchanged. Then repeat.</span></span><br><span class="line">        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + <span class="number">1</span>]] + parts[min_idx + <span class="number">2</span> :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> visualise:</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    tokens = [mergeable_ranks[part] <span class="keyword">for</span> part <span class="keyword">in</span> parts]</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bpe_train</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    data: <span class="built_in">str</span>, vocab_size: <span class="built_in">int</span>, pat_str: <span class="built_in">str</span>, visualise: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;colour&quot;</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">bytes</span>, <span class="built_in">int</span>]:</span></span><br><span class="line">    <span class="comment"># First, add tokens for each individual byte value</span></span><br><span class="line">    <span class="keyword">if</span> vocab_size &lt; <span class="number">2</span>**<span class="number">8</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;vocab_size must be at least 256, so we can encode all bytes&quot;</span>)</span><br><span class="line">    ranks = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>**<span class="number">8</span>):</span><br><span class="line">        ranks[<span class="built_in">bytes</span>([i])] = i</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Splinter up our data into lists of bytes</span></span><br><span class="line">    <span class="comment"># data = &quot;Hello world&quot;</span></span><br><span class="line">    <span class="comment"># words = [</span></span><br><span class="line">    <span class="comment">#     [b&#x27;H&#x27;, b&#x27;e&#x27;, b&#x27;l&#x27;, b&#x27;l&#x27;, b&#x27;o&#x27;],</span></span><br><span class="line">    <span class="comment">#     [b&#x27; &#x27;, b&#x27;w&#x27;, b&#x27;o&#x27;, b&#x27;r&#x27;, b&#x27;l&#x27;, b&#x27;d&#x27;]</span></span><br><span class="line">    <span class="comment"># ]</span></span><br><span class="line">    words: <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">bytes</span>]] = [</span><br><span class="line">        [<span class="built_in">bytes</span>([b]) <span class="keyword">for</span> b <span class="keyword">in</span> word.encode(<span class="string">&quot;utf-8&quot;</span>)] <span class="keyword">for</span> word <span class="keyword">in</span> regex.findall(pat_str, data)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Now, use our data to figure out which merges we should make</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(ranks) &lt; vocab_size:</span><br><span class="line">        <span class="comment"># Find the most common pair. This will become our next token</span></span><br><span class="line">        stats = collections.Counter()</span><br><span class="line">        <span class="keyword">for</span> piece <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(piece[:-<span class="number">1</span>], piece[<span class="number">1</span>:]):</span><br><span class="line">                stats[pair] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        most_common_pair = <span class="built_in">max</span>(stats, key=<span class="keyword">lambda</span> x: stats[x])</span><br><span class="line">        token_bytes = most_common_pair[<span class="number">0</span>] + most_common_pair[<span class="number">1</span>]</span><br><span class="line">        token = <span class="built_in">len</span>(ranks)</span><br><span class="line">        <span class="comment"># Add the new token!</span></span><br><span class="line">        ranks[token_bytes] = token</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Now merge that most common pair in all the words. That is, update our training data</span></span><br><span class="line">        <span class="comment"># to reflect our decision to make that pair into a new token.</span></span><br><span class="line">        new_words = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            new_word = []</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> (word[i], word[i + <span class="number">1</span>]) == most_common_pair:</span><br><span class="line">                    <span class="comment"># We found our pair! Merge it</span></span><br><span class="line">                    new_word.append(token_bytes)</span><br><span class="line">                    i += <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_word.append(word[i])</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(word) - <span class="number">1</span>:</span><br><span class="line">                new_word.append(word[i])</span><br><span class="line">            new_words.append(new_word)</span><br><span class="line">        words = new_words</span><br><span class="line"></span><br><span class="line">        <span class="comment"># See the intermediate merges play out!</span></span><br><span class="line">        <span class="keyword">if</span> visualise:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;The current most common pair is <span class="subst">&#123;most_common_pair[<span class="number">0</span>]&#125;</span> + <span class="subst">&#123;most_common_pair[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;So we made <span class="subst">&#123;token_bytes&#125;</span> our <span class="subst">&#123;<span class="built_in">len</span>(ranks)&#125;</span>th token&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> visualise <span class="keyword">in</span> [<span class="string">&quot;colour&quot;</span>, <span class="string">&quot;color&quot;</span>]:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Now the first fifty words in our training data look like:&quot;</span>)</span><br><span class="line">                visualise_tokens([token <span class="keyword">for</span> word <span class="keyword">in</span> words[:<span class="number">50</span>] <span class="keyword">for</span> token <span class="keyword">in</span> word])</span><br><span class="line">            <span class="keyword">elif</span> visualise == <span class="string">&quot;simple&quot;</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Now the first twenty words in our training data look like:&quot;</span>)</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> words[:<span class="number">20</span>]:</span><br><span class="line">                    <span class="built_in">print</span>(word)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ranks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualise_tokens</span>(<span class="params">token_values: <span class="built_in">list</span>[<span class="built_in">bytes</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    backgrounds = itertools.cycle(</span><br><span class="line">        [<span class="string">f&quot;\u001b[48;5;<span class="subst">&#123;i&#125;</span>m&quot;</span>.encode() <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">167</span>, <span class="number">179</span>, <span class="number">185</span>, <span class="number">77</span>, <span class="number">80</span>, <span class="number">68</span>, <span class="number">134</span>]]</span><br><span class="line">    )</span><br><span class="line">    interleaved = itertools.chain.from_iterable(<span class="built_in">zip</span>(backgrounds, token_values))</span><br><span class="line">    <span class="built_in">print</span>((<span class="string">b&quot;&quot;</span>.join(interleaved) + <span class="string">&quot;\u001b[0m&quot;</span>.encode()).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_simple_encoding</span>():</span></span><br><span class="line">    gpt2_pattern = (</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?[\p&#123;L&#125;]+| ?[\p&#123;N&#125;]+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(__file__, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.read()</span><br><span class="line"></span><br><span class="line">    enc = SimpleBytePairEncoding.train(data, vocab_size=<span class="number">600</span>, pat_str=gpt2_pattern)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;This is the sequence of merges performed in order to encode &#x27;hello world&#x27;:&quot;</span>)</span><br><span class="line">    tokens = enc.encode(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line">    <span class="keyword">assert</span> enc.decode(tokens) == <span class="string">&quot;hello world&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> enc.decode_bytes(tokens) == <span class="string">b&quot;hello world&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> enc.decode_tokens_bytes(tokens) == [<span class="string">b&quot;hello&quot;</span>, <span class="string">b&quot; world&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> enc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train a BPE tokeniser on a small amount of text</span></span><br><span class="line">enc = train_simple_encoding()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualise how the GPT-4 encoder encodes text</span></span><br><span class="line">enc = SimpleBytePairEncoding.from_tiktoken(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line">y = enc.encode(<span class="string">&quot;hello world aaaaaaaaaaaa&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><p>WordPiece<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
">[9]</span></a></sup><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)
">[10]</span></a></sup> can be viewed as a <strong>language-modeling based</strong> BPE variant. It trains with similar process to the BPE but uses disparate merge rule: WordPiece select the <span class="label warning">unit pair that maximizes the likelihood of training data at utmost</span>, rather than choose the most frequent pair. <strong>WordPiece chooses the subword pair that has the maximum mutual information value</strong>.</p>
<p>WordPiece scores the likelihood of possible pairs using an <em>n</em>-gram LM. <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
">[9]</span></a></sup> mentioned that training LMs for every possible merge is prohibit, they used aggressive heuristics to reduce the budget. However, the public training implementation is unavailable.</p>
<p>The BERT tokenization applies two tokenizers one after another:</p>
<ul>
<li><strong>BasicTokenizer</strong>:<ol>
<li>Convert text to unicode.</li>
<li>Clean text: invalid character removal and whitespace cleanup.</li>
<li>Use whitespace to seperate Chinese characters.</li>
<li>Whitespace tokenization.</li>
<li>Lowercase &amp; Strips accents.</li>
<li>Split punctuations.</li>
</ol>
</li>
<li><strong>WordpieceTokenizer</strong>:<ol>
<li>Convert texts to unicode.</li>
<li>Apply WordPiece, a greedy longest-match-first algorithm to perform tokenization given vocabulary.</li>
</ol>
</li>
</ul>
<p><img width="50%" data-src="/notes/images/wordpiece.gif" alt="WordPiece" /></p>
<div class="note info">
            <p>All Chinese inputs are split into characters as if no wordpiece applied. </p>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># BERT Implementation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs end-to-end tokenziation.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_file, do_lower_case=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.vocab = load_vocab(vocab_file)</span><br><span class="line">    self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text):</span><br><span class="line">      <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">        split_tokens.append(sub_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span>(<span class="params">self, ids</span>):</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.inv_vocab, ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, do_lower_case=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      do_lower_case: Whether to lower case the input.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">    <span class="comment"># models. This is also applied to the English models now, but it doesn&#x27;t</span></span><br><span class="line">    <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">    <span class="comment"># and generally don&#x27;t have any Chinese data in them (there are Chinese</span></span><br><span class="line">    <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">    <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line"></span><br><span class="line">    orig_tokens = whitespace_tokenize(text)</span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">      <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line"></span><br><span class="line">    output_tokens = whitespace_tokenize(<span class="string">&quot; &quot;</span>.join(split_tokens))</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = unicodedata.normalize(<span class="string">&quot;NFD&quot;</span>, text)</span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">      cat = unicodedata.category(char)</span><br><span class="line">      <span class="keyword">if</span> cat == <span class="string">&quot;Mn&quot;</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      output.append(char)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    chars = <span class="built_in">list</span>(text)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    start_new_word = <span class="literal">True</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">      char = chars[i]</span><br><span class="line">      <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">        output.append([char])</span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> start_new_word:</span><br><span class="line">          output.append([])</span><br><span class="line">        start_new_word = <span class="literal">False</span></span><br><span class="line">        output[-<span class="number">1</span>].append(char)</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [<span class="string">&quot;&quot;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">      cp = <span class="built_in">ord</span>(char)</span><br><span class="line">      <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">        output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">        output.append(char)</span><br><span class="line">        output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output.append(char)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span>(<span class="params">self, cp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:</span></span><br><span class="line">    <span class="comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span></span><br><span class="line">    <span class="comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span></span><br><span class="line">    <span class="comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span></span><br><span class="line">    <span class="comment"># space-separated words, so they are not treated specially and handled</span></span><br><span class="line">    <span class="comment"># like the all of the other languages.</span></span><br><span class="line">    <span class="keyword">if</span> ((cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>) <span class="keyword">or</span></span><br><span class="line">        (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">        (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)):  <span class="comment">#</span></span><br><span class="line">      <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">      cp = <span class="built_in">ord</span>(char)</span><br><span class="line">      <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xfffd</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">        output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output.append(char)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs WordPiece tokenziation.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab, unk_token=<span class="string">&quot;[UNK]&quot;</span>, max_input_chars_per_word=<span class="number">200</span></span>):</span></span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">    using the given vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string">      input = &quot;unaffable&quot;</span></span><br><span class="line"><span class="string">      output = [&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">        already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">    output_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">      chars = <span class="built_in">list</span>(token)</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">len</span>(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      is_bad = <span class="literal">False</span></span><br><span class="line">      start = <span class="number">0</span></span><br><span class="line">      sub_tokens = []</span><br><span class="line">      <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">        end = <span class="built_in">len</span>(chars)</span><br><span class="line">        cur_substr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; end:</span><br><span class="line">          substr = <span class="string">&quot;&quot;</span>.join(chars[start:end])</span><br><span class="line">          <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            substr = <span class="string">&quot;##&quot;</span> + substr</span><br><span class="line">          <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          is_bad = <span class="literal">True</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_whitespace</span>(<span class="params">char</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Checks whether `chars` is a whitespace character.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># \t, \n, and \r are technically contorl characters but we treat them</span></span><br><span class="line">  <span class="comment"># as whitespace since they are generally considered as such.</span></span><br><span class="line">  <span class="keyword">if</span> char == <span class="string">&quot; &quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\t&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\n&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\r&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  cat = unicodedata.category(char)</span><br><span class="line">  <span class="keyword">if</span> cat == <span class="string">&quot;Zs&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_control</span>(<span class="params">char</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Checks whether `chars` is a control character.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># These are technically control characters but we count them as whitespace</span></span><br><span class="line">  <span class="comment"># characters.</span></span><br><span class="line">  <span class="keyword">if</span> char == <span class="string">&quot;\t&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\n&quot;</span> <span class="keyword">or</span> char == <span class="string">&quot;\r&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">  cat = unicodedata.category(char)</span><br><span class="line">  <span class="keyword">if</span> cat <span class="keyword">in</span> (<span class="string">&quot;Cc&quot;</span>, <span class="string">&quot;Cf&quot;</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_punctuation</span>(<span class="params">char</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Checks whether `chars` is a punctuation character.&quot;&quot;&quot;</span></span><br><span class="line">  cp = <span class="built_in">ord</span>(char)</span><br><span class="line">  <span class="comment"># We treat all non-letter/number ASCII as punctuation.</span></span><br><span class="line">  <span class="comment"># Characters such as &quot;^&quot;, &quot;$&quot;, and &quot;`&quot; are not in the Unicode</span></span><br><span class="line">  <span class="comment"># Punctuation class but we treat them as punctuation anyways, for</span></span><br><span class="line">  <span class="comment"># consistency.</span></span><br><span class="line">  <span class="keyword">if</span> ((cp &gt;= <span class="number">33</span> <span class="keyword">and</span> cp &lt;= <span class="number">47</span>) <span class="keyword">or</span> (cp &gt;= <span class="number">58</span> <span class="keyword">and</span> cp &lt;= <span class="number">64</span>) <span class="keyword">or</span></span><br><span class="line">      (cp &gt;= <span class="number">91</span> <span class="keyword">and</span> cp &lt;= <span class="number">96</span>) <span class="keyword">or</span> (cp &gt;= <span class="number">123</span> <span class="keyword">and</span> cp &lt;= <span class="number">126</span>)):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  cat = unicodedata.category(char)</span><br><span class="line">  <span class="keyword">if</span> cat.startswith(<span class="string">&quot;P&quot;</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    vocab_file=<span class="string">&quot;./cased_L-12_H-768_A-12/vocab.txt&quot;</span></span><br><span class="line">    tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=<span class="literal">True</span>)</span><br><span class="line">    output_tokens = tokenizer.tokenize(<span class="string">&quot;&quot;&quot;This text is included to </span></span><br><span class="line"><span class="string">        make sure Unicode is handled properly: 力加勝北区ᴵᴺᵀᵃছজটডণত&quot;&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Unigram-Language-Model"><a href="#Unigram-Language-Model" class="headerlink" title="Unigram Language Model"></a>Unigram Language Model</h2><p>Unigram Language Model<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://www.aclweb.org/anthology/P18-1007.pdf)
">[11]</span></a></sup> initializes its base vocabulary with a large # of vocabulary and gradually removes a portion (e.g., 20%) of units according to the likelihood change. It use a unigram LM to evaluate the likelihood increase after subword removal, where the probability of each unit is computed using EM algorithm. The drop process will stop until reach the pre-defined vocabulary size.</p>
<p><img width="50%" data-src="/notes/images/Unigram-LM-tokenization.png" alt="Ungram LM algorithm <small>[11]</small>" /></p>
<p>Since unigram is not based on merge rules (in contrast to BPE and WordPiece), there has several ways of tokenizing new text after training. Therefore, unigram also saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. It simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their possibilities.</p>
<p>Assume that the set of all possible tokenizations for a word $x_i$ is defined as $S(x_i)$, the overall loss is defined as:</p>
<script type="math/tex; mode=display">
\mathcal{L} = - \sum_{i=1}^N \log \bigg( \sum_{x \in S(x_i)} p(x) \bigg)</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> digamma</span><br><span class="line"></span><br><span class="line"><span class="comment"># To efficiently determine the next possible words</span></span><br><span class="line"><span class="comment"># We need a Trie data structure</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, word, value</span>):</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                node[ch] = &#123;&#125;</span><br><span class="line">            node = node[ch]</span><br><span class="line">        node[<span class="string">&#x27;&lt;END&gt;&#x27;</span>] = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            node = node[ch]</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;&lt;END&gt;&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> node[<span class="string">&#x27;&lt;END&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_value</span>(<span class="params">self, word, value</span>):</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;word not in trie&quot;</span>)</span><br><span class="line">            node = node[ch]</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;&lt;END&gt;&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;word not in trie&quot;</span>)</span><br><span class="line">        node[<span class="string">&#x27;&lt;END&gt;&#x27;</span>] = value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentencePieceTrainer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.trie = <span class="literal">None</span></span><br><span class="line">        self.maxlen = <span class="literal">None</span></span><br><span class="line">        self.vocab_size = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_trie</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        trie = Trie()</span><br><span class="line">        norm = <span class="built_in">sum</span>(<span class="built_in">list</span>(tokens.values()))</span><br><span class="line">        logsum = digamma(norm)</span><br><span class="line"></span><br><span class="line">        maxlen = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> tok, val <span class="keyword">in</span> tokens.items():</span><br><span class="line">            trie.add(tok, digamma(val)-logsum)</span><br><span class="line">            maxlen = <span class="built_in">max</span>(maxlen, <span class="built_in">len</span>(tok))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trie, maxlen</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_step</span>(<span class="params">self, text, trie</span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># d[i] contains the maximum log_prob of any tokenization</span></span><br><span class="line">        <span class="comment"># of text[:i], initialized to 0 (i.e. log(0)=-infty)</span></span><br><span class="line">        d = [-np.inf]*(N+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># p[i] (stands for parent) contains the number of characters of</span></span><br><span class="line">        <span class="comment"># the final token in the most likely sequence that ends at index i</span></span><br><span class="line">        p = [<span class="literal">None</span>]*(N+<span class="number">1</span>)</span><br><span class="line">        d[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># find all possible final words. Have to look back</span></span><br><span class="line">            <span class="comment"># a distance set by the length of the longest token</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i-self.maxlen, <span class="number">0</span>), i):</span><br><span class="line"></span><br><span class="line">                final_token = text[j:i]</span><br><span class="line">                final_value = trie.get_value(final_token)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># if the current ending word has a higher log-probability,</span></span><br><span class="line">                <span class="comment"># save that value and store the word (i.e. # chars to backtrack)</span></span><br><span class="line">                <span class="keyword">if</span> final_value <span class="keyword">and</span> d[j]+final_value &gt; d[i]:</span><br><span class="line">                    d[i] = d[j]+final_value</span><br><span class="line">                    p[i] = <span class="built_in">len</span>(final_token)</span><br><span class="line">            <span class="keyword">if</span> p[i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Encountered unknown token &#x27;<span class="subst">&#123;text[i-<span class="number">1</span>]&#125;</span>&#x27;.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        loss = d[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> loss, p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward_step</span>(<span class="params">self, text, p</span>):</span></span><br><span class="line">        idx = <span class="built_in">len</span>(p)</span><br><span class="line">        tokenization = []</span><br><span class="line">        <span class="keyword">while</span> idx &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># move back the number of steps p tells you to</span></span><br><span class="line">            next_idx = idx-p[idx-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># extract the final token</span></span><br><span class="line">            tok = text[next_idx-<span class="number">1</span>:idx-<span class="number">1</span>]</span><br><span class="line">            tokenization.append(tok)</span><br><span class="line"></span><br><span class="line">            idx = next_idx</span><br><span class="line">        tokenization = <span class="built_in">list</span>(<span class="built_in">reversed</span>(tokenization))</span><br><span class="line">        <span class="keyword">return</span> tokenization</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">E_step</span>(<span class="params">self, tokenization, trie</span>):</span></span><br><span class="line">        <span class="comment"># get the new token counts based on updated tokenization</span></span><br><span class="line">        counts = collections.Counter(tokenization)</span><br><span class="line">        norm = <span class="built_in">sum</span>(<span class="built_in">list</span>(counts.values()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bayesianify them: https://cs.stanford.edu/~pliang/papers/tutorial-acl2007-talk.pdf</span></span><br><span class="line">        <span class="comment"># https://github.com/google/sentencepiece/blob/master/src/unigram_model_trainer.cc</span></span><br><span class="line">        <span class="comment"># we are returning the log probabilties here (alpha=0 prior)</span></span><br><span class="line">        logsum = digamma(norm)</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> counts.items():</span><br><span class="line">            counts[k] = digamma(v)-logsum</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> counts.items():</span><br><span class="line">            trie.set_value(k, v)</span><br><span class="line">        <span class="keyword">return</span> trie</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">M_step</span>(<span class="params">self, text, trie</span>):</span></span><br><span class="line">        loss, p = self.forward_step(text, trie)</span><br><span class="line">        tokenization = self.backward_step(text, p)</span><br><span class="line">        <span class="keyword">return</span> tokenization, loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">EM_step</span>(<span class="params">self, text, tokenization, trie</span>):</span></span><br><span class="line">        trie = self.E_step(tokenization, trie)</span><br><span class="line">        tokenization, loss = self.M_step(text, trie)</span><br><span class="line">        <span class="keyword">return</span> loss, tokenization, trie</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">EM_round</span>(<span class="params">self, text, tokens, delta=<span class="number">0.01</span>, max_iter=<span class="number">10</span></span>):</span></span><br><span class="line">        tokenization, old_loss = self.M_step(text, self.trie)</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;EM iter <span class="subst">&#123;step&#125;</span>: &quot;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            loss, tokenization, trie = self.EM_step(text, tokenization, self.trie)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Loss=<span class="subst">&#123;loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(old_loss-loss) &lt; delta:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            old_loss = loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_tokens</span>(<span class="params">self, tokens, characters, vocab_size, trim_frac=<span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Tokens are passed by reference and modified in place.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            True: to indicate to caller that more rounds are needed</span></span><br><span class="line"><span class="string">            False: to indicate we successfully hit the target vocab size</span></span><br><span class="line"><span class="string">            ValueError: if the vocab size cannot be reached.&quot;&quot;&quot;</span></span><br><span class="line">        sorted_tokens = tokens.most_common()</span><br><span class="line">        N = <span class="built_in">len</span>(sorted_tokens)</span><br><span class="line">        n_trim = <span class="built_in">int</span>(trim_frac*N)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(N)):</span><br><span class="line">            <span class="keyword">if</span> N &lt;= vocab_size:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> n_trim &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            tok = sorted_tokens[i][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> tok <span class="keyword">not</span> <span class="keyword">in</span> characters:</span><br><span class="line">                self.trie.set_value(tok, <span class="number">0</span>) <span class="comment"># we need to delete it from the trie (that sticks around)</span></span><br><span class="line">                tokens.pop(tok) <span class="comment"># also need to delete from tokens, so the next round doesn&#x27;t see it</span></span><br><span class="line">                n_trim -= <span class="number">1</span></span><br><span class="line">                N -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n_trim &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Could not reduce tokens further. Please increase vocab size&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, text, tokens, characters, vocab_size, delta=<span class="number">0.01</span>, max_iter=<span class="number">5</span>, max_rounds=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; To turn off pruning, just set max_rounds=1 &quot;&quot;&quot;</span></span><br><span class="line">        text = re.sub(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;_&#x27;</span>, text)</span><br><span class="line">        <span class="keyword">if</span> vocab_size &gt; <span class="built_in">len</span>(tokens):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Vocab size is larger than the availble number of tokens <span class="subst">&#123;<span class="built_in">len</span>(tokens)&#125;</span>.&quot;</span>)</span><br><span class="line">        self.trie, self.maxlen = self._initialize_trie(tokens)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_rounds+<span class="number">1</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;--- Round <span class="subst">&#123;i&#125;</span>. Vocab size: <span class="subst">&#123;<span class="built_in">len</span>(tokens)&#125;</span> ---&quot;</span>)</span><br><span class="line">            self.EM_round(text, tokens, delta, max_iter)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.prune_tokens(tokens, characters, vocab_size):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generalized_forward_step</span>(<span class="params">self, text, trie, nbest_size=<span class="number">1</span></span>):</span></span><br><span class="line">        N = <span class="built_in">len</span>(text)</span><br><span class="line">        d = [-np.inf]*(N+<span class="number">1</span>)</span><br><span class="line">        p = [<span class="literal">None</span>]*(N+<span class="number">1</span>)</span><br><span class="line">        d[<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">            d_queue = []</span><br><span class="line">            p_queue = []</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i-self.maxlen, <span class="number">0</span>), i):</span><br><span class="line">                final_token = text[j:i]</span><br><span class="line">                final_value = trie.get_value(final_token)</span><br><span class="line">                <span class="keyword">if</span> final_value:</span><br><span class="line">                    curr_d = d[j]+final_value</span><br><span class="line">                    curr_p = <span class="built_in">len</span>(final_token)</span><br><span class="line">                    d[i] = <span class="built_in">max</span>(d[i], curr_d)</span><br><span class="line">                    d_queue.append(curr_d)</span><br><span class="line">                    p_queue.append(curr_p)</span><br><span class="line">            ids = np.argsort(d_queue)[-nbest_size:]</span><br><span class="line">            p[i] = [p_queue[z] <span class="keyword">for</span> z <span class="keyword">in</span> ids]</span><br><span class="line">        <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generalized_backward_step</span>(<span class="params">self, text, p</span>):</span></span><br><span class="line">        idx = <span class="built_in">len</span>(p)</span><br><span class="line">        tokenization = []</span><br><span class="line">        <span class="keyword">while</span> idx &gt; <span class="number">1</span>:</span><br><span class="line">            back_steps = np.random.choice(p[idx-<span class="number">1</span>])</span><br><span class="line">            next_idx = idx-back_steps</span><br><span class="line">            tok = text[next_idx-<span class="number">1</span>:idx-<span class="number">1</span>]</span><br><span class="line">            tokenization.append(tok)</span><br><span class="line">            idx = next_idx</span><br><span class="line">        tokenization = <span class="built_in">list</span>(<span class="built_in">reversed</span>(tokenization))</span><br><span class="line">        <span class="keyword">return</span> tokenization</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text, nbest_size=<span class="number">1</span></span>):</span></span><br><span class="line">        text = re.sub(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;_&#x27;</span>, text)</span><br><span class="line">        <span class="keyword">if</span> self.trie <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Trainer has not yet been fit. Cannot tokenize.&quot;</span>)</span><br><span class="line">        p = self.generalized_forward_step(text, self.trie, nbest_size)</span><br><span class="line">        tokenization = self.generalized_backward_step(text, p)</span><br><span class="line">        <span class="keyword">return</span> tokenization</span><br></pre></td></tr></table></figure>
<p>Refer to <sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)
">[18]</span></a></sup> for details.</p>
<h2 id="SentencePiece-Library"><a href="#SentencePiece-Library" class="headerlink" title="SentencePiece Library"></a>SentencePiece Library</h2><p>SentencePiece<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226)
">[12]</span></a></sup><sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SentencePiece](https://github.com/google/sentencepiece)
">[17]</span></a></sup> includes the space in the base vocabulary then use BPE or unigram algorithm to tokenize. XLNet, T5, ALBERT use SentencePiece for subword tokenization. It uses the unigram by default.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SentencePiece</span></span><br><span class="line">--byte_fallback: (<span class="built_in">type</span>: bool, default: <span class="literal">false</span>)</span><br><span class="line">    decompose unknown pieces into UTF-8 byte pieces.    </span><br><span class="line">    Note: need to <span class="built_in">set</span> --character_coverage less than 1.0, otherwise byte-fall-backed tokens may not appear <span class="keyword">in</span> the training data.</span><br><span class="line">--character_coverage: (<span class="built_in">type</span>: double; default:0.9995)</span><br><span class="line">    character coverage of determining the minimal symbols.</span><br><span class="line">    </span><br><span class="line"><span class="comment"># see: https://github.com/google/sentencepiece/blob/master/doc/options.md</span></span><br></pre></td></tr></table></figure>
<p>Pros：</p>
<ol>
<li>C++ implementations makes it blazingly fast to tokenize.</li>
<li>It is <strong>whitespace agnostic</strong>, supporting to train non-whitespace delineated languages, such as Chinese and Japanese with the same ease as English or French.<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)
">[18]</span></a></sup></li>
<li>It works at the byte level.</li>
</ol>
<h3 id="Basic-usage"><a href="#Basic-usage" class="headerlink" title="Basic usage"></a>Basic usage</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># env / data</span></span><br><span class="line">pip install sentencepiece</span><br><span class="line">wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line"><span class="comment"># train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`</span></span><br><span class="line"><span class="comment"># `m.vocab` is just a reference. not used in the segmentation.</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># makes segmenter instance and loads the model file (m.model)</span></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode: text =&gt; id</span></span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;This is a test&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_ids(<span class="string">&#x27;This is a test&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># decode: id =&gt; text</span></span><br><span class="line"><span class="built_in">print</span>(sp.decode_pieces([<span class="string">&#x27;▁This&#x27;</span>, <span class="string">&#x27;▁is&#x27;</span>, <span class="string">&#x27;▁a&#x27;</span>, <span class="string">&#x27;▁t&#x27;</span>, <span class="string">&#x27;est&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(sp.decode_ids([<span class="number">209</span>, <span class="number">31</span>, <span class="number">9</span>, <span class="number">375</span>, <span class="number">586</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns vocab size</span></span><br><span class="line"><span class="built_in">print</span>(sp.get_piece_size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># id &lt;=&gt; piece conversion</span></span><br><span class="line"><span class="built_in">print</span>(sp.id_to_piece(<span class="number">209</span>))</span><br><span class="line"><span class="built_in">print</span>(sp.piece_to_id(<span class="string">&#x27;▁This&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns 0 for unknown tokens (we can change the id for UNK)</span></span><br><span class="line"><span class="built_in">print</span>(sp.piece_to_id(<span class="string">&#x27;__MUST_BE_UNKNOWN__&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;unk&gt;, &lt;s&gt;, &lt;/s&gt; are defined by default. Their ids are (0, 1, 2)</span></span><br><span class="line"><span class="comment"># &lt;s&gt; and &lt;/s&gt; are defined as &#x27;control&#x27; symbol.</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">  <span class="built_in">print</span>(sp.id_to_piece(<span class="built_in">id</span>), sp.is_control(<span class="built_in">id</span>))</span><br></pre></td></tr></table></figure>
<h3 id="User-defined-and-control-symbols"><a href="#User-defined-and-control-symbols" class="headerlink" title="User defined and control symbols"></a>User defined and control symbols</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Example of user defined symbols</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_user --user_defined_symbols=&lt;sep&gt;,&lt;cls&gt; --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp_user = spm.SentencePieceProcessor()</span><br><span class="line">sp_user.load(<span class="string">&#x27;m_user.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ids are reserved in both mode.</span></span><br><span class="line"><span class="comment"># &lt;unk&gt;=0, &lt;s&gt;=1, &lt;/s&gt;=2, &lt;sep&gt;=3, &lt;cls&gt;=4</span></span><br><span class="line"><span class="comment"># user defined symbols allow these symbol to apper in the text.</span></span><br><span class="line"><span class="built_in">print</span>(sp_user.encode_as_pieces(<span class="string">&#x27;this is a test&lt;sep&gt; hello world&lt;cls&gt;&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sp_user.piece_to_id(<span class="string">&#x27;&lt;sep&gt;&#x27;</span>))  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(sp_user.piece_to_id(<span class="string">&#x27;&lt;cls&gt;&#x27;</span>))  <span class="comment"># 4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;3=&#x27;</span>, sp_user.decode_ids([<span class="number">3</span>]))  <span class="comment"># decoded to &lt;sep&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;4=&#x27;</span>, sp_user.decode_ids([<span class="number">4</span>]))  <span class="comment"># decoded to &lt;cls&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="Unigram-sampling-and-nbest-segmentation-for-subword-regularization"><a href="#Unigram-sampling-and-nbest-segmentation-for-subword-regularization" class="headerlink" title="Unigram: sampling and nbest segmentation for subword regularization"></a>Unigram: sampling and nbest segmentation for subword regularization</h3><p>When <code>--model_type=unigram</code> (default) is used, we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://www.aclweb.org/anthology/P18-1007.pdf)
">[11]</span></a></sup> for more detail. <code>nbest_size</code> is the number of highest-ranked groups of tokens to sample from at each time, where <strong>-1</strong> means all of the possibilities.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Can obtain different segmentations per request.</span></span><br><span class="line"><span class="comment"># There are two hyperparamenters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  <span class="built_in">print</span>(sp.sample_encode_as_pieces(<span class="string">&#x27;hello world&#x27;</span>, -<span class="number">1</span>, <span class="number">0.1</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  <span class="built_in">print</span>(sp.sample_encode_as_ids(<span class="string">&#x27;hello world&#x27;</span>, -<span class="number">1</span>, <span class="number">0.1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sample</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    result = sp.encode(<span class="string">&#x27;This is a test&#x27;</span>, out_type=<span class="built_in">str</span>, enable_sampling=<span class="literal">True</span>, alpha=<span class="number">0.1</span>, nbest_size=-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get 10 best</span></span><br><span class="line"><span class="built_in">print</span>(sp.nbest_encode_as_pieces(<span class="string">&#x27;hello world&#x27;</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(sp.nbest_encode_as_ids(<span class="string">&#x27;hello world&#x27;</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h3 id="BPE-model"><a href="#BPE-model" class="headerlink" title="BPE model"></a>BPE model</h3><p>Sentencepiece also supports BPE (byte pair encoding) model by setting <code>--model_type=bpe</code>. The BPE model does not support sampling and n-best segmentation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe&#x27;</span>)</span><br><span class="line">sp_bpe = spm.SentencePieceProcessor()</span><br><span class="line">sp_bpe.load(<span class="string">&#x27;m_bpe.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;*** BPE ***&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(sp_bpe.encode_as_pieces(<span class="string">&#x27;thisisatesthelloworld&#x27;</span>)) <span class="comment"># [&#x27;▁this&#x27;, &#x27;is&#x27;, &#x27;at&#x27;, &#x27;est&#x27;, &#x27;he&#x27;, &#x27;llow&#x27;, &#x27;or&#x27;, &#x27;ld&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(sp_bpe.nbest_encode_as_pieces(<span class="string">&#x27;hello world&#x27;</span>, <span class="number">5</span>))  <span class="comment"># [] (returns an empty list)</span></span><br></pre></td></tr></table></figure>
<h3 id="Character-and-word-model"><a href="#Character-and-word-model" class="headerlink" title="Character and word model"></a>Character and word model</h3><p>Sentencepiece supports character and word segmentation with <code>--model_type=char</code> and <code>--model_type=character</code> flags.<br>In <code>word</code> segmentation, sentencepiece just segments tokens with whitespaces, so the input text must be pre-tokenized. We can apply different segmentation algorithm transparently without changing pre/post processors.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># char model</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_char --model_type=char --vocab_size=400&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp_char = spm.SentencePieceProcessor()</span><br><span class="line">sp_char.load(<span class="string">&#x27;m_char.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sp_char.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(sp_char.encode_as_ids(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word model</span></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m_word --model_type=word --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp_word = spm.SentencePieceProcessor()</span><br><span class="line">sp_word.load(<span class="string">&#x27;m_word.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sp_word.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))  <span class="comment"># &#x27;.&#x27; will not be one token.</span></span><br><span class="line"><span class="built_in">print</span>(sp_word.encode_as_ids(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="Text-normalization"><a href="#Text-normalization" class="headerlink" title="Text normalization"></a>Text normalization</h3><p>Sentencepiece provides the following general pre-defined normalization rules. We can change the normalizer with <code>--normaliation_rule_name=&lt;NAME&gt;</code> flag.</p>
<ul>
<li><strong>nmt_nfkc</strong>: NFKC normalization with some additional normalization around spaces. (default)</li>
<li><strong>nfkc</strong>: original: NFKC normalization.</li>
<li><strong>nmt_nfkc_cf</strong>: nmt_nfkc + Unicode case folding (mostly lower casing)</li>
<li><strong>nfkc_cf</strong>: nfkc + Unicode case folding.</li>
<li><strong>identity</strong>: no normalization</li>
</ul>
<p>The TSV file is fed with <code>--normalization_rule_tsv=&lt;FILE&gt;</code> flag.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tocode</span>(<span class="params">s</span>):</span>                                                                               </span><br><span class="line">    out = []                                                                                 </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> s:                                                                              </span><br><span class="line">        out.append(<span class="built_in">str</span>(<span class="built_in">hex</span>(<span class="built_in">ord</span>(c))).replace(<span class="string">&#x27;0x&#x27;</span>, <span class="string">&#x27;U+&#x27;</span>))                                     </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(out)          </span><br><span class="line"></span><br><span class="line"><span class="comment"># TSV format:  source Unicode code points &lt;tab&gt; target code points</span></span><br><span class="line"><span class="comment"># normalize &quot;don&#x27;t =&gt; do not,  I&#x27;m =&gt; I am&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;normalization_rule.tsv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  f.write(tocode(<span class="string">&quot;I&#x27;m&quot;</span>) + <span class="string">&#x27;\t&#x27;</span> + tocode(<span class="string">&quot;I am&quot;</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">  f.write(tocode(<span class="string">&quot;don&#x27;t&quot;</span>) + <span class="string">&#x27;\t&#x27;</span> + tocode(<span class="string">&quot;do not&quot;</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">open</span>(<span class="string">&#x27;normalization_rule.tsv&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).read())</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_tsv=normalization_rule.tsv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line"><span class="comment"># m.model embeds the normalization rule compiled into an FST.</span></span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&quot;I&#x27;m busy&quot;</span>))  <span class="comment"># normalzied to `I am busy&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&quot;I don&#x27;t know it.&quot;</span>))  <span class="comment"># normalized to &#x27;I do not know it.&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="Vocabulary-restriction"><a href="#Vocabulary-restriction" class="headerlink" title="Vocabulary restriction"></a>Vocabulary restriction</h3><p>We can encode the text only using the tokens specified with <code>set_vocabulary</code> method.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gets all tokens as Python list.</span></span><br><span class="line">vocabs = [sp.id_to_piece(<span class="built_in">id</span>) <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(sp.get_piece_size())]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Aggregates the frequency of each token in the training data.</span></span><br><span class="line">freq = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;botchan.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        line = line.rstrip()</span><br><span class="line">        <span class="keyword">for</span> piece <span class="keyword">in</span> sp.encode_as_pieces(line):</span><br><span class="line">            freq.setdefault(piece, <span class="number">0</span>)</span><br><span class="line">            freq[piece] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"><span class="comment"># only uses the token appearing more than 1000 times in the training data.</span></span><br><span class="line">vocabs = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x : x <span class="keyword">in</span> freq <span class="keyword">and</span> freq[x] &gt; <span class="number">1000</span>, vocabs))</span><br><span class="line">sp.set_vocabulary(vocabs)</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the restriction</span></span><br><span class="line">sp.reset_vocabulary()</span><br><span class="line"><span class="built_in">print</span>(sp.encode_as_pieces(<span class="string">&#x27;this is a test.&#x27;</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="Extracting-crossing-words-pieces"><a href="#Extracting-crossing-words-pieces" class="headerlink" title="Extracting crossing-words pieces"></a>Extracting crossing-words pieces</h3><p>Sentencepieces does not extract pieces crossing multiple <strong>words</strong> (here the <strong>word</strong> means the space delimited tokens). The piece will never contain the whitespace marker (_) in the middle.</p>
<p><code>--split_by_whtespace=false</code> disables this restriction and allows to extract pieces crossing multiple words. In CJK (Chinese/Japanese/Korean), this flag will not affect the final segmentation results so much as words are not tokenized with whitespaces in CJK.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000 --split_by_whitespace=false&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gets all tokens as Python list.</span></span><br><span class="line">vocabs = [sp.id_to_piece(<span class="built_in">id</span>) <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(sp.get_piece_size())]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> piece <span class="keyword">in</span> vocabs[<span class="number">0</span>:<span class="number">500</span>]:</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">&#x27;\w+▁\w+&#x27;</span>, piece):</span><br><span class="line">        <span class="built_in">print</span>(piece)</span><br></pre></td></tr></table></figure>
<h3 id="Getting-byte-offsets-of-tokens"><a href="#Getting-byte-offsets-of-tokens" class="headerlink" title="Getting byte offsets of tokens"></a>Getting byte offsets of tokens</h3><p>Sentencepiece keeps track of byte offset (span) of each token, which is useful for highlighting the token on top of unnormalized text.</p>
<p>We first need to install <strong>protobuf</strong> module and <strong>sentencepiece_pb2.py</strong> as the byte offsets and all other meta data for segementation are encoded in protocol buffer. <code>encode_as_serialized_proto</code> method resturns serialized SentencePieceText proto. You can get the deserialized object by calling ParseFromString method.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install protobuf</span><br><span class="line">wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece_pb2</span><br><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.train(<span class="string">&#x27;--input=botchan.txt --model_prefix=m --vocab_size=2000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(<span class="string">&#x27;m.model&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One best result</span></span><br><span class="line">spt = sentencepiece_pb2.SentencePieceText()</span><br><span class="line">spt.ParseFromString(sp.encode_as_serialized_proto(<span class="string">&#x27;ｈｅｌｌｏ&#x27;</span>)) <span class="comment"># Full width hello</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># begin/end (offsets) are pointing to the original input.</span></span><br><span class="line"><span class="built_in">print</span>(spt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nbest results</span></span><br><span class="line">nspt = sentencepiece_pb2.NBestSentencePieceText()</span><br><span class="line">nspt.ParseFromString(sp.nbest_encode_as_serialized_proto(<span class="string">&#x27;ｈｅｌｌｏ&#x27;</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># print(nspt)</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">text: &quot;\357\275\210\357\275\205\357\275\214\357\275\214\357\275\217&quot;</span></span><br><span class="line"><span class="string">pieces &#123;</span></span><br><span class="line"><span class="string">  piece: &quot;\342\226\201he&quot;</span></span><br><span class="line"><span class="string">  id: 28</span></span><br><span class="line"><span class="string">  surface: &quot;\357\275\210\357\275\205&quot;</span></span><br><span class="line"><span class="string">  begin: 0</span></span><br><span class="line"><span class="string">  end: 6</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">pieces &#123;</span></span><br><span class="line"><span class="string">  piece: &quot;ll&quot;</span></span><br><span class="line"><span class="string">  id: 98</span></span><br><span class="line"><span class="string">  surface: &quot;\357\275\214\357\275\214&quot;</span></span><br><span class="line"><span class="string">  begin: 6</span></span><br><span class="line"><span class="string">  end: 12</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">pieces &#123;</span></span><br><span class="line"><span class="string">  piece: &quot;o&quot;</span></span><br><span class="line"><span class="string">  id: 38</span></span><br><span class="line"><span class="string">  surface: &quot;\357\275\217&quot;</span></span><br><span class="line"><span class="string">  begin: 12</span></span><br><span class="line"><span class="string">  end: 15</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="Add-new-special-tokens"><a href="#Add-new-special-tokens" class="headerlink" title="Add new special tokens"></a>Add new special tokens</h3><p>For the need of expanding new special tokens to pre-trained sentencepiece model, such as <code>[MASK0-99]</code>, <code>[DOMAIN0-99]</code>, and so on.<br>Ref: <sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[sentencepiece/python/add_new_vocab.ipynb](https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb)
">[19]</span></a></sup><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Run this code in google/sentencepiece/python/</span></span><br><span class="line"><span class="comment"># Load pre-trained sentencepiece model</span></span><br><span class="line"><span class="keyword">import</span> sentencepiece_model_pb2 <span class="keyword">as</span> model</span><br><span class="line">m = model.ModelProto()</span><br><span class="line">m.ParseFromString(<span class="built_in">open</span>(<span class="string">&quot;old.model&quot;</span>, <span class="string">&quot;rb&quot;</span>).read())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the list of new tokens want to add</span></span><br><span class="line">special_tokens = <span class="built_in">open</span>(<span class="string">&quot;special_tokens.txt&quot;</span>, <span class="string">&quot;r&quot;</span>).read().split(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add new tokens to sentencepiece model</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> special_tokens:</span><br><span class="line">    new_token = model.ModelProto().SentencePiece()</span><br><span class="line">    new_token.piece = token</span><br><span class="line">    new_token.score = <span class="number">0</span></span><br><span class="line">    m.pieces.append(new_token)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Save new sentencepiece model</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;new.model&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(m.SerializeToString())</span><br></pre></td></tr></table></figure></p>
<h3 id="Handle-whitespaces-newlines"><a href="#Handle-whitespaces-newlines" class="headerlink" title="Handle whitespaces/newlines"></a>Handle whitespaces/newlines</h3><p><a href="https://github.com/google/sentencepiece/issues/684">GitHub Issue</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--remove_extra_whitespaces=<span class="literal">false</span></span><br><span class="line"><span class="comment"># In addition, newlines are all normalized whitespaces internally by default. You can stop all normalizations with </span></span><br><span class="line">--normalization_rule_name=identity</span><br></pre></td></tr></table></figure></p>
<p>Ref:<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub issue: Manually modifying SentencePiece model?](https://github.com/google/sentencepiece/issues/121)
">[20]</span></a></sup>.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">% <span class="built_in">cd</span> src</span><br><span class="line">% protoc --python_out=. sentencepiece_model.proto</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; import sentencepiece_model_pb2 as model</span><br><span class="line">&gt;&gt;&gt; m = model.ModelProto()</span><br><span class="line">&gt;&gt;&gt; m.ParseFromString(open(<span class="string">&#x27;../python/test/test_ja_model.model&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>).<span class="built_in">read</span>())</span><br><span class="line">352301</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> p <span class="keyword">in</span> m.pieces:</span><br><span class="line">...     p.score += 10.0</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; with open(<span class="string">&#x27;new.model&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) as f:</span><br><span class="line">...     f.write(m.SerializeToString())</span><br></pre></td></tr></table></figure><br>Refer to <a href="https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=T7F349Sd2Bzg">Sentencepiece python module example</a></p>
<h2 id="Huggingface-tokenizers"><a href="#Huggingface-tokenizers" class="headerlink" title="Huggingface tokenizers"></a>Huggingface tokenizers</h2><h3 id="Add-special-tokens"><a href="#Add-special-tokens" class="headerlink" title="Add special tokens"></a>Add special tokens</h3><p><sup id="fnref:22"><a href="#fn:22" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[why does huggingface t5 tokenizer ignore some of the whitespaces?](https://stackoverflow.com/questions/72214408/why-does-huggingface-t5-tokenizer-ignore-some-of-the-whitespaces)
">[22]</span></a></sup><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> AddedToken</span><br><span class="line">tokenizer.add_special_tokens(&#123;<span class="string">&quot;additional_special_tokens&quot;</span>: [AddedToken(<span class="string">&quot;\n&quot;</span>)]&#125;)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.special_tokens_map)</span><br></pre></td></tr></table></figure></p>
<h3 id="Handle-non-space-separated-language"><a href="#Handle-non-space-separated-language" class="headerlink" title="Handle non-space-separated language"></a>Handle non-space-separated language</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/huggingface/tokenizers/issues/990</span></span><br><span class="line">from tokenizers import trainers, models, Tokenizer, pre_tokenizers</span><br><span class="line"></span><br><span class="line">pre_tokenizer = pre_tokenizers.Sequence(</span><br><span class="line">    [</span><br><span class="line">        pre_tokenizers.WhitespaceSplit(),</span><br><span class="line">        pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pre_tokenizer.pre_tokenize_str(<span class="string">&quot;私 は りんご が 好き です&quot;</span>))</span><br><span class="line"><span class="comment"># [(&#x27;ç§ģ&#x27;, (0, 1)), (&#x27;ãģ¯&#x27;, (2, 3)), (&#x27;ãĤĬãĤĵãģĶ&#x27;, (4, 7)), (&#x27;ãģĮ&#x27;, (8, 9)), (&#x27;å¥½ãģį&#x27;, (10, 12)), (&#x27;ãģ§ãģĻ&#x27;, (13, 15))]</span></span><br></pre></td></tr></table></figure>
<h2 id="OpenAI-tiktoken"><a href="#OpenAI-tiktoken" class="headerlink" title="OpenAI tiktoken"></a>OpenAI tiktoken</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># install</span></span><br><span class="line">pip install --upgrade tiktoken</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"></span><br><span class="line">enc = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line"></span><br><span class="line">toks = enc.encode(<span class="string">&quot;tiktoken tokenizer&quot;</span>)</span><br><span class="line"></span><br><span class="line">x = enc.decode([<span class="number">83</span>, <span class="number">1609</span>, <span class="number">5963</span>, <span class="number">374</span>, <span class="number">2294</span>, <span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Refer to <sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
">[23]</span></a></sup><sup id="fnref:24"><a href="#fn:24" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[OpenAI tiktoken](https://github.com/openai/tiktoken)
">[24]</span></a></sup><sup id="fnref:25"><a href="#fn:25" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[OpenAI API Tokenizer](https://platform.openai.com/tokenizer)">[25]</span></a></sup>.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>OpenAI models</strong></th>
<th><strong>Vocab</strong></th>
<th><strong>#Vocab</strong></th>
<th><strong>#Numeric<br> token</strong></th>
<th><strong>Task</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt2</td>
<td>gpt2</td>
<td>50257</td>
<td>1691</td>
<td>open source</td>
</tr>
<tr>
<td>davinci/curie/gabbage/ada<br>text-{name}-001 (name=davinci/curie/gabbage/ada)</td>
<td>r50k_base</td>
<td>50257</td>
<td>1691</td>
<td>old text</td>
</tr>
<tr>
<td>text-similarity-davinci-001<br>text-search-{name}-doc-001<br>(name=davinci/curie/gabbage/ada)<br>code-search-{name}-code-001<br>(name=gabbage/ada)</td>
<td>r50k_base</td>
<td>50257</td>
<td>1691</td>
<td>old embedding</td>
</tr>
<tr>
<td>text-davinci-003<br>text-davinci-002</td>
<td>p50k_base</td>
<td>50281<br>(add whitespace tokens of length 2-24)</td>
<td>1691</td>
<td>text</td>
</tr>
<tr>
<td>code-davinci-001/002<br>code-cushman-001/002<br>davinci/cushman-codex</td>
<td>p50k_base</td>
<td>50281<br>(add whitespace tokens of length 2-24)</td>
<td>1691</td>
<td>code</td>
</tr>
<tr>
<td>text/code-davinci-edit-001</td>
<td>p50k_edit</td>
<td>50284 (add 3 special tokens)</td>
<td>1691</td>
<td>edit</td>
</tr>
<tr>
<td>text-embedding-ada-002</td>
<td>cl100k_base</td>
<td>100256</td>
<td>1122</td>
<td>embedding</td>
</tr>
<tr>
<td>gpt-3.5-turbo<br>gpt-4</td>
<td>cl100k_base</td>
<td>100256</td>
<td>1122</td>
<td>chat</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Empirical-Analysis"><a href="#Empirical-Analysis" class="headerlink" title="Empirical Analysis"></a>Empirical Analysis</h2><h3 id="Unigram-vs-Char-based-BPE"><a href="#Unigram-vs-Char-based-BPE" class="headerlink" title="Unigram vs Char-based BPE"></a>Unigram vs Char-based BPE</h3><p><strong>Unigram aligns better than char-based BPE does in morphology.</strong> <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720)3720)
">[15]</span></a></sup> argued that Unigram LM tokenization can recover subword units that align with morphology much better than BPE do, using SentencePiece<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226)
">[12]</span></a></sup> implementation on English and Japanese Wikipedia. </p>
<p>It can be seen from the below figure that Unigram tends to produce longer subword units than BPE on average and have more tokens of moderate frequency.</p>
<p><img data-src="/notes/images/Unigram-vs-BPE-token-length-and-frequency.png" alt="English subword token vocabulary comparison between Unigram and BPE tokenization."></p>
<p>As shown in the table, BPE tokenization tends to merge common tokens, such as English inflectional suffixes and Japanese particles, into their neighbors even though resulting units are not semantically meaningful. This may be due to the greedy construction of BPE tokenization.</p>
<p><img data-src="/notes/images/Unigram-vs-BPE-evaluation.png" alt="Unigram vs char-based BPE tokenization &lt;small&gt;[11]&lt;/small&gt;"></p>
<p><sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720)3720)
">[15]</span></a></sup> found that segmentations produced by Unigram LM align more closely to the morphological references in both English and Japanese.<br><img data-src="/notes/images/Unigram-vs-BPE-in-segmentation.png" alt="Subword boundaries between tokenized subwords and morphological segmentations."></p>
<p><strong>Models using Unigram outperform counterparts using BPE in finetuning downstream tasks.</strong> <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://arxiv.org/pdf/2004.03720)3720)
">[15]</span></a></sup> claimed that fine-tuning models pretrained with unigram LM tokenization produces better performance than with BPE tokenization for experimented tasks.</p>
<p><img data-src="/notes/images/Unigram-vs-BPE-finetuning-tasks.png" alt="Unigram vs char-based BPE on finetuning downstream tasks"></p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2021tokenization-PTMs,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Word Tokenization for Pre-trained Models&#125;&#125;,</span><br><span class="line">  year = &#123;2021&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2021/11/29/Subword-Tokenization-in-NLP/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization">Word Tokenization: How to Handle Out-Of-Vocabulary Words?</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c">A comprehensive guide to subword tokenisers</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://huggingface.co/blog/how-to-train">How to train a new language model from scratch using Transformers and Tokenizers</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb#scrollTo=tCvb9epa2iZV">Colab: train your tokenizer</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/google-research/bert">Github: Google BERT</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://huggingface.co/docs/tokenizers/python/latest/quicktour.html">Hugginface tokenizer</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/">How to Train BPE, WordPiece, and Unigram Tokenizers from Scratch using Hugging Face</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.aclweb.org/anthology/P16-1162.pdf">[BPE] Neural Machine Translation of Rare Words with Subword Units
Rico</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1609.08144.pdf">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/P18-1007.pdf">Taku Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1808.06226">Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/#Subword-Tokenization">Word Tokenization: How to Handle Out-Of-Vocabulary Words?</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.persagen.com/files/misc/radford2019language.pdf">[GPT-2]Language models are unsupervised multitask learners</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/2004.03720">Byte Pair Encoding is Suboptimal for Language Model Pretraining</a>3720)<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15">SentencePiece Tokenizer Demystified
</a><a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb">SentencePiece Python Module Example</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/google/sentencepiece">SentencePiece</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15">SentencePiece Tokenizer Demystified</a><a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb">sentencepiece/python/add_new_vocab.ipynb</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/google/sentencepiece/issues/121">GitHub issue: Manually modifying SentencePiece model?</a><a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://medium.com/geekculture/easy-sentencepiece-for-subword-tokenization-in-python-and-tensorflow-4361a1ed8e39">Easy SentencePiece for Subword Tokenization in Python and Tensorflow</a><a href="#fnref:21" rev="footnote"> ↩</a></span></li><li id="fn:22"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/72214408/why-does-huggingface-t5-tokenizer-ignore-some-of-the-whitespaces">why does huggingface t5 tokenizer ignore some of the whitespaces?</a><a href="#fnref:22" rev="footnote"> ↩</a></span></li><li id="fn:23"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">How to count tokens with tiktoken</a><a href="#fnref:23" rev="footnote"> ↩</a></span></li><li id="fn:24"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/openai/tiktoken">OpenAI tiktoken</a><a href="#fnref:24" rev="footnote"> ↩</a></span></li><li id="fn:25"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">25.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://platform.openai.com/tokenizer">OpenAI API Tokenizer</a><a href="#fnref:25" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>LLM</category>
        <category>Tokenization</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Pre-training</tag>
        <tag>NLP</tag>
        <tag>Tokenization</tag>
      </tags>
  </entry>
  <entry>
    <title>Multimodal Tokenization with Vector Quantization: A Review</title>
    <url>/notes/2024/05/24/Tokenization-with-Vector-Quantization/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A review of multimodal tokenization approaches using vector quantization<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Vector quantization (wiki)](https://en.wikipedia.org/wiki/Vector_quantization)">[1]</span></a></sup> approaches.<br><span id="more"></span></p>
<h1 id="Codebook-Learning-with-Vector-Quantization"><a href="#Codebook-Learning-with-Vector-Quantization" class="headerlink" title="Codebook Learning with Vector Quantization"></a>Codebook Learning with Vector Quantization</h1><h2 id="VQ-VAE-NeurIPS’17"><a href="#VQ-VAE-NeurIPS’17" class="headerlink" title="VQ-VAE (NeurIPS’17)"></a>VQ-VAE (NeurIPS’17)</h2><p>Vector-quantized AutoEncoder (VQ-VAE)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Van Den Oord, Aaron, and Oriol Vinyals. "[Neural discrete representation learning.](https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)" Advances in neural information processing systems 30 (2017).">[2]</span></a></sup> combines the variational autoencoder (VAE) with vector quantization (VQ)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Vector quantization (wiki)](https://en.wikipedia.org/wiki/Vector_quantization)">[1]</span></a></sup>, using the parameterization of the posterior distribution of (discrete) latents given an observation. </p>
<div class="note info">
            <p>It does not suffer from large variance, and avoids the ‘’posterior collapse’’ issue which has been problematic with many VAEs that have a strong decoder, often caused by latents being ignored.</p>
          </div>
<h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>VAEs encompass following parts:<br>(1) an encoder network parameterized by a posterior distribution $q(z|x)$ of discrete latent random variables $z$ given the input data $x$, (2) a prior distribution $p(z)$, and (3) a decoder with a distribution $p(x|z)$ over input data.</p>
<h3 id="Discrete-latents"><a href="#Discrete-latents" class="headerlink" title="Discrete latents"></a>Discrete latents</h3><p>VQ-VAE defines the posterior and prior distributions as categorical, and the samples drawn from these distributions index an embedding table, which are used as the decoder inputs.</p>
<p><img data-src="/notes/images/VQ-VAE.png" alt="VQ-VAE"></p>
<p>VQ-VAE defines the latent embedding space $e \in \mathbb{R}^{K \times D}$ where $K$ is the $K$-way categorical embedding table size, $D$ is the size of latent embedding vector $e_i \in \mathbb{R}^D, i \in 1,2,\cdots, K$. The encoder takes the input $x$ to get the output $z_e(x)$. The discrete latent variables $z$ are then calculated by nearest neighbour look-up using shared embedding space $e$. The posterior categorical distribution $q(z|x)$ are defined as 1-hot distribution:</p>
<script type="math/tex; mode=display">
\begin{align}
q(z=k|x)=\begin{cases}1&\text{for k}=\text{argmin}_j\|z_e(x)-e_j\|_2,\\0&\text{otherwise}\end{cases}, \label{eq:posterior}
\end{align}</script><p>We view this as a VAE that can bound $\log p(x)$ with the ELBO. The distribution $q(z=k|x)$ is deterministic and by defining a simple uniform prior over $z$ we obtain a KL divergence constant and equal to $\log K$.</p>
<p>The representation $z_e(x)$ is passed through the discretization bottleneck followed by mapping onto the nearest element of embedding $e$. The input to the decoder is the nearest embedding vector $e_k$ as follows:</p>
<script type="math/tex; mode=display">
\begin{align}
z_q(x)=e_k,\quad\text{where}\quad k=\text{argmin}_j\|z_e(x)-e_j\|_2 \label{eq:emb}
\end{align}</script><p>This can be treated as an autoencoder with a particular non-linearity that maps the latents to 1-of-$K$ embedding vectors.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>The Eq.$\eqref{eq:emb}$ and $\eqref{eq:posterior}$ approximate the gradient using straight-through estimator and just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$.</p>
<script type="math/tex; mode=display">
\begin{align} 
    \mathcal{L} &{}= \underbrace{\log p(x|z_q(x))}_{\text{reconstruction loss}} + \underbrace{\|\|\mathrm{sg}[z_e(x)]-e\|\|_2^2}_{\text{codebook loss}} + \underbrace{\beta\|\|z_e(x)-\mathrm{sg}[e]\|\|_2^2}_{\text{commitment loss}}, \\
    &{}= \underbrace{\Vert x - D(e) \Vert_2^2}_{\text{reconstruction loss}} + \underbrace{\Vert \text{sg}[E(x)] - e \Vert_2^2}_{\text{codebook loss}} + \underbrace{\beta  \Vert \text{sg}[e] - E(x) \Vert_2^2}_{\text{commitment loss}}  \label{eq:vq_loss}
\end{align}</script><p>Here, $\beta=0.25$.</p>
<h4 id="EMA-Update"><a href="#EMA-Update" class="headerlink" title="EMA Update"></a>EMA Update</h4><p>VQVAE can use exponential moving average (EMA) updates for the codebook, as the replacement for the codebook loss, the 2nd term in Eq.$\eqref{eq:vq_loss}$.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    N_{i}^{(t)} &{}:=N_{i}^{(t-1)}*\gamma+n_{i}^{(t)}(1-\gamma) \\
    \quad m_{i}^{(t)} &{}:=m_{i}^{(t-1)}*\gamma+\sum_{j}^{n_{i}^{(t)}}E(x)_{i,j}^{(t)}(1-\gamma)\\
    e_{i}^{(t)} &{}:=\frac{m_{i}^{(t)}}{N_{i}^{(t)}}
\end{aligned}</script><p>where $n_{i}^{(t)}$ is the number of quantized vectors in $E(x)$, $\gamma \in [0,1] $ is a decay parameter. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reconstruction loss</span></span><br><span class="line">loss = F.mse_loss(quantize, x.detach())</span><br><span class="line"><span class="comment"># determine code to use for commitment loss</span></span><br><span class="line">maybe_detach = torch.detach <span class="keyword">if</span> <span class="keyword">not</span> self.learnable_codebook <span class="keyword">or</span> freeze_codebook <span class="keyword">else</span> identity</span><br><span class="line"></span><br><span class="line">commit_quantize = maybe_detach(quantize)            </span><br><span class="line"></span><br><span class="line"><span class="comment"># straight through</span></span><br><span class="line">quantize = x + (quantize - x).detach()</span><br><span class="line"></span><br><span class="line">commit_loss = F.mse_loss(commit_quantize, x)</span><br><span class="line"></span><br><span class="line">loss = loss + commit_loss * self.commitment_weight</span><br></pre></td></tr></table></figure>
<h2 id="VQVAE-2-NeurIPS’19"><a href="#VQVAE-2-NeurIPS’19" class="headerlink" title="VQVAE-2 (NeurIPS’19)"></a>VQVAE-2 (NeurIPS’19)</h2><p>VQVAE-2<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Razavi, Ali, Aaron Van den Oord, and Oriol Vinyals. "[Generating diverse high-fidelity images with vq-vae-2](https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf)." Advances in neural information processing systems 32 (2019). ">[3]</span></a></sup> introduces a multi-scale hierarchical structure to the original VQVAE framework, complemented by PixelCNN priors that govern the latent codes.</p>
<h3 id="Stage-1-Hierarchical-Latent-Codes"><a href="#Stage-1-Hierarchical-Latent-Codes" class="headerlink" title="Stage 1: Hierarchical Latent Codes"></a>Stage 1: Hierarchical Latent Codes</h3><div class="note info">
            <p><strong>Motivation:</strong> Hierarchical VQ models capture local features, such as textures, distinctly from global features, like the shape and geometry of objects.</p>
          </div>
<p>VQVAE-2<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Razavi, Ali, Aaron Van den Oord, and Oriol Vinyals. "[Generating diverse high-fidelity images with vq-vae-2](https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf)." Advances in neural information processing systems 32 (2019). ">[3]</span></a></sup> employs a hierarchical arrangement of VQ codes to effectively model large images. In this hierarchy, the <strong>top-level</strong> latent code encapsulates global information, while the <strong>bottom-level</strong> latent code, which is conditioned on the top-level code, is tasked with capturing local details.</p>
<p><img data-src="/notes/images/VQVAE-2.png" alt="VQVAE-2"></p>
<p>Without the conditioning of the bottom-level latent on the top-level latent, the top-level latent would be burdened with the task of encoding every minute detail from the pixels. By allowing each level in the hierarchy to focus on different aspects of the pixels, the model encourages the encoding of complementary information across each latent map. This strategy is instrumental in minimizing the reconstruction error during the encoding process. For a more in-depth understanding, refer to the algorithmic details provided.</p>
<h3 id="Stage-2-Learning-Priors-over-Latent-Codes"><a href="#Stage-2-Learning-Priors-over-Latent-Codes" class="headerlink" title="Stage 2: Learning Priors over Latent Codes"></a>Stage 2: Learning Priors over Latent Codes</h3><p>In the second stage, VQVAE-2 learns a prior for the latent codes. It involves fitting a prior distribution to the learned posterior, effectively achieving lossless compression of the latent space. This is accomplished by re-encoding the latent variables with a distribution that more accurately approximates their true underlying distribution. Also, they find that self-attention layers can capture correlations in spatial locations that are far apart in the image.</p>
<p><img data-src="/notes/images/VQVAE-2 algorithm.png" alt="VQVAE-2 algorithm"></p>
<h2 id="VQGAN-CVPR’21"><a href="#VQGAN-CVPR’21" class="headerlink" title="VQGAN (CVPR’21)"></a>VQGAN (CVPR’21)</h2><p>VQGAN<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Esser, Patrick, Robin Rombach, and Bjorn Ommer. "[Taming transformers for high-resolution image synthesis.](http://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf)" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.">[4]</span></a></sup> proposes to combine CNNs with transformer architectures to learn a codebook of contextually rich visual elements. The model then utilizes the transformer’s capability to capture long-range interactions within global compositions. To ensure that the codebook effectively captures perceptually significant local structures, VQGAN employs an adversarial training strategy, reducing the transformer’s need to model low-level statistics.</p>
<p>QGAN employs a discriminator and perceptual loss to maintain high perceptual quality even at increased compression rates. It utilizes a patch-based discriminator, $D$, which is trained to differentiate between real and reconstructed images:</p>
<p>\begin{equation}<br>    \mathcal{L}_{\mathrm{GAN}}({E,G,\mathcal{Z}},D)=[\log D(x)+\log(1-D(\hat{x}))]<br>\end{equation}</p>
<p>Here, $E$ denotes the encoder, $G$ is the generator, <script type="math/tex">\mathcal{Z} = \{z_{k}\}_{k=1}^{K} \subset \mathbb{R}^{n_{z}}</script> represents the learned sicrete codebook.</p>
<p><img data-src="/notes/images/VQGAN.png" alt=""></p>
<p>The optimization objective for VQGAN is formulated as a min-max problem:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \begin{aligned}\mathcal{Q}^{*}=\arg\operatorname*{min}_{E,G,\mathcal{Z}}\operatorname*{max}_{D}\mathbb{E}_{x\sim p(x)}\Big[\mathcal{L}_{\mathrm{VQ}}(E,G,\mathcal{Z}) +\lambda\mathcal{L}_{\mathrm{GAN}}(\{E,G,\mathcal{Z}\},D)\Big]\end{aligned}
\end{equation}</script><p>The adaptive weight $\lambda$ is computed as follows:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \lambda=\frac{\nabla_{G_L}[\mathcal{L}_{\mathrm{recon}}]}{\nabla_{G_L}[\mathcal{L}_{\mathrm{GAN}}]+\delta}
\end{equation}</script><p>where <script type="math/tex">\mathcal{L}_{\mathrm{recon}}</script> is the perceptual reconstruction loss, <script type="math/tex">\nabla_{G_L}[\cdot]</script> denotes the gradient with respect to the last layer of the decoder.</p>
<p>Through this adversarial process, VQGAN not only learns to compress visual information efficiently but also ensures that the resulting images are perceptually convincing, bridging the gap between high-level semantic understanding and low-level pixel accuracy.</p>
<p>In the second stage, it pretrains a transformer to predict rasterized image tokens autoregressively.</p>
<h2 id="iGPT-ICML’20"><a href="#iGPT-ICML’20" class="headerlink" title="iGPT (ICML’20)"></a>iGPT (ICML’20)</h2><p>iGPT<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Mark, et al. "Generative pretraining from pixels." International conference on machine learning. PMLR, 2020.">[10]</span></a></sup> delves into the realm of autoregressive pre-training applied directly to image pixels. The process begins with reducing the image to a lower resolution to manage the extensive context that high-resolution images entail. Subsequently, iGPT employs a clustering strategy to further compress the pixel information. By applying $k$-means clustering to the (R, G, B) values of each pixel with $k$ set to 512, the model effectively condenses the color space, reducing the context length by a factor of three.</p>
<p>However, even after these initial steps, the resulting context—such as $96^2 \times 3$ or $192^2 \times 3$ —can remain unwieldy for efficient processing. To address this, iGPT utilizes a Variational Autoencoder with Vector Quantization (VQ-VAE) that compresses the pixel space into a latent grid of $48^2$. This transformation significantly shrinks the context size while retaining the image’s critical features.</p>
<p>iGPT assesses the quality of the learned representations through two different methods:</p>
<ol>
<li><p>Linear Probe: This technique involves training a linear classifier on top of the frozen pre-trained representations to evaluate how well they capture the necessary information for accurate classification tasks.</p>
</li>
<li><p>Finetuning: Alternatively, the model fine-tunes the pre-trained representations on downstream tasks.</p>
</li>
</ol>
<p><img data-src="/notes/images/iGPT.png" alt=""></p>
<h2 id="DALL-E-ICML’21"><a href="#DALL-E-ICML’21" class="headerlink" title="DALL-E (ICML’21)"></a>DALL-E (ICML’21)</h2><p>DALL-E<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramesh, Aditya, et al. "Zero-shot text-to-image generation." International conference on machine learning. Pmlr, 2021.">[11]</span></a></sup> applies a transformer that autoregressively models the text and image tokens as a single stream of data. It uses two-stage training procedure:</p>
<ol>
<li>Stage 1: Train a discrete VAE to compress each $256 \times 256$ RGB image into a $32 \times 32$ grid of image tokens, each element of which can assume 8192 possible values.</li>
<li>Stage 2: Concatenate up to 256 BPE-encoded text tokens with the $1024 (32 \times 32)$ image tokens, and train an autoregressive transformer to model the joint distribution over the text and image tokens.</li>
</ol>
<p>The overall procedure can be viewed as maximizing the evidence lower bound (ELB) on the joint likelihood of the model distribution over iamges $x$, captions $y$, and the tokens $z$ for the encoded RGB image. We model this distribution using the factorization <script type="math/tex">p_{\theta,\psi}(x,y,z)=p_{\theta}(x\mid y,z)p_{\psi}(y,z)</script>, which yields the lower bound:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \ln p_{\theta,\psi}(x,y)\geqslant\mathbb{E}_{z\sim q_{\phi}(z\mid x)}\left(\ln p_{\theta}(x\mid y,z)-\beta D_{\mathrm{KL}}(q_{\phi}(y,z\mid x),p_{\psi}(y,z))\right)
\end{equation}</script><p>where:</p>
<ul>
<li>$q_\phi$ denotes the distribution over the $32\times 32$ image tokens generated by the dVAE encoder given the RAB image $x$;</li>
<li>$p_\theta$ denotes  the distribution over the RGB images generated by the dVAE decoder given the image tokens;</li>
<li>$p_\phi$ denotes the joint distribution over the text image tokens modeled by transformer.</li>
</ul>
<h3 id="Stage-1-Visual-Codebook-Learning"><a href="#Stage-1-Visual-Codebook-Learning" class="headerlink" title="Stage 1: Visual Codebook Learning"></a>Stage 1: Visual Codebook Learning</h3><p>DALL-E firstly train a dVAE using gumbel-softmax relaxation instead of the straight-through estimator used in VQVAE. Each $256 \times 256$ RGB image is transformed into a $32 \times 32$ grid of discrete tokens through a discrete Variational Autoencoder (dVAE). These tokens can each take on one of 8192 unique values, resulting in a compact encoding of the visual information.</p>
<p>The dVAE leverages a Gumbel-Softmax relaxation technique, as opposed to the straight-through estimator often used in VQ-VAE. Its architecture comprises convolutional ResNets with bottleneck-style blocks, utilizing 3x3 convolutions and 1x1 convolutions for skip connections. Downscaling of feature maps is performed by max-pooling in the encoder, while the decoder employs nearest-neighbor upsampling for reconstruction.</p>
<h3 id="Stage-2-Prior-Learning"><a href="#Stage-2-Prior-Learning" class="headerlink" title="Stage 2: Prior Learning"></a>Stage 2: Prior Learning</h3><p>The subsequent stage is focused on modeling the relationship between text and images:  The model concatenates up to 256 BPE-encoded text tokens with the 1024 image tokens from Stage 1. An autoregressive transformer is then trained to capture the joint distribution of both text and image tokens.</p>
<p>DALL-E normalizes the cross-entropy losses for text and image tokens by their respective totals in the data batch. The text token loss is weighted by $1/8$, and the image token loss by $7/8$, reflecting a higher emphasis on image modeling.</p>
<h2 id="ViT-VQGAN-ICLR’22"><a href="#ViT-VQGAN-ICLR’22" class="headerlink" title="ViT-VQGAN (ICLR’22)"></a>ViT-VQGAN (ICLR’22)</h2><p>ViT-VQGAN<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. "[Vector-quantized image modeling with improved vqgan](https://arxiv.org/pdf/2110.04627)." arXiv preprint arXiv:2110.04627 (2021).">[5]</span></a></sup> leverages ViT-based VQGAN to encode discrete latent codes, and adopt combined objectives such as logit-laplace loss, L2 loss, adversarial loss, and perceptual loss.</p>
<p><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. "[Vector-quantized image modeling with improved vqgan](https://arxiv.org/pdf/2110.04627)." arXiv preprint arXiv:2110.04627 (2021).">[5]</span></a></sup> uses a combination of logit-laplace loss, L2 loss, perceptual loss based on VGG net, and GAN loss with a StyleGAN discriminator:</p>
<script type="math/tex; mode=display">
\begin{equation}
    L=L_{\mathrm{VQ}}+0.1 L_{\mathrm{Adv}}+0.1 L_{\mathrm{Perceptual}}+0.1 L_{\mathrm{Logit-laplace}}+ 1.0L_{2}
\end{equation}</script><p><img data-src="/notes/images/ViT-VQGAN.png" alt=""></p>
<div class="note info">
            <p><strong>Dimension reduction</strong>: <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. "[Vector-quantized image modeling with improved vqgan](https://arxiv.org/pdf/2110.04627)." arXiv preprint arXiv:2110.04627 (2021).">[5]</span></a></sup> finds that reducing the dimensionality of the lookup space can significantly enhance the reconstruction process. By reducing the dimensions from 256 to 32 through a linear mapping applied after the encoder’s output, the model can achieve a more efficient and accurate reconstruction of the input data.</p>
          </div>
<div class="note success">
            <p><strong>L2-normalized codes</strong>: It applies L2 norm on encoded latents $z_e(x)$ and codebook latents $e$. The codebook variables are initialized from a normal distribution. This normalization process projects all latent variables onto the surface of a hypersphere, which means that the Euclidean distance between L2-normalized latents transitions to measuring the cosine similarity between two vectors <script type="math/tex">\|\ell_2(z_e(x))-\ell_2(e_j)\|_2^2</script>. This shift to cosine similarity offers a more consistent and reliable way to compare the angles between vectors, which is particularly useful in high-dimensional spaces where Euclidean distances can become inflated and less meaningful.</p>
          </div>
<h2 id="RQ-VAE-CVPR’22"><a href="#RQ-VAE-CVPR’22" class="headerlink" title="RQ-VAE (CVPR’22)"></a>RQ-VAE (CVPR’22)</h2><p>The Residual-Quantized Variational Autoencoder (RQ-VAE)<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lee, D., Kim, C., Kim, S., Cho, M., & Han, W. S. (2022). [Autoregressive image generation using residual quantization](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11523-11532).">[7]</span></a></sup> incorporates residual quantization (RQ) to progressively refine the quantization of a feature map in a hierarchical, coarse-to-fine approach. At each quantized position, RQ-VAE employs a sequence of $D$ residual quantization iterations, yielding $D$ discrete codes. RQ’s ability to generate a vast number of compositions—exponential in the number of iterations ($D$)—allows RQ-VAE to closely approximate feature maps without depending on an excessively large codebook. This efficiency in representation enables a reduction in the spatial resolution of the quantized feature map without compromising the integrity of the encoded image.</p>
<p><img data-src="/notes/images/RQVAE.png" alt="RQVAE"></p>
<p>The dual-stage framework combines RQ-VAE with an RQ-Transformer, which is designed for the autoregressive modeling of images:</p>
<p>Stage 1: RQ-VAE encodes an image into a stacked map of $D$ discrete codes using a codebook.<br>Stage 2: RQ-Transformer addresses the training challenges of autoregressive models, particularly exposure bias.</p>
<h3 id="Stage-1-RQ-VAE"><a href="#Stage-1-RQ-VAE" class="headerlink" title="Stage 1: RQ-VAE"></a>Stage 1: RQ-VAE</h3><div class="note info">
            <p><strong>Reducing Spatial Resolution</strong>: While VQ-VAE performs a form of lossy compression on images and necessitates a balance between dimensionality reduction and information preservation, it typically requires $HW \log_2 K$ bits to encode an image using a codebook of size $K$. According to rate-distortion theory, the minimum reconstruction error is contingent on the bit count. To reduce spatial dimensions from $(H,W)$ to $(H/2,W/2)$ while maintaining reconstruction quality, the codebook would need to increase to a size of $K^4$. However, a VQ-VAE with an expansive codebook is impractical due to the potential for codebook collapse and unstable training dynamics.</p>
          </div>
<p>Instead of enlarging the codebook, RQ-VAE applies residual quantization to discretize a vector $z$. Given a quantization depth $D$, RQ represents $z$ with a sequence of $D$ codes:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \mathcal{RQ}(\mathbf{z};\mathcal{C},D)=(k_{1},\cdots,k_{D})\in[K]^{D}
\end{equation}</script><p>Here $\mathcal{C}$ is the codebook of size $|\mathcal{C}|=K$, and $k<em>d$ is the code assigned to vector $z$ at depth $d$. Starting from the initial residual $r_0 = z$, RQ iteratively computes the code $k_d$ for the residual $r</em>{d-1}$, and the subsequent residual $r_d$ is determined as follows:</p>
<script type="math/tex; mode=display">
\begin{equation}
    k_{d}=\mathcal{Q}(\mathbf{r}_{d-1};\mathcal{C}),\\\mathbf{r}_{d}=\mathbf{r}_{d-1}-\mathbf{e}(k_{d}),
\end{equation}</script><p>This process is repeated for $d=1,\cdots, D$. </p>
<div class="note info">
            <p>While traditional VQ segments the entire vector space <script type="math/tex">\mathbb{R}^n_z</script> into $K$ distinct clusters, RQ with a depth $D$ can partition this space into $K^D$ clusters at most. This means that RQ with depth $D$ has a comparable partitioning capacity to that of a VQ with $K^D$ codes.</p>
          </div>
<p>RQ-VAE augments the encoder-decoder structure of VQ-VAE by replacing VQ with the RQ module outlined above. With a depth of $D$, RQ-VAE represents a feature map $Z$ as a stacked map of codes <script type="math/tex">\mathbf{M}\in[K]^{H\times W\times D}</script> and constructs <script type="math/tex">\hat{\mathbf{Z}}^{(d)}\in\mathbb{R}^{H\times W\times n_{z}}</script>, which is quantized feature map at depth $d$ for each $d \in [D]$ such that:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\mathrm{M}_{hw} &=\mathcal{RQ}(E(\mathbf{X})_{hw};\mathcal{C},D), \\
\hat{\mathbf{Z}}_{hw}^{(d)} &=\sum_{d^{\prime}=1}^d\mathbf{e}(\mathbf{M}_{hwd^{\prime}}). 
\end{aligned}
\end{equation}</script><p>Finally, the decoder $G$ reconstructs the input image from $\hat{\mathbf{Z}}$ as $\hat{\mathbf{X}} = G(\hat{\mathbf{Z}})$. </p>
<p>The RQ-VAE training loss is as follows:</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathcal{L}=\mathcal{L}_{\mathrm{recon}}+\beta\mathcal{L}_{\mathrm{commit}}
\end{equation}</script><p>Note that it applies the exponential moving average (EMA) of the clusted features for the codebook update.</p>
<h3 id="Stage-2-RQ-Transformer"><a href="#Stage-2-RQ-Transformer" class="headerlink" title="Stage 2: RQ-Transformer"></a>Stage 2: RQ-Transformer</h3><p>In the second stage, the RQ-Transformer employs a two-pronged approach to model images autoregressively. This stage is pivotal in enhancing the predictive accuracy of the model and can be broken down into two components:</p>
<ol>
<li><p><strong>Spatial Transformer</strong>: This module captures the contextual information by summarizing the data from preceding positions in the image. It acts like a lens, focusing on relevant areas to create a context vector that encapsulates the essence of what has been encoded so far.</p>
</li>
<li><p><strong>Depth Transformer</strong>: Building upon the foundation laid by the Spatial Transformer, the Depth Transformer then takes a step-by-step approach to anticipate the sequence of $D$ codes for each position in the image. It does this by considering the context vector, which provides the necessary backdrop against which the predictions are made.</p>
</li>
</ol>
<p>By combining these two transformers, the RQ-Transformer adeptly synthesizes the spatial nuances and the depth-wise details, thereby generating a comprehensive representation of the image at each step.</p>
<h3 id="Contextual-RQ-Transformer-NeurIPS’22"><a href="#Contextual-RQ-Transformer-NeurIPS’22" class="headerlink" title="Contextual RQ-Transformer (NeurIPS’22)"></a>Contextual RQ-Transformer (NeurIPS’22)</h3><p>Contextual RQ-Transformer<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lee, Doyup, et al. "Draft-and-revise: Effective image generation with contextual rq-transformer." Advances in Neural Information Processing Systems 35 (2022): 30127-30138.">[9]</span></a></sup> uses two-stage framework: (1) RQVAE tokenization; (2) Contextual RQ-transformer.</p>
<p><img data-src="/notes/images/Contextual RQ-Transformer.png" alt="Contextual RQ-Transformer"></p>
<h4 id="RQVAE-tokenization"><a href="#RQVAE-tokenization" class="headerlink" title="RQVAE tokenization"></a>RQVAE tokenization</h4><p>The first stage of the Contextual RQ-Transformer employs the RQ-VAE—a powerful tokenizer capable of condensing high-dimensional data into a discrete set of latent tokens. </p>
<h4 id="Bidirectional-context-integration"><a href="#Bidirectional-context-integration" class="headerlink" title="Bidirectional context integration"></a>Bidirectional context integration</h4><p>Once the data is tokenized, the Contextual RQ-Transformer performs two key operations to model the relationships within the tokenized sequence:</p>
<ol>
<li><p><strong>Bidirectional Spatial Attention</strong>: Utilizing bidirectional attention mechanisms, the model predicts the masked positions in the sequence, given a masked scheduling function. This approach allows the model to consider both past and future context, leading to a more accurate and coherent understanding of the data.</p>
</li>
<li><p><strong>Autoregressive Depth</strong>: The model employs autoregressive transformers to process the sequence depth-wise. This structure is akin to modifying the lower layers of a RQ-Transformer<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lee, D., Kim, C., Kim, S., Cho, M., & Han, W. S. (2022). [Autoregressive image generation using residual quantization](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11523-11532).">[7]</span></a></sup> from a causal (unidirectional) to a bidirectional model. By doing so, the contextual RQ-Transformer captures the sequential dependencies with greater precision.</p>
</li>
</ol>
<h2 id="HQ-VAE-NeurIPS’22"><a href="#HQ-VAE-NeurIPS’22" class="headerlink" title="HQ-VAE (NeurIPS’22)"></a>HQ-VAE (NeurIPS’22)</h2><p>HQ-VAE<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="You, Tackgeun, et al. "Locally hierarchical auto-regressive modeling for image generation." Advances in Neural Information Processing Systems 35 (2022): 16360-16372.">[12]</span></a></sup> adopts a hierarchical VQ scheme to encode input data using two levels of discrete codes, top $\mathbf{t}$ and bottom $\mathbf{b}$, respectively. It transforms the feature map $\mathbf{z} \in \mathbb{R}^{rl \times rl \times d}$ into two code maps $(\mathbf{t}, \mathbf{b})$, where $\mathbf{t} \in \mathcal{Z}^{l\times l}$ and $\mathbf{b} \in \mathcal{Z}^{rl\times rl}$ with an interger scaling factor $r \in \{1,2,\cdots \}$.</p>
<p>It first captures the high-level information of a feature map by quantizing its downsampled version using the top codes:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \mathbf{z}^{\mathrm{top}}=\mathrm{Downsample}(\mathbf{z};r),\quad t_{ij}=VQ^{\mathrm{top}}(\mathbf{z}_{ij}^{\mathrm{top}};\mathcal{C}_{ij}^{\mathrm{top}}),\quad\mathbf{e}_{ij}^{\mathrm{top}}=\mathcal{C}^{\mathrm{top}}[t_{ij}],
\end{equation}</script><p>where $\mathcal{C}^{\mathrm{top}}$ is the codebook of top codes. Then given the top code map $\mathbf{t}$, the bottom codes are derived at:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \mathbf{z}^{\text{bot}}=\mathbf{z}-\text{Upsample}(\mathbf{e}^{\text{top}};r),\quad b_{ij}=VQ^{\text{bot}}(\mathbf{z}^{\text{bot}};\mathcal{C}^{\text{bot}}),\quad\mathbf{e}^{\text{bot}}=\mathcal{C}^{\text{bot}}[b_{ij}],
\end{equation}</script><p>where $\mathcal{C}^{\mathrm{bot}}$ is the codebook of bottom codes. </p>
<p><img data-src="/notes/images/HQ-VAE.png" alt=""></p>
<h2 id="LFQ-MagViT-V2-ICLR’24"><a href="#LFQ-MagViT-V2-ICLR’24" class="headerlink" title="LFQ (MagViT-V2; ICLR’24)"></a>LFQ (MagViT-V2; ICLR’24)</h2><p>MagViT-V2<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Lijun, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng et al. "[Language Model Beats Diffusion--Tokenizer is Key to Visual Generation](https://arxiv.org/pdf/2310.05737)." ICLR 2024.">[8]</span></a></sup> proposed the lookup-free quantization (LFQ) method that assumes independent codebook dimensins and binary latents. Specifically, the latent space of LFQ is decomposed as Cartesian product of single-dimensional variables, as <script type="math/tex">\mathbb{C}=\times_{i=1}^{\mathrm{log}_{2}^{*}K}C_{i}</script>. Given a feature vector $\mathbf{z} \in \mathbb{R}^{\log_2 K}$, each dimension of the quantized representation $q(\mathbf{z})$ is obtained from:</p>
<script type="math/tex; mode=display">
\begin{equation}
    q(\mathrm{z}_i)=C_{i,j},\text{where}j=\arg\min_k\|\mathrm{z}_i-C_{i,k}\|,
\end{equation}</script><p>where $C_{i,j}$ is the $j$-th value in $C_i$. With $C_i = \{-1,1\}$, the $\arg\min$ can be computed by the sign function as:</p>
<script type="math/tex; mode=display">
\begin{equation}
    q(\mathbf{z}_i)=\mathrm{sign}(\mathbf{z}_i)=-\mathbb{1}\{\mathbf{z}_i\leqslant0\}+\mathbb{1}\{\mathbf{z}_i>0\}.
\end{equation}</script><p>With LFQ, the token index for $q(\mathbf{z})$ is given by:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \text{Index}(\mathbf{z})=\sum_{i=1}^{\log_{2}K}\operatorname{arg}\operatorname*{min}_{k}\|\mathbf{z}_{i}-C_{i,k}\|\prod_{b=0}^{i-1}|C_{b}|=\sum_{i=1}^{\operatorname{log}_{2}K}2^{i-1}\mathbb{1}\{\mathbf{z}_{i}>0\}
\end{equation}</script><p>where $|C_0|=1$ sets the virtual basis.</p>
<p>It adds an entropy penalty during training to encourage codebook utilization:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \mathcal{L}_\text{entropy}=\mathbb{E}[H(q(\mathbf{z}))]-H[\mathbb{E}(q(\mathbf{z}))].
\end{equation}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Lookup Free Quantization</span></span><br><span class="line"><span class="string">Proposed in https://arxiv.org/abs/2310.05737</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In the simplest setup, each dimension is quantized into &#123;-1, 1&#125;.</span></span><br><span class="line"><span class="string">An entropy penalty is used to encourage utilization.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log2, ceil</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, einsum</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Module</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, reduce, pack, unpack</span><br><span class="line"></span><br><span class="line"><span class="comment"># constants</span></span><br><span class="line"></span><br><span class="line">Return = namedtuple(<span class="string">&#x27;Return&#x27;</span>, [<span class="string">&#x27;quantized&#x27;</span>, <span class="string">&#x27;indices&#x27;</span>, <span class="string">&#x27;entropy_aux_loss&#x27;</span>])</span><br><span class="line"></span><br><span class="line">LossBreakdown = namedtuple(<span class="string">&#x27;LossBreakdown&#x27;</span>, [<span class="string">&#x27;per_sample_entropy&#x27;</span>, <span class="string">&#x27;batch_entropy&#x27;</span>, <span class="string">&#x27;commitment&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span>(<span class="params">v</span>):</span></span><br><span class="line">    <span class="keyword">return</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default</span>(<span class="params">*args</span>):</span></span><br><span class="line">    <span class="keyword">for</span> arg <span class="keyword">in</span> args:</span><br><span class="line">        <span class="keyword">if</span> exists(arg):</span><br><span class="line">            <span class="keyword">return</span> arg() <span class="keyword">if</span> <span class="built_in">callable</span>(arg) <span class="keyword">else</span> arg</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pack_one</span>(<span class="params">t, pattern</span>):</span></span><br><span class="line">    <span class="keyword">return</span> pack([t], pattern)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpack_one</span>(<span class="params">t, ps, pattern</span>):</span></span><br><span class="line">    <span class="keyword">return</span> unpack(t, ps, pattern)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># entropy</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span>(<span class="params">t, eps = <span class="number">1e-5</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> t.clamp(<span class="built_in">min</span> = eps).log()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">prob</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (-prob * log(prob)).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># class</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LFQ</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        *,</span></span></span><br><span class="line"><span class="params"><span class="function">        dim = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        codebook_size = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        entropy_loss_weight = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        commitment_loss_weight = <span class="number">0.25</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        diversity_gamma = <span class="number">1.</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        straight_through_activation = nn.Identity(<span class="params"></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">        num_codebooks = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        keep_num_codebooks_dim = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        codebook_scale = <span class="number">1.</span>,            <span class="comment"># for residual LFQ, codebook scaled down by 2x at each layer</span></span></span></span><br><span class="line"><span class="params"><span class="function">        frac_per_sample_entropy = <span class="number">1.</span>    <span class="comment"># make less than 1. to only use a random fraction of the probs for per sample entropy</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># some assert validations</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> exists(dim) <span class="keyword">or</span> exists(codebook_size), <span class="string">&#x27;either dim or codebook_size must be specified for LFQ&#x27;</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> exists(codebook_size) <span class="keyword">or</span> log2(codebook_size).is_integer(), <span class="string">f&#x27;your codebook size must be a power of 2 for lookup free quantization (suggested <span class="subst">&#123;<span class="number">2</span> ** ceil(log2(codebook_size))&#125;</span>)&#x27;</span></span><br><span class="line"></span><br><span class="line">        codebook_size = default(codebook_size, <span class="keyword">lambda</span>: <span class="number">2</span> ** dim)</span><br><span class="line">        codebook_dim = <span class="built_in">int</span>(log2(codebook_size))</span><br><span class="line"></span><br><span class="line">        codebook_dims = codebook_dim * num_codebooks</span><br><span class="line">        dim = default(dim, codebook_dims)</span><br><span class="line"></span><br><span class="line">        has_projections = dim != codebook_dims</span><br><span class="line">        self.project_in = nn.Linear(dim, codebook_dims) <span class="keyword">if</span> has_projections <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.project_out = nn.Linear(codebook_dims, dim) <span class="keyword">if</span> has_projections <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.has_projections = has_projections</span><br><span class="line"></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.codebook_dim = codebook_dim</span><br><span class="line">        self.num_codebooks = num_codebooks</span><br><span class="line"></span><br><span class="line">        keep_num_codebooks_dim = default(keep_num_codebooks_dim, num_codebooks &gt; <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> (num_codebooks &gt; <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">not</span> keep_num_codebooks_dim)</span><br><span class="line">        self.keep_num_codebooks_dim = keep_num_codebooks_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># straight through activation</span></span><br><span class="line"></span><br><span class="line">        self.activation = straight_through_activation</span><br><span class="line"></span><br><span class="line">        <span class="comment"># entropy aux loss related weights</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt; frac_per_sample_entropy &lt;= <span class="number">1.</span></span><br><span class="line">        self.frac_per_sample_entropy = frac_per_sample_entropy</span><br><span class="line"></span><br><span class="line">        self.diversity_gamma = diversity_gamma</span><br><span class="line">        self.entropy_loss_weight = entropy_loss_weight</span><br><span class="line"></span><br><span class="line">        <span class="comment"># codebook scale</span></span><br><span class="line"></span><br><span class="line">        self.codebook_scale = codebook_scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># commitment loss</span></span><br><span class="line"></span><br><span class="line">        self.commitment_loss_weight = commitment_loss_weight</span><br><span class="line"></span><br><span class="line">        <span class="comment"># for no auxiliary loss, during inference</span></span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;mask&#x27;</span>, <span class="number">2</span> ** torch.arange(codebook_dim - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;zero&#x27;</span>, torch.tensor(<span class="number">0.</span>), persistent = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># codes</span></span><br><span class="line"></span><br><span class="line">        all_codes = torch.arange(codebook_size)</span><br><span class="line">        bits = ((all_codes[..., <span class="literal">None</span>].<span class="built_in">int</span>() &amp; self.mask) != <span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">        codebook = self.bits_to_codes(bits)</span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;codebook&#x27;</span>, codebook, persistent = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bits_to_codes</span>(<span class="params">self, bits</span>):</span></span><br><span class="line">        <span class="keyword">return</span> bits * self.codebook_scale * <span class="number">2</span> - self.codebook_scale <span class="comment"># [-1 ,1]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dtype</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.codebook.dtype</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">indices_to_codes</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        indices,</span></span></span><br><span class="line"><span class="params"><span class="function">        project_out = <span class="literal">True</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        is_img_or_video = indices.ndim &gt;= (<span class="number">3</span> + <span class="built_in">int</span>(self.keep_num_codebooks_dim))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.keep_num_codebooks_dim:</span><br><span class="line">            indices = rearrange(indices, <span class="string">&#x27;... -&gt; ... 1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># indices to codes, which are bits of either -1 or 1</span></span><br><span class="line"></span><br><span class="line">        bits = ((indices[..., <span class="literal">None</span>].<span class="built_in">int</span>() &amp; self.mask) != <span class="number">0</span>).to(self.dtype)</span><br><span class="line"></span><br><span class="line">        codes = self.bits_to_codes(bits)</span><br><span class="line"></span><br><span class="line">        codes = rearrange(codes, <span class="string">&#x27;... c d -&gt; ... (c d)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># whether to project codes out to original dimensions</span></span><br><span class="line">        <span class="comment"># if the input feature dimensions were not log2(codebook size)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> project_out:</span><br><span class="line">            codes = self.project_out(codes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># rearrange codes back to original shape</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_img_or_video:</span><br><span class="line">            codes = rearrange(codes, <span class="string">&#x27;b ... d -&gt; b d ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> codes</span><br><span class="line"></span><br><span class="line"><span class="meta">    @autocast(<span class="params">enabled = <span class="literal">False</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        x,</span></span></span><br><span class="line"><span class="params"><span class="function">        inv_temperature = <span class="number">100.</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        return_loss_breakdown = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        mask = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        einstein notation</span></span><br><span class="line"><span class="string">        b - batch</span></span><br><span class="line"><span class="string">        n - sequence (or flattened spatial dimensions)</span></span><br><span class="line"><span class="string">        d - feature dimension, which is also log2(codebook size)</span></span><br><span class="line"><span class="string">        c - number of codebook dim</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">        is_img_or_video = x.ndim &gt;= <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># standardize image or video into (batch, seq, dimension)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_img_or_video:</span><br><span class="line">            x = rearrange(x, <span class="string">&#x27;b d ... -&gt; b ... d&#x27;</span>)</span><br><span class="line">            x, ps = pack_one(x, <span class="string">&#x27;b * d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> x.shape[-<span class="number">1</span>] == self.dim, <span class="string">f&#x27;expected dimension of <span class="subst">&#123;self.dim&#125;</span> but received <span class="subst">&#123;x.shape[-<span class="number">1</span>]&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">        x = self.project_in(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split out number of codebooks</span></span><br><span class="line"></span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;b n (c d) -&gt; b n c d&#x27;</span>, c = self.num_codebooks)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># quantize by eq 3.</span></span><br><span class="line"></span><br><span class="line">        original_input = x</span><br><span class="line"></span><br><span class="line">        codebook_value = torch.ones_like(x) * self.codebook_scale</span><br><span class="line">        quantized = torch.where(x &gt; <span class="number">0</span>, codebook_value, -codebook_value)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use straight-through gradients (optionally with custom activation fn) if training</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            x = self.activation(x)</span><br><span class="line">            x = x + (quantized - x).detach()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = quantized</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate indices</span></span><br><span class="line"></span><br><span class="line">        indices = reduce((x &gt; <span class="number">0</span>).<span class="built_in">int</span>() * self.mask.<span class="built_in">int</span>(), <span class="string">&#x27;b n c d -&gt; b n c&#x27;</span>, <span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># entropy aux loss</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="comment"># the same as euclidean distance up to a constant</span></span><br><span class="line">            distance = -<span class="number">2</span> * einsum(<span class="string">&#x27;... i d, j d -&gt; ... i j&#x27;</span>, original_input, self.codebook)</span><br><span class="line"></span><br><span class="line">            prob = (-distance * inv_temperature).softmax(dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># account for mask</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> exists(mask):</span><br><span class="line">                prob = prob[mask]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = rearrange(prob, <span class="string">&#x27;b n ... -&gt; (b n) ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># whether to only use a fraction of probs, for reducing memory</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.frac_per_sample_entropy &lt; <span class="number">1.</span>:</span><br><span class="line">                num_tokens = prob.shape[<span class="number">0</span>]</span><br><span class="line">                num_sampled_tokens = <span class="built_in">int</span>(num_tokens * self.frac_per_sample_entropy)</span><br><span class="line">                rand_mask = torch.randn(num_tokens).argsort(dim = -<span class="number">1</span>) &lt; num_sampled_tokens</span><br><span class="line">                per_sample_probs = prob[rand_mask]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                per_sample_probs = prob</span><br><span class="line"></span><br><span class="line">            <span class="comment"># calculate per sample entropy</span></span><br><span class="line"></span><br><span class="line">            per_sample_entropy = entropy(per_sample_probs).mean()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># distribution over all available tokens in the batch</span></span><br><span class="line"></span><br><span class="line">            avg_prob = reduce(per_sample_probs, <span class="string">&#x27;... c d -&gt; c d&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">            codebook_entropy = entropy(avg_prob).mean()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 1. entropy will be nudged to be low for each code, to encourage the network to output confident predictions</span></span><br><span class="line">            <span class="comment"># 2. codebook entropy will be nudged to be high, to encourage all codes to be uniformly used within the batch</span></span><br><span class="line"></span><br><span class="line">            entropy_aux_loss = per_sample_entropy - self.diversity_gamma * codebook_entropy</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># if not training, just return dummy 0</span></span><br><span class="line">            entropy_aux_loss = per_sample_entropy = codebook_entropy = self.zero</span><br><span class="line"></span><br><span class="line">        <span class="comment"># commit loss</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            commit_loss = F.mse_loss(original_input, quantized.detach(), reduction = <span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> exists(mask):</span><br><span class="line">                commit_loss = commit_loss[mask]</span><br><span class="line"></span><br><span class="line">            commit_loss = commit_loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            commit_loss = self.zero</span><br><span class="line"></span><br><span class="line">        <span class="comment"># merge back codebook dim</span></span><br><span class="line"></span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;b n c d -&gt; b n (c d)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project out to feature dimension if needed</span></span><br><span class="line"></span><br><span class="line">        x = self.project_out(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reconstitute image or video dimensions</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_img_or_video:</span><br><span class="line">            x = unpack_one(x, ps, <span class="string">&#x27;b * d&#x27;</span>)</span><br><span class="line">            x = rearrange(x, <span class="string">&#x27;b ... d -&gt; b d ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            indices = unpack_one(indices, ps, <span class="string">&#x27;b * c&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># whether to remove single codebook dim</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.keep_num_codebooks_dim:</span><br><span class="line">            indices = rearrange(indices, <span class="string">&#x27;... 1 -&gt; ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># complete aux loss</span></span><br><span class="line"></span><br><span class="line">        aux_loss = entropy_aux_loss * self.entropy_loss_weight + commit_loss * self.commitment_loss_weight</span><br><span class="line"></span><br><span class="line">        ret = Return(x, indices, aux_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_loss_breakdown:</span><br><span class="line">            <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret, LossBreakdown(per_sample_entropy, codebook_entropy, commit_loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="FSQ"><a href="#FSQ" class="headerlink" title="FSQ"></a>FSQ</h2><p>Finite Scalar Quantization (FSQ)<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mentzer, Fabian, et al. "Finite scalar quantization: Vq-vae made simple." arXiv preprint arXiv:2309.15505 (2023).">[13]</span></a></sup> is a technique that applies a bounding function $f$ to a $d$-dimensional representation $z \in \mathbb{R}^d$, subsequently rounding the result to an integer. The choice of $f$ is critical, as it determines the quantization scheme. Specifically, $f$ is selected such that the output $\hat{z} = \text{round}(f(z))$ can take one of $L$ unique values. An illustrative example of $f$ is given by:</p>
<script type="math/tex; mode=display">f: z \mapsto \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)</script><p>This approach effectively maps $z$ to a quantized representation $\hat{z}$ that belongs to a codebook $\mathcal{C}$, where $\mathcal{C}$ is constructed as the product of per-channel codebook sets. Consequently, the number of distinct codebook entries is given by:</p>
<script type="math/tex; mode=display">|\mathcal{C}| = L^d</script><p>For each vector $\hat{z} \in \mathcal{C}$, there exists a bijective mapping to an integer in the range ${1, \cdots, L^d}$, simplifying the encoding and decoding processes.</p>
<p><strong>Generalized FSQ</strong>: </p>
<p>The concept can be further generalized to handle heterogeneous channels, where the $i$-th channel is mapped to $L_i$ unique values. This generalization yields a more flexible codebook with a total number of entries as follows:</p>
<script type="math/tex; mode=display">|\mathcal{C}| = \prod_{i=1}^d L_i</script><p><strong>Gradient Propagation via Straight-Through Estimator (STE)</strong>:</p>
<p>To enable gradient propagation through the discrete <code>round</code> operation, we employ the Straight-Through Estimator (STE) method. This involves replacing the gradients with a simple identity term that ignores the rounding operation during backpropagation. Specifically, the STE-based rounding function is implemented as:</p>
<script type="math/tex; mode=display">\text{round_ste}: x \mapsto x + \text{sg}(\text{round}(x) - x)</script><p>Here, <code>sg</code> represents the stop gradient operation, which blocks gradients from flowing through the second term, effectively treating it as a constant during backpropagation. This allows gradients to “pass through” the rounding operation, enabling training of neural networks utilizing FSQ.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">round_ste</span>(<span class="params">z: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Round with straight through gradients.&quot;&quot;&quot;</span></span><br><span class="line">    zhat = z.<span class="built_in">round</span>()</span><br><span class="line">    <span class="keyword">return</span> z + (zhat - z).detach()</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/FSQ.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Finite Scalar Quantization: VQ-VAE Made Simple - https://arxiv.org/abs/2309.15505</span></span><br><span class="line"><span class="string">Code adapted from Jax version in Appendix A.1</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Tuple</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Module</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor, int32</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, pack, unpack</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span>(<span class="params">v</span>):</span></span><br><span class="line">    <span class="keyword">return</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default</span>(<span class="params">*args</span>):</span></span><br><span class="line">    <span class="keyword">for</span> arg <span class="keyword">in</span> args:</span><br><span class="line">        <span class="keyword">if</span> exists(arg):</span><br><span class="line">            <span class="keyword">return</span> arg</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pack_one</span>(<span class="params">t, pattern</span>):</span></span><br><span class="line">    <span class="keyword">return</span> pack([t], pattern)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpack_one</span>(<span class="params">t, ps, pattern</span>):</span></span><br><span class="line">    <span class="keyword">return</span> unpack(t, ps, pattern)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor helpers</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">round_ste</span>(<span class="params">z: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Round with straight through gradients.&quot;&quot;&quot;</span></span><br><span class="line">    zhat = z.<span class="built_in">round</span>()</span><br><span class="line">    <span class="keyword">return</span> z + (zhat - z).detach()</span><br><span class="line"></span><br><span class="line"><span class="comment"># main class</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FSQ</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        levels: <span class="type">List</span>[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        dim: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_codebooks = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        keep_num_codebooks_dim: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        scale: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        allowed_dtypes: <span class="type">Tuple</span>[torch.dtype, ...] = (<span class="params">torch.float32, torch.float64</span>)</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        _levels = torch.tensor(levels, dtype=int32)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;_levels&quot;</span>, _levels, persistent = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        _basis = torch.cumprod(torch.tensor([<span class="number">1</span>] + levels[:-<span class="number">1</span>]), dim=<span class="number">0</span>, dtype=int32)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;_basis&quot;</span>, _basis, persistent = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.scale = scale</span><br><span class="line"></span><br><span class="line">        codebook_dim = <span class="built_in">len</span>(levels)</span><br><span class="line">        self.codebook_dim = codebook_dim</span><br><span class="line"></span><br><span class="line">        effective_codebook_dim = codebook_dim * num_codebooks</span><br><span class="line">        self.num_codebooks = num_codebooks</span><br><span class="line">        self.effective_codebook_dim = effective_codebook_dim</span><br><span class="line"></span><br><span class="line">        keep_num_codebooks_dim = default(keep_num_codebooks_dim, num_codebooks &gt; <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> (num_codebooks &gt; <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">not</span> keep_num_codebooks_dim)</span><br><span class="line">        self.keep_num_codebooks_dim = keep_num_codebooks_dim</span><br><span class="line"></span><br><span class="line">        self.dim = default(dim, <span class="built_in">len</span>(_levels) * num_codebooks)</span><br><span class="line"></span><br><span class="line">        has_projections = self.dim != effective_codebook_dim</span><br><span class="line">        self.project_in = nn.Linear(self.dim, effective_codebook_dim) <span class="keyword">if</span> has_projections <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.project_out = nn.Linear(effective_codebook_dim, self.dim) <span class="keyword">if</span> has_projections <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.has_projections = has_projections</span><br><span class="line"></span><br><span class="line">        self.codebook_size = self._levels.prod().item()</span><br><span class="line"></span><br><span class="line">        implicit_codebook = self.indices_to_codes(torch.arange(self.codebook_size), project_out = <span class="literal">False</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;implicit_codebook&quot;</span>, implicit_codebook, persistent = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.allowed_dtypes = allowed_dtypes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bound</span>(<span class="params">self, z: Tensor, eps: <span class="built_in">float</span> = <span class="number">1e-3</span></span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Bound `z`, an array of shape (..., d).&quot;&quot;&quot;</span></span><br><span class="line">        half_l = (self._levels - <span class="number">1</span>) * (<span class="number">1</span> + eps) / <span class="number">2</span></span><br><span class="line">        offset = torch.where(self._levels % <span class="number">2</span> == <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.0</span>)</span><br><span class="line">        shift = (offset / half_l).atanh()</span><br><span class="line">        <span class="keyword">return</span> (z + shift).tanh() * half_l - offset</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quantize</span>(<span class="params">self, z: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Quantizes z, returns quantized zhat, same shape as z.&quot;&quot;&quot;</span></span><br><span class="line">        quantized = round_ste(self.bound(z))</span><br><span class="line">        half_width = self._levels // <span class="number">2</span> <span class="comment"># Renormalize to [-1, 1].</span></span><br><span class="line">        <span class="keyword">return</span> quantized / half_width</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_scale_and_shift</span>(<span class="params">self, zhat_normalized: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        half_width = self._levels // <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> (zhat_normalized * half_width) + half_width</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_scale_and_shift_inverse</span>(<span class="params">self, zhat: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        half_width = self._levels // <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> (zhat - half_width) / half_width</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">codes_to_indices</span>(<span class="params">self, zhat: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts a `code` to an index in the codebook.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> zhat.shape[-<span class="number">1</span>] == self.codebook_dim</span><br><span class="line">        zhat = self._scale_and_shift(zhat)</span><br><span class="line">        <span class="keyword">return</span> (zhat * self._basis).<span class="built_in">sum</span>(dim=-<span class="number">1</span>).to(int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">indices_to_codes</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        indices: Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        project_out = <span class="literal">True</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Inverse of `codes_to_indices`.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        is_img_or_video = indices.ndim &gt;= (<span class="number">3</span> + <span class="built_in">int</span>(self.keep_num_codebooks_dim))</span><br><span class="line"></span><br><span class="line">        indices = rearrange(indices, <span class="string">&#x27;... -&gt; ... 1&#x27;</span>)</span><br><span class="line">        codes_non_centered = (indices // self._basis) % self._levels</span><br><span class="line">        codes = self._scale_and_shift_inverse(codes_non_centered)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.keep_num_codebooks_dim:</span><br><span class="line">            codes = rearrange(codes, <span class="string">&#x27;... c d -&gt; ... (c d)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> project_out:</span><br><span class="line">            codes = self.project_out(codes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_img_or_video:</span><br><span class="line">            codes = rearrange(codes, <span class="string">&#x27;b ... d -&gt; b d ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> codes</span><br><span class="line"></span><br><span class="line"><span class="meta">    @autocast(<span class="params">enabled = <span class="literal">False</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, z: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        einstein notation</span></span><br><span class="line"><span class="string">        b - batch</span></span><br><span class="line"><span class="string">        n - sequence (or flattened spatial dimensions)</span></span><br><span class="line"><span class="string">        d - feature dimension</span></span><br><span class="line"><span class="string">        c - number of codebook dim</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        orig_dtype = z.dtype</span><br><span class="line">        is_img_or_video = z.ndim &gt;= <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># standardize image or video into (batch, seq, dimension)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_img_or_video:</span><br><span class="line">            z = rearrange(z, <span class="string">&#x27;b d ... -&gt; b ... d&#x27;</span>)</span><br><span class="line">            z, ps = pack_one(z, <span class="string">&#x27;b * d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> z.shape[-<span class="number">1</span>] == self.dim, <span class="string">f&#x27;expected dimension of <span class="subst">&#123;self.dim&#125;</span> but found dimension of <span class="subst">&#123;z.shape[-<span class="number">1</span>]&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">        z = self.project_in(z)</span><br><span class="line"></span><br><span class="line">        z = rearrange(z, <span class="string">&#x27;b n (c d) -&gt; b n c d&#x27;</span>, c = self.num_codebooks)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># make sure allowed dtype before quantizing</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> z.dtype <span class="keyword">not</span> <span class="keyword">in</span> self.allowed_dtypes:</span><br><span class="line">            z = z.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">        codes = self.quantize(z)</span><br><span class="line">        indices = self.codes_to_indices(codes)</span><br><span class="line"></span><br><span class="line">        codes = rearrange(codes, <span class="string">&#x27;b n c d -&gt; b n (c d)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cast codes back to original dtype</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> codes.dtype != orig_dtype:</span><br><span class="line">            codes = codes.<span class="built_in">type</span>(orig_dtype)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># project out</span></span><br><span class="line"></span><br><span class="line">        out = self.project_out(codes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reconstitute image or video dimensions</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_img_or_video:</span><br><span class="line">            out = unpack_one(out, ps, <span class="string">&#x27;b * d&#x27;</span>)</span><br><span class="line">            out = rearrange(out, <span class="string">&#x27;b ... d -&gt; b d ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            indices = unpack_one(indices, ps, <span class="string">&#x27;b * c&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.keep_num_codebooks_dim:</span><br><span class="line">            indices = rearrange(indices, <span class="string">&#x27;... 1 -&gt; ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># return quantized output and indices</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, indices</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Related work: Binary Spherical Quantization (BSQ)<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhao, Yue, Yuanjun Xiong, and Philipp Krähenbühl. "Image and Video Tokenization with Binary Spherical Quantization." arXiv preprint arXiv:2406.07548 (2024).">[14]</span></a></sup>.</p>
<h1 id="Prior-Learning"><a href="#Prior-Learning" class="headerlink" title="Prior Learning"></a>Prior Learning</h1><p>In the second stage, existing literature often applies a causal or bidirectional language models for prior learning.</p>
<h2 id="Causal-Transformer-Modeling"><a href="#Causal-Transformer-Modeling" class="headerlink" title="Causal Transformer Modeling"></a>Causal Transformer Modeling</h2><p>It learns an autoregressive language models, such as VQGAN<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Esser, Patrick, Robin Rombach, and Bjorn Ommer. "[Taming transformers for high-resolution image synthesis.](http://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf)" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.">[4]</span></a></sup>, ViT-VQGAN<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. "[Vector-quantized image modeling with improved vqgan](https://arxiv.org/pdf/2110.04627)." arXiv preprint arXiv:2110.04627 (2021).">[5]</span></a></sup>, DALL-E<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramesh, Aditya, et al. "Zero-shot text-to-image generation." International conference on machine learning. Pmlr, 2021.">[11]</span></a></sup>, iGPT<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Mark, et al. "Generative pretraining from pixels." International conference on machine learning. PMLR, 2020.">[10]</span></a></sup>,  etc.</p>
<h2 id="Bidirectional-Transformer-Modeling"><a href="#Bidirectional-Transformer-Modeling" class="headerlink" title="Bidirectional Transformer Modeling"></a>Bidirectional Transformer Modeling</h2><p>Another way for image prior learning applies bidirectional modeling, such as MaskGIT<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chang, Huiwen, et al. "Maskgit: Masked generative image transformer." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.">[15]</span></a></sup>, MagViT-V2<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Lijun, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng et al. "[Language Model Beats Diffusion--Tokenizer is Key to Visual Generation](https://arxiv.org/pdf/2310.05737)." ICLR 2024.">[8]</span></a></sup>, Muse<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chang, Huiwen, et al. "Muse: Text-to-image generation via masked generative transformers." arXiv preprint arXiv:2301.00704 (2023).">[16]</span></a></sup>.</p>
<h3 id="MaskGIT-CVPR’22"><a href="#MaskGIT-CVPR’22" class="headerlink" title="MaskGIT (CVPR’22)"></a>MaskGIT (CVPR’22)</h3><p>MaskGIT<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chang, Huiwen, et al. "Maskgit: Masked generative image transformer." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.">[15]</span></a></sup> consists of two stages:</p>
<ol>
<li>VQ tokenizer training as in VQGAN;</li>
<li>Masked Visual Token Modeling (MVTM) on a bidirectional transformer.</li>
</ol>
<h4 id="Masked-Visual-Token-Modeling-MVTM"><a href="#Masked-Visual-Token-Modeling-MVTM" class="headerlink" title="Masked Visual Token Modeling (MVTM)"></a>Masked Visual Token Modeling (MVTM)</h4><p>MaskGIT utilizes a mask scheduling function to strategically mask input latent tokens in bidirectional transformers. Subsequently, the masked token is refined through optimization based on the cross-entropy loss calculated between the ground-truth and predicted tokens, closely resembling the approach employed in masked language models.</p>
<p><img data-src="/notes/images/MaskGIT-pipeline.png" alt=""></p>
<h4 id="Iterative-Decoding"><a href="#Iterative-Decoding" class="headerlink" title="Iterative Decoding"></a>Iterative Decoding</h4><p>Autoregressive decoding, known for its sequential left-to-right approach, inherently leads to slower image generation. MaskGIT overcomes this limitation by incorporating bidirectional decoding, enabling parallel processing for faster results. Below is a detailed breakdown of MaskGIT’s decoding process:</p>
<p><strong>Decoding Process</strong>:</p>
<p><img data-src="/notes/images/MaskGIT.png" alt="MaskGIT"></p>
<p><strong>(1) Predict</strong>. At each iteration $t$, MaskGIT utilizes the current masked tokens $Y_M^{(t)}$ to predict probabilities $p^{(t)} \in \mathbb{R}^{N \times K}$ for <strong>all masked locations in parallel</strong>. This step leverages the capabilities of bidirectional transformers to simultaneously assess potential replacements for each masked token.</p>
<p><strong>(2) Sample</strong>. For each masked location $i$, MaskGIT samples tokens $y_i^{(t)}$ based on the predicted probabilities $p_i^{(t)} \in \mathbb{R}^{K}$ over all possible tokens in the codebook. The sampled token’s prediction score serves as a “confidence” score, indicating the model’s certainty in its prediction. For unmasked positions in $Y_M^{(t)}$, the confidence score is set to 1.0, representing absolute certainty.</p>
<p><strong>(3) Mask Schedule</strong>. The number of tokens to mask at iteration $t$ is determined using the mask scheduling function $\gamma$:</p>
<script type="math/tex; mode=display">n = \lceil\gamma\left(\frac{t}{T}\right)N\rceil</script><p>Here, $N$ is the input length, $T$ is the total number of iterations, and $n$ is the number of tokens to be masked. As $t$ progresses, $\gamma$ ensures a decreasing mask ratio, allowing the model to gradually generate more tokens until all are uncovered within $T$ steps.</p>
<p><strong>(4) Mask</strong>. To obtain $Y_M^{(t+1)}$ for iteration $t+1$, MaskGIT masks $n$ tokens in $Y_M^{(t)}$ based on their confidence scores. The mask $M^{(t+1)}$ is calculated as follows:</p>
<p>\begin{equation}<br>    m_i^{(t+1)}=\begin{cases}1,&amp;\text{if }c_i&lt;\text{ sorted}_j(c_j)[n].\\<br>    0,&amp;\text{ otherwise.}\end{cases},<br>\end{equation}</p>
<p>where $c_i$ is the confidence score for the $i$-th token, and $\text{sorted}_j(c_j)[n]$ represents the $n$-th smallest confidence score among all tokens.</p>
<p><strong>Synthesis in $T$ Steps</strong>:  MaskGIT’s decoding algorithm systematically generates an image in $T$ iterations. At each step, the model simultaneously predicts probabilities for all masked tokens but only retains the most confident predictions. The remaining tokens are masked out and re-predicted in the next iteration. This process, with a progressively decreasing mask ratio, ensures that all tokens are generated within $T$ steps, leading to faster and more efficient image generation.</p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2024VQ-Review,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Multimodal Tokenization with Vector Quantization: A Review&#125;&#125;,</span><br><span class="line">  year = &#123;2024&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2024/05/24/Vector-Quantization/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Vector_quantization">Vector quantization (wiki)</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Van Den Oord, Aaron, and Oriol Vinyals. &quot;<a href="https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf">Neural discrete representation learning.</a>&quot; Advances in neural information processing systems 30 (2017).<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Razavi, Ali, Aaron Van den Oord, and Oriol Vinyals. &quot;<a href="https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf">Generating diverse high-fidelity images with vq-vae-2</a>.&quot; Advances in neural information processing systems 32 (2019).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Esser, Patrick, Robin Rombach, and Bjorn Ommer. &quot;<a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf">Taming transformers for high-resolution image synthesis.</a>&quot; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. &quot;<a href="https://arxiv.org/pdf/2110.04627">Vector-quantized image modeling with improved vqgan</a>.&quot; arXiv preprint arXiv:2110.04627 (2021).<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yu, Lijun, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann et al. &quot;<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf">Magvit: Masked generative video transformer</a>.&quot; In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10459-10469. 2023.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lee, D., Kim, C., Kim, S., Cho, M., &amp; Han, W. S. (2022). <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf">Autoregressive image generation using residual quantization</a>. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11523-11532).<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yu, Lijun, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng et al. &quot;<a href="https://arxiv.org/pdf/2310.05737">Language Model Beats Diffusion--Tokenizer is Key to Visual Generation</a>.&quot; ICLR 2024.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lee, Doyup, et al. &quot;Draft-and-revise: Effective image generation with contextual rq-transformer.&quot; Advances in Neural Information Processing Systems 35 (2022): 30127-30138.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, Mark, et al. &quot;Generative pretraining from pixels.&quot; International conference on machine learning. PMLR, 2020.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ramesh, Aditya, et al. &quot;Zero-shot text-to-image generation.&quot; International conference on machine learning. Pmlr, 2021.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">You, Tackgeun, et al. &quot;Locally hierarchical auto-regressive modeling for image generation.&quot; Advances in Neural Information Processing Systems 35 (2022): 16360-16372.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mentzer, Fabian, et al. &quot;Finite scalar quantization: Vq-vae made simple.&quot; arXiv preprint arXiv:2309.15505 (2023).<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhao, Yue, Yuanjun Xiong, and Philipp Krähenbühl. &quot;Image and Video Tokenization with Binary Spherical Quantization.&quot; arXiv preprint arXiv:2406.07548 (2024).<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chang, Huiwen, et al. &quot;Maskgit: Masked generative image transformer.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chang, Huiwen, et al. &quot;Muse: Text-to-image generation via masked generative transformers.&quot; arXiv preprint arXiv:2301.00704 (2023).<a href="#fnref:16" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>LMM</category>
        <category>Tokenization</category>
      </categories>
      <tags>
        <tag>Tokenization</tag>
        <tag>Multimodality</tag>
        <tag>LMM</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode practice: random</title>
    <url>/notes/2019/03/01/Leetcode/Leetcode-practice-random/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Leetcode practice of the <code>random</code> topic.<br><span id="more"></span></p>
<h3 id=""><a href="#" class="headerlink" title=""></a><span class="label primary">Leetcode 470. Implement Rand10() Using Rand7()</span></h3><div class="note info">
            <p>Given a function <code>rand7</code> which generates a uniform random integer in the range 1 to 7, write a function <code>rand10</code> which generates a uniform random integer in the range 1 to 10.<br>Do NOT use system’s <code>Math.random()</code>.</p>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The rand7() API is already defined for you.</span></span><br><span class="line"><span class="comment"># def rand7():</span></span><br><span class="line"><span class="comment"># @return a random integer in the range 1 to 7</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rand10</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># the maximum common multiple n*10 (n=4) that is less than 49 (7*7)</span></span><br><span class="line">        reject_threshold = <span class="number">40</span></span><br><span class="line">        <span class="comment"># threshold &lt;= res &lt;= 7*7, to enter the following loop</span></span><br><span class="line">        res = <span class="number">41</span></span><br><span class="line">        <span class="keyword">while</span> res &gt; reject_threshold:</span><br><span class="line">            res = (rand7()-<span class="number">1</span>)*<span class="number">7</span> + rand7() <span class="comment"># generate 0 - 7*7</span></span><br><span class="line">        <span class="keyword">return</span> res%<span class="number">10</span> + <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Algorithms</category>
        <category>LeetCode</category>
        <category>Random</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Adversarial Networks</title>
    <url>/notes/2020/01/15/Generative/Generative-Adversarial-Networks/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>GANs are widely applied to estimate generative models without any explicit density function, which instead take the game-theoretic approach: learn to generate from training distribution via 2-player games.</p>
<span id="more"></span>
<h1 id="Generative-Adversarial-Networks-GAN"><a href="#Generative-Adversarial-Networks-GAN" class="headerlink" title="Generative Adversarial Networks (GAN)"></a>Generative Adversarial Networks (GAN)</h1><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>GANs consist of two components:</p>
<ul>
<li>Generator network $G$: try to fool the discriminator by generating real-looking images</li>
<li>Discriminator network $D$: try to distinguish between real and fake images</li>
</ul>
<p><img data-src='/notes/images/GAN-schematic.png' width='80%' /></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Train joinly in <strong>minimax game</strong> -&gt; minimax objective function</p>
<script type="math/tex; mode=display">\min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim p_\text{data}(x)}\big[ \log \underbrace{D(x)}_\text{$D$ output for real data $x$} \big] + \mathbb{E}_{z \sim p_z(z)} \bigg[ \log \big(1- \underbrace{D(G(z))}_\text{$D$ output for generated fake data $G(z)$}\big) \bigg]</script><p>where </p>
<ul>
<li>$D$ outputs likelihood in (0,1) of real image</li>
</ul>
<div class="note warning">
            <ul><li>Discriminator $D$ aims to <strong>maximize the objective</strong> such that $D(x) \approx 1$(real) and <script type="math/tex">D(G(z)) \approx 0</script> (fake).</li><li>Generator $G$ aims to <strong>minimize the objective</strong> such that $D(G(z)) \approx 1$ (to fool the $D$)</li></ul>
          </div>
<p>The objective is also:</p>
<ol>
<li><strong>Gradient ascent</strong> on discriminator $D$:<script type="math/tex; mode=display">\max_{D} \bigg[ \mathbb{E}_{x\sim p_\text{data}} \log D(x) + \mathbb{E}_{z\sim p(z)} \log \big( 1-D(G(z)) \big) \bigg]</script></li>
<li></li>
</ol>
<ul>
<li>(not work well) <em>Gradient descent</em> on $G$ minimizes the $D$ being correct:<script type="math/tex; mode=display">\min_G \mathbb{E}_{z \sim p(z)} \log \big( 1- D(G(z)) \big)</script><blockquote>
<p>Problems: when samples are likely fake, the gradient the the left region of the figure is relatively flat!</p>
</blockquote>
</li>
<li>Instead, <strong>gradient ascent</strong> on $G$ maximize the likelihood of $D$ being wrong<script type="math/tex; mode=display">\color{red}{\max}_G \mathbb{E}_{z \sim p(z)} \color{red}{\log D(G(z))}</script></li>
</ul>
<p><img data-src="/notes/images/Generator-objective.svg" width="70%"/></p>
<h3 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h3><p>Minibatch SGD training</p>
<ol>
<li><strong>for</strong> number of training iterations <strong>do</strong><ol>
<li><strong>for</strong> $k$ steps <strong>do</strong><ul>
<li>Sample minibatch of $m$ noise samples <script type="math/tex">\{ z^{(1)}, z^{(2)},  \cdots,z^{(m)} \}</script> from noise distribution <script type="math/tex">p_g(z)</script></li>
<li>Sample minibatch of $m$ examples <script type="math/tex">\{ x^{(1)}, x^{(2)},  \cdots,x^{(m)} \}</script> from data generating distribution <script type="math/tex">p_\text{data}(x)</script></li>
<li>Update the discriminator by ascending its stochastic gradient:<script type="math/tex; mode=display">\nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^m \bigg[ \log D(x^{(i)}) + \log \big( 1-D(G(z^{(i)})) \big) \bigg]</script></li>
</ul>
</li>
<li>Sample minibatch of $m$ noise samples  $m$ noise samples <script type="math/tex">\{ z^{(1)}, z^{(2)},  \cdots,z^{(m)} \}</script> from noise distribution <script type="math/tex">p_g(z)</script></li>
<li>Update the $G$ by ascending its stochastic gradient:<script type="math/tex; mode=display">\nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^m \log \bigg(D_{\theta_d} \big( G_{\theta_g(z^{(i)})} \big) \bigg)</script></li>
</ol>
</li>
</ol>
<h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p>After training, use $G$ to generate new images</p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><strong>Parzen-window density estimator</strong><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Theis, L., Oord, A.V., & Bethge, M. (2015). [A note on the evaluation of generative models](https://arxiv.org/pdf/1511.01844.pdf). CoRR, abs/1511.01844.
">[9]</span></a></sup> should be avoided for evaluation.</p>
<ul>
<li>a.k.a. Kernel Density Esitimator (KDE)</li>
<li>An estimator with kernel $K$ and brandwidth $h$:<script type="math/tex; mode=display">
\hat{p}_h (x) = \frac{1}{nh} \sum_i K \bigg( \frac{x-x_i}{h} \bigg)</script></li>
<li>In generative model evaluation, $K$ is usually density function of standard Gaussian distribution.</li>
<li>Parzen-window estimator can be <strong>unreliable</strong></li>
</ul>
<p>The average <strong>log-likelihood</strong><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Theis, L., Oord, A.V., & Bethge, M. (2015). [A note on the evaluation of generative models](https://arxiv.org/pdf/1511.01844.pdf). CoRR, abs/1511.01844.
">[9]</span></a></sup> is also not correlated with the sample qualities, i.e., a model have poor log-likelihood and produce great samples, or have great log-likelihood and produce poor samples.</p>
<h2 id="Theoretical-Results"><a href="#Theoretical-Results" class="headerlink" title="Theoretical Results"></a>Theoretical Results</h2><p>The global optimum <script type="math/tex">p_g=p_\text{data}</script></p>
<ul>
<li>For fixed $G$, the optimal $D$ is <script type="math/tex; mode=display">
\begin{equation}
D^*_G(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}
\end{equation}</script></li>
</ul>
<p>The training criteron</p>
<script type="math/tex; mode=display">
\begin{align}
C(G) &= \max_D (G,D) \\
&= \mathbb{E}_{x \sim p_\text{data}} \big[ \log D^*_G(x) \big] + \mathbb{E}_{z \sim p_z} \big[ \log\big(1 - D^*_G(G(z))\big) \big]\\
&= \mathbb{E}_{x \sim p_\text{data}} \big[ \log D^*_G(x) \big] + \mathbb{E}_{x\sim p_g} \big[ \log (1 - D_G^*(x)) \big]\\
&= \mathbb{E}_{x \sim p_\text{data}} \bigg[ \log \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)} \bigg] + \mathbb{E}_{x\sim p_g}\bigg[ \log \frac{p_g(x)}{p_\text{data}(x) + p_g(x)} \bigg]
\end{align}</script><ol>
<li>The global minimum of the virtual training criterion $C(G)$ gets the value $-\log4$ iff <script type="math/tex">p_g = p_\text{data}</script>.<script type="math/tex; mode=display">
\begin{align}
C(G) &= -\log(4) + \mathbb{KL}\bigg( p_\text{data} \Vert \frac{p_\text{data} + p_g}{2} \bigg)  + \mathbb{KL}\bigg( p_g \Vert \frac{p_\text{data} + p_g}{2} \bigg)\\
&= -\log(4) + 2 \cdot \text{JSD} (p_\text{data} \Vert p_g)
\end{align}</script></li>
</ol>
<div class="note info">
            <p>The salient <em>difference</em> between VAEs and GANs when using backprop:</p><ul><li><strong>VAEs</strong> cannot have discrete variables at the <strong>input</strong> to the generator </li><li><strong>GANs</strong> cannot have discrete variable at the <strong>output</strong> of the generator.</li></ul>
          </div>
<h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>Pros:</p>
<ul>
<li>Beautiful, sota samples!</li>
</ul>
<p>Cons:</p>
<ul>
<li>Trickier / unstable to train</li>
<li>Cannot solve inference queries such as $p(x)$, $p(z \vert x)$</li>
</ul>
<p>Active research</p>
<ul>
<li>Better loss function, more stable training (Wasserstein GAN, LSGAN, …)</li>
<li>Conditional GANs, GANs for all kinds of applications</li>
</ul>
<div class="note info">
            <p>Generative models:</p><ul><li><em>PixelCNN</em>/ <em>PixelRNN</em>: Explicit density model, optimizes exact likelihood, good samples. But inefficient sequential generation</li><li><em>VAE</em>: Optimize variational lower bound on likelihood. Useful latent representations, inference queries. But current sample quality not the best.</li><li><em>GAN</em>: Game-theoretic approach, best samples! But can be tricky and unstable to train, no inference queries.</li></ul>
          </div>
<h1 id="Deep-Convolutional-GAN-DCGAN"><a href="#Deep-Convolutional-GAN-DCGAN" class="headerlink" title="Deep Convolutional GAN (DCGAN)"></a>Deep Convolutional GAN (DCGAN)</h1><div class="note success">
            <p><strong>DCGAN</strong> architecture<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Radford, A., Metz, L., & Chintala, S. (2015). [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf). CoRR, abs/1511.06434.">[2]</span></a></sup></p><ul><li>Generator $G$ =&gt; upsampling network with fractionally-strided convolutions</li><li>Discriminator $D$ =&gt; CNN</li><li>Replace pooling layers with <strong>strided convolutions</strong> ($D$) and <strong>fractional-strided convolutions</strong> ($G$)</li><li><em>BatchNorm</em> in both $G$ and $D$</li><li>Remove FC hidden layer for deeper architectures</li><li>Use <em>ReLU</em> activation in $G$ for all layers except for the output which uses $\tanh$</li><li>Use <em>LeakyReLU</em> in $D$ for all layers</li></ul>
          </div>
<p><img data-src="/notes/images/DCGAN.png" alt="upload successful"></p>
<h2 id="Vector-Arithmetic"><a href="#Vector-Arithmetic" class="headerlink" title="Vector Arithmetic"></a>Vector Arithmetic</h2><p>Inspired by <em>Word2Vec</em> that vec(“King”) - vec(“man”) + vec(“woman”) = vec(“Queen”), DCGAN found similar latent semantic representation space in $Z$ representations in the generator $G$. </p>
<ul>
<li>The arithmetic between single samples is unstable, thus DCGAN takes the average vector of three examplars.</li>
<li><p>z(“smiling woman”) - z(“neutral woman”) + z(“neutral man”) = z(“smiling man”)<br><img data-src="/notes/images/DCGAN-vector-semantics.png" alt="DCGAN vector semantics"></p>
</li>
<li><p>z(“man with glasses”) - z(“man without glasses”) + z(“woman without glasses”) = z(“woman with glasses”)<br><img data-src="/notes/images/DCGAN-vector-semantics-glasses.png" alt="DCGAN glasses"></p>
</li>
</ul>
<h1 id="Improved-Techniques-for-Training-GANs"><a href="#Improved-Techniques-for-Training-GANs" class="headerlink" title="Improved Techniques for Training GANs"></a>Improved Techniques for Training GANs</h1><ul>
<li><p><strong>Feature Matching</strong><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Salimans, T., Goodfellow, I.J., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016). [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498.pdf). ArXiv, abs/1606.03498.
">[8]</span></a></sup> specifies the objective for $G$ to prevent the overtraining on the current $D$. Let $\mathbf{f(x)}$ be activations on intermediate layers of the $D$, the objective is:</p>
<script type="math/tex; mode=display">\Vert \mathbb{E}_{x \sim p_\text{data}} \mathbf{f(x)} - \mathbb{E}_{z \sim p_z(z)} \mathbf{f}(G(z)) \Vert_2^2</script></li>
<li><p><strong>Minibatch Discrimination</strong> is applied to allow the $D$ to look at multiple data in combination so as to tell the output of $G$ to be more dissimilar to each other. Let <script type="math/tex">f(\mathbf{x_i}) \in \mathbb{R}^A</script> be a feature vector for input <script type="math/tex">\mathbf{x}_i</script>, which is multiplied with a tensor $T \in \mathbb{R}^{A \times B \times C}$:</p>
<script type="math/tex; mode=display">
\begin{align}
M_i &= \mathbf{f(x_i)} \cdot T  \in \mathbb{R}^{B \times C}\\
c_b(\mathbf{x}_i, \mathbf{x}_j) &= \exp( - \Vert M_{i,b} - M_{j,b} \Vert_{L_1})\\
o(\mathbf{x}_i)_b &= \sum_{j=1}^n c_b (\mathbf{x}_i, \mathbf{x}_j) \in \mathbb{R} \\
o(\mathbf{x}_i) &= \big[ o(\mathbf{x}_i)_1, o(\mathbf{x}_i)_2, \cdots, o(\mathbf{x}_i)_B \big] \in \mathbb{R}^B \\
o(\mathbf{X}) & \in \mathbb{R}^{n \times B}
\end{align}</script></li>
</ul>
<p><img data-src="/notes/images/GAN-minibatch-discrimination.png" width="65%"/></p>
<p>Allows to incoporate side information from other samples and is superior to feature matching in the unconditional setting. This helps addressing mode collapse by allowing $D$ to detect if the generated samples are too close to each other.</p>
<ul>
<li><p><strong>Historical averaging</strong> modifies each palyer’s cost as the term </p>
<script type="math/tex; mode=display">\bigg\Vert \mathbf{\theta} - \frac{1}{t} \sum_{t=1}^t \mathbf{\theta}[i] \bigg\Vert^2</script><p>where $\mathbf{\theta}[i]$ is the parameters at past time $i$.</p>
</li>
<li><p><strong>One-sided label smoothing</strong> applies smoothed values like .0 or .1 as the target of 0 and 1.</p>
<script type="math/tex; mode=display">
\begin{align}
\text{regular $D$ cost} &= CE(\color{blue}{1.}, D(\text{data})) + CE (\color{blue}{0.}, , D(\text{samples}))\\
\text{one-sided label smooothed} &= CE(\color{red}{.9}, D(\text{data})) + CE (\color{red}{.1}, , D(\text{samples}))
\end{align}</script></li>
</ul>
<ul>
<li><strong>Virtual batch normalization</strong> uses a reference batch (fixed) to compute normalization statistics and constract a batch containing the sample and reference batch.</li>
</ul>
<ul>
<li><em>Inception score</em> <script type="math/tex">\exp \big( \mathbb{E}_x KL(p(y \vert \mathbb{x}) \Vert p(y) ) \big)</script>has been empirically shown to be well correlated with human judgement.</li>
</ul>
<h1 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h1><p>Conditional GAN<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mirza, M., & Osindero, S. (2014). [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf). ArXiv, abs/1411.1784.
">[6]</span></a></sup> feeds the input $y$ to conditional on both the generator $G$ and discriminator $D$.</p>
<ul>
<li>In $G$, the prior input noise <script type="math/tex">p_z(z)</script> and $y$ are combined in joint hidden representations;</li>
<li>In $D$, $x$ and $y$ are presented as inputs.</li>
</ul>
<p>The objective function is:</p>
<script type="math/tex; mode=display">
\max_{D} \bigg[ \mathbb{E}_{x\sim p_\text{data}} \log D(x \color{red}{|y}) + \mathbb{E}_{z\sim p(z)} \log \big( 1-D(G(z\color{red}{|y})) \big) \bigg]</script><p><img data-src="/notes/images/Conditional-GAN.png" width="60%"/></p>
<h1 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h1><p>InfoGAN <sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, Xi, et al. [Infogan: Interpretable representation learning by information maximizing generative adversarial nets.](http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf) Advances in neural information processing systems. 2016.">[20]</span></a></sup> maximizes the mutual information between a small subset of the latent variables and the observation. Given the generator $G$ with both the incompressible noise $z$ and the latent code $c$, the generator becomes $G(z,c)$. InfoGAN applies the information-theoretic regularization, $i.e.$, the high mutual information (MI) between latent codes $c$ and generator distribution $G(z,c)$.</p>
<p>InfoGAN aims to solve the information-regularized minimax game:</p>
<script type="math/tex; mode=display">
\min_G \max_D V_I (D,G) - \lambda I (c; G(z,c))</script><p>In practice, the mutual information term $I(c; G(z,c))$ is hard to maximize directly as it requires the posterior $P(c \vert x)$. InfoGAN defines an auxiliary distribution $Q(c \vert x)$ to approximate $P(c \vert x)$:</p>
<script type="math/tex; mode=display">
\begin{align}
I(c; G(z,c)) &{}= H(c) - H(c \vert G(z,c)) \\
            &{}= \mathbb{E}_{x \sim G(z,c)} \big[ \mathbb{E}_{c^\prime \sim P(c \vert x)} [ \log P(c^\prime \vert x) ] \big] + H(c) \\
            &{}= \mathbb{E}_{x \sim G(z,c)} \big[ \underbrace{\mathbb{D}_\textrm{KL} (P(\cdot \vert x) \Vert Q(\cdot \vert x))}_{\geq 0} + \mathbb{E}_{c^\prime \sim P(c\vert x)} [ \log Q(c^\prime \vert x) ]  \big] + H(c) \\
            &{}\geq \mathbb{E}_{x \sim G(z,c)} \big[ \mathbb{E}_{c^\prime \sim P(c \vert x)} [ \log P(c^\prime \vert x) ] \big] + H(c)
\end{align}</script><p>The variational lower bound <script type="math/tex">L_1 (G,Q)</script> of the mutual information $I(c;G(z,c))$:</p>
<script type="math/tex; mode=display">
\begin{align}
L_1 (G,Q) &{}= \mathbb{E}_{c \sim P(c), x\sim G(z,c)} [\log Q (c \vert x)] + H(c) \\
&{}= \mathbb{E}_{x \sim G(z,c)} \big[ \mathbb{E}_{c^\prime \sim P(c\vert x)}[\log Q(c^\prime \vert x)] \big] + H(c) \\
&{}\leq I(c; G(z,c))
\end{align}</script><p>where <script type="math/tex">L_1</script> can be maximized w.r.t. $Q$ directly and w.r.t. $G$ via the reparametrization trick. Hence <script type="math/tex">L_1 (G,Q)</script> can be added to the GAN’s objectives with no changes to GAN’s training procedure. </p>
<p>Thus, InfoGAN defines the minimax game with a variational regularization of mutua information and a hyperparameter $\lambda$:</p>
<script type="math/tex; mode=display">\min_{G,Q} \max_D V_\textrm{InfoGAN} (D,G,Q) = V(D,G) - \lambda L_1 (C,Q)</script><h1 id="Wasserstein-GAN-WGAN"><a href="#Wasserstein-GAN-WGAN" class="headerlink" title="Wasserstein GAN (WGAN)"></a>Wasserstein GAN (WGAN)</h1><p>WGAN<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arjovsky, M., Chintala, S., & Bottou, L. (2017). [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf). ArXiv, abs/1701.07875.
">[7]</span></a></sup> designed the objective function such that $G$ minimizes the Earth Mover/Wasserstein distance between data and generative distributions.<br>It improved the stability of learning, avoiding to balance generator $G$ and discriminator $D$’s capacity properly. It also got rid of the <em>mode collapse</em>.</p>
<h2 id="EM-distance"><a href="#EM-distance" class="headerlink" title="EM distance"></a>EM distance</h2><p>It applies <strong>Earth Mover</strong> (EM) distance or Wasserstein-1：</p>
<script type="math/tex; mode=display">
W(\mathbb{P}_r, \mathbb{P}_g) = \inf_{\gamma \in \prod (\mathbb{P}_r,\mathbb{P}_g)} \mathbb{E}_{(x,y) \sim y} \big[ \Vert x - y \Vert \big]</script><ul>
<li>where <script type="math/tex">\prod (\mathbb{P}_r,\mathbb{P}_g)</script> denotes the set of all joint distributions $\gamma(x,y)$ whose marginals are respectively <script type="math/tex">\mathbb{P}_r</script> and <script type="math/tex">\mathbb{P}_g</script>.</li>
<li>“Intuitively, $\gamma(x,y)$ indicates how much ‘mass’ must be transported from $x$ to $y$ in order to transform the distributions <script type="math/tex">\mathbb{P}_r</script> into <script type="math/tex">\mathbb{P}_g</script>. The EM distance is the ‘cost’ of the optimal transport plan.”<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arjovsky, M., Chintala, S., & Bottou, L. (2017). [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf). ArXiv, abs/1701.07875.
">[7]</span></a></sup><br>-In EM distance, the infimum $\inf$ is intractable to compute! </li>
</ul>
<p>Thus it applies <strong>Kantorovinch-Rubinstein duality</strong>:</p>
<script type="math/tex; mode=display">
W(\mathbb{P}_r, \mathbb{P}_g) = \sup_{\Vert f \Vert_L \leq 1} \mathbb{E}_{x\sim \mathbb{P}_r} [f(x)] - \mathbb{E}_{x \sim \mathbb{P}_\theta}[f(x)]</script><ul>
<li>where the supremum is over all the 1-Lipschitz functions $f: \chi \rightarrow \mathbb{R}$</li>
</ul>
<div class="note info">
            <p>$f: X \rightarrow Y$ is $K$-Lipschitz if for distance functions <script type="math/tex">d_X</script> and <script type="math/tex">d_Y</script> on $X$ and $Y$, <script type="math/tex">d_Y\big( f(x_1), f(x_2) \big) \leq K d_x (x_1, x_2)</script></p>
          </div>
<p>Assume that we search over a parameterized family of functions <script type="math/tex">f_w</script> which $w \in \mathcal{W}$:</p>
<script type="math/tex; mode=display">
\begin{align}
\max_{w \in \mathcal{W}} \mathbb{E}_{x \in \mathbb{P}_r} \big[ f_w(x) \big] &\leq \sup_{\Vert f_L \Vert \leq K} \mathbb{E}_{x \sim \mathbb{P}_r} [f(x)] - \mathbb{E}_{x \sim P_g}[f(x)]\\
&=K \cdot W(\mathbb{P}_r, \mathbb{P}_g)
\end{align}</script><ul>
<li>For <script type="math/tex">\mathbb{P}_g</script> induced by <script type="math/tex">g_\theta (z)</script> we can backprop through <script type="math/tex">\mathbb{E}_{x \sim P_r}[f_w(x)]</script>: <script type="math/tex">- \mathbb{E}_{z \sim p(z)} [\nabla_\theta f_w (g_\theta (z))]</script></li>
</ul>
<h2 id="WGAN-pseudocode"><a href="#WGAN-pseudocode" class="headerlink" title="WGAN pseudocode"></a>WGAN pseudocode</h2><p>Given:</p>
<ul>
<li>$\alpha=1e-5$: learning rate</li>
<li>$c=0.01$: clipping hyperparameter</li>
<li>$m=64$: batch size</li>
<li>$n_\text{critic}=5$: the # of iterations of the critic per generator iteration</li>
<li><script type="math/tex">w_0</script>: initial critic parameters; <script type="math/tex">\theta_0</script>: initial generator’s parameters</li>
</ul>
<p><strong>while</strong> $\theta$ has not converged <strong>do</strong></p>
<ol>
<li>for $t=0,\cdots, n_\text{critic}$ do:<ol>
<li>Sample <script type="math/tex">\{x^{(i)}\}_{i=1}^m \sim \mathbb{P}_r</script>, a batch from the real data</li>
<li>Sample <script type="math/tex">\{z^{(i)}\}_{i=1}^m \sim p(z)</script> a batch of prior samples</li>
<li>Update <script type="math/tex">g_w \leftarrow \nabla_w [\color{red}{ \frac{1}{m} \sum_{i=1}^m f_w(x^{(i)}) - \frac{1}{m} \sum_{i=1}^m f_w \big( g_\theta (z^{(i)}) \big)} ]</script></li>
<li>Update <script type="math/tex">w \leftarrow w + \alpha \cdot \text{RMSProp}(w, g_w)</script></li>
<li>Update <script type="math/tex">w \leftarrow \text{clip}(w, -c, c)</script></li>
</ol>
</li>
<li>Sample <script type="math/tex">\{z^{(i)}\}_{i=1}^m \sim p(z)</script> a batch of prior samples</li>
<li>Update <script type="math/tex">g_\theta \leftarrow \color{blue}{ - \nabla_\theta \frac{1}{m} \sum_{i=1}^m f_w \big( g_\theta (z^{(i)}) \big) }</script></li>
<li>Update <script type="math/tex">\theta \leftarrow \theta - \alpha \cdot \text{RMSProp}(\theta, g_\theta)</script></li>
</ol>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img data-src="/notes/images/WGAN-gradients.png" width="50%"/></p>
<ul>
<li>It can be seen that the gradient of regular GAN’s discriminator could get vanishing gradients whereas the WGAN’s critic cannot saturate and converge to a linear function with gradients everywhere.</li>
</ul>
<ul>
<li><p>Momentum-based optimizer like Adam perform worse since the loss for the critic is non-stationary, whereas <strong>RMSProp</strong> is known to perform well even on very non-stationary problems.</p>
</li>
<li><p>No mode collapse has been evidenced during WGAN experiments.</p>
</li>
</ul>
<h1 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h1><p><strong>Problems</strong> of WGAN: </p>
<ul>
<li>the <strong>weight clipping</strong> for $k$-Lipshitz constraint biases the $D$ towards much simpler functions. </li>
<li>“It is observed that our NNs try to attain the maximum gradient norm $k$ and end up learning extremely simple functions”<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A.C. (2017). [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf). NIPS.
">[10]</span></a></sup> (see figures below).</li>
</ul>
<p><img data-src="/notes/images/WGAN-GP-value-surface.png" alt="WGAN-GP value surface"></p>
<p><img data-src="/notes/images/WGAN-GP-weights.png" alt="WGAN-GP weights"></p>
<h2 id="Gradient-Penalty"><a href="#Gradient-Penalty" class="headerlink" title="Gradient Penalty"></a>Gradient Penalty</h2><p>The <em>gradient penalty</em><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A.C. (2017). [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf). NIPS.
">[10]</span></a></sup> (WGAN-GP) is an alternative to implement the Lipschitz constraints. “The differentiable function is 1-Lipschtiz iff it has gradients with norm at most 1 everywhere”<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A.C. (2017). [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf). NIPS.
">[10]</span></a></sup>.</p>
<p>The new objective is:</p>
<script type="math/tex; mode=display">
L = \underbrace{\mathbb{E}_{\mathbf{\tilde{x}} \in \mathbb{P}_g} \big[ D(\color{red}{\mathbf{\tilde{x}}}) - \mathbb{E}_{\mathbf{x} \sim \mathbb{P}_r}[D(\color{blue}{\mathbf{x}})]  \big]}_\textit{original critic loss} + \underbrace{\lambda \mathbb{E}_{\mathbf{\hat{x}} \in \mathbb{P}_{\mathbf{\hat{x}}}} \big[ ( \Vert \nabla_{\mathbf{\hat{x}}} D( \color{orange}{ \mathbf{\hat{x}} }) \Vert_2 -1 )^2 \big] }_{\color{green}{\textit{gradient penalty}}}</script><ul>
<li><strong>Sample distribution</strong>: define <script type="math/tex">\mathbb{P}_\mathbf{\hat{x}}</script> sampling uniformly along straight lines between pairs $\hat{\mathbf{x}}$ and $\mathbf{x}$ sampled from the data distribution <script type="math/tex">\mathbb{P}_r</script> and generator distribution <script type="math/tex">\mathbb{P}_g</script>, i.e.: <script type="math/tex; mode=display">\color{orange}{ \hat{\mathbf{x}} } \leftarrow \epsilon \color{blue}{\mathbf{x}} + (1-\epsilon) \color{red}{\mathbf{\tilde{x}}}</script></li>
</ul>
<h2 id="Pseudocode-1"><a href="#Pseudocode-1" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><p>Given:</p>
<ul>
<li>gradient penalty coefficient $\lambda = 10$</li>
<li># of critic iterations per generator iteration <script type="math/tex">n_\text{critic}</script></li>
<li>batch size $m$</li>
<li>Adam hyperparameters <script type="math/tex">\alpha=1e-3, \beta_1=0, beta_2 = 0.9</script></li>
<li>Initial critic parameters <script type="math/tex">w_0</script>, initial generator parameters <script type="math/tex">\theta_0</script></li>
</ul>
<p><strong>while</strong> $\theta$ has not converged <strong>do</strong></p>
<ol>
<li>for <script type="math/tex">t= 1, \cdots, n_\text{critic}</script> do:<ol>
<li>for $i=1, \cdots, m$ do:<ol>
<li>Sample real data <script type="math/tex">\mathbf{x} \in \mathbb{P}_r</script>, latent variable <script type="math/tex">z \in p(z)</script>, a random number $\epsilon \sim U[0,1]$</li>
<li>forward pass of $G$: <script type="math/tex">\tilde{\mathbf{x}} \leftarrow G_\theta(z)</script></li>
<li>$ \color{orange}{ \hat{\mathbf{x}}} \leftarrow \epsilon \color{blue}{\mathbf{x}} + (1-\epsilon) \color{red}{\mathbf{\tilde{x}}} $</li>
<li>Objective: <script type="math/tex">L^{(i)} \leftarrow D_w(\color{red}{\tilde{\mathbf{x}}}) - D_w(\color{blue}{\mathbf{x}}) + \color{green}{ \lambda( \Vert \nabla_{\color{orange}{ \mathbf{\hat{x}} }} D_w( \color{orange}{ \mathbf{\hat{x}} }) \Vert_2 -1)^2 }</script></li>
</ol>
</li>
<li>Update $D$: <script type="math/tex">w \leftarrow(\nabla_w \frac{1}{m}\sum_{i=1}^m L^{(i)}, w, \alpha, \beta_1, \beta_2 )</script></li>
</ol>
</li>
<li>Sample a batch of latent variables <script type="math/tex">\{z^{(i)}\}_{i=1}^m \sim p(z)</script></li>
<li>Update $G$: <script type="math/tex">\theta \leftarrow \text{Adam}(\nabla_\theta \frac{1}{m}\sum_{i=1}^m - D_w(G_\theta (z)), \theta, \alpha, \beta_1, \beta_2 )</script></li>
</ol>
<h1 id="Spectral-Normalization-GAN-SN-GAN"><a href="#Spectral-Normalization-GAN-SN-GAN" class="headerlink" title="Spectral Normalization GAN (SN-GAN)"></a>Spectral Normalization GAN (SN-GAN)</h1><p><strong>Spectral normalization</strong><sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/pdf/1802.05957.pdf). ArXiv, abs/1802.05957.
">[11]</span></a></sup> is a novel weight normalization method to stablize the training of discriminator $D$. </p>
<p>For a linear layer <script type="math/tex">g(\mathbf{h})= W \mathbf{h}</script>, the norm is given by: </p>
<script type="math/tex; mode=display">
\begin{align}
\Vert g \Vert_\text{Lip} &= \sup_{\mathbf{h}} \sigma (\nabla g(\mathbf{h})) \\
& = \sup_{\mathbf{h}} \sigma (W) \\
& = \sigma (W)
\end{align}</script><p>If the Lipchiz norm of the activation function <script type="math/tex">\Vert a_l \Vert_\text{Lip} = 1</script>, we can use the inequality <script type="math/tex">\Vert g_1 \circ g_2 \Vert_\text{Lip} \leq \Vert g_1 \Vert_\text{Lip} \cdot \Vert g_2 \Vert_\text{Lip}</script> to observe the following bound on <script type="math/tex">\Vert f \Vert_\text{Lip}</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
\Vert f \Vert_\text{Lip} &\leq \Vert (\mathbf{h} \mapsto W^{L+1}\mathbf{h}_L) \Vert_\text{Lip} \cdot \Vert a_L \Vert_\text{Lip} \cdot \Vert (\mathbf{h}_{L-1} \mapsto W^{L}\mathbf{h}_{L-1}) \Vert_\text{Lip} \\
& \cdots \Vert a_1 \Vert_\text{Lip} \cdot \Vert (\mathbf{h}_0 \mapsto W^{1}\mathbf{h}_0) \Vert_\text{Lip} \\
&= \prod_{l=1}^{L+1} \Vert (\mathbf{h}_{l-1} \mapsto W^l \mathbf{h}_{l-1}) \Vert_\text{Lip} \\
& = \prod_{l=1}^{L+1} \sigma (W^l)
\end{align}</script><p>Our <em>spectral normalization</em> normalizes the spectral norm of the weigth matrix $W$ so that it satisfies the Lipschitz constraint $\sigma(W)=1$:</p>
<script type="math/tex; mode=display">
\bar{W}_\text{SN} (W) := \frac{W}{\sigma(W)}</script><h1 id="Projection-Discriminator"><a href="#Projection-Discriminator" class="headerlink" title="Projection Discriminator"></a>Projection Discriminator</h1><p><sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Miyato, T., & Koyama, M. (2018). [cGANs with Projection Discriminator](https://arxiv.org/pdf/1802.05637.pdf). ArXiv, abs/1802.05637.
">[14]</span></a></sup> proposed a novel <strong>projection based discriminator</strong> to incorporate conditional information into the $D$ of GANs that respects the role of the conditional information.</p>
<p><img data-src="/notes/images/Discriminator-4-cGAN.png" width="80%"/></p>
<h1 id="Self-Attention-GAN-SAGAN"><a href="#Self-Attention-GAN-SAGAN" class="headerlink" title="Self-Attention GAN (SAGAN)"></a>Self-Attention GAN (SAGAN)</h1><p>SA-GAN<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, H., Goodfellow, I.J., Metaxas, D.N., & Odena, A. (2018). [Self-Attention Generative Adversarial Networks](https://arxiv.org/pdf/1805.08318.pdf). ArXiv, abs/1805.08318.
">[17]</span></a></sup> applies self-attention op to capture the long0range depdencies in images, whereas conventional convolution op learns the information in a local neighborhood which requires deep layers to model larger receptive regions.</p>
<h2 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h2><p>Given images features from prev layer <script type="math/tex">\mathbf{x} \in \mathbb{R}^{C \times N}</script>, firstly transform into two feature spaces $f$, $g$:</p>
<script type="math/tex; mode=display">
\begin{align}
f(\mathbf{x}) &= \mathbf{W}_f(\mathbf{x}) & \rightarrow \text{Q}\\
g(\mathbf{x}) &= \mathbf{W}_g(\mathbf{x}) & \rightarrow  \text{K} \\
h(\mathbf{x_i}) &= \mathbf{W}_h(\mathbf{x_i}) & \rightarrow  \text{V} 
\end{align}</script><p><img data-src="/notes/images/SAGAN.png" width="85%"/></p>
<p>Thus,</p>
<script type="math/tex; mode=display">
\begin{align}
s_{ij} &= f(\mathbf{x}_i)^\top g(\mathbf{x}_j) & \\
\beta_{ji} &= \frac{\exp (s_{ij})}{\sum_{i=1}^N \exp (s_{ij})} & \text{attn weights} \\
\mathbf{c_j} & =  \sum_{i=1}^N \beta_{j,i} \mathbf{h} (\mathbf{x_i}) & \text{Self attn}\\
\mathbf{o_j} &= \mathbf{W_v} \mathbf{c_j} & \text{post transformation}
\end{align}</script><ul>
<li>where <script type="math/tex">\{ \mathbf{W_g, W_f, W_h} \} \in \mathbb{R}^{\bar{C} \times C}</script>, <script type="math/tex">\mathbf{W}_v \in \mathbb{R}^{C \times \bar{C}}</script> are 1x1 convolutions.</li>
<li>$C$ is the # of channels, $N$ is the # of feature locations</li>
</ul>
<p>Further, a scale parameter are applied on the attention layer output:</p>
<script type="math/tex; mode=display">\mathbf{y_i} = \gamma \mathbf{o_i} + \mathbf{x_i}</script><p>where $\gamma$ is a learnable scalar and is initialized as 0.</p>
<h2 id="Hinge-adversarial-loss"><a href="#Hinge-adversarial-loss" class="headerlink" title="Hinge adversarial loss"></a>Hinge adversarial loss</h2><p>SAGAN applies the hinge adversarial loss (<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lim, J.H., & Ye, J.C. (2017). [Geometric GAN](https://arxiv.org/pdf/1705.02894.pdf). ArXiv, abs/1705.02894.
">[13]</span></a></sup>)</p>
<script type="math/tex; mode=display">
\begin{align}
L_D &= - \mathbb{E}_{(x,y) \sim p_\text{data}} [\min(0, -1+ D(x,y))] - \mathbb{E}_{z \sim p_z, y \sim p_\text{data}} [\min (0, -1-D(G(z), y))] \\
L_G &= - \mathbb{E}_{z \sim p_z, y \sim p_\text{data}} D(G(z), y)
\end{align}</script><h2 id="Stablizing-techniques"><a href="#Stablizing-techniques" class="headerlink" title="Stablizing techniques"></a>Stablizing techniques</h2><ul>
<li>Spectral normalization <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/pdf/1802.05957.pdf). ArXiv, abs/1802.05957.
">[11]</span></a></sup></li>
<li>two timescale update rule (TTUR) <sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://pdfs.semanticscholar.org/4d6f/5a0f5e9941731d0c058e23bb9c5c8e348d08.pdf?_ga=2.151752505.1023325089.1578882675-1863904407.1553653768). NIPS.
">[12]</span></a></sup></li>
</ul>
<h1 id="BigGAN"><a href="#BigGAN" class="headerlink" title="BigGAN"></a>BigGAN</h1><ul>
<li>Large batch size, model size</li>
<li>Fuse class information at all levels (cGAN)</li>
<li>Hinge loss</li>
<li>Orthonormal regularization &amp; Truncation trick</li>
</ul>
<p>BigGAN <sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="(BigGAN) Brock, A., Donahue, J., & Simonyan, K. (2018). [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/pdf/1809.11096.pdf). ArXiv, abs/1809.11096.
">[18]</span></a></sup> applies onthogonal regularization to directly enforces the orthogonality condition:</p>
<script type="math/tex; mode=display">
\begin{align}
R_\beta (W) &= \beta \Vert W^\top W - I \Vert_F^2 \\
R_\beta (W) &= \beta \Vert W^\top W  \odot (\mathbf{1} - I) \Vert_F^2
\end{align}</script><p><strong>BigGAN architecture</strong>:<br><img data-src="/notes/images/BigGAN generator.png" width="30%"/></p>
<h1 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h1><p>StyleGAN<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Karras, T., Laine, S., & Aila, T. (2018). [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/pdf/1812.04948.pdf). CVPR.
">[19]</span></a></sup> automatically learned unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stocastic variation in generation.</p>
<p><img data-src="/notes/images/StyleGAN.png" width="70%"/></p>
<ul>
<li>Given a latent code $\mathbf{z}$ in the input latent space $\mathcal{Z}$, a non-linear mapping network $f: \mathcal{Z} \rightarrow \mathcal{W}$ first produces $\mathbf{w} \in \mathcal{W}$. Here $f$ is an 8-layer MLP.</li>
<li><p>The learned affine transformations then specialize $\mathbf{w}$ to <em>styles</em> <script type="math/tex">\mathbf{y}= (\color{blue}{\mathbf{y}_s}, \color{red}{\mathbf{y}_b)}</script> that control the <strong>adaptive instance normalization</strong> (AdaIN):</p>
<script type="math/tex; mode=display">
\text{AdaIN}(\mathbf{x}_i, \mathbf{y}) = \color{blue}{\mathbf{y}_s} \frac{\mathbf{x}_i - \mu(\mathbf{x}_i)}{\sigma (\mathbf{x}_i)} + \color{red}{\mathbf{y}_b}</script><p>where each feature map $\mathbf{x}_i$ is normalized separately, and then scaled and biased using learned style $\mathbf{y}$.</p>
</li>
<li><p>The explicit noise inputs are broadcast to all feature maps using learned per-feature scaling factors and then added to the convolution output.</p>
</li>
</ul>
<p><img data-src="/notes/images/style-GAN-image.png" width="80%"/></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/pdf/1406.2661.pdf">Generative Adversarial Networks</a>. ArXiv, abs/1406.2661.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Radford, A., Metz, L., &amp; Chintala, S. (2015). <a href="https://arxiv.org/pdf/1511.06434.pdf">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a>. CoRR, abs/1511.06434.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Goodfellow, I.J. (2017). <a href="https://arxiv.org/pdf/1701.00160.pdf">NIPS 2016 Tutorial: Generative Adversarial Networks</a>. ArXiv, abs/1701.00160.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepgenerativemodels.github.io/notes/gan/">cs236 notes</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture13.pdf">cs231 slides</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mirza, M., &amp; Osindero, S. (2014). <a href="https://arxiv.org/pdf/1411.1784.pdf">Conditional Generative Adversarial Nets</a>. ArXiv, abs/1411.1784.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). <a href="https://arxiv.org/pdf/1701.07875.pdf">Wasserstein GAN</a>. ArXiv, abs/1701.07875.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Salimans, T., Goodfellow, I.J., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). <a href="https://arxiv.org/pdf/1606.03498.pdf">Improved Techniques for Training GANs</a>. ArXiv, abs/1606.03498.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Theis, L., Oord, A.V., &amp; Bethge, M. (2015). <a href="https://arxiv.org/pdf/1511.01844.pdf">A note on the evaluation of generative models</a>. CoRR, abs/1511.01844.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A.C. (2017). <a href="https://arxiv.org/pdf/1704.00028.pdf">Improved Training of Wasserstein GANs</a>. NIPS.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Miyato, T., Kataoka, T., Koyama, M., &amp; Yoshida, Y. (2018). <a href="https://arxiv.org/pdf/1802.05957.pdf">Spectral Normalization for Generative Adversarial Networks</a>. ArXiv, abs/1802.05957.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., &amp; Hochreiter, S. (2017). <a href="https://pdfs.semanticscholar.org/4d6f/5a0f5e9941731d0c058e23bb9c5c8e348d08.pdf?_ga=2.151752505.1023325089.1578882675-1863904407.1553653768">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</a>. NIPS.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lim, J.H., &amp; Ye, J.C. (2017). <a href="https://arxiv.org/pdf/1705.02894.pdf">Geometric GAN</a>. ArXiv, abs/1705.02894.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Miyato, T., &amp; Koyama, M. (2018). <a href="https://arxiv.org/pdf/1802.05637.pdf">cGANs with Projection Discriminator</a>. ArXiv, abs/1802.05637.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhao, J.J., Mathieu, M., &amp; LeCun, Y. (2016). <a href="https://arxiv.org/pdf/1609.03126.pdf">Energy-based Generative Adversarial Network</a>. ArXiv, abs/1609.03126.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Nowozin, S., Cseke, B., &amp; Tomioka, R. (2016). <a href="https://arxiv.org/pdf/1606.00709.pdf">f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</a>. ArXiv, abs/1606.00709.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, H., Goodfellow, I.J., Metaxas, D.N., &amp; Odena, A. (2018). <a href="https://arxiv.org/pdf/1805.08318.pdf">Self-Attention Generative Adversarial Networks</a>. ArXiv, abs/1805.08318.<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">(BigGAN) Brock, A., Donahue, J., &amp; Simonyan, K. (2018). <a href="https://arxiv.org/pdf/1809.11096.pdf">Large Scale GAN Training for High Fidelity Natural Image Synthesis</a>. ArXiv, abs/1809.11096.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Karras, T., Laine, S., &amp; Aila, T. (2018). <a href="https://arxiv.org/pdf/1812.04948.pdf">A Style-Based Generator Architecture for Generative Adversarial Networks</a>. CVPR.<a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, Xi, et al. <a href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf">Infogan: Interpretable representation learning by information maximizing generative adversarial nets.</a> Advances in neural information processing systems. 2016.<a href="#fnref:20" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Unsupervised learning</category>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>Unsupervised learning</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Likelihood-based Generative Models I: Autoregressive Models</title>
    <url>/notes/2019/12/17/Generative/Likelihood-based-autoregressive-models/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>The brain has about 10<sup>14</sup> synapses and we only live for about 10<sup>9</sup> seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception) is the only place we can get 10<sup>5</sup> dimensions of constraint per second.<br><b>(Geoffrey Hinton)</b></p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<span id="more"></span>
<p><strong>Unsupervised learning</strong> can be used to capture rich patterns in raw data with deep networks in a <strong>label-free</strong> way.</p>
<ul>
<li>Generative models: recreate raw data distribution</li>
<li>Goal: learn some underlying hidden structure of the data</li>
<li>Self-supervised learning: “puzzle” tasks that require semantic understanding to improve downstream tasks.</li>
<li>Examples: clustering , dimensionality reduction, compression, feature learning, density estimation</li>
</ul>
<div class="note info">
            <p>Main generative models:</p><ul><li>Autoregressive</li><li>Normalizing flow</li><li>Variational Autoencoder</li><li>Generative Adversarial Networks</li></ul>
          </div>
<p>Real applications:</p>
<ul>
<li>generating data: synthesizing images, videos, speech, texts</li>
<li>compressing data: constructing efficient codes</li>
<li>anomaly detection</li>
</ul>
<p><img data-src="/notes/images/taxonomy-of-generative-models.png" alt="upload successful"></p>
<center> Image source: <sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Stanford cs231n: Generative models](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)">[20]</span></a></sup> </center>


<div class="note info">
            <p><strong>Likelihood-based models</strong>:</p><ul><li>estimate <script type="math/tex">p_\text{data}</script> from samples <script type="math/tex">x^{(1)},\cdots,x^{(n)} \sim p_\text{data}(x)</script></li></ul>
          </div>
<p>Given a dataset <script type="math/tex">\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(n)}</script>, find $\theta$ by solving the optimization problem:</p>
<script type="math/tex; mode=display">\arg\min_\theta J(\theta,\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(n)}) = \frac{1}{n}\sum_{i=1}^n - \log p_\theta (\mathbf{x}^{(i)})</script><p>which is equivalent to minimizing KL divergence between the empirical data distribution and the model:</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{p}_\text{data}(\mathbf{x}) &= \frac{1}{n} \sum_{i=1}^n \mathbb{I}[\mathbf{x}=\mathbf{x}^{(i)}] \\
\mathbb{KL}(\hat{p}_\text{data} \| p_\theta) &= \mathbb{E}_{\mathbb{x}\sim \hat{p}_\text{data}} [- \log p_\theta(\mathbf{x})] - \mathbb{H}(\hat{p}_\text{data})
\end{align}</script><ul>
<li>MLE + SGD</li>
<li>p<sub>$\theta$</sub> $\rightarrow$ NN</li>
</ul>
<h1 id="Autoregressive-models"><a href="#Autoregressive-models" class="headerlink" title="Autoregressive models"></a>Autoregressive models</h1><p>Autoregressive (AR) models share parameters among conditional distributions:</p>
<ol>
<li>RNNs</li>
<li>Masking convolutions &amp; attentions</li>
</ol>
<p>The AR property is that each output <script type="math/tex">x_d</script> only dependes on the previous input units <script type="math/tex">\mathbf{x}_{<d}</script>, but not on the future:</p>
<script type="math/tex; mode=display">p(\mathbf{x}) = \prod_{d=1}^D p(x_d \vert \mathbf{x}_{<d})</script><p>AR models can only model <strong>discrete data</strong>.</p>
<h1 id="RNN-AR-models"><a href="#RNN-AR-models" class="headerlink" title="RNN AR models"></a>RNN AR models</h1><p>RNNs privode a <strong>compact, shared parameterization</strong> of a sequence of conditional distributions, shown to excel in handwriting generation, character prediction, machine translation, etc.</p>
<h2 id="RNN-LM"><a href="#RNN-LM" class="headerlink" title="RNN LM"></a>RNN LM</h2><p>Given sequence of characters $\mathbf{x}$, $i$ indicates the position of characters:</p>
<script type="math/tex; mode=display">\log p(\mathbf{x}) = \sum_{i=1}^d \log p(x_i \vert \mathbf{x}_{1:i-1})</script><p>Raw LSTM layers:</p>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  &= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c) \\
\mathbf{c}^c_j &= \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j} \\
\mathbf{h}_j^c &= \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)
\end{align}</script><h2 id="PixelRNN"><a href="#PixelRNN" class="headerlink" title="PixelRNN"></a>PixelRNN</h2><p>Like in LM, AR models cast the joint distribution of pixels in the images to a product of conditional distributions, turning the joint modeling problem into a sequential problem with factorization, where one learns to predict the next pixel given all preivous generated pixels.</p>
<p><em>PixelRNN</em> leverages two-dimensional RNNs and residual connections<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup> in generative image modeling. </p>
<h3 id="Pixel-by-pixel-generation"><a href="#Pixel-by-pixel-generation" class="headerlink" title="Pixel-by-pixel generation"></a>Pixel-by-pixel generation</h3><p>The probability $p(\mathbf{x})$ to each image $\mathbf{x}$ of $n \times n$ pixels. The image $\mathbf{x}$ is tiled as 1-D sequence <script type="math/tex">x_1, \cdots, x_{n^2}</script> where pixels are taken from the image row by row. The joint distribution $p(\mathbf{x})$ is the product of the conditional probability over pixels:</p>
<script type="math/tex; mode=display">p(\mathbf{x}) = \prod_{i=1}^{n^2} p(x_i \vert x_1, \cdots, x_{i-1})</script><p>where each pixel is conditioned on all the previous generated pixels, whose generation is in the <em>raster scan order</em>: row by row and pixel by pixel within each row.</p>
<p>Taking into account RGB color channels of each pixel, the distribution of pixel $x_i$ is:</p>
<script type="math/tex; mode=display">p(x_i \vert \mathbf{x}_{<i}) = p(\color{red}{x_{i,R}} \vert \mathbf{x}_{<i}) p(\color{green}{x_{i,G}} \vert \mathbf{x}_{<i}, \color{red}{x_{i,R}}) p(\color{blue}{x_{i, B}} \vert \mathbf{x}_{<i}, \color{red}{x_{i, R}}, \color{green}{x_{i, G}})</script><p><img data-src='/notes/images/Generative-image-modeling-RGB.png' width='40%'/></p>
<p><em>PixelRNN</em> employs the discrete distribution with a 256-way softmax. Each channel variable <script type="math/tex">x_{x,*}</script> takes the scalar values from 0 to 255. The advantages:</p>
<ol>
<li>to be arbitrarily multimodal without prior on the shape;</li>
<li>achieve better results than continuous distribution and easy to learn.</li>
</ol>
<div class="note info">
            <ul><li>Training &amp; evaluation -&gt; the pixel distribution is parallel distribution (<em>teacher forcing</em>)</li><li>Generation -&gt; sequential, row by row and pixel by pixel.</li></ul>
          </div>
<p><img data-src="/notes/images/PixelRNN-state-mapping.png" width="80%" /></p>
<center>Image source: <i>PixelRNN</i><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup></center>

<h3 id="Row-LSTM"><a href="#Row-LSTM" class="headerlink" title="Row LSTM"></a>Row LSTM</h3><p><strong>Row LSTM</strong> is a <strong>unidirectional LSTM layer</strong> that takes the image <strong>row by row</strong> from top down to bottom computing features with 1-D convolution for a whole row at once.</p>
<p>It captures a roughly <u><strong>“triangular context”</strong> above the pixel</u> with kernel size of $k \times 1$ in temporal convolutions where $k \leq 3$, of which the larger kernel size captures the broader contexts and weight sharing guarantees the translation invariance.</p>
<div class="note warning">
            <ul><li><em>Row LSTMs</em> have the <strong>triangular receptive field</strong>, unable capturing the entire available context.</li></ul>
          </div>
<p>The computation proceeds as follows.</p>
<ul>
<li>LSTM layers have an <em>input-to-state</em> (is) and recurrent <em>state-to-state</em> (ss) component that together determine the four gates inside the LSTM core.</li>
<li>The <em>input-to-state</em> component is precomputed using 1-D  masked convolution with kernel size $k \times 1$ horizontally, where  <script type="math/tex">\{ \mathbf{i}^c_j, \mathbf{o}^c_j, \mathbf{f}^c_j, \tilde{c}^c_j\} \in \mathbb{R}^{h \times n \times n}</script>, $h$ denotes the # of output feature maps.</li>
<li>The row-wise <em>state-to-state</em> component of the LSTM layer takes the previous hidden and cell state <script type="math/tex">\mathbf{h}_{j-1}^c</script> and <script type="math/tex">\mathbf{c}_{j-1}^c</script>, where <script type="math/tex">\{\mathbf{x}_i, \mathbf{h}_{j-1}^c, \mathbf{c}_{j-1}^c\} \in \mathbb{R}^{h \times n \times 1}</script>, the weights $\mathbf{K}^{ss}$ and $\mathbf{K}^{is}$ represent the kernel weights of <em>state-to-state</em> and <em>input-to-state</em> components, $\circledast$ denotes the convolution operation.</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  &= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  ( \color{red}{ \mathbf{K}^\text{ss} \circledast \mathbf{h}_{j-1}^c} \color{blue}{+} \color{red}{ \mathbf{K}^\text{is} \circledast \mathbf{x}_{j} }) \\
\mathbf{c}^c_j &= \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j} \\
\mathbf{h}_j^c &= \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)
\end{align}</script><h3 id="Diagonal-BiLSTM"><a href="#Diagonal-BiLSTM" class="headerlink" title="Diagonal BiLSTM"></a>Diagonal BiLSTM</h3><p><em>Diagonal BiLSTM</em> is designed to impede the drawbacks of limited triangular receptive fields of <em>Row LSTM</em> and could capture the entire available context.</p>
<p><em>Diagonal BiLSTM</em> skews the input $\mathbf{x} \in \mathbb{R}^{n \times n}$ into $\mathbb{R}^{n \times (2n-1)}$ by shifting the $i$-th row with $(i-1)$ position offsets, i.e. each row is one position right shift compared with the previous row (see below figure).</p>
<ul>
<li>The <em>input-to-state</em> components of each direction adopt a $1 \times 1$ convolution $K^\text{is}$, and the output of $(\mathbf{K}^\text{is} \circledast \mathbf{x}) \in \mathbb{R}^{4h \times n \times n}$</li>
<li>The <em>state-to-state</em> recurrent component uses a <strong>column-wise 1D convolution</strong> $K^\text{ss}$ with kernel size $2 \times 1$.<br>  <small>Why 2x1? Larger sizes do not broaden the already global receptive fields.</small></li>
<li>For bi-LSTMs, the right-to-left directional LSTM is shifted down by one row and added to the left-to-right LSTM outputs.</li>
</ul>
<p><img data-src='/notes/images/PixelRNN-diagonal-biLSTM.png' width='80%'/></p>
<ul>
<li>Train the <em>pixelRNN</em> of up to 12 layers of depth with residual connections and layer-to-output skip connections.</li>
</ul>
<p><img data-src="/notes/images/pixelRNN-skip-connection.png" alt="upload successful"></p>
<p><center> Image source: <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup><center></p>
<h1 id="Masking-based-AR-models"><a href="#Masking-based-AR-models" class="headerlink" title="Masking-based AR models"></a>Masking-based AR models</h1><p>Key property: parallelized computation of all conditions</p>
<ol>
<li>Masked MLP (MADE)</li>
<li>Masked convolutions &amp; self-attention (PixelCNN families and <em>PixelSNAIL</em>)<ul>
<li>also share parameters across time</li>
</ul>
</li>
</ol>
<p><img data-src='/notes/images/pixelCNN-visualization.png' width='50%'/></p>
<h2 id="MADE"><a href="#MADE" class="headerlink" title="MADE"></a>MADE</h2><p>Masked Autoencoder Distribution Estimator (MADE) (Deepmind &amp; Iain Murray)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Germain, M., Gregor, K., Murray, I., & Larochelle, H. (2015, June). [Made: Masked autoencoder for distribution estimation](http://www.jmlr.org/proceedings/papers/v37/germain15.pdf). In International Conference on Machine Learning (pp. 881-889).
">[3]</span></a></sup> masks the autoencoder’s parameters to respect autoregressive properties that each input only reconstructed from previous input in a given ordering.</p>
<p>MADE zeros out the connections of layer connections by elementwise-multiplying a binary mask matrix on the weight matrices, setting the weight connectivities as 0s for removing.</p>
<p>For masked autoencoder with $L&gt;1$ hidden layers, let </p>
<ul>
<li>$D$ denote the dimension of input $\mathbf{x}$, $\mathbf{M}$ denote the connection mask; </li>
<li>in $l$-th layer, $K^l$ be the # of hidden states, $m^l(k)$ represent the maximum number of connected input of the $k$-th unit.</li>
</ul>
<blockquote>
<p>For $l$-th layer in masked autoencoders, the mask of weight matrices $\mathbf{W}$:</p>
<script type="math/tex; mode=display">
\mathbf{M}_{k^\prime, k}^{\mathbf{W}^l} = \mathbf{1}_{m^l(k^\prime) \leq m^{l-1}(k)} = \left\{
                \begin{array}{ll}
                  1 & \text{if} \; m^l(k^\prime) \leq m^{l-1}(k) \\
                  0 & \text{otherwise}
                \end{array}
              \right.</script><p>For the output mask of weight matrices $\mathbf{V}$:</p>
<script type="math/tex; mode=display">
\mathbf{M}_{d, k}^{\mathbf{V}} = \mathbf{1}_{d > m^L(k)} = \left\{
                \begin{array}{ll}
                  1 & \text{if} \; d > m^L(k) \\
                  0 & \text{otherwise}
                \end{array}
              \right.</script></blockquote>
<p><img data-src='/notes/images/MADE.png' width='70%'/></p>
<p><center> Image source:<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Germain, M., Gregor, K., Murray, I., & Larochelle, H. (2015, June). [Made: Masked autoencoder for distribution estimation](http://www.jmlr.org/proceedings/papers/v37/germain15.pdf). In International Conference on Machine Learning (pp. 881-889).
">[3]</span></a></sup> <center></p>
<h2 id="PixelCNN-families"><a href="#PixelCNN-families" class="headerlink" title="PixelCNN families"></a>PixelCNN families</h2><h3 id="PixelCNN"><a href="#PixelCNN" class="headerlink" title="PixelCNN"></a>PixelCNN</h3><p>PixelCNN<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). [Pixel recurrent neural networks](https://arxiv.org/pdf/1601.06759.pdf). (Google DeepMind). ICML 2016.
">[1]</span></a></sup> adopts multiple conv layers <em>without pooling</em> to preserve the spatial resolution and masks the future context.</p>
<ul>
<li><em>Drawbacks</em>: PixelRNNs cannot consider the pixels on the right side of the current position (as Fig. below).</li>
</ul>
<p><img data-src='/notes/images/pixelCNN.png' width='50%'/></p>
<h3 id="Gated-PixelCNN"><a href="#Gated-PixelCNN" class="headerlink" title="Gated PixelCNN"></a>Gated PixelCNN</h3><p><em>Gated PixelCNN</em> takes into account both the <strong>vertical stack</strong> and the <strong>horizontal stack</strong> by combing both the pixels of region above and those on the left of the current row, wherein the convolutions of vertical stack are not masked. (See <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Tutorial: Gated PixelCNN](http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html)
">[5]</span></a></sup> for the tutorial.)</p>
<ul>
<li>advantages: deal with “blind spot” of the receptive field in <em>PixelCNNs</em>.</li>
</ul>
<p><img data-src='/notes/images/gatedPixelCNN.png' width='50%'/></p>
<p><em>Gated PixelCNNs</em> replace the ReLU between masked convolutions in the original pixelCNN with the gated activation function:</p>
<script type="math/tex; mode=display">\mathbf{y} = \tanh (w_{k,f} \circledast \mathbf{x}) \odot \sigma (W_{k,g} \circledast \mathbf{x})</script><p>where $p$ represents the # of feature maps, $\circledast$ denotes convolution operations, where it is masked in horizontal stack but unmasked in the vertical stack.</p>
<p><img data-src='/notes/images/Gated-PixelCNN.png' width='100%'/></p>
<h4 id="Conditional-PixelCNN"><a href="#Conditional-PixelCNN" class="headerlink" title="Conditional PixelCNN"></a>Conditional PixelCNN</h4><p>Given high-level latent representation $\mathbf{h}$, we model the conditional PixelCNN models:</p>
<script type="math/tex; mode=display">p(\mathbf{x} \vert \mathbf{h}) = \prod_{i=1}^{n^2} p(x_i \vert x_1, \cdots, x_{i-1}, \mathbf{h})</script><p>Add terms pf $\mathbf{h}$ before the non-linearities:</p>
<script type="math/tex; mode=display">\mathbf{y} = \tanh (W_{k,f} \circledast \mathbf{x}  \color{red}{+ V_{k,f}^\top \mathbf{h}} ) \odot \sigma (W_{k,g} \circledast \mathbf{x} \color{red}{+ V_{k,g}^\top \mathbf{h} })</script><p>where $k$ is the layer number.<br>Condition on <em>what</em>: </p>
<ul>
<li>class-dependent: $\mathbf{h} \rightarrow \text{1-hot}$, is equivalent to adding a class-dependent bias at each layer. </li>
</ul>
<p>Condition on <em>where</em>:</p>
<ul>
<li>location-dependent: use Transposed convolution to map $\mathbf{h}$ to a spatial representation $\color{red}{\mathbf{s} = \text{deconv}(\mathbf{h})}$ to produce the output $\mathbf{s}$ of the same shape as the image. It can be seen as adding a location dependent bias:<script type="math/tex; mode=display">\mathbf{y} = \tanh (W_{k,f} \circledast \mathbf{x}  \color{red}{+ V_{k,f} \circledast \mathbf{s}} ) \odot \sigma (W_{k,g} \circledast \mathbf{x} \color{red}{+ V_{k,g} \circledast \mathbf{s} })</script></li>
</ul>
<h3 id="PixelCNN-1"><a href="#PixelCNN-1" class="headerlink" title="PixelCNN++"></a>PixelCNN++</h3><p>Background: </p>
<ul>
<li>previous 256-way softmax is very costly and slow to compute, and makes the gradient w.r.t parameters very sparse.</li>
<li>the model does not know that the value 128 is close to that of 127 and 129. Especially unobserved sub-pixels will be assigned with 0 probability.</li>
</ul>
<p><em>PixelCNN++</em><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). [Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications](https://arxiv.org/pdf/1701.05517). arXiv preprint arXiv:1701.05517.
">[6]</span></a></sup> assumes the latent color intensity $\nu$ with continuous distribution and takes the continuous univariate distribution to be <strong>a mixture of logistic distributions</strong>. </p>
<ul>
<li>$\sigma(x) = -1 / (1+ \exp(- x))$.</li>
<li>For all sub-pixels except the edges 0 and 255:<script type="math/tex; mode=display">
\begin{align}
\nu & \sim \sum_{i=1}^K \pi_i \text{logistic}(\mu_i, s_i) \\
P(x \vert \pi, \mu, s) &= \sum_{i=1}^K \pi_i [ \sigma \bigg( (\frac{x+0.5 - \mu_i}{s_i}) \bigg) - \sigma \bigg( (\frac{x-0.5 - \mu_i}{s_i}) \bigg)  ]
\end{align}</script></li>
<li>For edge cases, <ol>
<li>when $x=0$, set $x-0.5 \rightarrow -\infty$</li>
<li>when $x=255$, set $x+0.5 \rightarrow +\infty$</li>
</ol>
</li>
</ul>
<div class="note info">
            <p><strong>Logistic distribution</strong>:</p><script type="math/tex; mode=display">\begin{align}F(x) &= \big(1+ e^{-\frac{x-\mu}{s}}\big)^{-1} \\&= \frac{1}{2} [1+\tanh (\frac{x-\mu}{2 s})]\end{align}</script><p>where the mean $ \mu \in (-\infty, +\infty)$,  std deviation $s &gt;0$</p>
          </div>
<ul>
<li><em>PixelCNN++</em> does not use deep networks to model the relationship between color channels. For the pixel <script type="math/tex">(r_{i,j}, g_{i,j}, b_{i,j})</script> at the location $(i,j)$ in the image, with the contexts <script type="math/tex">C_{i,j}</script>:</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
p(\color{red}{r}_{i,j}, \color{green}{g}_{i,j}, \color{blue}{b}_{i,j} \vert C_{i,j}) &= P \bigg(\color{red}{r}_{i,j} \vert \mu_\color{red}{r}(C_{i,j}), s_\color{red}{r}(C_{i,j})\bigg) \times P\bigg(\color{green}{g}_{i,j} \vert \mu_\color{green}{g} (C_{i,j}, \color{red}{r}_{i,j}), s_\color{green}{g}(C_{i,j})\bigg) \\ & \times P\bigg(\color{blue}{b}_{i,j} \vert \mu_\color{blue}{b} (C_{i,j}, \color{red}{r}_{i,j}, \color{green}{g}_{i,j}), s_\color{blue}{b}(C_{i,j}) \bigg) \\
\mu_\color{green}{g}(C_{i,j}, \color{red}{r}_{i,j}) &= \mu_\color{green}{g}(C_{i,j}) + \alpha(C_{i,j})\color{red}{r}_{i,j} \\
\mu_\color{blue}{b}(C_{i,j}, \color{red}{r}_{i,j}, \color{green}{g}_{i,j}) &= \mu_\color{blue}{b}(C_{i,j}) + \beta (C_{i,j}) \color{red}{r}_{i,j} + \gamma (C_{i,j}) \color{blue}{b}_{i,j}
\end{align}</script><p>where $\alpha$, $\gamma$, $\beta$ are scalar coefficents depdenting on the mixture component and previous pixels.</p>
<ul>
<li>As shown in the figure, it applies convolutions of stride 2 for downsampling and transposed strided convolution for upsampling. It also uses shor-cut connections recover the information loss from convolutions in the lower layers, similar to VAE<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). [Improved variational inference with inverse autoregressive flow](https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf). In Advances in neural information processing systems (pp. 4743-4751).
">[8]</span></a></sup> and U-Net<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ronneberger, O., Fischer, P., & Brox, T. (2015, October). [U-net: Convolutional networks for biomedical image segmentation](https://arxiv.org/pdf/1505.04597.pdf)%E5%92%8C[Tiramisu](https://arxiv.org/abs/1611.09326). In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.
">[7]</span></a></sup>. </li>
</ul>
<p><img data-src="/notes/images/PixelCNN++.png" alt="upload successful"></p>
<h2 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h2><ul>
<li>Masked convolutions: masked convolution has limited receptive field and thus requires deep stacked layers of a linearly increased number. It requires expand the kernel size or incease the layer depth to enlarge the effective receptive fields. (see below figure)</li>
</ul>
<p><img data-src="/notes/images/WaveNet-causal-conv.png" alt="upload successful"></p>
<p>WaveNet<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). [Wavenet: A generative model for raw audio](https://arxiv.org/pdf/1609.03499.pdf). arXiv preprint arXiv:1609.03499.
">[11]</span></a></sup><sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[(DeepMind blog) WaveNet: A generative model for raw audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)
">[12]</span></a></sup> (van den Ood <em>et al.</em>, DeepMind 2016) leverages dilated masked casual convolution to exponentially increase the receptive field. It is applied in TTS, ASR, music generation, audio modeling, etc.</p>
<p><img data-src="/notes/images/Dilated-causal-conv.png" alt="upload successful"></p>
<h3 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h3><p>It uses the same gated activation unit in <em>PixelCNN</em>, outperforming ReLU:</p>
<script type="math/tex; mode=display">
\mathbf{z} = \tanh (W_{f,k} \circledast \mathbf{x}) \odot \sigma(W_{g,k} \circledast \mathbf{x})</script><p>The overall model structure is:<br><img data-src="/notes/images/WaveNet-Model.png" alt="upload successful"></p>
<h3 id="Conditional-WaveNet"><a href="#Conditional-WaveNet" class="headerlink" title="Conditional WaveNet"></a>Conditional WaveNet</h3><p>Like the conditional <em>Gated PixelCNN</em>, WaveNet can be also conditional on a hidden representation $\mathbf{h}$.</p>
<ul>
<li>Global conditioning on a single representation vector $\mathbf{h}$ that influences the output distribution of all timesteps, e.g. a speaker embedding in a TTS model:<script type="math/tex; mode=display">
\mathbf{z} = \tanh (W_{f,k} \circledast \mathbf{x} + \color{red}{V_{f,k}^\top \mathbf{h}}) \odot \sigma(W_{g,k} \circledast \mathbf{x} + \color{red}{V_{g,k}^\top \mathbf{h}})</script></li>
<li>Local conditioning on a second timeseries <script type="math/tex">h_t</script>, possibly with a lower sampling frequency than the audio, e.g. linguistic features in a TTS model. WaveNet learns the upsampling on this time series using a <strong>transposed convolution</strong>: <script type="math/tex">\mathbf{y} = f(\mathbf{h})</script><script type="math/tex; mode=display">
\mathbf{z} = \tanh (W_{f,k} \circledast \mathbf{x} + \color{red}{V_{f,k}  \circledast f(\mathbf{h})}) \odot \sigma(W_{g,k} \circledast \mathbf{x} + \color{red}{V_{g,k}  \circledast f(\mathbf{h})})</script></li>
</ul>
<h3 id="Softmax-distribution"><a href="#Softmax-distribution" class="headerlink" title="Softmax distribution"></a>Softmax distribution</h3><p>The raw audio output is stored as a sequence of 16-bit scalar values (one per time step), thus the softmax output is 2<sup>16</sup>=65,536 probabilities per timestep. WaveNet applies a <strong>$\mu$-law companding transformation</strong> to the data and thenquantize it to 256 possible values:</p>
<script type="math/tex; mode=display">
f(x_t) = \text{sign} (x_t) \frac{\ln(1 + \mu |x_t|)}{\ln(1 + \mu)}</script><p>where <script type="math/tex">x_t \in (-1,1)</script>, $u=255$. The reconstruction signal after quantization sounded similar to the original.</p>
<h3 id="Fast-generation-via-caching"><a href="#Fast-generation-via-caching" class="headerlink" title="Fast generation via caching"></a>Fast generation via caching</h3><div class="note dander">
            <p><strong>Problems</strong>: During generation, convolutional AR models redundantly compute states, impeding the speed of generation process. Such states can be cached and reused to expedite the generation.<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... & Huang, T. S. (2017). [Fast generation for convolutional autoregressive models](https://arxiv.org/pdf/1704.06001). arXiv preprint arXiv:1704.06001.">[14]</span></a></sup></p>
          </div>
<p>The convolutional autoregressive generative model could cache and reuse the previously computed hidden states to accelerate the generation.</p>
<p>The below figure shows the model with 2 convolutional and 2 transposed convolutional layers with strid of 2, wherein blue dots indicate the cached states and orange bots are computed in the current step. The computation process can be seen as:</p>
<p><img data-src="/notes/images/conv-AR-fast-generation.png" alt="upload successful"></p>
<p><center> Image source: <sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... & Huang, T. S. (2017). [Fast generation for convolutional autoregressive models](https://arxiv.org/pdf/1704.06001). arXiv preprint arXiv:1704.06001.
">[14]</span></a></sup> </center></p>
<ul>
<li>This can also scale to 2D to apply on <strong>PixelCNN families</strong><sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... & Huang, T. S. (2017). [Fast generation for convolutional autoregressive models](https://arxiv.org/pdf/1704.06001). arXiv preprint arXiv:1704.06001.
">[14]</span></a></sup>.</li>
</ul>
<h2 id="PixelSNAIL"><a href="#PixelSNAIL" class="headerlink" title="PixelSNAIL"></a>PixelSNAIL</h2><p><em>PixelSNAIL</em><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, X., Mishra, N., Rohaninejad, M., & Abbeel, P. (2017). [Pixelsnail: An improved autoregressive generative model](https://arxiv.org/pdf/1712.09763). ICML 2018.
">[10]</span></a></sup> adopt masked self-attention approaches inspired by <em>SNAIL</em><sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mishra, N., Rohaninejad, M., Chen, X., & Abbeel, P. (2017). [A simple neural attentive meta-learner](https://arxiv.org/pdf/1707.03141). arXiv preprint arXiv:1707.03141.
">[9]</span></a></sup>. </p>
<h3 id="Model-architecture-1"><a href="#Model-architecture-1" class="headerlink" title="Model architecture"></a>Model architecture</h3><p>The overall model structure:</p>
<p><img data-src="/notes/images/PixelSNAIL-model.png" alt="upload successful"></p>
<ul>
<li><p>It uses the self-attention block with shape <script type="math/tex">H \times W \times C_1 \rightarrow H \times W \times C_2</script> (see below figure):</p>
<ul>
<li>Key f<sub>k</sub>: <script type="math/tex">C_1 \rightarrow \text{d}_\text{key}</script></li>
<li>Query f<sub>q</sub>: <script type="math/tex">C_1 \rightarrow \text{d}_\text{key}</script></li>
<li><p>Value f<sub>v</sub>(x): <script type="math/tex">C_1 \rightarrow C_2</script></p>
<p>Given 2D feature map $\mathbf{y}= {y_1, y_2, \cdots, y_N }$, the attention mapping is:</p>
<script type="math/tex; mode=display">
\begin{align}
z_i & = \sum_{j<i} e_{ij} f_v(y_i) \\
e_i &= \text{softmax}([f_k(y_1)^\top f_q(y_i), \cdots, f_k(y_i-1)^\top f_q(y_i)])
\end{align}</script><p>where the summation over all previous history, i.e. $j&lt;i$.</p>
</li>
</ul>
</li>
</ul>
<p><img data-src="/notes/images/PixelSNAIL-attn-block.png" width="60%"/></p>
<ul>
<li>It applies 2D convolutions with gated activation functions as gated &amp;PixelCNN* and residual connections as the figure.</li>
</ul>
<p><img data-src="/notes/images/PixelSNAIL-residual-blocks.png" width="60%"/></p>
<ul>
<li>It adopts the <strong>zigzag ordering</strong> rather than <em>PixelCNN</em>-like raster scan ordering.</li>
</ul>
<p><img data-src="/notes/images/Image-generative-ordering.png" alt="upload successful"></p>
<ul>
<li>It employs the <strong>discretized mixture of logistics</strong> of <em>PixelCNN++</em> as the output distribution.</li>
</ul>
<p>In comparison,</p>
<ul>
<li><em>Gated PixelCNN</em> and <em>PixelCNN++</em> apply causal convolutions (dilated and strided conv, respectively) over the sequence, allowing the <strong>high-brandwidth access</strong> to the previous pixels. However, caual convolutions are limited to the receptive field due to their finite sizes.</li>
<li><strong>PixelSNAIL</strong> achieves a much larger receptive field size (see below figure).</li>
</ul>
<p><img data-src="/notes/images/PixelCNN-effective-receptive-field.png" alt="upload successful"></p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>“The basic difference between AR models with Generative Adversarial Networks (GANs) is that GANs <strong>implicitly learn data distribution</strong> whereas AR models learn the explicit distribution governed by a prior. “<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[(TowardsDataScience blog) Auto-Regressive Generative Models (PixelRNN, PixelCNN++)](https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173)
[^16:] [CS294-158 Lecture 2 slides](https://drive.google.com/file/d/194FouvI7xJM0bG4AaEsHo8PSSOjUozN6/view)
">[15]</span></a></sup><br><div class="note info">
            <p><strong>Pros:</strong></p><ul><li>expressivity (explicit learn): AR factorization is general; can explicitly compute likelihood $p(x)$</li><li>explicit likelihood of training data gives good evaluation metric</li><li>good samples</li><li>generalization: meaningful parameter sharing has good inductive bias</li><li>the training is more stable than GANs</li><li>it works for both discrete and continuous data (It is hard to learn discrete data like text for GANs)</li></ul>
          </div><br><div class="note danger">
            <p><strong>Cons:</strong></p><ul><li>Sequential generation =&gt; slow!</li><li>Low sampling efficency: sampling each pixel = 1 forward pass!</li></ul>
          </div><br><!-- todo: 17 / 18 /19 reading --></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oord, A. V. D., Kalchbrenner, N., &amp; Kavukcuoglu, K. (2016). <a href="https://arxiv.org/pdf/1601.06759.pdf">Pixel recurrent neural networks</a>. (Google DeepMind). ICML 2016.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks. Andrej Karpathy blog</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Germain, M., Gregor, K., Murray, I., &amp; Larochelle, H. (2015, June). <a href="http://www.jmlr.org/proceedings/papers/v37/germain15.pdf">Made: Masked autoencoder for distribution estimation</a>. In International Conference on Machine Learning (pp. 881-889).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., &amp; Graves, A. (2016). <a href="https://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders.pdf">Conditional image generation with pixelcnn decoders</a>. In Advances in neural information processing systems (pp. 4790-4798).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html">Tutorial: Gated PixelCNN</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Salimans, T., Karpathy, A., Chen, X., &amp; Kingma, D. P. (2017). <a href="https://arxiv.org/pdf/1701.05517">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</a>. arXiv preprint arXiv:1701.05517.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ronneberger, O., Fischer, P., &amp; Brox, T. (2015, October). <a href="https://arxiv.org/pdf/1505.04597.pdf">U-net: Convolutional networks for biomedical image segmentation</a>%E5%92%8C<a href="https://arxiv.org/abs/1611.09326">Tiramisu</a>. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016). <a href="https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf">Improved variational inference with inverse autoregressive flow</a>. In Advances in neural information processing systems (pp. 4743-4751).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mishra, N., Rohaninejad, M., Chen, X., &amp; Abbeel, P. (2017). <a href="https://arxiv.org/pdf/1707.03141">A simple neural attentive meta-learner</a>. arXiv preprint arXiv:1707.03141.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, X., Mishra, N., Rohaninejad, M., &amp; Abbeel, P. (2017). <a href="https://arxiv.org/pdf/1712.09763">Pixelsnail: An improved autoregressive generative model</a>. ICML 2018.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... &amp; Kavukcuoglu, K. (2016). <a href="https://arxiv.org/pdf/1609.03499.pdf">Wavenet: A generative model for raw audio</a>. arXiv preprint arXiv:1609.03499.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">(DeepMind blog) WaveNet: A generative model for raw audio</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ramachandran, P., Paine, T. L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., ... &amp; Huang, T. S. (2017). <a href="https://arxiv.org/pdf/1704.06001">Fast generation for convolutional autoregressive models</a>. arXiv preprint arXiv:1704.06001.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173">(TowardsDataScience blog) Auto-Regressive Generative Models (PixelRNN, PixelCNN++)</a>
[^16:] <a href="https://drive.google.com/file/d/194FouvI7xJM0bG4AaEsHo8PSSOjUozN6/view">CS294-158 Lecture 2 slides</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1703.03664.pdf">Parallel Multiscale Autoregressive Density Estimation</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1612.08185.pdf">PixelCNN Models with Auxiliary Variables for Natural Image Modeling</a><a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1812.01608.pdf">GENERATING HIGH FIDELITY IMAGES
WITH SUBSCALE PIXEL NETWORKS
AND MULTIDIMENSIONAL UPSCALING</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf">Stanford cs231n: Generative models</a><a href="#fnref:20" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Unsupervised learning</category>
        <category>Likelihood-based models</category>
        <category>Autoregressive models</category>
      </categories>
      <tags>
        <tag>Unsupervised learning</tag>
        <tag>Autoregressive models</tag>
      </tags>
  </entry>
  <entry>
    <title>Likelihood-based Generative Models II: Flow  Models</title>
    <url>/notes/2019/12/22/Generative/Likelihood-based-generative-flow-models/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Flow models are used to learn <strong>continuous data</strong>.<br><span id="more"></span></p>
<div class="note warning">
            <p><strong>Discrete autoregressive models</strong> tractably learn the joint distribution by decomposing it into a product of conditionals using the <em>probability chain rule</em> according to a fixed ordering over dimensions.</p><p>The sequential nature limits its computational efficiency:</p><ol><li>its sampling procedure is sequential and non-parallelizable;</li><li>there is no natural latent representation associated with autoregressive models.</li></ol><p>To sum up:</p><ul><li>Pros: <ul><li>fast evaluation of p(x)</li><li>great compression performance</li><li>good samples with carefully designed dependency structure</li></ul></li><li>Cons:<ul><li>slow sampling (without significant engineering)</li><li>discrete data only</li></ul></li></ul>
          </div>
<h1 id="Flows-explanation"><a href="#Flows-explanation" class="headerlink" title="Flows explanation"></a><strong>Flows explanation</strong></h1><blockquote>
<p>Ideas: instead of using a pdf over $\mathbf{x}$, work with a flow from $\mathbf{x}$ to $\mathbf{z}$</p>
</blockquote>
<h2 id="Flows-in-1D"><a href="#Flows-in-1D" class="headerlink" title="Flows in 1D"></a><strong>Flows</strong> in 1D</h2><p><b>Flow from $\mathbf{x}$ to $\mathbf{z}$ </b>: an invertible differentiable mapping $f$ from $\mathbf{x}$ (data) to $\mathbf{z}$ (noise): </p>
<script type="math/tex; mode=display">\mathbf{z}=f_\theta(\mathbf{x})</script><p><strong>Training</strong></p>
<ul>
<li>$\mathbf{x}$ to $\mathbf{z}$: transform the data distribution into a <strong>base distribution p(z)</strong><ul>
<li>common choices: uniform, std Gaussian</li>
</ul>
</li>
<li>Training the CDF is equivalent to the PDF: <script type="math/tex">f_\theta (\mathbf{x}) = \int_{-\infty}^\mathbf{x} p_\theta (t) dt</script></li>
</ul>
<p><strong>Ancestral sampling</strong> (a.k.a <em>forward sampling</em>):</p>
<ul>
<li>$\mathbf{z}$ to $\mathbf{x}$: mapping $\mathbf{z} \sim p(\mathbf{z})$ through the flow’s inverse will yield the data distribution $\mathbf{x}=f_\theta^{-1}(\mathbf{z})$</li>
</ul>
<p><img data-src="/notes/images/flow-models.png" alt=""></p>
<div class="note warning">
            <p><em>Change of variables</em>:</p><script type="math/tex; mode=display">\begin{align}z &= f_\theta(x) \\p_\theta(x) dx &= p(z) dz \\p_\theta(x) &= p \big(f_\theta(x)\big) \bigg|\frac{\partial f_\theta(x)}{\partial x}\bigg|\end{align}</script>
          </div>
<p><strong>Fitting flows</strong>:</p>
<ul>
<li>fit with MLE: <script type="math/tex">\arg\min_\theta \mathbb{E}_x [- \log p_\theta (x)]</script></li>
<li><script type="math/tex">p_\theta (x)</script> is the density of $x$ under the sampling process and is calculated using <em>Change of variables</em>.</li>
</ul>
<script type="math/tex; mode=display">\mathbf{z} \sim p(\mathbf{z}) \quad \mathbf{x} = f^{-1}_\theta (\mathbf{z})</script><p>CDF flows: </p>
<script type="math/tex; mode=display">
\begin{align}
P(\mathbf{x} \in [x_0, x_0+dx]) &=  P(\mathbf{z} \in [f(x_0), f(x_0+dx)]) \\
p(x)dx &= p(z) f^\prime(x)dx
\end{align}</script><ul>
<li>Recover the original objective with CDF flows:<script type="math/tex; mode=display">
\begin{align}
\mathbf{z} &=  \text{CDF}_\theta (x) \\
\log p(x) &= \log p(z) + \log \text{CDF}_\theta^\prime (x) \\
&= \underbrace{\log p(z)}_\text{predefined pdf} + \log \text{PDF}_\theta (x) 
\end{align}</script></li>
<li>Flows can be more general than CDFs.</li>
</ul>
<h2 id="Flows-in-high-dimensions"><a href="#Flows-in-high-dimensions" class="headerlink" title="Flows in high dimensions"></a>Flows in high dimensions</h2><p>For high-dimensional data, $x$ and $z$ have the same dimension.</p>
<p><img data-src="/notes/images/flow-high-dimension.png" alt=""></p>
<p><em>Change of variables</em>: For $z \sim p(z)$, ancestral sampling process f<sup>-1</sup> linearly transforms a small cube $dz$ to a small parallelepiped $dx$.</p>
<script type="math/tex; mode=display">p(x) = p(z) \frac{\text{vol}(dz)}{\text{vol}(dx)} = p(z) \bigg| \text{det} \frac{dz}{dx} \bigg|</script><p><strong>Intuition</strong>: $x$ is likely if it maps to a “large” region in $z$ space.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Train with MLE:</p>
<script type="math/tex; mode=display">
\begin{align}
\textit{change-of-variables formula} & \quad p_\theta(\mathbf{x}) = p\big(f_\theta(\mathbf{x})\big) \bigg| \text{det} \frac{\partial f_\theta(\mathbf{x})}{\partial \mathbf{x}} \bigg| \\
\text{training objective} & \quad \arg\min_\theta \mathbb{E}_\mathbf{x} [-\log p_\theta (\mathbf{x})] = \mathbb{E}_\mathbf{x} \bigg[ -\log p(f_\theta(\mathbf{x})) - \log \text{det} \bigg| \frac{\partial f_\theta(\mathbf{x})}{\partial \mathbf{x}} \bigg| \bigg]
\end{align}</script><p>where the Jacobian determinant must be easy to calculate and differentiate!</p>
<h3 id="Constructing-flows-composition"><a href="#Constructing-flows-composition" class="headerlink" title="Constructing flows: composition"></a>Constructing flows: composition</h3><ul>
<li>Fows<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rezende, D.J., & Mohamed, S. (2015). [Variational Inference with Normalizing Flows](https://arxiv.org/pdf/1505.05770.pdf). ArXiv, abs/1505.05770.
">[5]</span></a></sup> can be composed:<script type="math/tex; mode=display">
\begin{align}
x &\rightarrow f_1 \rightarrow f_2 \rightarrow \cdots \rightarrow f_k \rightarrow z \\
z &= f_k \circ \cdots \circ f_1(x) \\
x &= f_1^{-1} \circ \cdots \circ f_k^{-1}(z) \\
\log p_\theta(x) &= \log p_\theta(z) + \sum_{i=1}^k \log \bigg| \text{det} \frac{\partial f_i}{\partial f_{i-1}} \bigg|
\end{align}</script></li>
</ul>
<h3 id="Affine-flows-multivariate-Gaussian"><a href="#Affine-flows-multivariate-Gaussian" class="headerlink" title="Affine flows (multivariate Gaussian)"></a>Affine flows (multivariate Gaussian)</h3><p>Affine flows (multivariate Gaussian)</p>
<ul>
<li>Parameters: an invertible matrix $A$ and a vector $b$</li>
<li>$f(x) = A^{-1} (x-b)$</li>
</ul>
<p><strong>Sampling</strong>: $x= Az + b$, where $z \sim \mathcal{N}(0,1)$</p>
<p>Log likelihood is expensive when dimension is large:</p>
<ul>
<li>the Jacobian of $f$ is $A^{-1}$</li>
<li>Log likelihood involves calculating $\text{det}(A)$</li>
</ul>
<h3 id="Elementwise-flows"><a href="#Elementwise-flows" class="headerlink" title="Elementwise flows"></a>Elementwise flows</h3><script type="math/tex; mode=display">f_\theta((x_1, \cdots, x_d)) = (f_\theta(x_1), \cdots, f_\theta(x_d))</script><ul>
<li>flexible: it can use elementwise affine functions or CDF flows</li>
</ul>
<p>The Jacobian is diaganal, so the determinant is easy to evaluate:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} &= \text{diag}(f_\theta^\prime(x_1), \cdots, f^\prime_\theta(x_d)) \\
\text{det} \frac{\partial \mathbf{z}}{\partial \mathbf{x}} &= \prod_{i=1}^d f^\prime_\theta(x_i)
\end{align}</script><h3 id="More-flow-types"><a href="#More-flow-types" class="headerlink" title="More flow types"></a>More flow types</h3><p>Coupling layers (NICE<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dinh, L., Krueger, D., & Bengio, Y. (2014). [NICE: Non-linear Independent Components Estimation](https://arxiv.org/pdf/1410.8516.pdf). CoRR, abs/1410.8516.
">[1]</span></a></sup>/RealNVP<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2016). [Density estimation using Real NVP](https://arxiv.org/pdf/1605.08803.pdf). ArXiv, abs/1605.08803.
">[2]</span></a></sup>), directed graphical models, invertible 1x1 convs<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kingma, D.P., & Dhariwal, P. (2018). [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/pdf/1807.03039.pdf). NeurIPS.
">[9]</span></a></sup>, FFJORD</p>
<h1 id="NICE"><a href="#NICE" class="headerlink" title="NICE"></a>NICE</h1><div class="note info">
            <p><em>A good representation is one in which the distribution of the data is easy to model.</em></p>
          </div>
<p>Consider the problem of learning a pdf from a parametric family of densities <script type="math/tex">\{p_\theta, \theta \in \Theta \}</script> over finite dataset $\mathcal{D}$ of $N$ samples, of each in a space $\chi$, typically $\chi=\mathbb{R}^D$</p>
<p><strong>N</strong>on-linear <strong>I</strong>ndependent <strong>C</strong>omponent <strong>E</strong>stimation (NICE)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dinh, L., Krueger, D., & Bengio, Y. (2014). [NICE: Non-linear Independent Components Estimation](https://arxiv.org/pdf/1410.8516.pdf). CoRR, abs/1410.8516.
">[1]</span></a></sup> learns a non-linear  deterministic transformation $h=f(\mathbf{x})$ to map the input $\mathbf{x}$ into the latent space with a factorized distribution, i.e. independent latent variables <script type="math/tex">h_d</script>:</p>
<script type="math/tex; mode=display">
p_{H}(h) = \prod_d p_{H_d}(h_d)</script><p>Apply the <em>change of variables</em> $h=f(x)$, assuming that $f$ is invertible and differentiable, the dimension of $h$ and $x$ are the same in order to fit the distribution <script type="math/tex">p_H</script>:</p>
<script type="math/tex; mode=display">p_X(x) = p_H(f(x)) \bigg| \text{det} (\frac{\partial f(x)}{\partial x}) \bigg|</script><p>where <script type="math/tex">\frac{\partial f(x)}{\partial x}</script> is the Jocobian matrix of function $f$ at $x$.</p>
<script type="math/tex; mode=display">
\begin{align}
h &\sim p_H(h) \\
x &= f^{-1}(h)
\end{align}</script><p>where <script type="math/tex">p_H(h)</script>, the <em>prior distribution</em>, is a predefined PDF, e.g. isotropic Gaussian. If the prior distribution is factorial (i.e. with independent dimensions), then we have the deterministic transform of a factorial distribution:</p>
<script type="math/tex; mode=display">
\begin{align}
\log(p_X(x)) &= \log (p_H(f(x))) + \log (\bigg| \text{det}(\frac{\partial f(x)}{\partial x}) \bigg|)
& \text{MLE} \\
\log(p_X(x)) &= \color{red}{\sum_{d=1}^D \log \big(p_{H_d}(f_d(x)) \big)} + \log \big(\bigg| \text{det}(\frac{\partial f(x)}{\partial x}) \bigg|\big) & \text{NICE}
\end{align}</script><p>where <script type="math/tex">f(x) = (f_d(x))_{d\leq D}</script></p>
<div class="note info">
            <p><strong>NICE</strong> can be viewed as an <em>invertible preprocessing transform</em> of the dataset. </p><ul><li>Invertible preprocessings can increase likelihood arbitrarily simply by contracting the data. The factorized prior <script type="math/tex">p_H</script> encourages to discover meaningful structures in the dataset.</li></ul>
          </div>
<h2 id="Coupling-layer"><a href="#Coupling-layer" class="headerlink" title="Coupling layer"></a>Coupling layer</h2><p>Given $x \in \chi$, <script type="math/tex">I_1</script>, <script type="math/tex">I_2</script> are partitions of $[1,D]$ such that $d=|I_1|$ and $m$ is a function. Define <script type="math/tex">y = (y_1, y_2)</script> where:</p>
<script type="math/tex; mode=display">
\begin{align}
x_{I_1} &= x_{I_1} \\
y_{I_2} &= g(x_{I_2}; \color{red}{m}(x_{I_1}))
\end{align}</script><p>where g: <script type="math/tex">\mathbb{R}^{D-d} \times m(\mathbb{R}^d) \rightarrow \mathbb{R}^{D-d}</script> is the coupling law. If <script type="math/tex">I_1 = [1,d]</script> and <script type="math/tex">I_2 = [d,D]</script>, the Jacobian is:</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial x} =
\begin{bmatrix}
I_d & 0\\
\frac{\partial y_{I_2}}{\partial x_{I_1}} & \frac{\partial y_{I_2}}{\partial x_{I_2}}
\end{bmatrix}</script><p>where<script type="math/tex">I_d</script> is the identity matrix of size $d$, thus <script type="math/tex">\text{det} \frac{\partial y}{\partial x} = \text{det} \frac{\partial y_{I_2}}{\partial x_{I_2}}</script></p>
<p>The inverted mapping is:</p>
<script type="math/tex; mode=display">
\begin{align}
x_{I_1} &= y_{I_1} \\
x_{I_2} &= g^{-1}(y_{I_2}; m(y_{I_1}))
\end{align}</script><h3 id="Additive-coupling-layer"><a href="#Additive-coupling-layer" class="headerlink" title="Additive coupling layer"></a>Additive coupling layer</h3><p>NICE applies <script type="math/tex">g(a;b) = a+b</script> so that taking <script type="math/tex">a=x_{I_2}</script> and <script type="math/tex">b=m(y_{I_1})</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
y_{I_2} &= x_{I_2} + m(x_{I_1}) \\
x_{I_2} &= y_{I_2} - m(y_{I_1})
\end{align}</script><h1 id="RealNVP"><a href="#RealNVP" class="headerlink" title="RealNVP"></a>RealNVP</h1><p>Real-valued non-volume preserving (real NVP)</p>
<h2 id="Affine-coupling-layer"><a href="#Affine-coupling-layer" class="headerlink" title="Affine coupling layer"></a><strong>Affine coupling layer</strong></h2><ul>
<li>Given a $D$ dimensional input $x$ and $d&lt;D$, the output $y$ of an affine coupling layer follows:<script type="math/tex; mode=display">
\begin{align}
y_{1:d} &= x_{1:d} \\
y_{d+1:D} &= x_{d+1:D} \odot \exp(s(x_{1:d})) + t(x_{1:d})
\end{align}</script>where $s$ and $t$ represent <em>scale</em> and <em>translation</em> functions from $\mathbb{R}^{d} \rightarrow \mathbb{R}^{D-d}$.</li>
</ul>
<p>The Jocobian is:</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial x} =
\begin{bmatrix}
\mathbb{I}_d & 0\\
\frac{\partial y_{d+1:D}}{\partial x_{1:d}^\top} & \text{diag} (\exp[s(x_{1:d})])
\end{bmatrix}</script><p>The determinant can be computed as: <script type="math/tex">\exp \big[\sum_j s(x_{1:d})_j \big]</script></p>
<h2 id="Masked-convolution"><a href="#Masked-convolution" class="headerlink" title="Masked convolution"></a>Masked convolution</h2><p>Partitioning adopts the binary mask $b$:</p>
<script type="math/tex; mode=display">y = b \odot x + (1-b) \odot \big( x \odot \exp(\color{blue}{s}(b \odot x)) + \color{blue}{t}(b \odot x) \big)</script><p>where $s(\cdot)$ and $t(\cdot)$ are rectified CNNs.</p>
<p>Two partitioning are applied:</p>
<ol>
<li>spatial checkerboard patterns</li>
<li>channel-wise masking<br><img data-src="/notes/images/RealNVP-mask.png" alt="upload successful"></li>
</ol>
<h1 id="Autoregressive-flows"><a href="#Autoregressive-flows" class="headerlink" title="Autoregressive flows"></a>Autoregressive flows</h1><p>Construct flows from directed acyclic graphs.</p>
<p><strong>Autoregressive flows</strong>:</p>
<ul>
<li>The sampling process of a Bayes net is a flow:<ul>
<li>if autoregressive, called <em>autoregressive flow</em>:<script type="math/tex; mode=display">
\begin{align}
  \textbf{encoder} & \quad \textbf{decoder} \\
z_1 \sim f_\theta(x_1) &\quad x_1 = f_\theta^{-1}(z_1) \\
z_2 \sim f_\theta(x_2 \vert x_1) & \quad x_2 = f_\theta^{-1}(z_2;x_1) \\
z_3 \sim f_\theta(x_3 \vert x_1, x_2) & \quad x_3 = f_\theta^{-1}(z_3;x_1, x_2)
\end{align}</script></li>
</ul>
</li>
<li>Sampling is an <strong>invertible</strong> mapping from $z$ to $x$</li>
<li>The DAG structure causes the Jacobian to be <em>triangular</em>, when variables are ordered by topological sort.</li>
</ul>
<p><em>*How to fit autoregressive flow</em>?</p>
<ul>
<li>map $\mathbf{x}$ to $\mathbf{z}$</li>
<li>fully parallelizable</li>
</ul>
<script type="math/tex; mode=display">p_\theta(\mathbf{x}) = p(f_\theta(\mathbf{x})) \bigg| \text{det} \frac{\partial f_\theta(\mathbf{x})}{\partial \mathbf{x}} \bigg|</script><h2 id="Variational-lossy-autoencoder-VLAE"><a href="#Variational-lossy-autoencoder-VLAE" class="headerlink" title="Variational lossy autoencoder (VLAE)"></a>Variational lossy autoencoder (VLAE)</h2><p>Variational Lossy Autoencoder (VLAE)<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, X., Kingma, D.P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I., & Abbeel, P. (2016). [Variational Lossy Autoencoder](https://arxiv.org/pdf/1611.02731.pdf). ArXiv, abs/1611.02731.
">[6]</span></a></sup> force the global latent code to discard irrelevant information and thus encode the data in a lossy fassion.</p>
<p>For an <strong>autoregressive flow</strong> $f$, some continuous noise source $\epsilon$ is transformed into latent code $\mathbf{z}$: $\mathbf{z} = f(\epsilon)$. Assuming the density function for noise source is $u(\epsilon)$, then $\log p(\mathbf{z}) = \log u(\epsilon) + \log \text{det}\frac{d \epsilon}{d \mathbf{z}}$</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}(\mathbf{x}; \theta) &= \mathbb{E}_{\mathbf{z} \sim q(\mathbf{z} \vert \mathbf{x}) } [\log (\mathbf{x} \vert \mathbf{z}) + \log p(\mathbf{z}) -\log q(\mathbf{z} \vert \mathbf{x})] \\
&= \mathbb{E}_{\mathbb{z} \sim q(\mathbb{z} \vert \mathbb{x}), \epsilon= f^{-1}(\mathbb{z})} \bigg[ \log p(\mathbf{x} \vert f(\epsilon)) + \log u(\epsilon) + \log \text{det} \frac{d \epsilon}{d \mathbf{z}} - \log q(\mathbf{\mathbf{z} \vert \mathbf{x}}) \bigg] \\
&= \mathbb{E}_{\mathbb{z} \sim q(\mathbb{z} \vert \mathbb{x}), \epsilon=f^{-1}(\mathbb{z})} \bigg[ \log p(\mathbf{x} \vert f(\epsilon)) + \log u(\epsilon) + \underbrace{(\log q(\mathbf{z} \vert \mathbf{x}) -\log \text{det}\frac{d \epsilon}{d \mathbf{z}} )}_\text{IAF posterior}  \bigg]
\end{align}</script><h2 id="Masked-autoregressive-flow-MAF"><a href="#Masked-autoregressive-flow-MAF" class="headerlink" title="Masked autoregressive flow (MAF)"></a>Masked autoregressive flow (MAF)</h2><p>Considering an AR model whose conditionals are parameterized as single Gaussians, i.e., the $i$-th conditional is given by:</p>
<script type="math/tex; mode=display">p(x_i \vert \mathbf{x}_{1: i-1}) = \mathcal{N}(x_i \vert \mu_i, (\exp \alpha_i)^2)</script><p>where <script type="math/tex">f_{\mu_i}</script> and <script type="math/tex">f_{\alpha_i}</script> are unconstrained scalar functions that compute the mean and log standard deviation of the $i$-th conditional given all previous variables</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_i = f_{\mu_i}(\mathbf{x}_{1：i-1}) & \quad\text{mean}\\
\alpha = f_{\alpha_i} (\mathbf{x}_{1:i-1}) & \quad \log \sigma_i
\end{align}</script><p>Thus the data can be generated by:</p>
<script type="math/tex; mode=display">x_i = \mu_i \exp \alpha_i + \mu_i</script><script type="math/tex; mode=display">
\begin{align}
\mu_i &= f_{\mu_i}(\mathbf{x}_{1：i-1}) & \quad\text{mean of $i$-th data point}\\
\alpha &= f_{\alpha_i} (\mathbf{x}_{1:i-1}) & \quad \log \sigma_i \\
u_i &\sim \mathcal{N}(0,1)
\end{align}</script><ul>
<li>The vector of random numbers (with <code>randn()</code>) <script type="math/tex">\mathbf{\mu} = (\mu_1, \mu_2, \cdots, \mu_I)</script> is used to generate data.</li>
</ul>
<p>Given data point $\mathbf{x}$, the random number vector $\mathbf{u}$ is:</p>
<script type="math/tex; mode=display">u_i = (x_i - \mu_i) \exp(- \alpha_i)</script><ul>
<li>where <script type="math/tex">\mu_i = f_{\mu_i}(\mathbf{x}_{1：i-1})</script> and <script type="math/tex">\alpha = f_{\alpha_i} (\mathbf{x}_{1:i-1})</script>.</li>
</ul>
<p>Due to the AR structure, the Jacobian of $f^{-1}$ is triangular by design, hence:</p>
<script type="math/tex; mode=display">\bigg| \text{det} \bigg( \frac{\partial f^{-1}}{\partial \mathbf{x}} \bigg) = \exp \bigg( -\sum_I \alpha_i \bigg) \bigg|</script><ul>
<li>where <script type="math/tex">\alpha_i = f_{\alpha_i} (\mathbf{x}_{1:i-1})</script>.</li>
<li>This can be equivalently interpreted as a normalizing flow.</li>
</ul>
<p><strong>Masking</strong></p>
<ul>
<li>The functions <script type="math/tex">f_{\mu_i}, f_{\alpha_i}</script> are with masking, like MADE.</li>
</ul>
<p>It is derived that MAF and IAF has the theoretical equivalence (See <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Papamakarios, G., Murray, I., & Pavlakou, T. (2017). [Masked Autoregressive Flow for Density Estimation](https://arxiv.org/pdf/1705.07057.pdf). NIPS.
">[7]</span></a></sup>).</p>
<h1 id="Inverse-autoregressive-flow-IAF"><a href="#Inverse-autoregressive-flow-IAF" class="headerlink" title="Inverse autoregressive flow (IAF)"></a>Inverse autoregressive flow (IAF)</h1><p>Inverse autoregressive flow (IAF)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kingma, D.P., Salimans, T., & Welling, M. (2017). [Improved Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/pdf/1606.04934.pdf). ArXiv, abs/1606.04934.
">[8]</span></a></sup> is the inverse of an AR flow:</p>
<ul>
<li>$\mathbf{x} \rightarrow \mathbf{z}$ has the same structure as the <strong>sampling</strong> in an AR model</li>
<li><script type="math/tex">\mathbf{z} \rightarrow \mathbf{x}</script> has the same structure as <strong>loglikelihood</strong> computation of an AR model. Thus, <strong>IAF sampling is fast</strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
    \textbf{encoder} & \quad \textbf{decoder} \\
    z_1 \sim f_\theta^{-1}(x_1) &\quad x_1 = f_\theta(z_1) \\
    z_2 \sim f_\theta^{-1}(x_2 \vert z_1) & \quad x_2 = f_\theta(z_2;z_1) \\
    z_3 \sim f_\theta^{-1}(x_3 \vert z_1, z_2) & \quad x_3 = f_\theta(z_3;z_1, z_2)
\end{align}</script><p><img data-src="/notes/images/IAF.png" alt="upload successful"></p>
<p>IAF approximates the posterior $q(\mathbf{z} \vert \mathbf{x})$ with the chain.  At $t$-th step of the flow, IAFapply the AR NN with inputs $\mathbf{z}_{t-1}$ and $\mathbf{h}$, and outputs <script type="math/tex">\mathbf{\mu}_t</script> and <script type="math/tex">\sigma_t</script>. (See the figure)</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{z}_0 &= \mathbf{\mu}_0 \odot \epsilon & \text{initialize at }t=0\\
\mathbf{z}_t &= \mathbf{\mu}_t + \sigma_t \odot \mathbf{z}_{t-1} & \text{at } t=\{1,\cdots,T\}
\end{align}</script><p>The density at $T$-th step:</p>
<script type="math/tex; mode=display">
\log q(\mathbf{z}_T \vert \mathbf{x}) = -\sum_{i=1}^D \bigg( \frac{1}{2}\epsilon_i^2 + \frac{1}{2}\log(2\pi) + \sum_{t=0}^T \log \sigma_{t,i} \bigg)</script><p>The output of autogressive NN:</p>
<script type="math/tex; mode=display">[\mathbf{m}, \mathbf{s}] \rightarrow \text{AutoregressiveNN}[t](\mathbf{z,h;\theta})</script><ul>
<li>where <script type="math/tex; mode=display">
\begin{align}
\mathbf{z} &\leftarrow \sigma \odot \mathbf{z} + (1-\sigma) \odot \mathbf{m} \\
\sigma &\leftarrow \text{sigmoid}(\mathbf{s})
\end{align}</script></li>
</ul>
<h2 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><p>Given:</p>
<ul>
<li>$\mathbf{x}$: a datapoint</li>
<li>$\mathbf{\theta}$: NN parameters</li>
<li>$\textrm{EncoderNN}(\mathbf{x}; \mathbf{\theta})$: encoder network, with output $\mathbf{h}$</li>
<li>$\text{AutoregressiveNN}[*](\mathbf{z,h;\theta})$: autoregressive networks, with input $\mathbf{h}$</li>
<li>let $l$ denote the scalar value of $\log q(\mathbf{z} \vert \mathbf{x})$, evaluated at sample $\mathbf{z}$</li>
</ul>
<p><strong>IAF algorithm</strong>:</p>
<ol>
<li>$[\mathbf{\mu},\sigma, \mathbf{h}] \leftarrow$ EncoderNN$(\mathbf{x; \theta})$</li>
<li>$\mathbf{\epsilon} \sim \mathcal{N}(0, I)$</li>
<li>$\mathbf{z} \leftarrow \sigma \odot \epsilon + \mu$</li>
<li>$l \leftarrow -\text{ sum}(\log \sigma + \frac{1}{2}\epsilon^2) + \frac{1}{2}\log(2\pi)$</li>
<li>for $t = {1,\cdots, T}$:<ol>
<li>$[\mathbf{m}, \mathbf{s}] \rightarrow \text{AutoregressiveNN}[t](\mathbf{z,h;\theta})$</li>
<li>$\sigma \leftarrow \text{sigmoid}(\mathbf{s})$</li>
<li>$\mathbf{z} \leftarrow \sigma \odot \mathbf{z} + (1-\sigma) \odot \mathbf{m}$</li>
<li>$l \leftarrow -\text{sum}(\log \sigma) + l$</li>
</ol>
</li>
</ol>
<h2 id="AF-vs-IAF"><a href="#AF-vs-IAF" class="headerlink" title="AF vs IAF"></a>AF vs IAF</h2><p><strong>AF</strong>:</p>
<ul>
<li>Fast evaluation of $p(x)$ for arbitrary $x$</li>
<li>Slow sampling</li>
</ul>
<p><strong>IAF</strong>:</p>
<ul>
<li>Slow evaluation of $p(x)$ for arbitrary $x$, so training directly by MLE is slow</li>
<li>Fast sampling</li>
<li>Fast evaluation of $p(x)$ if $x$ is a sample</li>
</ul>
<p>Parallel WaveNet, IAF-VAE exploit IAF’s fast sampling.</p>
<h1 id="Glow"><a href="#Glow" class="headerlink" title="Glow"></a>Glow</h1><p>Glow<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kingma, D.P., & Dhariwal, P. (2018). [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/pdf/1807.03039.pdf). NeurIPS.
">[9]</span></a></sup></p>
<ul>
<li>Activation norm</li>
<li>Invertible 1x1 convolution</li>
<li>Affine coupling layers</li>
</ul>
<p><img data-src="/notes/images/Glow.png" alt="upload successful"></p>
<h1 id="Flow"><a href="#Flow" class="headerlink" title="Flow++"></a>Flow++</h1><p>Flow++<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ho, J., Chen, X., Srinivas, A., Duan, Y., & Abbeel, P. (2019). [Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design](https://arxiv.org/pdf/1902.00275.pdf). ICML.
">[10]</span></a></sup></p>
<ul>
<li>Dequantization via VI</li>
<li>Improved coupling layers<ul>
<li>Apply CDF for a mixture of $K$ logistics</li>
<li>stacking conv and multi-head self-attention</li>
</ul>
</li>
</ul>
<!--
todo:
1. 
-->
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dinh, L., Krueger, D., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/pdf/1410.8516.pdf">NICE: Non-linear Independent Components Estimation</a>. CoRR, abs/1410.8516.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2016). <a href="https://arxiv.org/pdf/1605.08803.pdf">Density estimation using Real NVP</a>. ArXiv, abs/1605.08803.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://drive.google.com/file/d/1S9reenpExqKzkGz7kZ8JBbAJcJt1E7dP/view">CS294-158 Lecture 2c+3a slides</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ermongroup.github.io/cs228-notes/inference/sampling/">CS228 notes: sampling methods</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rezende, D.J., &amp; Mohamed, S. (2015). <a href="https://arxiv.org/pdf/1505.05770.pdf">Variational Inference with Normalizing Flows</a>. ArXiv, abs/1505.05770.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, X., Kingma, D.P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2016). <a href="https://arxiv.org/pdf/1611.02731.pdf">Variational Lossy Autoencoder</a>. ArXiv, abs/1611.02731.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Papamakarios, G., Murray, I., &amp; Pavlakou, T. (2017). <a href="https://arxiv.org/pdf/1705.07057.pdf">Masked Autoregressive Flow for Density Estimation</a>. NIPS.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kingma, D.P., Salimans, T., &amp; Welling, M. (2017). <a href="https://arxiv.org/pdf/1606.04934.pdf">Improved Variational Inference with Inverse Autoregressive Flow</a>. ArXiv, abs/1606.04934.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kingma, D.P., &amp; Dhariwal, P. (2018). <a href="https://arxiv.org/pdf/1807.03039.pdf">Glow: Generative Flow with Invertible 1x1 Convolutions</a>. NeurIPS.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ho, J., Chen, X., Srinivas, A., Duan, Y., &amp; Abbeel, P. (2019). <a href="https://arxiv.org/pdf/1902.00275.pdf">Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design</a>. ICML.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Müller, T., McWilliams, B., Rousselle, F., Gross, M., &amp; Novák, J. (2018). <a href="http://dl.acm.org/citation.cfm?id=3341156">Neural Importance Sampling</a>. ACM Trans. Graph., 38, 145:1-145:19.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Grathwohl, W., Chen, R.T., Bettencourt, J., Sutskever, I., &amp; Duvenaud, D. (2018). <a href="https://arxiv.org/pdf/1810.01367.pdf">FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models</a>. ArXiv, abs/1810.01367.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Huang, C., Krueger, D., Lacoste, A., &amp; Courville, A.C. (2018). <a href="https://arxiv.org/pdf/1804.00779.pdf">Neural Autoregressive Flows</a>. ArXiv, abs/1804.00779.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oord, A.V., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G.V., Lockhart, E., Cobo, L.C., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., &amp; Hassabis, D. (2017). <a href="https://arxiv.org/pdf/1711.10433.pdf">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</a>. ArXiv, abs/1711.10433.<a href="#fnref:14" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Unsupervised learning</category>
        <category>Likelihood-based models</category>
        <category>Flow models</category>
      </categories>
      <tags>
        <tag>Unsupervised learning</tag>
        <tag>Likelihood-based models</tag>
        <tag>Flow models</tag>
      </tags>
  </entry>
  <entry>
    <title>Variational Autoencoders</title>
    <url>/notes/2020/01/09/Generative/Variational-AutoEncoders/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>This is a concise introduction of Variational Autoencoder (VAE).<br><span id="more"></span></p>
<p><img data-src="/notes/images/taxonomy-of-generative-models.png" alt="Taxonomy of generative models"></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li><p>PixelCNN define tractable density function with MLE:</p>
<script type="math/tex; mode=display">p(\theta) = \prod_{i=1}^n p_\theta (x_i \vert x_1, \cdots, x_{i-1})</script></li>
<li><p>VAE define the intractable density function with latent $\mathbf{z}$:</p>
<script type="math/tex; mode=display">p(\theta) = \color{red}{\int} p_\theta (z) p_\theta (x \vert z) dz</script></li>
</ul>
<p>This cannot directly optimize, VAEs derive and optimize the <em>lower bound</em> on likelihood instead.</p>
<h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><p>Autoencoder (AE) encodes the inputs into latent representations $\mathbf{z}$ with dimension reduction to capture meaningful factors of variation in data. Then employ $\mathbf{z}$ to reconstruct original data by autoencoding itself.</p>
<p><img data-src="/notes/images/Autoencoder.png" width="60%" /></p>
<ul>
<li>After training, throw away the decoder and <strong>only retain the encoder</strong>. </li>
<li>Encoder can be used to initialize the supervised model on downstream tasks.</li>
</ul>
<h1 id="Variational-Autoencoder"><a href="#Variational-Autoencoder" class="headerlink" title="Variational Autoencoder"></a>Variational Autoencoder</h1><p>Assume training data <script type="math/tex">\{ x^{(i)}\}_{i=1}^N</script> is generated from underlying unobserved (latent) representation $\mathbf{z}$.</p>
<div class="note info">
            <p><strong>Intuition</strong>: </p><ul><li>$\mathbf{x}$ -&gt; image</li><li>$\mathbf{z}$ -&gt; latent factors used to generate $\mathbf{x}$: attributes, orientation, pose, how much smile, <em>etc</em>. Choose prior $p(z)$ to be simple, e.g. Gaussian. </li></ul>
          </div>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p><strong><span style="color:red;"> Intractable integral </span></strong> to MLE of training data:</p>
<script type="math/tex; mode=display">p(\theta) = \color{red}{\int} \color{green}{\overbrace{p_\theta (z)}^{\checkmark\text{Gaussian prior}}} \color{green}{\underbrace{p_\theta (x \vert z)}_{\checkmark\text{decoder NN}}} dz</script><p>where it is <em>intractable</em> to compute $p(x \vert z)$ for every $z$, i.e. integral. The intractability is marked in red.</p>
<p>Thus, the posterior density is also intractable due to the intractable data likelihood:</p>
<script type="math/tex; mode=display">p_\theta (z \vert x) = \frac{ \color{green}{p_\theta (x \vert z) p_\theta (z)}}{ \color{red}{p_\theta (x)}}</script><p><img data-src="/notes/images/VAE-decoder.png" width="40%" /></p>
<center> VAE Decoder<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Doersch, C. (2016). [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf). ArXiv, abs/1606.05908.
">[5]</span></a></sup></center>

<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p><strong>Encoder</strong> -&gt; “recognition / inference” networks. </p>
<ul>
<li>Define encoder network <script type="math/tex">q_\phi (z \vert x)</script> that approximates the intractable true posterior <script type="math/tex">p_\theta(z \vert x)</script>. VAE makes the variational approximate posterior be a multivariate Gaussian with diagonal covariance for data point $\mathbf{x}^{(i)}$:<script type="math/tex; mode=display">
\log q_\phi (\mathbf{z} \vert \mathbf{x}^{(i)}) = \log \mathcal{N} (\mathbf{z}; \mathbf{\mu}^{(i)}, \mathbf{\sigma}^{2(i)}\mathbf{I})</script></li>
</ul>
<p>where</p>
<ul>
<li>For Gaussian MLP encoder or decoder<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kingma, D. P., & Welling, M. (2013). [Auto-encoding variational bayes](https://arxiv.org/pdf/1312.6114.pdf). arXiv preprint arXiv:1312.6114.
">[4]</span></a></sup>,<script type="math/tex; mode=display">
\begin{align}
\mu &= \mathbf{W_4 h + b_4} \\
\log \sigma^2 &= \mathbf{W_5 h + b_5} \\
h &= \tanh (\mathbf{W_3 z + b_3})
\end{align}</script></li>
</ul>
<div class="note info">
            <p>Use NN to model $\log \sigma^2$ instead of $\sigma^2$ is because that $\log \sigma^2 \in (-\infty, \infty)$ whereas $\sigma^2 \geq 0$</p>
          </div>
<p><strong>Decoder</strong> -&gt; “generation” networks <script type="math/tex">p_\theta (x \vert z)</script></p>
<p><img data-src="/notes/images/VAE-networks.png" alt="upload successful"></p>
<script type="math/tex; mode=display">
\begin{align}
\log p_\theta (x^{(i)}) &= \mathbb{E}_{z \sim q_\phi (z \vert x^{(i)})} \bigg[ \log p_\theta (x^{(i)}) \bigg] & p_\theta (x^{(i)}) \text{ does not depend on }z \\
&= \mathbb{E}_z \bigg[ \log \frac{p_\theta (x^{(i)} \vert z) p_\theta (z)}{p_\theta (z \vert x^{(i)})} \bigg] & \text{Bayes rule} \\
&= \mathbb{E}_z \bigg[ \log \frac{p_\theta(x^{(i)} \vert z) p_\theta (z)}{p_\theta (z \vert x^{(i)})} \frac{q_\phi (z \vert x^{(i)})}{q_\phi (z \vert x^{(i)})} \bigg] & \text{multiply by constant} \\
&= \mathbb{E}_z \bigg[ \log p_\theta (x^{(i)} \vert z)\bigg] - \mathbb{E}_z \bigg[\log \frac{q_\phi (z \vert x^{(i)})}{p_\theta (z)} \bigg] + \mathbb{E}_z \bigg[ \log \frac{q_\phi (z \vert x^{(i)})}{p_\theta (z \vert x^{(i)})} \bigg] & \text{logarithms} \\
&= \underbrace{ \mathbb{E}_z \bigg[ \log \color{green}{\overbrace{p_\theta (x^{(i)} \vert z)}^\text{decoder}} \bigg] - \mathbb{KL} \big( \color{blue}{ \overbrace{q_\phi (z \vert x^{(i)})}^\text{encoder}} \| \color{blue}{\overbrace{p_\theta (z)}^{z\,\text{prior}} } \big) }_{\mathcal{L}(x^{(i)}, \theta, \phi)} + \underbrace{\mathbb{KL} \big(  q_\phi(z \vert x^{(i)}) \| \color{red}{ \overbrace{p_\theta (z \vert x^{(i)})}^{\text{intactable!}} } \big)}_{\geq 0}
\end{align}</script><ul>
<li>The first RHS term represents <strong>tractable lower bound</strong> <script type="math/tex">\mathcal{L}(x^{(i)}, \theta, \phi)</script>, wherein <script type="math/tex">p_\theta (x \vert z)</script> and $\mathbb{KL}$ terms are differentiable.</li>
<li>Thus, the variational lower bound (ELBO) is derived：<script type="math/tex; mode=display">\log p_\theta (x^{(i)}) \geq \mathcal{L}(x^{(i)}, \theta, \phi)</script></li>
<li>Training: maximize lower bound<script type="math/tex; mode=display">\theta^*, \phi^* = \arg\max_{\theta, \phi} \sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)</script></li>
</ul>
<script type="math/tex; mode=display">
\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_z \bigg[ \log \color{green}{\overbrace{p_\theta (x^{(i)} \vert z)}^\text{decoder}} \bigg] - \mathbb{KL} \big( \color{blue}{ \overbrace{q_\phi (z \vert x^{(i)})}^\text{encoder}} \| \color{blue}{\overbrace{p_\theta (z)}^{z\,\text{prior}} } \big)</script><p>where</p>
<ul>
<li>the fist term <script type="math/tex">\mathbb{E}_z \bigg[ \log \color{green}{p_\theta (x^{(i)} \vert z)} \bigg]</script>: reconstruct the input data. It is a <em>negative reconstruction error</em>.</li>
<li>the second term <script type="math/tex">\mathbb{KL} \big( \color{blue}{q_\phi (z \vert x^{(i)})} \| \color{blue}{p_\theta (z)} \big)</script> make approximate posterior distribution close to the prior. It acts as a regularizer.</li>
</ul>
<p>The derived estimator when using isotropic multivariate Gaussian <script type="math/tex">p_\theta(\mathbf{z})=\mathcal{N}(\mathbf{z}; \mathbf{0,I})</script>:</p>
<script type="math/tex; mode=display">
\mathcal{L}(\theta, \phi;x^{(i)}) \simeq \frac{1}{2} \sum_{j=1}^D\bigg( 1+ \log((\sigma^{(i)}_j)^2) - (\mu^{(i)}_j)^2 - (\sigma_j^{(i)})^2 \bigg) + \frac{1}{L} \sum_{l=1}^L \log p_\theta (\mathbf{x}^{(i)} \vert \mathbf{z}^{(i,l)})</script><p>where <script type="math/tex">\mathbf{z}^{(i,l)} = \mu^{i} + \sigma^{(i)} \odot \epsilon^{(l)}</script> and <script type="math/tex">\epsilon^{(l)} \sim \mathcal{N}(0, \mathbf{I})</script>, <script type="math/tex">\mu_j</script> and <script type="math/tex">\sigma_j</script> denote the $j$-th element of mean and variance vectors.</p>
<p><img data-src="/notes/images/VAE-training.png" alt="upload successful"></p>
<h3 id="Reparameterization-trick"><a href="#Reparameterization-trick" class="headerlink" title="Reparameterization trick"></a>Reparameterization trick</h3><p>Given the deterministic mapping <script type="math/tex">\mathbf{z}= g_\phi (\epsilon, x)</script>, we know that </p>
<script type="math/tex; mode=display">q_\phi (\mathbf{z} \vert \mathbf{x})\prod_i dz_i = p(\mathbf{\epsilon})\prod_i d\epsilon_i</script><p>Thus, </p>
<script type="math/tex; mode=display">
\begin{align}
\int q_\phi (\mathbf{z \vert x}) f(\mathbf{z})d\mathbf{z} &= \int p(\mathbf{\epsilon}) f(g_\phi(\mathbf{\epsilon}, \mathbf{x})) d\mathbf{\epsilon} & \\
& \simeq \frac{1}{L} \sum_{l=1}^L f(g_\phi (\mathbf{x}, \epsilon^{(l)})) & \text{where }\epsilon^{(l)}\sim p(\epsilon)
\end{align}</script><p>Take the univariate Gaussian case for example: $z \sim p(z \vert x) = \mathcal{N}(\mu, \sigma^2)$, the valid reparameterization is: $z = \mu + \sigma \epsilon$, where the auxiliary noise variable $\epsilon \sim \mathcal{N}(0,1)$.<br>Thus,</p>
<script type="math/tex; mode=display">\begin{align}
\mathbb{E}_{\mathcal{N}(z;\mu,\sigma^2)} [f(z)] &= \mathbb{E}_{\mathcal{N}}[f(\mu+ \sigma \epsilon)] & \\
\end{align}</script><p><img data-src="/notes/images/VAE-reparameterization-trick.png" width="30%"/></p>
<h2 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h2><ul>
<li>After training, remove the encoder network, and use decoder network to generate. </li>
<li><strong>Sample $z$ from prior</strong> as the input!</li>
<li>Diagonal prior on $z$ -&gt; independent latent variables!</li>
</ul>
<p><img data-src="/notes/images/VAE-generation.png" alt="upload successful"></p>
<ul>
<li><em>Different dimensions of $z$ encode interpretable factors of variation</em>.</li>
<li>Good feature representation that can be computed using <script type="math/tex">q_\phi (z \vert x)</script><br><img data-src="/notes/images/VAE-generated-samples.svg" width="70%" /></li>
</ul>
<h2 id="Pros-amp-cons"><a href="#Pros-amp-cons" class="headerlink" title="Pros &amp; cons"></a>Pros &amp; cons</h2><ul>
<li>Probabilistic spin to traditional autoencoders =&gt; allows generating data</li>
<li>Defines an intractable density =&gt; derive and optimize a (variational) lower bound</li>
</ul>
<p><strong>Pros</strong>:</p>
<ul>
<li>Principles approach to generative models</li>
<li>Allows inference of $q(z \vert x)$, can be useful feature representation for downstream tasks</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li>Maximizes lower bound of likelihood: okay, but not as good evalution as <em>PixelRNN / PixelCNN</em>！</li>
<li>loert quality compared to the sota (GANs)</li>
</ul>
<h1 id="Variational-Graph-Auto-Encoder-VGAE"><a href="#Variational-Graph-Auto-Encoder-VGAE" class="headerlink" title="Variational Graph Auto-Encoder (VGAE)"></a>Variational Graph Auto-Encoder (VGAE)</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Given an undirected, unweighted graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with $N=|V|$ nodes, the ajacency matrix $\mathbf{A}$ with self-connection (i.e., the diagonal is set to 1), degree matrix $\mathbf{D}$, stochastic latent variable <script type="math/tex">\mathbf{z}_i</script> in matrix $\mathbf{Z} \in \mathbb{R}^{N \times F}$, node features $\mathbf{X} \in \mathbb{R}^{N \times D}$. <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kipf, T., & Welling, M. (2016). [Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308). ArXiv, abs/1611.07308.">[7]</span></a></sup></p>
<h2 id="Inference-model"><a href="#Inference-model" class="headerlink" title="Inference model"></a>Inference model</h2><p>Apply a 2-layer Graph Convolutional Networks (GCN) to for parameterization:</p>
<script type="math/tex; mode=display">
\begin{align}
q(\mathbf{Z} \vert \mathbf{X}, \mathbf{A}) &= \prod_{i=1}^N q(\mathbf{z}_i \vert \mathbf{X}, \mathbf{A}) \\
q(\mathbf{z}_i \vert \mathbf{X}, \mathbf{A}) &= \mathcal{N}(\mathbf{z}_i \vert \mathbf{\mu}_i, \text{diag}(\mathbf{\sigma}_i^2))
\end{align}</script><p>where </p>
<ul>
<li>Mean: <script type="math/tex">\mu = \text{GCN}_\mu (\mathbf{X}, \mathbf{A})</script></li>
<li>Variance: <script type="math/tex">\log \sigma = \text{GCN}_\sigma (\mathbf{X}, \mathbf{A})</script></li>
</ul>
<p>The two-layer GCN is defined as <script type="math/tex">\text{GCN}(\mathbf{X}, \mathbf{A}) = \tilde{\mathbf{A}} \text{ReLU}(\tilde{\mathbf{A}}\mathbf{X}\mathbf{W}_0)\mathbf{W}_1</script><br>where <script type="math/tex">\tilde{\mathbf{A}} = \mathbf{D}^{-1/2} \mathbf{A}\mathbf{D}^{-1/2}</script> is the semmetrically normalized adjacency matrix.</p>
<h2 id="Generative-model"><a href="#Generative-model" class="headerlink" title="Generative model"></a>Generative model</h2><p>The generative model applies an inner product between latent variables:</p>
<script type="math/tex; mode=display">
\begin{align}
p(\mathbf{A}\vert \mathbf{Z}) &= \prod_{i=1}^N \prod_{j=1}^N p(A_{ij} \vert \mathbf{z}_i, \mathbf{z}_j) \\
p(A_{ij}=1 \vert \mathbf{z}_i, \mathbf{z}_j)) &= \sigma(\mathbf{z}_i^\top \mathbf{z}_j)
\end{align}</script><p>where <script type="math/tex">A_{ij}</script> are elements of ajacency matrix $\mathbf{A}$ and $\sigma(\cdot)$ represents the sigmoid function.</p>
<h2 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h2><p>Optimize the variational lower bound (ELBO) <script type="math/tex">\mathcal{L}</script> w.r.t the variational parameters <script type="math/tex">\mathbf{W}_i</script>:</p>
<script type="math/tex; mode=display">
\mathcal{L} = \mathbb{E}_{q(\mathbf{Z} \vert \mathbf{X},\mathbf{A})} \big[\log p(\mathbf{A} \vert \mathbf{Z})\big] - \mathbb{KL}\big[q(\mathbf{Z} \vert \mathbf{X}, \mathbf{A}) \Vert p(\mathbf{Z})\big]</script><p>where the Gaussian prior <script type="math/tex">p(\mathbf{Z}) = \prod_I p(\mathbf{z}_i) = \prod_i \mathcal{N}(\mathbf{z}_i \vert 0, \mathbf{I})</script></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf">Stanford cs231n: Generative models</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.deeplearningbook.org/contents/generative_models.html">I. Goodfellow et. al, Deep Learning</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Goodfellow, I. (2016). <a href="https://arxiv.org/abs/1701.00160">Tutorial: Generative adversarial networks. In NIPS.</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kingma, D. P., &amp; Welling, M. (2013). <a href="https://arxiv.org/pdf/1312.6114.pdf">Auto-encoding variational bayes</a>. arXiv preprint arXiv:1312.6114.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Doersch, C. (2016). <a href="https://arxiv.org/pdf/1606.05908.pdf">Tutorial on Variational Autoencoders</a>. ArXiv, abs/1606.05908.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepgenerativemodels.github.io/notes/vae/">cs236 VAE notes</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kipf, T., &amp; Welling, M. (2016). <a href="https://arxiv.org/abs/1611.07308">Variational Graph Auto-Encoders</a>. ArXiv, abs/1611.07308.<a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Unsupervised learning</category>
        <category>VAE</category>
      </categories>
      <tags>
        <tag>Unsupervised learning</tag>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>Clustering Methods: A Note</title>
    <url>/notes/2020/03/20/ML/Clustering-methods/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Notes of clustering approaches.<br><span id="more"></span></p>
<h1 id="Gaussian-Mixture-Models-GMMs"><a href="#Gaussian-Mixture-Models-GMMs" class="headerlink" title="Gaussian Mixture Models (GMMs)"></a>Gaussian Mixture Models (GMMs)</h1><h2 id="EM-algorithm"><a href="#EM-algorithm" class="headerlink" title="EM algorithm"></a>EM algorithm</h2><p>Given the fully observed variable <script type="math/tex">\mathbf{x}_i</script> of which $i$ indicates the $i$-th variable, <script type="math/tex">\mathbf{z}_i</script> be the hidden or missing variables. The objective is to maximize the log likelihood of observed data:</p>
<script type="math/tex; mode=display">
\mathcal{l}(\theta) = \sum_{i=1}^N \log p(\mathbf{x}_i \vert \mathbf{\theta}) = \sum_{i=1}^N \color{red}{\log} \big[ \sum_{\mathbf{z}_i} p(\mathbf{x}_i, \mathbf{z}_i  \vert \theta) \big]</script><p>It is hard to direct optimize due to the $\log$ cannot be merged into the inner sum.</p>
<p>EM define the <strong>complete data log likelihood</strong> as:</p>
<script type="math/tex; mode=display">
\mathcal{l}_c (\theta) \triangleq \sum_{i=1}^N \log  p(\mathbf{x}_i, \color{red}{\mathbf{z}_i}  \vert \theta)</script><p>This also cannot be computed since <script type="math/tex">\mathbf{z}_i</script> is unknown.</p>
<p>Then EM defines the <strong>expected complete data log likelihood</strong> as:</p>
<script type="math/tex; mode=display">
Q(\theta, \theta^{t-1}) = \mathbb{E}[ \mathcal{l}_c (\theta) \vert \mathcal{D}, \theta^{t-1} ]</script><p>where $t$ is the current iteratio number, $Q$ is the <strong>auxiliary function</strong>. The expectation is take w.r.t the old parameters $\theta^{t-1}$ and the observed data $\mathcal{D}$. </p>
<p>In the <strong>E-step</strong>, compute the <strong>expected sufficient statistics</strong> (ESS) <script type="math/tex">Q(\theta, \theta^{t-1})</script></p>
<p>In the <strong>M-step</strong>, the Q function is optimized w.r.t $\theta$:</p>
<script type="math/tex; mode=display">
\theta^t = \arg \max_\theta Q(\theta, \theta^{t-1})</script><p>To perform MAP estimation, the M step is modified as:</p>
<script type="math/tex; mode=display">
\theta^t = \arg\max_\theta Q(\theta, \theta^{t-1}) + \log p(\theta)</script><div class="note info">
            <ul><li>EM algorithm monotonically increase the log likelihood of the observed data (plus the log prior when doing MAP)</li></ul>
          </div>
<h2 id="EM-for-GMMs"><a href="#EM-for-GMMs" class="headerlink" title="EM for GMMs"></a>EM for GMMs</h2><p>Let $K$ be the # of mixture components.</p>
<h3 id="Auxiliary-function"><a href="#Auxiliary-function" class="headerlink" title="Auxiliary function"></a>Auxiliary function</h3><p>the expected complete data log likelihood is:</p>
<script type="math/tex; mode=display">
\begin{align}
Q(\theta, \theta^{t-1}) & \triangleq \mathbb{E} \big[ \sum_i p(\mathbf{x}_i, z_i \vert \theta) \big] \\
& = \sum_i \mathbb{E} \big[ \log \big[ \prod_{k=1}^K (\pi_k p(\mathbf{x}_i \vert \theta_k))^{\mathbb{I}(z_i=k)} \big] \big] \\
&= \sum_i \sum_k \mathbb{E} [ \mathbb{I}(z_i =k) \log [\pi_k p(\mathbf{x}_i \vert \theta_k)]] \\
&=\sum_i \sum_k p(z_i=k \vert \mathbf{x}_i, \theta^{t-1}) \log[\pi_k p(\mathbf{x}_i \vert \theta_k)] \\
&=\sum_i\sum_k r_{ik} \log \pi_k + \sum_i \sum_k r_{ik} \log p(\mathbf{x}_i \vert \theta_k)
\end{align}</script><p>where <script type="math/tex">r_{ik} \triangleq p(z_i \ k \vert \mathbf{x}_i, \theta^{t-1})</script> is the responsibility that cluster $k$ takes for data point $i$, which is computed at E-step.</p>
<h3 id="E-step"><a href="#E-step" class="headerlink" title="E step"></a>E step</h3><script type="math/tex; mode=display">
r_{ik} = \frac{\pi_k p(\mathbf{x}_i \vert \theta_k^{(t-1)})}{\sum_{k^\prime} \pi_{k^\prime} p(\mathbf{x}_i \vert \theta_{k^\prime}^{(t-1)})}</script><h3 id="M-step"><a href="#M-step" class="headerlink" title="M step"></a>M step</h3><p>In the M step, EM optimizes $Q$ w.r.t $\pi$ and <script type="math/tex">\theta_k</script>:</p>
<script type="math/tex; mode=display">
\pi_k = \frac{1}{N} \sum_i r_{ik} = \frac{r_k}{N}</script><p>where <script type="math/tex">r_k \triangleq \sum_i r_{ik}</script> is the weighted number of points assigned to cluster $k$.</p>
<p>The log likelihood:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{l}(\mu_k, \Sigma_k) &= \sum_k \sum_i r_{ik} \log p(\mathbf{x}_i \vert \theta_k) \\
&= -\frac{1}{2} \sum_i r_{ik} [\log \vert \Sigma_k \vert + (\mathbf{x}_i - \mathbf{\mu}_k)^\top \Sigma_k^{-1} (\mathbf{x}_i - \mathbf{\mu}_k)]
\end{align}</script><p>Thus,</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_k &= \frac{\sum_i r_{ik}\mathbf{x}_i}{r_k} \\
\Sigma_k &= \frac{\sum_i r_{ik} (\mathbf{x}_i - \mu_k)(\mathbf{x}_i - \mu_k)^\top}{r_k} \\
&= \frac{\sum_i r_{ik} \mathbf{x}_i \mathbf{x}_i^\top}{r_k} - \mu_k \mu_k^\top
\end{align}</script><p>After computing such new estimates, set <script type="math/tex">\theta^t = (\pi_k, \mu_k, \Sigma_k)</script> for $k=1:K$ and go to the next $E$ step.</p>
<div class="note info">
            <ul><li>The mean of cluster $k$ is just the weighted average of all poitns assigned to cluster $k$.</li><li>The covariance is proportional to the weighted empirical scatter matrix.</li></ul>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GMM</span>(<span class="params">Clustering</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, n_clusters, init_centroids=<span class="literal">None</span>, seed=<span class="number">2020</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GMM, self).__init__(dim, n_clusters, seed)</span><br><span class="line">        self.points = tf.placeholder(tf.float64, [<span class="literal">None</span>, dim], name=<span class="string">&#x27;points&#x27;</span>)</span><br><span class="line">        num_points = tf.shape(self.points)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># choose N centroids -&gt; (n_clusters, D)</span></span><br><span class="line">        <span class="keyword">if</span> init_centroids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.mu = tf.Variable(tf.<span class="built_in">slice</span>(tf.random.shuffle(self.points, seed=seed), [<span class="number">0</span>, <span class="number">0</span>], [n_clusters, -<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.mu = tf.Variable(init_centroids, dtype=tf.float64, name=<span class="string">&#x27;init&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand dim -&gt; (1, n_points, D)</span></span><br><span class="line">        xs_expanded = tf.expand_dims(self.points, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># (n_clusters, 1, D)</span></span><br><span class="line">        centroid_expanded = tf.expand_dims(self.mu, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># init variances</span></span><br><span class="line">        self.var = tf.Variable(tf.cast(tf.ones([n_clusters, self.dim]), tf.float64) / n_clusters)</span><br><span class="line">        <span class="comment"># init weights</span></span><br><span class="line">        self.weight = tf.Variable(tf.cast(tf.fill([n_clusters], <span class="number">1.</span> / n_clusters), tf.float64))</span><br><span class="line"></span><br><span class="line">        log_2piD = tf.constant(np.log(<span class="number">2</span> * np.pi) * dim, dtype=tf.float64)</span><br><span class="line">        <span class="comment"># E step: recompute the responsibilities w.r.t the current parameters</span></span><br><span class="line">        distances = tf.squared_difference(xs_expanded, centroid_expanded)</span><br><span class="line">        distance_x_inv_var = tf.reduce_sum(distances / tf.expand_dims(self.var, <span class="number">1</span>), <span class="number">2</span>)</span><br><span class="line">        log_coef = tf.expand_dims(log_2piD + tf.reduce_sum(tf.log(self.var), <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">        log_comp = -<span class="number">.5</span> * (log_coef + distance_x_inv_var)</span><br><span class="line">        log_weighted = log_comp + tf.expand_dims(tf.log(self.weight), <span class="number">1</span>)</span><br><span class="line">        log_shift = tf.reduce_max(log_weighted, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        weight = tf.exp(log_weighted - log_shift)</span><br><span class="line">        weight_sum = tf.reduce_sum(weight, <span class="number">0</span>)</span><br><span class="line">        gamma = weight / weight_sum</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M step: maximizing parameters w.r.t the computed responsibilities</span></span><br><span class="line">        gamma_sum = tf.reduce_sum(gamma, <span class="number">1</span>)</span><br><span class="line">        gamma_weighted = gamma / tf.expand_dims(gamma_sum, <span class="number">1</span>)</span><br><span class="line">        gamma_expanded = tf.expand_dims(gamma_weighted, <span class="number">2</span>)</span><br><span class="line">        self.mu_ = tf.reduce_sum(xs_expanded * gamma_expanded, <span class="number">1</span>)</span><br><span class="line">        distances = tf.squared_difference(xs_expanded, tf.expand_dims(self.mu_, <span class="number">1</span>))</span><br><span class="line">        self.var_ = tf.reduce_sum(distances * gamma_expanded, <span class="number">1</span>)</span><br><span class="line">        self.weight_ = gamma_sum / tf.cast(num_points, tf.float64)</span><br><span class="line"></span><br><span class="line">        ll = tf.reduce_sum(tf.log(weight_sum)) + tf.reduce_sum(log_shift)</span><br><span class="line">        self.mean_ll = ll / tf.cast(num_points * dim, tf.float64)</span><br><span class="line"></span><br><span class="line">        self.train_op = tf.group(</span><br><span class="line">            self.mu.assign(self.mu_),</span><br><span class="line">            self.var.assign(self.var_),</span><br><span class="line">            self.weight.assign(self.weight_)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, points, n_iters=<span class="number">1000</span>, TOLERANCE=<span class="number">1e-8</span></span>):</span></span><br><span class="line">        prev_ll = - np.inf</span><br><span class="line">        <span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            sess.run(tf.compat.v1.global_variables_initializer())</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iters):</span><br><span class="line">                cur_ll, _ = sess.run(</span><br><span class="line">                    [self.mean_ll, self.train_op],</span><br><span class="line">                    feed_dict=&#123;self.points: points&#125;</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                    difference = np.<span class="built_in">abs</span>(cur_ll - prev_ll)</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&#x27;GMM step-<span class="subst">&#123;i&#125;</span>:\t mean likelihood <span class="subst">&#123;cur_ll&#125;</span> \t difference <span class="subst">&#123;difference&#125;</span>&#x27;</span>)</span><br><span class="line">                    <span class="keyword">if</span> difference &lt; TOLERANCE:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&#x27;GMM step-<span class="subst">&#123;i&#125;</span>:\t mean likelihood <span class="subst">&#123;cur_ll&#125;</span>&#x27;</span>)</span><br><span class="line">                prev_ll = cur_ll</span><br><span class="line">            mu = self.mu.<span class="built_in">eval</span>(sess)</span><br><span class="line">            var = self.var.<span class="built_in">eval</span>(sess)</span><br><span class="line">        <span class="keyword">return</span> mu, var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    n_points = <span class="number">200</span></span><br><span class="line">    n_clusters = <span class="number">3</span></span><br><span class="line">    D = <span class="number">100</span></span><br><span class="line">    points = np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, (n_points, D))  <span class="comment"># random</span></span><br><span class="line">    cluster = GMM(D, n_clusters, init_centroids=<span class="literal">None</span>)</span><br><span class="line">    mu, var = cluster.update(points)</span><br></pre></td></tr></table></figure>
<p>Multiplying multiple small probabilities can cause the arithmetic underflow, i.e.</p>
<script type="math/tex; mode=display">\log \sum_{i=1}^n \exp (x_i)</script><div class="note success">
            <p>To circumvent this, a common trick called <code>log sum exponential trick</code>:</p><script type="math/tex; mode=display">  \begin{align}  \log \sum_{i=1}^n \exp (x_i) &= \log \exp (b) \sum_{i=1}^n \exp (x_i -b) \\  &= b + \log \sum_{i=1}^n \exp (x_i -b)  \end{align}</script>
          </div>
<h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p>K-means algorithm is a variant of EM algorithm for GMM, which can be seen as a GMM with such assumptions: <script type="math/tex">\Sigma_k = \sigma^2 \mathbf{I}D</script> and <script type="math/tex">\pi_k = \frac{1}{K}</script> are fixed, only the cluster centers <script type="math/tex">\mathbf{\mu}_k \in \mathbb{R}^D</script> will be estimated.</p>
<p>In the E step, </p>
<script type="math/tex; mode=display">p(z_i = k \vert \mathbf{x}_i ,\theta) \approx \mathbb{I}(k = z_i^*)</script><p>where <script type="math/tex">z_i^* = \arg\max_k p(z_i=k \vert \mathbb{x}_i, \theta)</script>, which is also called <strong>hard EM</strong> since K-means makes the hard assignment of the points to clusters. </p>
<h3 id="E-step-1"><a href="#E-step-1" class="headerlink" title="E step"></a>E step</h3><p>Since the equal spherical convariance matrix is assumed, the most probable cluster for <script type="math/tex">\mathbf{x}_i</script> can be computed using the Euclidian distance between $N$ data points and $K$ cluster centroids:</p>
<script type="math/tex; mode=display">z_i^* = \arg \min_k  \Vert \mathbf{x}_i - \mu_k \Vert_2^2</script><p>This is equivalence to minimizing the <strong>pairwise squared deviations of points in the same cluster</strong>.</p>
<h3 id="M-step-1"><a href="#M-step-1" class="headerlink" title="M step"></a>M step</h3><p>Given the hard cluster assignments, the M step update the cluster centroid by computing the mean of all points assigned to it:</p>
<script type="math/tex; mode=display">
\mu_k = \frac{1}{N_k} \sum_{i:z_i=k} \mathbf{x}_i</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMeans</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; KMeans clustering &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, points, n_clusters, n_iter=<span class="number">300</span></span>):</span></span><br><span class="line">        self.points = points</span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line">        <span class="comment"># randomly choose N centroids -&gt; (n_clusters, D)</span></span><br><span class="line">        self.centroids = tf.Variable(tf.<span class="built_in">slice</span>(tf.random.shuffle(points), [<span class="number">0</span>, <span class="number">0</span>], [n_clusters, -<span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># expand dim -&gt; (1, n_points, D)</span></span><br><span class="line">        xs_expanded = tf.expand_dims(points, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># (n_clusters, 1, 2)</span></span><br><span class="line">        centroid_expanded = tf.expand_dims(self.centroids, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># calculate the distance between points and centroids -&gt; (n_clusters, n_points)</span></span><br><span class="line">        distances = tf.reduce_sum(tf.square(tf.subtract(xs_expanded, centroid_expanded)), -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (n_points,)</span></span><br><span class="line">        self.assignments = tf.argmin(distances, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># get mean points</span></span><br><span class="line">        means = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_clusters):</span><br><span class="line">            idx = tf.reshape(tf.where(tf.equal(self.assignments, c)), [<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">            c_points = tf.gather(points, idx)</span><br><span class="line">            means.append(tf.reduce_mean(c_points, reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># update centroids -&gt; (n_cluster, D)</span></span><br><span class="line">        new_centroids = tf.concat(means, <span class="number">0</span>)</span><br><span class="line">        self.update_centroids = tf.compat.v1.assign(self.centroids, new_centroids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        KMeans iterate</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">            points_vals: data points</span></span><br><span class="line"><span class="string">            centroid_vals: centroid points</span></span><br><span class="line"><span class="string">            assignment_vals: cluster categories</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            sess.run(tf.compat.v1.global_variables_initializer())</span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(self.n_iter):</span><br><span class="line">                _, centroid_vals, points_vals, assignment_vals = sess.run(</span><br><span class="line">                    [self.update_centroids, self.centroids, self.points, self.assignments])</span><br><span class="line">            <span class="comment"># print(f&#x27;centroids: &#123;centroid_vals&#125;&#x27;)</span></span><br><span class="line">        <span class="keyword">return</span> centroid_vals, points_vals, assignment_vals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n_points = <span class="number">200</span></span><br><span class="line">    n_clusters = <span class="number">3</span></span><br><span class="line">    n_iter = <span class="number">100</span></span><br><span class="line">    D = <span class="number">100</span></span><br><span class="line">    points = tf.constant(np.random.uniform(<span class="number">0</span>, <span class="number">10</span>, (n_points, D))) <span class="comment"># random</span></span><br><span class="line">    cluster = KMeans(points, n_clusters, n_iter)</span><br><span class="line">    centroid_vals, points_vals, assignment_vals = cluster.update()</span><br></pre></td></tr></table></figure>
<h1 id="Deep-Embedded-Clustering-DEC"><a href="#Deep-Embedded-Clustering-DEC" class="headerlink" title="Deep Embedded Clustering (DEC)"></a>Deep Embedded Clustering (DEC)</h1><p>Given a set of $n$ points <script type="math/tex">\{ x_i \in X \}_{i=1}^n</script> into $k$ clusters, each represented by a centroid <script type="math/tex">\mu_j</script>, $j=1, \cdots, k$.</p>
<p>Firstly apply linear transformation <script type="math/tex">f_\theta : X \rightarrow Z</script>, where $\theta$ are learnable parameters, $Z$ is the latent feature space.</p>
<p>Deep Embedded Clustering (DEC)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Xie, J., Girshick, R.B., & Farhadi, A. (2015). [Unsupervised Deep Embedding for Clustering Analysis](https://arxiv.org/pdf/1511.06335.pdf). ICML.
">[1]</span></a></sup> has two steps:</p>
<ol>
<li>parametrize initialization with a deep autoencoder</li>
<li>clustering, by computing an auxiliary target distribution and minimize KL divergence.</li>
</ol>
<p><img data-src="/notes/images/DEC.png" width="70%"/></p>
<h2 id="DEC-Clustering"><a href="#DEC-Clustering" class="headerlink" title="DEC Clustering"></a>DEC Clustering</h2><p>Given initial cluster centroids <script type="math/tex">\{ u_j\}_{k=1}^k</script>, DEC firstly computes the soft assignment between the embedded points and the cluster centroids; afterwards, update the deep mapping <script type="math/tex">f_\theta</script> with Stochastic Gradient Descent and refine the cluster centroids using an auxiliary target distribution. This process is repeated until convergence.</p>
<h3 id="Soft-Assignment"><a href="#Soft-Assignment" class="headerlink" title="Soft Assignment"></a>Soft Assignment</h3><p>DEC applies Student’s $t$-distribution as a kernel to measure the similarity between embedded point <script type="math/tex">z_i</script> and centroid <script type="math/tex">\mu_j</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
q_{ij} = \frac{(1+\Vert z_i - u_j \Vert^2 / \alpha)^{-\frac{\alpha+1}{2}}}{\sum_{j^\prime} (1+\Vert z_i - u_{j^\prime} \Vert^2 / \alpha)^{-\frac{\alpha+1}{2}}}
\end{align}</script><p>where <script type="math/tex">z_i = f_\theta (x_i) \in Z</script> corresponds to <script type="math/tex">x_i \in X</script> after embedding, $\alpha$ (set to 1) are the degrees of freedom of the Student’s $t$-distribution and <script type="math/tex">q_{ij}</script> can be interpreted as the probability of assigning sample $i$ to cluster $j$ (i.e., soft assignment).</p>
<h3 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL Divergence"></a>KL Divergence</h3><p>DEC iteratively refines the clusters by learning from their high confidence assignments with the auxiliary target distribution. KL divergence is computed beween the soft assignments <script type="math/tex">q_i</script> and the auxiliary distribution <script type="math/tex">p_i</script>:</p>
<script type="math/tex; mode=display">
L = \mathbb{KL}(P \vert Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q{ij}}</script><p>The choice of target distribution $P$ is critical. </p>
<script type="math/tex; mode=display">p_{ij} = \frac{q_{ij}^2 / f_j}{\sum_{j^\prime} q_{ij}^2 / f_{j^\prime}}</script><p>where <script type="math/tex">f_j = \sum_i q_{ij}</script> are soft cluster frequencies.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">q_&#123;ij&#125; = \frac&#123;(1+\Vert z_i - u_j \Vert^2 / \alpha)^&#123;-\frac&#123;\alpha+1&#125;&#123;2&#125;&#125;&#125;&#123;\sum_&#123;j^\prime&#125; (1+\Vert z_i - u_&#123;j^\prime&#125; \Vert^2 / \alpha)^&#123;-\frac&#123;\alpha+1&#125;&#123;2&#125;&#125;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># tensorflow code</span></span><br><span class="line"><span class="comment"># given data variables `zs` -&gt; shape (?, D) ; `us` -&gt; shape (K, D) </span></span><br><span class="line"><span class="comment"># where D is the dimension, K is the cluster number</span></span><br><span class="line">zs = tf.expand_dims(zs, <span class="number">1</span>) <span class="comment"># (?, 1, D)</span></span><br><span class="line">us = tf.expand_dims(us, <span class="number">0</span>) <span class="comment"># (1, K, D)</span></span><br><span class="line">q = tf.<span class="built_in">pow</span>(tf.reduce_sum(tf.math.squared_difference(zs, us), axis=-<span class="number">1</span>) / model_config.alpha + <span class="number">1</span>, - (model_config.alpha + <span class="number">1</span>) / <span class="number">2</span>)</span><br><span class="line">q /= tf.reduce_sum(q, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">                    </span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> p_&#123;ij&#125; = \frac&#123;q_&#123;ij&#125;^2 / f_j&#125;&#123;\sum_&#123;j^\prime q_&#123;ij&#125;^2 / f_&#123;j^\prime&#125;&#125;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># norm along seq axis</span></span><br><span class="line">p = tf.square(q) / tf.reduce_sum(q, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># norm along cluster axis</span></span><br><span class="line">p /= tf.reduce_sum(p, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">kl_div = tf.reduce_sum(p * tf.math.log(p / q))</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Xie, J., Girshick, R.B., &amp; Farhadi, A. (2015). <a href="https://arxiv.org/pdf/1511.06335.pdf">Unsupervised Deep Embedding for Clustering Analysis</a>. ICML.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/K-means_clustering">Kmeans clustering</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>ML</category>
        <category>Clustering</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>Clustering</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Augmentation for Deep Learning Models</title>
    <url>/notes/2018/11/27/ML/Data-augmentation-for-Deep-Learning-models/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Neural nets require large scale dataset during training process. However, it is quite expensive to have the access to enough data size. One approach to deal with this issue is <em>Data augmentation</em>, which means increasing the number of data points.<br><span id="more"></span></p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><h4 id="It-works-when-we-can-find-appropriate-invariant-properties-that-the-model-should-posses"><a href="#It-works-when-we-can-find-appropriate-invariant-properties-that-the-model-should-posses" class="headerlink" title="It works when we can find appropriate invariant properties that the model should posses"></a><strong>It works when we can find appropriate invariant properties that the model should posses</strong></h4><hr>
<h3 id="Image-recognition"><a href="#Image-recognition" class="headerlink" title="Image-recognition"></a>Image-recognition</h3><ul>
<li>rescaling or applying affine distortions to images (translating, scalingt, rotating, flipping of the input image)</li>
</ul>
<h3 id="Speech-recognition"><a href="#Speech-recognition" class="headerlink" title="Speech-recognition"></a>Speech-recognition</h3><ul>
<li>adding a background audio track or applying small shifts along the time dimension  (add artificial <strong>noise background</strong>, change the <strong>tone</strong> or <strong>speed</strong> of speech signal (see <a href="https://arxiv.org/pdf/1412.5567.pdf">DeepSpeech: Scaling up endto-end<br>speech recognition</a>))</li>
</ul>
<hr>
<h3 id="Text-classification"><a href="#Text-classification" class="headerlink" title="Text-classification"></a>Text-classification</h3><p>Unlike image and speech, data augmentation using <strong>signal transformation</strong> is not reasonable, because <strong>exact order of characters may form rigorous syntactic and semantic meaning</strong>.</p>
<h4 id="Best-way"><a href="#Best-way" class="headerlink" title="Best way:"></a><strong>Best way:</strong></h4><ul>
<li>human rephrases of sentences -&gt; unrealistic and expensive</li>
</ul>
<h4 id="Choices"><a href="#Choices" class="headerlink" title="Choices"></a><strong>Choices</strong></h4><ul>
<li><strong>synonyms replacement</strong>: replace words or phrases with synonyms</li>
<li><strong>back-translation</strong>: use [english - ‘intermediate language’ - english] translastion. <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wieting, J., Mallinson, J., & Gimpel, K. (2017). [Learning Paraphrastic Sentence Embeddings
from Back-Translated Bitext](https://www.aclweb.org/anthology/D17-1026). arXiv preprint arXiv:1706.01847.
">[2]</span></a></sup></li>
<li><strong>data noising</strong>: <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Xie, Z., Wang, S. I., Li, J., Lévy, D., Nie, A., Jurafsky, D., & Ng, A. Y. (2017). [Data noising as smoothing in neural network language models](https://arxiv.org/pdf/1703.02573.pdf). arXiv preprint arXiv:1703.02573.
">[3]</span></a></sup></li>
<li><strong>contextual augmentation</strong>: <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kobayashi, S. (2018). [Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations](http://aclweb.org/anthology/N18-2072). arXiv preprint arXiv:1805.06201.">[5]</span></a></sup> </li>
</ul>
<hr>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, X., &amp; LeCun, Y. (2015). Text understanding from scratch. arXiv preprint arXiv:1502.01710.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wieting, J., Mallinson, J., &amp; Gimpel, K. (2017). <a href="https://www.aclweb.org/anthology/D17-1026">Learning Paraphrastic Sentence Embeddings
from Back-Translated Bitext</a>. arXiv preprint arXiv:1706.01847.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Xie, Z., Wang, S. I., Li, J., Lévy, D., Nie, A., Jurafsky, D., &amp; Ng, A. Y. (2017). <a href="https://arxiv.org/pdf/1703.02573.pdf">Data noising as smoothing in neural network language models</a>. arXiv preprint arXiv:1703.02573.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://forums.fast.ai/t/data-augmentation-for-nlp/229">fast.ai forum: data augmentation for nlp</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kobayashi, S. (2018). <a href="http://aclweb.org/anthology/N18-2072">Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations</a>. arXiv preprint arXiv:1805.06201.<a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>ML</category>
        <category>Data augmentation</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Data augmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>An Overview of Ensemble Learning</title>
    <url>/notes/2019/01/26/ML/Ensemble-learning-overview/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><strong>Ensemble learning</strong> combines multiple weak learners to build a strong learner.</p>
<span id="more"></span>
<h1 id="Simple-techniques"><a href="#Simple-techniques" class="headerlink" title="Simple techniques"></a>Simple techniques</h1><h2 id="Majority-voting"><a href="#Majority-voting" class="headerlink" title="Majority voting"></a>Majority voting</h2><p><code>Majority voting</code> is generally used in <strong>classification</strong> tasks: take the <code>majority</code> of the model predictions as the final prediction.</p>
<h2 id="Averaging"><a href="#Averaging" class="headerlink" title="Averaging"></a>Averaging</h2><p>Take the <code>average of predictions</code> from all the models as the final prediction, in <code>regression</code> or <code>classification</code> (the average of probabilities) tasks.</p>
<h2 id="Weighted-average"><a href="#Weighted-average" class="headerlink" title="Weighted average"></a>Weighted average</h2><p>Assign <code>different weights</code> defining the importance of each model for different models.</p>
<h1 id="Advanced-ensemble-techniques"><a href="#Advanced-ensemble-techniques" class="headerlink" title="Advanced ensemble techniques"></a>Advanced ensemble techniques</h1><h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>Stacking (a.k.a. Stacked Generalization, or Super Learner) employs a number of first-layer individual learners (model 1-5 / Tier-1 in the below figures) generated from the training data set, followed by a second-level learner  (model 6 / Tier-2, a.k.a. <em>meta-learner</em>).</p>
<p><img data-src="/notes/images/stacked-generalization.png" alt="upload successful"><br><img data-src="/notes/images/stacking.png" alt="upload successful"></p>
<p><img data-src="/notes/images/stacking-alg.png" alt="upload successful"></p>
<h2 id="Blending"><a href="#Blending" class="headerlink" title="Blending"></a>Blending</h2><p>Similar to stacking, but use <strong>only a devset</strong> from the training set to make predictions. The devset and the predictions are used to build the model on test set.</p>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging(<strong>B</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong>):</p>
<ol>
<li>Bootstrapped subsampling;</li>
<li>Fit the base model on each of these subsets;</li>
<li>Models are run <strong>in parallel</strong> and independent of each other;</li>
<li>The final prediction are determined by combining all model predictions. </li>
</ol>
<p><img data-src="/notes/images/bagging.png" alt="upload successful"></p>
<h3 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h3><p><img data-src="/notes/images/random-forest-alg.png" alt="random forest"></p>
<p><strong>Pros</strong>:</p>
<ol>
<li>Robust against outliers and noise;</li>
<li><em>Reduce variance</em> and typically avoids overfitting;</li>
<li>Fast run time;</li>
</ol>
<p><strong>Cons</strong>:</p>
<ol>
<li>Can be slow to score as complexity increases;</li>
<li>Lack of transparency due to the complexity of multiple trees;</li>
</ol>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting is a <strong>sequential process</strong>, where each subsequent model attempts to correct the errors of the previous model. </p>
<ol>
<li>Create a subset of all dataset.</li>
<li>Initially, all data points are given the same weights.</li>
<li>Fit a base model on this subset.</li>
<li>Use this base model to predict on the whole dataset.</li>
<li>Calculate errors using golden standard and predictions.</li>
<li>The wrongly predicted data are given <strong>higher weights</strong>.</li>
<li>Another model is created with step 3-6. (in order to <strong>correct the errors from the previous model</strong>)</li>
<li>Multiple models are created, each correcting the error of the previous model.</li>
<li>The final model (strong learner) is the weighted mean of all the models.</li>
</ol>
<p><img data-src="/notes/images/boosting-1.png" alt="upload successful"><br><img data-src="/notes/images/boosting-2.png" alt="upload successful"></p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>Adaptive Boosting</p>
<p><img data-src="/notes/images/AdaBoost.png" alt="upload successful"></p>
<p><strong>Pros</strong>:</p>
<ol>
<li>Often the best possible model;</li>
<li>Directly optimize the <strong>cost function</strong>;</li>
</ol>
<p><strong>Cons</strong>:</p>
<ol>
<li>Not robust against outliers and noise;</li>
<li>Can overfit;</li>
<li>Need to find proper stopping point;</li>
</ol>
<h3 id="Comparing-bagging-and-boosting"><a href="#Comparing-bagging-and-boosting" class="headerlink" title="Comparing bagging and boosting"></a>Comparing bagging and boosting</h3><div class="note danger">
            <p>Model error arises from noise, bias, and variance.</p><ul><li><em>Noise</em> is error by the target function;</li><li><em>Bias</em> is where the algorithm cannot learn the target;</li><li><em>Variance</em> comes from sampling.</li></ul><p><code>Boosting</code> is recommended on models that have a <strong>high bias</strong>, not Bagging.<br>Conversely, <code>Bagging</code> is recommend for cases of <strong>high variance</strong>, rather than Boosting.</p>
          </div>
<h3 id="GBM-Gradient-Boosted-Models"><a href="#GBM-Gradient-Boosted-Models" class="headerlink" title="GBM(Gradient Boosted Models)"></a>GBM(Gradient Boosted Models)</h3><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, T., & Guestrin, C. (2016). [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf). KDD.
">[1]</span></a></sup></p>
<h3 id="Light-GBM"><a href="#Light-GBM" class="headerlink" title="Light GBM"></a>Light GBM</h3><p>It is useful for large-size dataset.</p>
<h3 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h3><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, T., &amp; Guestrin, C. (2016). <a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a>. KDD.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhou Z. (2016) <a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf">Ensemble learning</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>ML</category>
        <category>Ensemble learning</category>
      </categories>
      <tags>
        <tag>Ensemble learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Automatic Evaluation Metrics for Language Generation</title>
    <url>/notes/2020/06/05/NLG/Automatic-Evaluation-Metrics-for-Language-Generation/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A summary of the automatic evaluation metric for natural language generation (NLG) applications.</p>
<p>The human evaluation considers the aspects of <strong>adequacy, fidelity, and fluency</strong>, but it is quite expensive. </p>
<ul>
<li><strong>Adequacy</strong>: Does the output <strong>convey the same meaning</strong> as the input sentence? Is part of the message <strong>lost, added, or distorted</strong>?</li>
<li><strong>Fluency</strong>: Is the output good fluent English? This involves both <strong>grammatical correctness</strong> and <strong>idiomatic word choices</strong>.</li>
</ul>
<p>Thus, a useful metric for automatic evaluation in NLG applications holds the promise, such as machine translation, text summarization, image captioning, dialogue generation, poetry/story generation, etc.<br><span id="more"></span></p>
<p>Goals for evaluation metrics:</p>
<ul>
<li><strong>Low cost</strong>: reduce time and money cost.</li>
<li><strong>Tunable</strong>: automatically optimize system performance towards metric.</li>
<li><strong>Meaningful</strong>: give the intuitive interpretation of quality.</li>
<li><strong>Consistent</strong>: repeated use of metric should provide the same results.</li>
<li><strong>Correct</strong>: The metric must rank better systems higher.</li>
</ul>
<p>Evaluation task: for NLG tasks, given candidate hypothesis and human references, compute the similarity between them and provide the corresponding score.</p>
<h1 id="BLEU-ACL-2002"><a href="#BLEU-ACL-2002" class="headerlink" title="BLEU (ACL 2002)"></a>BLEU (ACL 2002)</h1><p>Designing for machine translation evaluation, BiLingual Evaluation Understudy (BLEU) <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Papineni, K., Roukos, S., Ward, T., & Zhu, W. (2001). [Bleu: a Method for Automatic Evaluation of Machine Translation](https://pdfs.semanticscholar.org/8ff9/3cfd37dced279134c9d642337a2085b31f59.pdf?_ga=2.205976467.1470216667.1588583316-1863904407.1553653768). ACL.
">[1]</span></a></sup> measures the $n$-gram overlap between translation output and reference, by computing the precision for n-grams of size 1 to N (N=4).</p>
<p>BLEU take the geometric mean of the test corpus’s modified $n$-gram precision scores <script type="math/tex">p_n</script>, using $n$-grams up to length $N$ and positive weights <script type="math/tex">w_n</script> summing to one.  To compute the modified $n$-gram precision, all candidate $n$-gram counts and their corresponding maximum reference counts are collected. The candidate counts are clipped by their corresponding reference maximum value, summed and divided by the total number of candidate $n$-grams. In other words, truncating the $n$-gram counts in candidate with the corresponding maximum in the reference.</p>
<script type="math/tex; mode=display">
p_n = \frac{\sum_{\mathcal{C} \in \{ \textrm{candidates} \}} \sum_{\textrm{n-gram} \in \mathcal{C}} \textrm{count}_\textrm{clip} (\textrm{n-gram}) }{\sum_{\mathcal{C}^\prime \in \{ \textrm{candidates} \}} \sum_{\textrm{n-gram} \in \mathcal{C}^\prime } \textrm{count}_\textrm{clip} (\textrm{n-gram}^\prime)}</script><p>Then multiply by an exponential <strong>brevity penalty</strong> (BP) factor, given $c$ as the length of the candidate translation and $r$ as the effective reference corpus length (summing the best match lengths, <em>i.e.</em>, the closest reference sentence length, forr each candidate sentence in the corpus).</p>
<script type="math/tex; mode=display">
\textrm{BP}=\left\{
                \begin{array}{ll}
                  1 & \textrm{if } c > r\\
                  \exp(1- \frac{r}{c}) & \textrm{if } c \leq r
                \end{array}
    \right.</script><p>Thus,</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{BLEU} &{}= \textrm{BP} \cdot \exp \big( \sum_{n=1}^N w_n \log p_n \big) \\
\log \text{BLEU} &{}= \min \big(1-\frac{r}{c}, 0 \big) + \sum_{n=1}^N w_n \log p_n
\end{align}</script><p>where $N=4$, and <script type="math/tex">w_n = 1/N</script> is usually uniform weights. That is,</p>
<script type="math/tex; mode=display">
\text{BLEU}_4 = \min \big( 1, \frac{\textrm{output-length}}{\textrm{reference-length}} (\prod_{i=1}^4 \textrm{precision}_t)^{1/4} \big)</script><div class="note info">
            <ol><li>The BLEU score ranges from 0 to 1.</li><li>1-gram tends to satisfy adequacy, whereas large $n$-gram accounts for fluency.</li><li>BLEU treats all systems similarly and uses multiple human translators with different styles to circumvent the variety differences such as “East Asian economy” with the reference “economy of East Asia”<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Papineni, K., Roukos, S., Ward, T., & Zhu, W. (2001). [Bleu: a Method for Automatic Evaluation of Machine Translation](https://pdfs.semanticscholar.org/8ff9/3cfd37dced279134c9d642337a2085b31f59.pdf?_ga=2.205976467.1470216667.1588583316-1863904407.1553653768). ACL.">[1]</span></a></sup>.</li></ol>
          </div>
<h1 id="METEOR-WMT-ACL-2004"><a href="#METEOR-WMT-ACL-2004" class="headerlink" title="METEOR (WMT@ACL 2004)"></a>METEOR (WMT@ACL 2004)</h1><p>Meteor<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Denkowski, M.J., & Lavie, A. (2014). [Meteor Universal: Language Specific Translation Evaluation for Any Target Language](https://pdfs.semanticscholar.org/dfca/8f68163389605aa2b23d04ccfb293d6484fe.pdf?_ga=2.130365455.1470216667.1588583316-1863904407.1553653768). WMT@ACL.
">[2]</span></a></sup> evaluates translation candidates in a more flexible way and calculates the sentence-level similarity scores. For a hypothesis-reference pair, the alignment is constructed by exhaustively identifying all possible matches between the sentences as follows:</p>
<ol>
<li><strong>Exact</strong>: identical surface forms.</li>
<li><strong>Stem</strong>: same stem words (using Snoball Stemmer toolkit).</li>
<li><strong>Synonym</strong>: share membership in any synonym set in WordNet database.</li>
<li><strong>Paraphrase</strong>: match if they are paraphrases.</li>
</ol>
<p>The final alignment solution using a beam search follows:</p>
<ol>
<li>Each word in each sentence is covered by zero or one match.</li>
<li>Maximize the number of covered words across both sentences.</li>
<li>Minimize the number of <em>chunks</em>, where a <em>chunk</em> is a series of contiguous and identically ordered words in both sentences.</li>
<li>Minimize the sum of absolute distances between match start indices in the two sentences.</li>
</ol>
<p>Content and function words are identical in the hypothesis <script type="math/tex">(h_c, h_f)</script> and reference <script type="math/tex">(r_c, r_f)</script> accoring to the function word list (any word whose relative frequency above 10^-3). For matches <script type="math/tex">m_i</script>, count the number of content and function words in hypothesis <script type="math/tex">m_i(h_c), m_i(h_f)</script> and reference  <script type="math/tex">m_i(r_c), m_i(r_f)</script>. Calculate weighted precision and recall using matcher weights <script type="math/tex">(w_i,\cdots,w_n)</script> and content-function word weight $\delta$:</p>
<script type="math/tex; mode=display">
\begin{align}
P &{}= \frac{\sum_i w_i \cdot \big( \delta \cdot m_i(h_c) + (1 - \delta) \cdot m_t (h_f) \big)}{ \delta \cdot \vert h_c \vert + (1-\delta) \cdot \vert h_f \vert }\\
R &{}= \frac{\sum_i w_i \cdot \big( \delta \cdot m_i(r_c) + (1 - \delta) \cdot m_t (r_f) \big)}{ \delta \cdot \vert r_c \vert + (1-\delta) \cdot \vert r_f \vert } \\
F_\text{mean} &{}= \frac{P \cdot R}{ \alpha \cdot P + (1-\alpha)\cdot R }
\end{align}</script><p>To account for gaps and differences in word order, a <strong>fracmentation penalty</strong> uses the total number of match words $m$ and number of chunks ($ch$):</p>
<script type="math/tex; mode=display">
\text{penalty} = \gamma \cdot \bigg( \frac{ch}{m} \bigg)^\beta</script><p>The Meteor score is calculated:</p>
<script type="math/tex; mode=display">\text{Meteor} = (1- \textrm{penalty} \cdot F_\textrm{mean})</script><p>where parameters ${ \alpha, \beta, \gamma, \delta, (w_i, \cdots, w_n)}$ are tuned to maximize correlatin with human judgements.</p>
<h1 id="ROUGE-ACL-2004"><a href="#ROUGE-ACL-2004" class="headerlink" title="ROUGE (ACL 2004)"></a>ROUGE (ACL 2004)</h1><p>Recall-Oriented Understudy for Gisting Evaluation (ROUGE)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lin, C. (2004). [ROUGE: A Package For Automatic Evaluation Of Summaries](https://pdfs.semanticscholar.org/de79/1d19d5abe0b4a419ff039c07f066f781ec9c.pdf?_ga=2.171319200.1470216667.1588583316-1863904407.1553653768). ACL 2004.
">[3]</span></a></sup> is an automatic evaluation metric for text summarization, consisting of ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S.</p>
<h2 id="ROUGE-N-N-gram-Co-Occurrence-Statistics"><a href="#ROUGE-N-N-gram-Co-Occurrence-Statistics" class="headerlink" title="ROUGE-N: N-gram Co-Occurrence Statistics"></a>ROUGE-N: N-gram Co-Occurrence Statistics</h2><p>ROUGE-N is an <strong>n-gram recall</strong> between a candidate summary and a set of reference summaries.</p>
<script type="math/tex; mode=display">
p_n = \frac{\sum_{\mathcal{S} \in \{ \textrm{References} \}} \sum_{\textrm{n-gram} \in \mathcal{S}} \textrm{count}_\textrm{match} (\textrm{n-gram}) }{\sum_{\mathcal{S}^\prime \in \{ \textrm{References} \}} \sum_{\textrm{n-gram} \in \mathcal{S}^\prime } \textrm{count} (\textrm{n-gram}^\prime)}</script><p>where <script type="math/tex">\textrm{count}_\textrm{match} (\textrm{n-gram})</script> represent the maximum number of n-grams co-occuring in candidate summry and a set of reference summaries. It is consistent with the intuition that a candidate summary that is more similar to many reference summaries is preferred.</p>
<div class="note warning">
            <p><strong>vs. BLEU</strong>: <em>BLEU</em> is a <strong>precision-based</strong> measure whereas <em>ROUGE-N</em> is <strong>recall-based</strong>.</p>
          </div>
<h2 id="ROUGE-L-Longest-Common-Subsequence"><a href="#ROUGE-L-Longest-Common-Subsequence" class="headerlink" title="ROUGE-L: Longest Common Subsequence"></a>ROUGE-L: Longest Common Subsequence</h2><h3 id="Sentence-level-LCS"><a href="#Sentence-level-LCS" class="headerlink" title="Sentence-level LCS"></a>Sentence-level LCS</h3><p>Given a reference summary $X$ of length $m$, and a candidate summary sentence $Y$ of length $n$, the sentence level Longest Common Subsequence (LCS) is:</p>
<script type="math/tex; mode=display">
\begin{align}
R_\text{lcs} &{}= \frac{\textrm{LCS} (X, Y)}{m} \\
P_\text{lcs} &{}= \frac{\textrm{LCS} (X, Y)}{n} \\
F_\text{lcs} &{}= \frac{(1 + \beta^2) R_\text{lcs} P_\text{lcs} }{R_\text{lcs} + \beta^2 P_\text{lcs}}
\end{align}</script><h3 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h3><p>ROUGE-L takes the sentence-level normalized pairwise LCS.</p>
<h2 id="ROUGE-W-Weighted-LCS"><a href="#ROUGE-W-Weighted-LCS" class="headerlink" title="ROUGE-W: Weighted LCS"></a>ROUGE-W: Weighted LCS</h2><p>It measures weighted LCS-based statistics that favor consecutive LCS.</p>
<h2 id="ROUGE-S-Skip-bigram-Co-Occurrence-Statistics"><a href="#ROUGE-S-Skip-bigram-Co-Occurrence-Statistics" class="headerlink" title="ROUGE-S: Skip-bigram Co-Occurrence Statistics"></a>ROUGE-S: Skip-bigram Co-Occurrence Statistics</h2><p>ROUGE-S considers the skip-bigram-based F-measure:</p>
<script type="math/tex; mode=display">
\begin{align}
R_\textrm{s2} &{}= \frac{\textrm{skip2}(X, Y)}{C_m^2} \\
P_\textrm{s2} &{}= \frac{\textrm{skip2}(X, Y)}{C_n^2} \\
F_\textrm{s2} &{}= \frac{(1+\beta^2)P_\textrm{s2}R_\textrm{s2} }{R_\textrm{s2} + \beta^2 P_\textrm{s2}}
\end{align}</script><p>where skip-bigram means any pair of words in the sentence regardless of the order and distance.</p>
<h1 id="CIDEr-CVPR-2015"><a href="#CIDEr-CVPR-2015" class="headerlink" title="CIDEr (CVPR 2015)"></a>CIDEr (CVPR 2015)</h1><p>Consensus-based Image Description Evaluation (CIDEr)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vedantam, R., Zitnick, C.L., & Parikh, D. (2015). [CIDEr: Consensus-based image description evaluation](https://arxiv.org/abs/1411.5726). 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4566-4575.
">[4]</span></a></sup> is proposed for evaluating the candidate sentence <script type="math/tex">c_i</script> compared with reference <script type="math/tex">S_i = \{ s_{i1}, \cdots, s_{im} \}</script> for image <script type="math/tex">I_i</script> image captioning. All words in both candidate and reference sentences are first mapped to their stem or root forms.</p>
<p>To penalize the commonly occurred words across all images, TF-IDF weighting for each $n$-gram is applied. Let the count that an $n$-gram <script type="math/tex">\omega_k</script> occurs in a reference sentence <script type="math/tex">s_{ij}</script> be <script type="math/tex">h_k(s_{ij})</script>, and that of candidate sentence <script type="math/tex">c_i</script> be <script type="math/tex">h_k(s_{ij})</script>. The TF-IDF weighting <script type="math/tex">g_k (s_{ij})</script> for each $n$-gram <script type="math/tex">\omega_k</script> using:</p>
<script type="math/tex; mode=display">
g_k (s_{ij}) = \underbrace{\frac{h_k(s_{ij})}{\sum_{\omega_l \in \Omega} h_l (s_{ij})}}_\text{TF} \underbrace{\log \bigg( \frac{\vert I \vert }{\sum_{i_p \in I} \min (1, \sum_q h_k (s_{pq})) } \bigg)}_\textrm{IDF}</script><p>where $\Omega$ is the vocabulary for all $n$-grams and $I$ is the image set. The second term (IDF) measures the word saliency by discounting popular words that are likely to be less visually informative.</p>
<p>The CIDEr<sub>n</sub> score for $n$-grams uses the average cosine similarity between the candidate sentence and reference sentences:</p>
<script type="math/tex; mode=display">\begin{align}
\textrm{CIDEr}_n (c_i, S_i) &{}= \frac{1}{m} \sum_j \frac{ \mathbf{g^n}(c_i) \cdot \mathbf{g^n}(s_{ij}) }{ \Vert \mathbf{g^n}(c_i) \Vert \Vert\mathbf{g^n}(s_{ij}) \Vert } \\
\textrm{CIDER}(c_i, S_i) &{}= \sum_{n=1}^N w_n \textrm{CIDEr}_n (c_, S-i)
\end{align}</script><p>where <script type="math/tex">\mathbf{g^n}(c_i)</script> is a vector formed by <script type="math/tex">g_k(c_i)</script> corresponding to all $n$-grams of length $n$. So is <script type="math/tex">\mathbf{g^n}(s_{ij})</script>. $w_n$ takes the uniform weights <script type="math/tex">1/N</script>, $N=4$.</p>
<h1 id="SPICE-ECCV-2016"><a href="#SPICE-ECCV-2016" class="headerlink" title="SPICE (ECCV 2016)"></a>SPICE (ECCV 2016)</h1><div class="note danger">
            <p>Previous evaluation methods are all $n$-gram based, sensitive to $n$-gram overlap. However, $n$-gram overlap is neither necessary nor sufficient for two sentences to convey the same meaning.</p>
          </div>
<p>Semantic Propositional Image Caption Evaluation (SPICE)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Anderson, P., Fernando, B., Johnson, M., & Gould, S. (2016). [SPICE: Semantic Propositional Image Caption Evaluation](https://arxiv.org/pdf/1607.08822.pdf). ECCV.
">[5]</span></a></sup> measures the semantic meanings rather than $n$-gram based methods.</p>
<p>Given a candidate caption $c$ and a set of reference captions <script type="math/tex">S = \{ s_1, \cdots, is_m \}</script> associated with an image $I$, the goal is to compute the score between $c$ and $S$. SPICE builds <strong>scene graph</strong> using Stanford Scene Graph Parser. A Probabilistic Context-Free Grammar (PCFG) dependency parser and additional linguistic rules are applied.</p>
<p>Given a set of object class $C$, a set of relation typles $R$, aset of attribute types $A$, a caption $c$, we parse $c$ to a scene graph:</p>
<script type="math/tex; mode=display">
G(c) = \langle O(c), E(c), K(c) \rangle</script><p>where $O(c) \subseteq C $ is the set of object in $c$, $E(c) \subseteq O(c) \times R \times O(c)$ is the set of hyper-edges of relations, $K(c) \subseteq O(c) \times A$ ia the set of attributes of objects.</p>
<p>Define the function $T$ that returns logical tuples from a scene graph as:</p>
<script type="math/tex; mode=display">T(G(c)) \triangleq O(c) \cup E(c) \cup K(c)</script><p>The presicion $P$, recall $R$, SPICE are computed as:</p>
<script type="math/tex; mode=display">
\begin{align}
P(c, S) &{}= \frac{\vert T(G(c)) \otimes T(G(s)) \vert}{\vert T(G(c)) \vert} \\
R(c, S) &{}= \frac{\vert T(G(c)) \otimes T(G(s)) \vert}{\vert T(G(S)) \vert} \\
\textrm{SPICE}(c,S) &{}= F_1(c,S) = \frac{2 \cdot P(c,S) \cdot R(c,S) }{P(c,S) + R(c,S)}
\end{align}</script><p>where $\otimes$ denotes the binary matching that returns tha matching tuples in two scene graphs, using the WordNet synonym matching approach of Meteor.</p>
<p>SPICE is also bounded between 0 and 1.</p>
<h1 id="BERTScore-ICLR-2020"><a href="#BERTScore-ICLR-2020" class="headerlink" title="BERTScore (ICLR 2020)"></a>BERTScore (ICLR 2020)</h1><p>BERTScore<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q. and Artzi, Y., 2020. [Bertscore: Evaluating text generation with bert](https://openreview.net/pdf?id=SkeHuCVFDr). ICLR.">[6]</span></a></sup></p>
<p><img data-src="/notes/images/BERTScore.png" alt="BERTScore"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W. (2001). <a href="https://pdfs.semanticscholar.org/8ff9/3cfd37dced279134c9d642337a2085b31f59.pdf?_ga=2.205976467.1470216667.1588583316-1863904407.1553653768">Bleu: a Method for Automatic Evaluation of Machine Translation</a>. ACL.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Denkowski, M.J., &amp; Lavie, A. (2014). <a href="https://pdfs.semanticscholar.org/dfca/8f68163389605aa2b23d04ccfb293d6484fe.pdf?_ga=2.130365455.1470216667.1588583316-1863904407.1553653768">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</a>. WMT@ACL.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lin, C. (2004). <a href="https://pdfs.semanticscholar.org/de79/1d19d5abe0b4a419ff039c07f066f781ec9c.pdf?_ga=2.171319200.1470216667.1588583316-1863904407.1553653768">ROUGE: A Package For Automatic Evaluation Of Summaries</a>. ACL 2004.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vedantam, R., Zitnick, C.L., &amp; Parikh, D. (2015). <a href="https://arxiv.org/abs/1411.5726">CIDEr: Consensus-based image description evaluation</a>. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4566-4575.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Anderson, P., Fernando, B., Johnson, M., &amp; Gould, S. (2016). <a href="https://arxiv.org/pdf/1607.08822.pdf">SPICE: Semantic Propositional Image Caption Evaluation</a>. ECCV.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q. and Artzi, Y., 2020. <a href="https://openreview.net/pdf?id=SkeHuCVFDr">Bertscore: Evaluating text generation with bert</a>. ICLR.<a href="#fnref:6" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>NLG</category>
        <category>NLG Evaluation</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>NLG</tag>
        <tag>NLG Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>Decoding in Text Generation</title>
    <url>/notes/2020/04/21/NLG/Decoding-Methods-in-Language-Generation/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Summary of common decoding strategies in language generation.<br><span id="more"></span></p>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p>Auto-regressive language modeling can be distangled as the accumulated product of conditional probabilites of the next word:</p>
<script type="math/tex; mode=display">
P(w_{1:T} \vert \mathbf{W}_0) = \prod_{t=1}^T P(w_t \vert w_{1:t-1}, \mathbf{W}_0) \quad \textrm{with } w_{1:0}= \emptyset</script><p>where <script type="math/tex">\mathbf{W}_0</script> is the initial contextual sequences, $T$ is the generated variable sequence length.</p>
<h1 id="Maximization-based-Decoding"><a href="#Maximization-based-Decoding" class="headerlink" title="Maximization-based Decoding"></a>Maximization-based Decoding</h1><p>Conventional decoding strategies aim to maximize the likelihood of generated sequences. In this way, generated texts with the highest score for long passages are often generic, repetitive, and awkward.</p>
<div class="note danger">
            <p>It assumes that <strong>models assign a higher probability to higher quality text</strong>.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Holtzman, A., Buys, J., Forbes, M., & Choi, Y. (2019). [The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751). arXiv preprint arXiv:1904.09751.">[5]</span></a></sup></p>
          </div>
<h2 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h2><p>Greedy search selects the token with the highest probability at each step:</p>
<script type="math/tex; mode=display">
w_t = \arg\max_w P(w \vert w_{1:t-1})</script><p>So short-sighted is the greedy 1-best search not to guarantee the optimal sequences with the highest probabilities.</p>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><p>As a heuristic graph search algorithm, beam search explores the decoding search space with a beam-first search in a greedy left-to-right manner, retaining only the best $B$ partial hypotheses and pruning the hypothesizes with low probabilities at each time step, where $B$ is the <em>beam width</em>.</p>
<p><strong>Length bias</strong><br>It is noticeable that such a strategy prefers shorter sentences since the accumulated multiplication of probabilities reduces the overall probability of the whole sentence. </p>
<div class="note info">
            <p>Beam search with a larger beam size posses the <strong><code>length bias</code></strong> towards outputs with the shorter length. </p><ol><li>A heuristic <strong>solution</strong> is to <strong>normalize the log probability by the length of the target sentence</strong>, <em>i.e.,</em>, searching for the sentence with the highest average log probability per word.<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). [On the properties of neural machine translation: Encoder-decoder approaches](https://arxiv.org/pdf/1409.1259.pdf). arXiv preprint arXiv:1409.1259.">[9]</span></a></sup></li><li>Dividing by $\textrm{length}^\alpha$ with $0 &lt; \alpha &lt; 1$ where $\alpha$ is optimized on a dev set.<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Klingner, J. (2016). [Google's neural machine translation system: Bridging the gap between human and machine translation](https://arxiv.org/pdf/1609.08144.pdf). arXiv preprint arXiv:1609.08144.">[10]</span></a></sup></li><li>Dividing by manually designed <script type="math/tex; mode=display">lp = \frac{(5 + \textrm{length})^\alpha}{(5 + 1)^\alpha}</script></li></ol>
          </div>
<p><strong>Lack of diversity / degenerate repetition</strong><br>The outputs of beam search on image captioning are near-duplicates with similar shared paths and minor variations<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D. (2016). [Diverse beam search: Decoding diverse solutions from neural sequence models](https://arxiv.org/pdf/1610.02424). arXiv preprint arXiv:1610.02424.
">[1]</span></a></sup>. Most completions tend to stem from a single, highly valued beam.</p>
<p><strong>Near-identical beams</strong><br>Generated sentences with top-$B$ beams are computationally wasteful due to the slight variation and near-identity with the same repeated computations.</p>
<p><strong>Loss-evaluation mismatch</strong><br><code>Loss-evaluation mismatch</code> is that improvements in posterior-probabilities not necessarily corresponding to task-specific metrics. Tuning the beam size as a hyperparameter leads to reduced beam widths, resulting in the side effect of decoding mostly bland, generic, and ‘safe’ outputs.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D. (2016). [Diverse beam search: Decoding diverse solutions from neural sequence models](https://arxiv.org/pdf/1610.02424). arXiv preprint arXiv:1610.02424.
">[1]</span></a></sup></p>
<p>As shown in the figure, it shows a discrepancy between human- and model- generated word probabilities: repetition of text decoded by beam search w.r.t the probability of tokens generation, while that of human generated text is with variance.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Holtzman, A., Buys, J., Forbes, M., & Choi, Y. (2019). [The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751). arXiv preprint arXiv:1904.09751.
">[5]</span></a></sup><br><img data-src="/notes/images/beam-search-vs-human-data.png" width="50%"/></p>
<center>image source:<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Holtzman, A., Buys, J., Forbes, M., & Choi, Y. (2019). [The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751). arXiv preprint arXiv:1904.09751.
">[5]</span></a></sup></center> 


<div class="note warning">
            <p>Q: <strong>Is the repetition truly due to the beam search?</strong></p>
          </div>
<h2 id="Diverse-Beam-Search"><a href="#Diverse-Beam-Search" class="headerlink" title="Diverse Beam Search"></a>Diverse Beam Search</h2><p>Diverse Beam Search<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D. (2016). [Diverse beam search: Decoding diverse solutions from neural sequence models](https://arxiv.org/pdf/1610.02424). arXiv preprint arXiv:1610.02424.
">[1]</span></a></sup> divides the beam budget $B$ into $G$ groups and greedily optimize each group using beam search while holding previous groups fixed, with each group consisting of $B^{\prime} = B / G$ budgets.</p>
<p>An extra loss term <script type="math/tex">\Delta (\mathbf{y}_{[t]}, Y_{[t]}^g) = \sum_{b=1}^{B^\prime} \delta (\mathbf{y}_{[t]}, \mathbf{y}_{b,[t]}^g)</script> is supplemented, where $\delta(\cdot, \cdot)$ is the measure of sequence dissimilarity, such as negative cost of co-occurring n-grams between two sentences. It is proved that <strong>Hamming distance</strong> achieves the best as the measurement.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D. (2016). [Diverse beam search: Decoding diverse solutions from neural sequence models](https://arxiv.org/pdf/1610.02424). arXiv preprint arXiv:1610.02424.
">[1]</span></a></sup></p>
<h2 id="Noisy-Parallel-Approximate-Decoding"><a href="#Noisy-Parallel-Approximate-Decoding" class="headerlink" title="Noisy Parallel Approximate Decoding"></a>Noisy Parallel Approximate Decoding</h2><p>Noisy Parallel Approximate Decoding (NPAD)<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cho, K. (2016). [Noisy parallel approximate decoding for conditional recurrent language model](https://arxiv.org/pdf/1906.06362.pdf). arXiv preprint arXiv:1605.03835.
">[16]</span></a></sup> injects randomly sampled standard gaussian noise <script type="math/tex">\epsilon \sim \mathcal{N}(\pmb{0}, \sigma_t^2  \pmb{I})</script> to the hidden state of the decoder at each step $t$. The Gaussian variance <script type="math/tex">\sigma_0</script> anneals from a starting $\sigma_0$ to $0$ as the decoding progresses. They applied linear annealing with <script type="math/tex">\sigma_t = \sigma_0 / t</script>.</p>
<h2 id="Top-g-Capping"><a href="#Top-g-Capping" class="headerlink" title="Top-$g$ Capping"></a>Top-$g$ Capping</h2><ol>
<li>At each step $t$, current hypotheses are grouped as $g$ groups according to the parental hypothesis they come from, keeping the top $g$ from each grouping. </li>
<li>The resulting $B \times g$ candidates are ranked, and the top $B$ is selected as hypotheses for the next step.<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ippolito, D., Kriz, R., Kustikova, M., Sedoc, J., & Callison-Burch, C. (2019). [Comparison of Diverse Decoding Methods from Conditional Language Models](https://arxiv.org/pdf/1906.06362). arXiv preprint arXiv:1906.06362.
">[15]</span></a></sup></li>
</ol>
<h2 id="Clustered-Beam-Search"><a href="#Clustered-Beam-Search" class="headerlink" title="Clustered Beam Search"></a>Clustered Beam Search</h2><p>Clustered Beam Search<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Tam, Y. C. (2020). [Cluster-based beam search for pointer-generator chatbot grounded by knowledge](http://workshop.colips.org/dstc7/papers/03.pdf). Computer Speech & Language, 101094.">[18]</span></a></sup> is applied for chatbots. It initially selects $B \times 2$ candidates according to the log probabilities. Afterward, K-means is used to cluster all candidates into $K$ clusters, keeping $B/K$ candidates in each cluster for the next step, wherein average word embeddings are adopted for clustering.</p>
<h2 id="Clustering-Post-Decoding"><a href="#Clustering-Post-Decoding" class="headerlink" title="Clustering Post-Decoding"></a>Clustering Post-Decoding</h2><p>Clustering Post-Decoding(PDC)<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ippolito, D., Kriz, R., Kustikova, M., Sedoc, J., & Callison-Burch, C. (2019). [Comparison of Diverse Decoding Methods from Conditional Language Models](https://arxiv.org/pdf/1906.06362). arXiv preprint arXiv:1906.06362.
">[15]</span></a></sup> leverages K-means clustering on sentence-level embeddings of BERT, chooses the highest-ranked candidate from each cluster after K-means clustering. Finally, remove groups with no greater than two hypotheses and sample a second candidate from each of the remaining clusters.</p>
<h2 id="Contrastive-Search"><a href="#Contrastive-Search" class="headerlink" title="Contrastive Search"></a>Contrastive Search</h2><h1 id="Sampling-based-Decoding"><a href="#Sampling-based-Decoding" class="headerlink" title="Sampling-based Decoding"></a>Sampling-based Decoding</h1><p>Sampling strategies rely on the <code>randomness</code> of predicted word probabilities.</p>
<h2 id="Multinomial-Sampling"><a href="#Multinomial-Sampling" class="headerlink" title="Multinomial Sampling"></a>Multinomial Sampling</h2><p>Simple sampling means directly sampling from probabilities predicted by the model, often leading to incoherent gibberish texts.</p>
<script type="math/tex; mode=display">
w_t \sim P(w \vert w_{1:t-1})</script><p>The <code>unreliable tail</code> that over-represented the relatively low-probability takes the blame for the degenerate sampling results.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Holtzman, A., Buys, J., Forbes, M., & Choi, Y. (2019). [The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751). arXiv preprint arXiv:1904.09751.
">[5]</span></a></sup></p>
<h2 id="Temperature-Sampling"><a href="#Temperature-Sampling" class="headerlink" title="Temperature Sampling"></a>Temperature Sampling</h2><p>Temperature Sampling<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fan, A., Lewis, M., & Dauphin, Y. (2018). [Hierarchical neural story generation](https://arxiv.org/pdf/1805.04833.pdf). arXiv preprint arXiv:1805.04833.
">[6]</span></a></sup> shapes the probability distribution via a temperature $T$ on logits $u$ to regulate the entropyu of the distribution:</p>
<script type="math/tex; mode=display">
p(w=V_l \vert w_{1:t-1}) = \frac{\exp(u_l / \color{blue}{T})}{\sum \exp(u_l^\prime / \color{blue}{T})}</script><p>Setting $t \in (0,1)$ warps the distribution towards high probability events, lowering the mass in the tail. Choosing a temperature higher than one leads to the output more random whilst making it less than one is similar to greedy sampling.<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ippolito, D., Kriz, R., Kustikova, M., Sedoc, J., & Callison-Burch, C. (2019). [Comparison of Diverse Decoding Methods from Conditional Language Models](https://arxiv.org/pdf/1906.06362). arXiv preprint arXiv:1906.06362.
">[15]</span></a></sup></p>
<h2 id="Top-k-Sampling"><a href="#Top-k-Sampling" class="headerlink" title="Top-$k$ Sampling"></a>Top-$k$ Sampling</h2><p>Top-$k$ sampling method samples from a truncated fixed range of top-$k$ most probable choices.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fan, A., Lewis, M., & Dauphin, Y. (2018). [Hierarchical neural story generation](https://arxiv.org/pdf/1805.04833.pdf). arXiv preprint arXiv:1805.04833.
">[6]</span></a></sup><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Holtzman, A., Buys, J., Forbes, M., Bosselut, A., Golub, D., & Choi, Y. (2018). [Learning to write with cooperative discriminators](https://arxiv.org/pdf/1805.06087). arXiv preprint arXiv:1805.06087.
">[13]</span></a></sup><sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). [Language models are unsupervised multitask learners](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf). OpenAI Blog, 1(8), 9.
">[14]</span></a></sup></p>
<h2 id="Top-p-Nucleus-Sampling"><a href="#Top-p-Nucleus-Sampling" class="headerlink" title="Top-$p$ (Nucleus) Sampling"></a>Top-$p$ (Nucleus) Sampling</h2><p>Nucleus Sampling<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Holtzman, A., Buys, J., Forbes, M., & Choi, Y. (2019). [The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751). arXiv preprint arXiv:1904.09751.
">[5]</span></a></sup> leverages the shape of the probability distribution to scale the sampled probabilities. It selects from tokens whose cumulative probability mass exceeds the pre-chosen threshold $p$. Variable is its sampling size according to the shape of cumulative mass probabilities.</p>
<p>Given the top $p$ vocabulary <script type="math/tex">V^{(p)} \subset V</script> as the subset:</p>
<script type="math/tex; mode=display">
\sum_{w \in V^{(p)}} P(w \vert w_{1: t-1}) \geq p</script><p>Let <script type="math/tex">p^\prime = \sum_{w \in V^{(p)}} P(x \vert w_{1:i-1})</script>. The original distribution is rescaled as:</p>
<script type="math/tex; mode=display">
P^\prime (w \vert w_{1:i-1}) =\left\{
                \begin{array}{ll}
                  P(w \vert w_{1:i-1}) / p^\prime & \textrm{if } w \in V^{(p)}\\
                  0 & \textrm{otherwise}
                \end{array}
    \right.</script><div class="note info">
            <ul><li>Top-$k$ sampling and Nucleus Sampling both sample from truncated regions of the original probability distribution, varying on different trustworthy prediction zone.</li><li>Nucleus Sampling can be treated as a top-$k$ sampling with dynamically adjusted $k$. </li></ul>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/facebookresearch/llama/blob/main/llama/generation.py</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_top_p</span>(<span class="params">probs, p</span>):</span></span><br><span class="line">    probs_sort, probs_idx = torch.sort(probs, dim=-<span class="number">1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    probs_sum = torch.cumsum(probs_sort, dim=-<span class="number">1</span>)</span><br><span class="line">    mask = probs_sum - probs_sort &gt; p</span><br><span class="line">    probs_sort[mask] = <span class="number">0.0</span></span><br><span class="line">    probs_sort.div_(probs_sort.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>))</span><br><span class="line">    next_token = torch.multinomial(probs_sort, num_samples=<span class="number">1</span>)</span><br><span class="line">    next_token = torch.gather(probs_idx, -<span class="number">1</span>, next_token)</span><br><span class="line">    <span class="keyword">return</span> next_token</span><br></pre></td></tr></table></figure>
<h2 id="Typical-Sampling"><a href="#Typical-Sampling" class="headerlink" title="Typical Sampling"></a>Typical Sampling</h2><h2 id="eta-Sampling"><a href="#eta-Sampling" class="headerlink" title="$\eta$-Sampling"></a>$\eta$-Sampling</h2><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., &amp; Batra, D. (2016). <a href="https://arxiv.org/pdf/1610.02424">Diverse beam search: Decoding diverse solutions from neural sequence models</a>. arXiv preprint arXiv:1610.02424.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shao, L., Gouws, S., Britz, D., Goldie, A., Strope, B., &amp; Kurzweil, R. (2017). <a href="https://arxiv.org/pdf/1701.03185">Generating high-quality and informative conversation responses with sequence-to-sequence models</a>. arXiv preprint arXiv:1701.03185.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Klein, G., Kim, Y., Deng, Y., Senellart, J., &amp; Rush, A. M. (2017). <a href="https://arxiv.org/pdf/1701.02810">Opennmt: Open-source toolkit for neural machine translation</a>. arXiv preprint arXiv:1701.02810.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Paulus, R., Xiong, C., &amp; Socher, R. (2017). <a href="https://arxiv.org/pdf/1705.04304">A deep reinforced model for abstractive summarization</a>. arXiv preprint arXiv:1705.04304.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Holtzman, A., Buys, J., Forbes, M., &amp; Choi, Y. (2019). <a href="https://arxiv.org/pdf/1904.09751">The curious case of neural text degeneration</a>. arXiv preprint arXiv:1904.09751.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Fan, A., Lewis, M., &amp; Dauphin, Y. (2018). <a href="https://arxiv.org/pdf/1805.04833.pdf">Hierarchical neural story generation</a>. arXiv preprint arXiv:1805.04833.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., &amp; Weston, J. (2019). <a href="https://arxiv.org/pdf/1908.04319">Neural text generation with unlikelihood training</a>. arXiv preprint arXiv:1908.04319.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Welleck, S., Kulikov, I., Kim, J., Pang, R. Y., &amp; Cho, K. (2020). <a href="https://arxiv.org/pdf/2002.02492">Consistency of a Recurrent Language Model With Respect to Incomplete Decoding</a>. arXiv preprint arXiv:2002.02492.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/pdf/1409.1259.pdf">On the properties of neural machine translation: Encoder-decoder approaches</a>. arXiv preprint arXiv:1409.1259.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... &amp; Klingner, J. (2016). <a href="https://arxiv.org/pdf/1609.08144.pdf">Google's neural machine translation system: Bridging the gap between human and machine translation</a>. arXiv preprint arXiv:1609.08144.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Li, J., Galley, M., Brockett, C., Gao, J., &amp; Dolan, B. (2015). <a href="https://arxiv.org/pdf/1510.03055.pdf">A diversity-promoting objective function for neural conversation models</a>). arXiv preprint arXiv:1510.03055.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., &amp; Yu, Y. (2018, June). <a href="https://dl.acm.org/doi/pdf/10.1145/3209978.3210080">Texygen: A benchmarking platform for text generation models</a>. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (pp. 1097-1100).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Holtzman, A., Buys, J., Forbes, M., Bosselut, A., Golub, D., &amp; Choi, Y. (2018). <a href="https://arxiv.org/pdf/1805.06087">Learning to write with cooperative discriminators</a>. arXiv preprint arXiv:1805.06087.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <a href="https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf">Language models are unsupervised multitask learners</a>. OpenAI Blog, 1(8), 9.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ippolito, D., Kriz, R., Kustikova, M., Sedoc, J., &amp; Callison-Burch, C. (2019). <a href="https://arxiv.org/pdf/1906.06362">Comparison of Diverse Decoding Methods from Conditional Language Models</a>. arXiv preprint arXiv:1906.06362.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cho, K. (2016). <a href="https://arxiv.org/pdf/1906.06362.pdf">Noisy parallel approximate decoding for conditional recurrent language model</a>. arXiv preprint arXiv:1605.03835.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kulikov, I., Miller, A. H., Cho, K., &amp; Weston, J. (2018). <a href="https://arxiv.org/pdf/1811.00907">Importance of a search strategy in neural dialogue modelling</a>. arXiv preprint arXiv:1811.00907.<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Tam, Y. C. (2020). <a href="http://workshop.colips.org/dstc7/papers/03.pdf">Cluster-based beam search for pointer-generator chatbot grounded by knowledge</a>. Computer Speech &amp; Language, 101094.<a href="#fnref:18" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Conditional LM</category>
        <category>Decoding</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Conditional LM</tag>
        <tag>Decoding</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Captioning: A Summary!</title>
    <url>/notes/2020/05/01/NLG/Image-Captioning-A-Summary/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A summary of image-to-text translation.<br><span id="more"></span></p>
<h1 id="Neural-Image-Captioning-CVPR-2015"><a href="#Neural-Image-Captioning-CVPR-2015" class="headerlink" title="Neural Image Captioning (CVPR 2015)"></a>Neural Image Captioning (CVPR 2015)</h1><p>As the first end-to-end neural model for image captioning tasks, Neural Image Captioning (NIC)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). [Show and tell: A neural image caption generator](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf). 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3156-3164.
">[1]</span></a></sup> combines the pretrained convolutional neural networks (CNNs) for image classification with recurrent networks (RNNs) for sequence modeling.<br><img data-src="/notes/images/ImgCpt-show-n-tell-cvpr2015.png" width="40%"/></p>
<center> Image source: <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). [Show and tell: A neural image caption generator](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf). 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3156-3164.
">[1]</span></a></sup> </center>



<p>Let $I$ denote the input image, <script type="math/tex">\mathbf{W}_e \in \mathbb{R}^{\vert V \vert \times D}</script> be the $D$-dimensional word embedding matrices of vocabulary $V$, <script type="math/tex">\mathbf{s}_t</script> be the one-hot vector of $t$-th word. </p>
<script type="math/tex; mode=display">
\begin{align}
x_{-1} &{}= \textrm{CNN}(I) & \\
x_t &{}= \mathbf{W}_e \mathbf{s}_t, & t \in \{ 0 \cdots N-1\} \\
p_{t+1} &{}= \textrm{LSTM}(x_t), & t \in \{0 \cdots N-1 \} \\
\mathcal{L} &{}= -\sum_{t=1}^N \log p_t (\mathbf{s}_t) & \textrm{NLL loss}
\end{align}</script><p><strong>Inference</strong>:  sampling or beam search.</p>
<h1 id="Show-Attend-and-Tell-ICML-2015"><a href="#Show-Attend-and-Tell-ICML-2015" class="headerlink" title="Show, Attend and Tell (ICML 2015)"></a>Show, Attend and Tell (ICML 2015)</h1><p>The model receives a single raw image and generates a caption <script type="math/tex">\mathbf{y}</script> encoded as a sequence of $1$-of-$V$ encoded words.</p>
<script type="math/tex; mode=display">y = \{ \mathbf{y}_1, \cdots, \mathbf{y}_C \}, \mathbf{y}_i \in \mathbb{R}^V</script><p>where $V$ is the vocabulary size and $C$ is the caption length.</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><ul>
<li>Encoder: employ CNNs (Oxford VGGnet) from lower convolutional layers (4-th convolutional layer before max-pooling. 14 x 14 x 512) to extract $L$ (14 x 14 =196) vectors for each image. $D$-dimensional (512) features corresponds to different part of the images.<script type="math/tex; mode=display">a = \{ \mathbf{a}_1, \cdots, \mathbf{a}_L \}, \mathbf{a}_i \in \mathbb{R}^D</script></li>
</ul>
<p><img data-src="/notes/images/ImgCpt-show-attend-and-tell.png" width="60%"/></p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><ul>
<li>Decoder: LSTM<script type="math/tex; mode=display">
\left[\begin{array}{c} \mathbf{i}_t\\ \mathbf{o}_t    \\ \mathbf{f}_t    \\ \mathbf{g}_t \end{array}\right]  = \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  T_{D+m+n, n} \left(\begin{array}{c} \mathbf{E} \mathbf{y}_{t-1}    \\ \mathbf{h}{t-1} \\ \hat{\mathbf{z}_t } \end{array}\right)</script><script type="math/tex; mode=display">\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \mathbf{g}_{t}</script><script type="math/tex; mode=display">\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)</script>where <script type="math/tex">T_{s,t}: \mathbb{R}^s \rightarrow \mathbb{R}^t</script> denotes affine transformation, <script type="math/tex">\mathbf{i}_t</script>, <script type="math/tex">\mathbf{f}_t</script>, <script type="math/tex">\mathbf{c}_t</script>, <script type="math/tex">\mathbf{o}_t</script>, <script type="math/tex">\mathbf{h}_t</script> are the input, forget, memory, output and hidden state of the LSTM, respectively. </li>
</ul>
<p>The context vector<script type="math/tex">\hat{\mathbf{z}_t} \in \mathbb{R}^{D}</script> is calculated as:</p>
<script type="math/tex; mode=display">
\begin{align}
e_{ti} &{}= \color{green}{f_\textrm{att}}(\mathbf{a}_i, \mathbf{h}_{t-1}) \\
\alpha_{ti} &{}= \frac{\exp(e_{ti})}{\sum_{k=1}^L \exp(e_{tk})} \\
\hat{\mathbf{z}_t} &{}= \phi(\{ \mathbf{a}_i \} \{ \alpha_i\})
\end{align}</script><p>where <script type="math/tex">\alpha_i</script> represents the position weight, indicating as either the probability that location $i$ is the right place to focus on, or as relative importance to give to location $i$ in blending the <script type="math/tex">\alpha_i</script>‘s together. “Where” the network looks next, <em>i.e.</em>, <script type="math/tex">\alpha_{ti}</script>, depends on the sequence of words that has already been generated, <em>i.e.</em>, <script type="math/tex">\mathbf{h}_{t-1}</script>.</p>
<p>The initial memory state <script type="math/tex">\mathbf{c}_0</script> and hidden state <script type="math/tex">\mathbf{h}_0</script> of the LSTM are linear projected outputs of an average of annotation vectors <script type="math/tex">\mathbf{a}_i, i=\{1, \cdots, L\}</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{c}_0 &{}= f_c (\frac{1}{L} \sum_{L}^1\mathbf{a}_i) \\
\mathbf{h}_0 &{}= f_c (\frac{1}{L} \sum_{L}^1\mathbf{a}_i) \\
\end{align}</script><p><img data-src="/notes/images/ImgCpt-show-attend-tell-attention-vis.png" alt="upload successful"></p>
<center> Image source: <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S., & Bengio, Y. (2015). [Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf). ICML.
">[2]</span></a></sup> </center>

<ul>
<li>Output: use a deep output layer with LSTM state, context vector and the previous word:<script type="math/tex; mode=display">p(\mathbf{y}_t \vert \mathbf{a}, \mathbf{y}_1^{t-1}) \propto \exp(\mathbf{L}_O (\mathbf{E}\mathbf{y}_{t-1} + \mathbf{L}_h \mathbf{h}_t + \mathbf{L}_z \hat{\mathbf{z}_t}))</script>where <script type="math/tex">\mathbf{L}_O \in \mathbb{R}^{V \times m}, \mathbf{L}_h \in \mathbb{R}^{m \times n}, \mathbf{L}_z \in \mathbb{R}^{m \times D}</script> and $\mathbb{E}$ are learnable parameters initialized randomly.</li>
</ul>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>f<sub>att</sub> has two alternatives: </p>
<ul>
<li>stochastic (hard) attention</li>
<li>deterministic (soft) attention.</li>
</ul>
<h3 id="Stochastic-Hard-Attention"><a href="#Stochastic-Hard-Attention" class="headerlink" title="Stochastic Hard Attention"></a>Stochastic Hard Attention</h3><p>Let the location variable <script type="math/tex">s_t \in \mathbb{R}^L</script> denote where the model to focus on when generating the $t$-th word. <script type="math/tex">s_{t,i}</script> is an indicator one-hot variable where the $i$-th location to focus is set to 1. They assign a multinouli distribution parameterized by <script type="math/tex">\alpha_i</script>. This method requires sampling the attention location <script type="math/tex">s_t</script> at each time $t$.</p>
<p>It is computed as:</p>
<script type="math/tex; mode=display">
\begin{align}
p(s_{t,i} = 1 \vert s_{j<t}, \mathbf{a}) &{}= \alpha_{t,i} \\
\hat{\mathbf{z}_t} &{}= \sum_i s_{t,i} \mathbf{a}_i \\
\end{align}</script><p>The objective function $L$ is defined as a variational lower bound on the marginal log-likelihood <script type="math/tex">\log p(\mathbf{y} \vert \mathbf{a})</script> of obsreving sequence of words $\mathbf{y}$ given image features $\mathbf{a}$. Let $W$ denote the parameters of the model.</p>
<script type="math/tex; mode=display">
\begin{align}
L_s &{}= \sum_s p(s \vert \mathbf{a}) \log p(\mathbf{y} \vert s, \mathbf{a}) \\
&{}\leq \sum_s p(s \vert \mathbf{a}) p(\mathbf{y} \vert s, \mathbf{a})\\
&{}= \log  p(\mathbf{y} \vert \mathbf{a})
\end{align}</script><p>By assuming <script type="math/tex">\tilde{s} \sim \textrm{Multinoulli}_L (\{ \alpha_i \})</script>, the location <script type="math/tex">s_t</script> is calculated by sampling with Monte Carlo method.</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L_s}{\partial W} &=\sum_s p(s \vert \mathbf{a}) \bigg[ \frac{\partial \log p(\mathbf{y} \vert s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \vert s, \mathbf{a}) \frac{\log p(s \vert \mathbf{a})}{\partial W} \bigg] \\
&{} \approx \frac{1}{N} \sum_{n=1}^N \bigg[ \frac{\partial \log p(\mathbf{y} \vert \tilde{s}^n, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \vert \tilde{s}^n, \mathbf{a}) \frac{\log p(\tilde{s}^n \vert \mathbf{a})}{\partial W} \bigg]
\end{align}</script><p>A moveing average baseline is used to reduce the variance in the Monte Carlo estimator:</p>
<script type="math/tex; mode=display">
b_k = 0.9 \times b_{k-1} + 0.1 \times \log p(\mathbf{y} \vert \tilde{s}_k, \mathbf{a})</script><p>Finally, the entropy term is added:</p>
<script type="math/tex; mode=display">
\frac{\partial L_s}{\partial W} \approx \frac{1}{N} \sum_{n=1}^N \bigg[ \frac{\partial \log p(\mathbf{y} \vert \tilde{s}^n, \mathbf{a})}{\partial W} + \color{green}{\lambda_t} (\log p(\mathbf{y} \vert \tilde{s}^n, \mathbf{a})  - b )\frac{\log p(\tilde{s}^n \vert \mathbf{a})}{\partial W} + \color{green}{\lambda_e} \frac{\partial H [\tilde{s}^n]}{\partial W} \bigg]</script><p>where <script type="math/tex">\lambda_r</script> and <script type="math/tex">\lambda_e</script> are discounting factors. </p>
<div class="note info">
            <p>This equation is equivalent to the REINFORCE learning, where the reward for selecting the attention is a real value proportional to the log-likelihood of the target sentence under the sampled attention rollouts.</p>
          </div>
<h3 id="Deterministic-Soft-Attention"><a href="#Deterministic-Soft-Attention" class="headerlink" title="Deterministic Soft Attention"></a>Deterministic Soft Attention</h3><p>Soft attention take the expectation of the context vector $\hat{\mathbf{z}}_t$ directly:</p>
<script type="math/tex; mode=display">\mathbb{E}_{p(s_t \vert a)} [\hat{\mathbf{z}}_t] = \sum_{i=1}^L \alpha_{t,i} \mathbf{a}_i</script><div class="note success">
            <p>Deterministic soft attention can be treated as an approximation to the marginal likelihood over the attention locations.</p>
          </div>
<p>The expectation <script type="math/tex">\mathbb{E}_{p(s_t \vert a)}</script> can be treated as the first order Taylor approximation using a single forward prop. </p>
<p>Let <script type="math/tex">\mathbf{n}_t = \mathbf{L}_O (\mathbf{E}\mathbf{y}_{t-1} + \mathbf{L}_h \mathbf{h}_t + \mathbf{L}_z \hat{\mathbf{z}_t})</script>, <script type="math/tex">\mathbf{n}_{t,i}</script> denote <script type="math/tex">\mathbf{n}_t</script> computed by setting the context vector $\hat{\mathbf{z}}$ value to <script type="math/tex">\mathbf{a}_i</script>. The normalized weighted geometric mean for the softmax $k$-th word prediction:</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{out}[p(y_t=k \vert \mathbf{a})] &{}= \frac{\prod_{i} \exp(n_{t,k,i})^{p(s_{t,i}=1 \vert a)}}{\sum_j \prod_{i} \exp(n_{t,k,i})^{p(s_{t,i}=1 \vert a)}}\\
&{}= \frac{\exp(\mathbb{\mathbf{E}_{p(s_t \vert a)}[n_{t,k}]})}{\sum_j \exp(\mathbb{\mathbf{E}_{p(s_t \vert a)}[n_{t,k}]})}
\end{align}</script><p>It show that the expectation of context vector <script type="math/tex">\mathbb{E} [\mathbf{n}_t] = \mathbf{L}_O (\mathbf{E}\mathbf{y}_{t-1} + \mathbf{L}_h \mathbb{E} [\mathbf{h}_t] + \mathbf{L}_z \mathbb{E} [\hat{\mathbf{z}_t}] )</script>.</p>
<h3 id="Doubly-Stochastic-Attention"><a href="#Doubly-Stochastic-Attention" class="headerlink" title="Doubly Stochastic Attention"></a>Doubly Stochastic Attention</h3><ul>
<li>Encourage <script type="math/tex">\sum_{i} \alpha_{ti} \approx 1</script></li>
<li>Adopt a gating scalar $\beta$ from previsou hidden state <script type="math/tex">\mathbf{h}_{t-1}</script> at each time step $t$<script type="math/tex; mode=display">
\begin{align}
\phi(\{ \mathbf{a}_i \}, \{ \alpha_i \}) &{}= \beta \sum_i^L \alpha_i \mathbf{a}_i \\
\beta_t &{}= \sigma(f_\beta(\mathbf{h}_{t-1}))
\end{align}</script></li>
<li>The model is trained end-to-end by minimizing the penalized negative log-likelihood:<script type="math/tex; mode=display">
L_d = -\log(P(\mathbf{y} \vert \mathbf{x})) + \lambda \sum_i^L (1 - \sum_{t}^C \alpha_{ti})^2</script></li>
</ul>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><ul>
<li>Trained both attention variants using SGD with an adaptive learning rate. They found that RMSProp worked best on Flickr8k, whereas Adam performed better on Flickr30k/MS COCO dataset.</li>
<li>Early stopping on BLEU score, dropout.</li>
<li>MS COCO: &lt; 3 days training on an NVIDIA Titan Black GPU.</li>
<li>Vocabulary size V=10k.</li>
<li>Problems: no public splits on Flickr30k and COCO datasets.</li>
<li>Single model w/o an ensemble.</li>
</ul>
<h1 id="Semantic-Attention-CVPR-2016"><a href="#Semantic-Attention-CVPR-2016" class="headerlink" title="Semantic Attention (CVPR 2016)"></a>Semantic Attention (CVPR 2016)</h1><p>Image captioning methods can be generally divided into two approaches: top-down and bottom-up.</p>
<ul>
<li>The top-down method starts from the image features and converts it into words end-to-end using RNNs. But it is hard to attend to fine details when describing the image.</li>
<li>The bottom-up method is free to operate on any image resolution but lacks end-to-end formulation.</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>Semantic Attention<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="You, Q., Jin, H., Wang, Z., Fang, C., & Luo, J. (2016). [Image Captioning with Semantic Attention](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780872). 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4651-4659.
">[4]</span></a></sup> extracts top-down and bottom-up features from an input image. Firstly, the global visual feature $\mathbf{v}$ is extracted from a classification CNN and a list of visual attributes or concepts <script type="math/tex">\{ A_i \}</script> that are detected using attribute detectors.</p>
<p>$\mathbf{v}$ is only used to initilize the input node <script type="math/tex">\mathbf{x}_0</script>.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{x}_0 &{}= \phi_0 (\mathbf{v}) = \mathbf{W}^{x,v} \mathbf{v}\\
\mathbf{h}_t &{}= \textrm{RNN} (\mathbf{h}_{t-1}, \mathbf{x}_t) \\
Y_t &\sim \mathbf{p}_t = \varphi (\mathbf{h}_t, \{ A_t \}) \\
\mathbf{x}_i &{}= \phi (Y_{t-1}, \{ A_i \}), t>0
\end{align}</script><p><img data-src="/notes/images/ImgCpt-Semantic-Attn.png" width="50%"/></p>
<h2 id="Input-attention-phi"><a href="#Input-attention-phi" class="headerlink" title="Input attention $\phi$"></a>Input attention $\phi$</h2><p>Both <script type="math/tex">Y_{t-1}</script> and <script type="math/tex">A_i</script> correspond to an one-hot entry in dictionay $\mathcal{Y}$, denoting as <script type="math/tex">\mathbf{y}_{t-1}</script> and <script type="math/tex">\mathbf{y}^i</script>, respectively. Let <script type="math/tex">\mathbf{E} \in \mathbf{R}^{d \times \vert \mathcal{Y} \vert}</script> with $d &lt;&lt; \vert \mathcal{Y} \vert$, the relevance score assigned to each detected attribute <script type="math/tex">A_i</script> based on its relevance between the previous predicted word <script type="math/tex">Y_{t-1}</script>:</p>
<script type="math/tex; mode=display">\alpha_t^i \propto \exp (\mathbf{y}_{t-1}^\top\mathbf{E}^\top \mathbf{U}\mathbf{E}\mathbf{y}^i)</script><p>where trainable parameters $\mathbf{U} \in \mathbb{R}^{d \times d}$</p>
<p>The attention score $\alpha$ measures the attention on different attributes. The weighted sum are added to the input space together with previous word:</p>
<script type="math/tex; mode=display">
\mathbf{x}_t = \mathbf{W}^{x, Y} \bigg(\mathbf{E} \mathbf{y}_{t-1} + \textrm{diag} (\mathbf{w}^{x,A}) \sum_{i} \alpha_t^i \mathbf{E} \mathbf{y}^i \bigg)</script><p>where <script type="math/tex">\mathbf{W}^{x, Y} \in \mathbf{R}^{m \times d}</script> is the project matrix, $\mathbf{w}^{x,A} \in \mathbb{R}^d$ models the relative importance of visual attributes in each dimension of the word space.</p>
<h2 id="Output-attention-varphi"><a href="#Output-attention-varphi" class="headerlink" title="Output attention $\varphi$"></a>Output attention $\varphi$</h2><p>Similarly, the score <script type="math/tex">\beta_t^i</script> for each attribute <script type="math/tex">A_i</script> is measured w.r.t <script type="math/tex">\mathbf{h}_t</script>:</p>
<script type="math/tex; mode=display">\beta_t^i \propto \exp(\mathbf{h}_t^\top \mathbf{V} \sigma(\mathbf{E}\mathbf{y}^i))</script><p>The sigmiod activation function $\sigma$ is applied as the output to hidden state in RNN to ensure the same nonlinear transform on compared feature vectors.</p>
<p>The distribution is generated by a linaer transform followed by a softmax normalization:</p>
<script type="math/tex; mode=display">
\mathbf{p}_t \propto \exp(\mathbf{E}^\top \mathbf{W}^{Y,h} (\mathbf{h}_t + \textrm{diag}(\mathbf{w}^{Y,A}) \sum_i \beta_t^i \sigma (\mathbf{E}\mathbf{y}^i)))</script><p>where $\mathbf{W}^{Y,h} \in \mathbf{R}^{d \times n}$ is the projection matrix and $\mathbf{w}^{Y,A} \in \mathbf{R}^n$ models the relative importance of visual attributes in each dimension of the RNN state space.</p>
<h2 id="Model-training"><a href="#Model-training" class="headerlink" title="Model training"></a>Model training</h2><p>The loss function is defined as the NLL loss combined with regularization terms on attention scores <script type="math/tex">\{ \alpha_t^i \}</script> and <script type="math/tex">\{ \beta_t^i \}</script>:</p>
<script type="math/tex; mode=display">
\min -\sum_t \log p(Y_t) + g(\mathbf{\alpha}) + g(\mathbf{\beta})</script><p>where <script type="math/tex">\mathbf{\alpha}</script> and <script type="math/tex">\mathbf{\beta}</script> are attention score matrices with $(t,i)$-th entries of <script type="math/tex">\alpha_t^i</script> and <script type="math/tex">\beta_t^i</script>. The regularization function $g$ is used to enfoce the completeness of attention paid to every attribute in <script type="math/tex">\{ A_i \}</script> as well as the sparsity of attention at any particular time step.</p>
<script type="math/tex; mode=display">
\begin{align}
g(\mathbf{\alpha}) &{}= \Vert \mathbf{\alpha} \Vert_{1,p} + \Vert \mathbf{\alpha}^\top \Vert_{q,1}\\
&{}= \big[ \sum_i [ \sum_t \alpha_t^i]^p \big]^{1/p} + \big[ \sum_t [ \sum_i \alpha_t^i]^q \big]^{1/q} 
\end{align}</script><p>where the first term with $p&gt;1$ penalizes excessive attention paid to any single attribute <script type="math/tex">A_i</script> accumulated over the entire sentence, and the second term $0&lt;q&lt;1$ penalizes diverted attention to multiple attributes at any particular time.</p>
<h1 id="SCA-CNN-CVPR-2017"><a href="#SCA-CNN-CVPR-2017" class="headerlink" title="SCA-CNN (CVPR 2017)"></a>SCA-CNN (CVPR 2017)</h1><p><strong>Motivation</strong>: </p>
<ol>
<li>low-layer filters detect low-level visual cues like edges and corners, while higher-level ones extract abstract semantic patterns like objects.</li>
<li>CNN extractors output a hierarchy of visual abstractions, which is spatial, channel-wise, and multi-layer. Previous work only takes into account the spatial characteristics, regardless of the channel-wise and multi-layer information.</li>
<li>SCA-CNN takes full advantage of such three characteristics of CNN features.</li>
</ol>
<h2 id="Spatial-and-Channel-wise-Attention-CNN"><a href="#Spatial-and-Channel-wise-Attention-CNN" class="headerlink" title="Spatial and Channel-wise Attention CNN"></a>Spatial and Channel-wise Attention CNN</h2><p>Spatial and Channel-wise Attention-based Convolutional Neural Network (SCA-CNN)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., & Chua, T. (2017). [SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8100150). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 6298-6306.
">[5]</span></a></sup> applies channel-wise attention and spatial attention at multiple layers.</p>
<p>At $l$-th layer, the spatial and channel-wise attention weights $\gamma^l$ are function of LSTM memory <script type="math/tex">\mathbf{h}_{t-1} \in \mathbb{R}^d</script> and input CNN features $\mathbf{V}^l$, where $d$ is the dimension of hidden state. SCA-CNN modulates <script type="math/tex">\mathbf{V}^l</script> using the spatial and channel attention weights $\gamma^l$ as follows:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{V} &{}= \textrm{CNN} (\mathbf{X}^{l-1}) \\
\gamma^l &{}= \Phi(\mathbf{h}_{t-1}, \mathbf{V}^l)\\
\mathbf{X}_l &{}= f(\mathbf{V}^l, \gamma^l)
\end{align}</script><p>where <script type="math/tex">\mathbf{X}^l</script> is the modulated feature, $\pmb{\Phi}(\cdot)$ is the spatial and channel-wise attention function, $f(\cdot)$ is a linear weighting function that modulates CNN features and attention weights.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{h}_t &{}= \textrm{LSTM}(\mathbf{h}_{t-1}, \mathbf{X}^L, y_{t-1})\\
y_t &\sim p_t = \textrm{softmax}(\mathbf{h}_t, y_{t-1})
\end{align}</script><p>The spatial attention weights $\alpha^l$ and channel-wise attention weights $\beta^l$ are learned separately:</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha^l &{}= \mathbf{\Phi}_s (\mathbf{h}_{t-1}, \mathbf{V}^l) \\
\beta^l &{}= \mathbf{\Phi}_c (\mathbf{h}_{t-1}, \mathbf{V}^l)
\end{align}</script><p>where <script type="math/tex">\mathbf{\Phi}_s</script> and <script type="math/tex">\mathbf{\Phi}_c</script> represent spatial and channel-wise model respectively, having the cost of <script type="math/tex">\mathcal{O}(W^lH^lk)</script> for spatial attention and $\mathcal{O}(C^lk)$ for channel-wise attention. $W$,$H$,$C$, $k$ represent the width, height, channel and mapping dimension.<br><img data-src="/notes/images/SCA-CNN.png" width="80%"/></p>
<h2 id="Spatial-Attention"><a href="#Spatial-Attention" class="headerlink" title="Spatial Attention"></a>Spatial Attention</h2><p>CNN features <script type="math/tex">\mathbf{V}=[\mathbf{v}_1, \mathbf{v}_3, \cdots ,\mathbf{v}_m]</script> is flattened features along width and height, where <script type="math/tex">\mathbf{v}_i \in \mathbb{R}^C</script>, and $m=W \cdot H$. <script type="math/tex">\mathbf{v}_i</script> is considered as the visual feature of the $i$-th location. Given the previous hidden state <script type="math/tex">\mathbf{h}_{t-1}</script>, a single-layer fully-connected layer followed by a softmax is applied to generate attention distributions $\alpha$ over the image regions.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{a} &{}= \tanh \big( (\mathbf{W}_s \mathbf{V} + b_s) \oplus \mathbf{W}_{hs}\mathbf{h}_{t-1}  \big) \\
\alpha &{}= \textrm{softmax}(\mathbf{W}_i \mathbf{a} + b_i)
\end{align}</script><p>where <script type="math/tex">\mathbf{W}_s \in \mathbb{R}^{k \times C}</script>, <script type="math/tex">\mathbf{W}_hs \in \mathbb{R}^{k \times d}</script>, <script type="math/tex">\mathbf{W}_i \in \mathbb{R}^{k}</script> are trainable matrices to obtain the same dimension $k$, $\oplus$ is the addition between a matrice and a vector, model biases <script type="math/tex">b_s \in \mathbb{R}^k, b_i \in \mathbb{R}</script>.</p>
<h2 id="Channel-wise-Attention"><a href="#Channel-wise-Attention" class="headerlink" title="Channel-wise Attention"></a>Channel-wise Attention</h2><p>Each CNN filter is a pattern detector on images, and each channel of a feature map in CNN is a response activation of the corresponding convolutional filter. Applying channel-wise attention mechanisms can be treated as selecting semantic attributes.</p>
<p>Firstly, CNN features $\mathbf{V}$ is reshaped to $\mathbf{U} = [\mathbf{u}_1, \mathbf{u}_2,\cdots, \mathbf{u}_C]$, where <script type="math/tex">\mathbf{u}_i \in \mathbb{R}^{W \times H}</script> represents the $i$-th channel of the feature map $\mathbf{V}$, $C$ is the channel number. The mean-pooling for each channel is applied to obtain the channel feature $\mathbf{v}$:</p>
<script type="math/tex; mode=display">
\mathbf{v} = [v_1, v_2, \cdots, v_C ], \quad \mathbf{v} \in \mathbb{R}^C</script><p>where scalar <script type="math/tex">v_i</script> is the mean of vector <script type="math/tex">\mathbf{u}_i</script>, which represents the $i$-th channel features. </p>
<p>The channel-wise attention model $\Phi_c$ can be defined as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{b} &{}= \tanh \big( (\mathbf{W}_c \otimes \mathbf{v} + b_c) \oplus \mathbf{W}_{hc} \mathbf{h}_{t-1} \big) \\
\beta &{}= \textrm{softmax} (\mathbf{W}^\prime_i \mathbf{b} + b^\prime_i)
\end{align}</script><p>where <script type="math/tex">\mathbf{W}_c \in \mathbb{R}^k, \mathbf{W}_{hc} \in \mathbb{R}^{k \times d}, \mathbf{W}^\prime_i \in \mathbb{R}^k</script> are trainable parameters, $\otimes$ represents the outer product of vectors, biases <script type="math/tex">b_c \in \mathbb{R}^k, b^\prime_i \in \mathbb{R}</script>.</p>
<h3 id="Channel-Spatial"><a href="#Channel-Spatial" class="headerlink" title="Channel-Spatial"></a>Channel-Spatial</h3><p>Apply channel-wise attention followed by feature map $\mathbf{X}$.</p>
<script type="math/tex; mode=display">
\begin{align}
\beta &{} = \Phi_c (\mathbf{h}_{t-1}, \mathbf{V}) \\
\alpha &{}= \Phi_s (\mathbf{h}_{t-1}, f_c (\mathbf{V}, \beta)) \\
\mathbf{X} &{}= f(\mathbf{V}, \alpha, \beta)
\end{align}</script><p>where <script type="math/tex">f_c(\cdot)</script> is a channel-wise multiplication for feature map channels and corresponding channel weights.</p>
<h1 id="Adaptive-Attention-CVPR-2017"><a href="#Adaptive-Attention-CVPR-2017" class="headerlink" title="Adaptive Attention (CVPR 2017)"></a>Adaptive Attention (CVPR 2017)</h1><p><strong>Motivation</strong>:</p>
<ol>
<li>Most attention-based methods force visual attention to be active for each generated word. However, not all words have corresponding visual signals. </li>
<li>Decoders require little to predict words like “the”/“of”. Besides, other words that may be predicted reliably just from the language model, <em>e.g.,</em>, “sign” after “behind a red stop” or “phone” following “talk on a cell”.</li>
</ol>
<p><img data-src="/notes/images/adaptive-attn.png" width="80%"/></p>
<p>Adaptive attention with a “visual sentinel”<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lu, J., Xiong, C., Parikh, D., & Socher, R. (2017). [Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8099828). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3242-3250.
">[6]</span></a></sup> is proposed to decide when to rely on the visual signals and when to just rely on the language model.</p>
<h2 id="Spatial-Attention-1"><a href="#Spatial-Attention-1" class="headerlink" title="Spatial Attention"></a>Spatial Attention</h2><p>A spatial attention (fig. (b)) is uesd to compute the context vector <script type="math/tex">\mathbf{c}_t</script> as:</p>
<script type="math/tex; mode=display">\mathbf{c}_t = g(\mathbf{V}, \mathbf{h}_t)</script><p>where $g$ is the attention function, <script type="math/tex">\mathbf{V}=[\mathbf{v}_1, \cdots, \mathbf{v}_k], \mathbf{v}_i \in \mathbb{R}^d</script> is the $d$-dimensional spatial image feature, <script type="math/tex">\mathbf{h}_t \in \mathbb{R}^d</script> is the hidden state of RNN at time $t$.</p>
<p><img data-src="/notes/images/Spatial-attn.png" width="60%"/></p>
<center> (a) Soft attention  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) Spatial attention </center>

<p>As in fig. (b), given the spatial image feature <script type="math/tex">\mathbf{V} \in \mathbb{R}^{d \times k}</script> and <script type="math/tex">\mathbf{h}_t</script>, the context vector can be computed as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{z}_t &{}= \mathbf{w}^\top_h \tanh \big(\mathbf{W}_v \mathbf{V} + (\mathbf{W}_g \mathbf{h}_t) \mathbb{I}^\top \big) \\
\mathbf{\alpha}_t &{}= \textrm{softmax} (\mathbf{z}_t)\\
\mathbf{c}_t &{}= \sum_{i=1}^k \mathbf{\alpha}_{ti} \mathbf{v}_{ti} \\
\log p(t_t \vert y-1, \cdots, y_{t-1}, \mathbf{I}) &{}= f(\mathbf{h}_t, \mathbf{c}_t)
\end{align}</script><p>where $\mathbb{I} \in \mathbb{R}^k$ is a vector with all elements set to 1, <script type="math/tex">\mathbf{W}_v,\mathbf{W}_v \in \mathbb{R}^{k \times d}</script>, <script type="math/tex">\mathbf{w}_h \in \mathbb{R}^k</script> are trainable parameters. $\mathbf{\alpha} \in \mathbb{R}^k$ is the attention weight over features in $\mathbf{V}$. $\mathbf{I}$ is the input image.</p>
<p>It uses the current hidden state rather than the previous one to generate the context vector, which can be treated as the residual visual information of current hidden state <script type="math/tex">\mathbf{h}_t</script>, diminishing the uncertainty or complements the informativeness of the current hidden state for next word prediction.</p>
<h2 id="Adaptive-Attention"><a href="#Adaptive-Attention" class="headerlink" title="Adaptive Attention"></a>Adaptive Attention</h2><p>Aforementioned spatial attention cannot determine when to leverage visual signals or language models. The visual sentinel vector <script type="math/tex">\mathbf{s}_t</script> is extended on LSTM:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{g}_t &{}= \sigma (\mathbf{W}_x \mathbf{x}_t + \mathbf{W}_h \mathbf{h}_{t-1}) \\
\mathbf{s}_t &{}= \mathbf{g}_t \odot \tanh (\mathbf{m}_t) \\
\hat{\mathbf{c}_t} &{}= \beta_t \mathbf{s}_t + (1- \beta_t) \mathbf{c}_t
\end{align}</script><p>where the new sentinel gate at time $t$ <script type="math/tex">\beta_t</script> controls the trade-off beween the image information and decoder memory.</p>
<p>The new sentinel gate <script type="math/tex">\beta_t</script> is computed as:</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\mathbf{\alpha}}_t &{}= \textrm{softmax}([ \mathbf{z}_t ; \mathbf{w}_h^\top \tanh \big(\mathbf{W}_s \mathbf{s}_t + \mathbf{W}_g \mathbf{h}_t \big) ]) \\
\beta_t &{}= \hat{\mathbf{\alpha}}_t [k+1]
\end{align}</script><p>where <script type="math/tex">\hat{\mathbf{\alpha}}_t \in \mathbb{R}^{k+1}</script> is the attention distribution over both the spatial image feature and visual sentinel vector. In which the last element serves as the gate value <script type="math/tex">\beta_t</script>.</p>
<p><img data-src="/notes/images/Adaptive-Attn-Model.png" width="60%"/></p>
<p>The probability over vocabulary at time $t$ is:</p>
<script type="math/tex; mode=display">\mathbf{p}_t = \textrm{softmax} \big( \mathbf{W}_p (\hat{\mathbf{c}}_t + \mathbf{h}_t) \big)</script><h1 id="Semantic-Compositional-Networks-CVPR-2017"><a href="#Semantic-Compositional-Networks-CVPR-2017" class="headerlink" title="Semantic Compositional Networks (CVPR 2017)"></a>Semantic Compositional Networks (CVPR 2017)</h1><p><strong>Motivation</strong>: LSTM-based generation is quite limited: it only uses semantic concepts through soft attention or initialization at the first step.</p>
<h2 id="Semantic-Compositional-Network"><a href="#Semantic-Compositional-Network" class="headerlink" title="Semantic Compositional Network"></a>Semantic Compositional Network</h2><p>Semantic Compositional Networks (SCN)<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., & Deng, L. (2017). [Semantic Compositional Networks for Visual Captioning](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8099610). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1141-1150.
">[7]</span></a></sup> detect the <strong>semantic concepts</strong>, <em>i.e.</em>, tags, from each input image. It uses the $K$ most common words in the training captions to determine the vocabulary of tags, including most frequent nouns, verbs, or adjectives. </p>
<p>The tag detection can be cast as a <strong>multi-label classification</strong> task. Given $N$ training examples, <script type="math/tex">\mathbf{y}_i = [ y_{i1}, \cdots, y_{iK}] \in \{ 0,1 \}^K</script> is the label’s dummy encoding of $i$-th image, wherein 1 and 0 indicate annotation or not respectively. Let <script type="math/tex">\mathbf{v}_i</script> and <script type="math/tex">\mathbf{s}_i</script> be the image feature vector and semantic feature vector of the $i$-th image, the cost function is:</p>
<script type="math/tex; mode=display">
\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K \big( y_{ik} \log s_{ik} + (1- y_{ik}) \log (1-s_{ik}) \big)</script><p>where <script type="math/tex">\mathbf{s}_{i} = \sigma \big( \textrm{MLP}(\mathbf{v}_i) \big)</script> is a $K$-dimensional vector with <script type="math/tex">\mathbf{s}_i = [ s_{i1},\cdots,s_{iK} ]</script>.</p>
<p><img data-src="/notes/images/ImgCpt-SCN-model.png" width="80%"/></p>
<h2 id="SCN-RNN"><a href="#SCN-RNN" class="headerlink" title="SCN-RNN"></a>SCN-RNN</h2><p>SCN injects the tag-dependent matrices:</p>
<script type="math/tex; mode=display">
\begin{align}
\tilde{\mathbf{x}}_{t-1} &{}= \mathbf{W}_b \mathbf{s} \odot \mathbf{W}_c \mathbf{x}_{t-1} \\
\tilde{\mathbf{h}}_{t-1} &{}= \mathbf{U}_b \mathbf{s} \odot \mathbf{U}_c \mathbf{h}_{t-1}\\
\mathbf{z} &{}= \mathbb{I}(t=1) \cdot \mathbf{C}\mathbf{v}\\
\mathbf{h}_t &{}= \sigma ( \mathbf{W}_a \tilde{\mathbf{x}}_{t-1} + \mathbf{U}_a \tilde{\mathbf{h}}_{t-1} + \mathbf{z} )
\end{align}</script><p>where <script type="math/tex">\mathbf{x}_t</script> is the $t$-th word in the generated caption, <script type="math/tex">\mathbf{x}_0</script> is defined as the ‘BOS’ token, $\odot$ is the Hadamard product. Trainable parameters <script type="math/tex">\{ \mathbf{W}_a, \mathbf{U}_a \} \in \mathbf{R}^{d \times d^\prime}</script>, <script type="math/tex">\{ \mathbf{W}_b, \mathbf{U}_b \} \in \mathbf{R}^{d^\prime \times K}</script>, where $d$ is the hidden dimension, $d^\prime$ is the number of factors. <script type="math/tex">\mathbf{W}_a</script> and <script type="math/tex">\mathbf{W}_b</script> are shared among all captions, capturing common linguistic patterns; <script type="math/tex">\mathbf{W}_b \mathbf{s}</script> accounts for semantic aspects of the image captured by $\mathbf{s}$.</p>
<p><img data-src="/notes/images/ImgCpt-SCN-RNN.png" width="50%"/></p>
<h2 id="SCN-LSTM"><a href="#SCN-LSTM" class="headerlink" title="SCN-LSTM"></a>SCN-LSTM</h2><p>SCN-RNN can be generalized using LSTM units:</p>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}_t \\ \mathbf{o}_t    \\ \mathbf{f}_t    \\ \tilde{c}_t \end{array}\right]  &{}= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \color{red}{\sigma} \end{array}\right]  (\mathbf{W}_{a} \tilde{\mathbf{x}}_{i, t-1} + \mathbf{U}_{a} \mathbf{h}_{t-1} + \mathbf{z}) \\
\mathbf{c}_t &{}= \mathbf{i}_t \odot \tilde{\mathbf{c}}_t + \mathbf{f}_t \odot \mathbf{c}_{t-1} \\
\mathbf{h}_{t} &{}= \mathbf{o}_t \odot \tanh (\mathbf{c}_t)
\end{align}</script><p>where <script type="math/tex">\mathbf{z}=\mathbb{I} (t=1) \cdot \mathbf{C}\mathbf{v}</script>. For $\star = i,f,o,c$, we define</p>
<script type="math/tex; mode=display">
\begin{align}
\tilde{\mathbf{x}}_{\star, t-1} &{}= \mathbf{W}_{\star b }\mathbf{s} \odot \mathbf{W}_{\star c}\mathbf{x}_{t-1} \\
\tilde{\mathbf{h}}_{\star, t-1} &{}= \mathbf{U}_{\star b }\mathbf{s} \odot \mathbf{U}_{\star c}\mathbf{x}_{t-1}
\end{align}</script><h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><p>Given image $\mathbf{I}$ and corresponding caption $\mathbf{X}$, the objective function is defined as:</p>
<script type="math/tex; mode=display">
\log p(\mathbf{X} \vert \mathbf{I}) = \sum_{t=1}^T p(\mathbf{x}_0, \cdots, \mathbf{x}_{t-1}, \mathbf{v}, \mathbf{s})</script><p>Averaged objectives among all (image, caption) pairs are used during training.</p>
<h1 id="Up-Down-Attention-CVPR-2018"><a href="#Up-Down-Attention-CVPR-2018" class="headerlink" title="Up-Down Attention (CVPR 2018)"></a>Up-Down Attention (CVPR 2018)</h1><p>Up-Down Attention<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., & Zhang, L. (2017). [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578734). 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6077-6086.
">[8]</span></a></sup> combines the bottom-up (based on Faster R-CNN<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ren, S., He, K., Girshick, R.B., & Sun, J. (2015). [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf). IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 1137-1149.
">[9]</span></a></sup>), a top-down attention mechanism to attend to attention at the level of objects and other salient image regions. <code>Top-down</code> uses the non-visual or task-specific contexts to predict an attention distribution over image regions using ResNet-101<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="He, K., Zhang, X., Ren, S., & Sun, J. (2016). [Deep Residual Learning for Image Recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780459). 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778.
">[10]</span></a></sup>, whereas <code>bottom-up</code>  proposes a set of salient image regions, wich each region represented by a pooled convolutional feature vector using Faster R-CNN.</p>
<p><img data-src="/notes/images/ImgCpt-Up-Down-attn-region.png" width="40%"/></p>
<p>As shown in the left figure, the input regions correspond to a uniform grid of equally sized and shaped neural receptive fields, irrespective of the content of the image. In contrast, the right focuses on the objects and salient image regions for attention.</p>
<h2 id="Bottom-Up-attention"><a href="#Bottom-Up-attention" class="headerlink" title="Bottom-Up attention"></a>Bottom-Up attention</h2><p>All regions whose class detection probability exceeds a confidence threshold are selected<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ren, S., He, K., Girshick, R.B., & Sun, J. (2015). [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf). IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 1137-1149.
">[9]</span></a></sup>. For each selected region $i$, <script type="math/tex">\mathbf{v}_i</script> is defined as the mean-pooled convolutional feature from this region.</p>
<h2 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h2><p>A top-down attention LSTM followed by a language model (LM) LSTM is used to generate captions.</p>
<h3 id="Attention-LSTM"><a href="#Attention-LSTM" class="headerlink" title="Attention LSTM"></a>Attention LSTM</h3><p>Let superscript denotes the layer number, <em>i.e.</em>, $\mathbf{h}^1$ indicates the hidden state in the first LSTM. The top-down attention LSTM receives the concatenated previous output of LM LSTM <script type="math/tex">\mathbf{h}^2_{t-1}</script>, mean-pooled image feature <script type="math/tex">\bar{\mathbf{v}}=\frac{1}{k}\sum_i \mathbf{v}_i</script> and the previous generated word vector <script type="math/tex">\mathbf{x}_t = \mathbf{W}_e \Pi_t</script>, where <script type="math/tex">\mathbf{W}_e \in \mathbb{R}^{\vert V \vert \times D}</script> is the word embedding matrix for vocabulary $V$, and <script type="math/tex">\Pi_t</script> is one-hot encoding of the input word at timestep $t$.</p>
<script type="math/tex; mode=display">\mathbf{x}_t^1 = [ \mathbf{h}_{t-1}^2, \bar{\mathbf{v}}, W_e \Pi_t ]</script><p>The output <script type="math/tex">\mathbf{h}_t^1</script> of the attention LSTM, a normalized attention weight <script type="math/tex">\alpha_{i,t}</script> for each of the $k$ image features <script type="math/tex">\mathbf{v}_i</script> at each time step $t$:</p>
<script type="math/tex; mode=display">
\begin{align}
a_{i,t} &{}= \mathbf{w}_a^\top \tanh \big( \mathbf{W}_{va}\mathbf{v}_i +  \mathbf{W}_{ha} \mathbf{h}_t^1 \big) \\
\pmb{\alpha}_1 &{}= \textrm{softmax} (\mathbf{a}_t) \\
\hat{\mathbf{v}}_t &{}= \sum_{i=1}^K \alpha_{i,t} \mathbf{v}_i
\end{align}</script><p>where <script type="math/tex">\hat{\mathbf{v}}_t</script> is the input to language LSTM, <script type="math/tex">\mathbf{W}_{va} \in \mathbb{R}^{H \times V}, \mathbf{W}_{ha} \in \mathbb{R}^{H \times M}, \mathbf{w}_{a} \in \mathbb{R}^H</script> are learnable parameters.</p>
<p><img data-src="/notes/images/ImgCpt-Up-Down-Decoder.png" alt="upload successful"></p>
<h3 id="Language-LSTM"><a href="#Language-LSTM" class="headerlink" title="Language LSTM"></a>Language LSTM</h3><p>The input to LM LSTM is concatated image features and attention LSTM output:</p>
<script type="math/tex; mode=display">\mathbf{x}_t^2 = [ \hat{\mathbf{v}}_t, \mathbf{h}_t^1 ]</script><p>The predicted caption sequences <script type="math/tex">y_{1:T}=(y_1, \cdots, y_T)</script>:</p>
<script type="math/tex; mode=display">p(y_t \vert y_{1:t-1}) = \textrm{softmax} (\mathbf{W}_p \mathbf{h}_t^2 + \mathbf{b}_p )</script><p>where <script type="math/tex">\mathbf{W}_p \in \mathbb{R}^{\vert V \vert \times M}</script> and <script type="math/tex">\mathbf{b}_p \in \mathbb{R}^{\vert V \vert}</script> are learnable weights and biases.</p>
<p>The probability of generated captions is:</p>
<script type="math/tex; mode=display">p(y_{1:T}) = \prod_{t=1}^T p(y_t \vert y_{1: t-1})</script><h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><ol>
<li><p>Cross-entropy<br>Given the ground truth sequence <script type="math/tex">t_{1:T}^*</script>, the corss entropy loss is:</p>
<script type="math/tex; mode=display">\mathcal{L}_{ce} (\theta) = - \sum_{t=1}^T \log \big( p_\theta(y_t^* \vert y_{1:t-1}^* ) \big)</script></li>
<li><p>Negative expected score</p>
<script type="math/tex; mode=display">
\mathcal{L}_r (\theta) = - \mathbb{E}_{y_{1:T \sim p_\theta}}[r(y_{1:T})]</script><p>where $r$ is the score function (<em>e.g.</em>, CIDEr).</p>
</li>
</ol>
<h1 id="Stylized-Image-Captioning"><a href="#Stylized-Image-Captioning" class="headerlink" title="Stylized Image Captioning"></a>Stylized Image Captioning</h1><h2 id="StyleNet-CVPR-2017"><a href="#StyleNet-CVPR-2017" class="headerlink" title="StyleNet (CVPR 2017)"></a>StyleNet (CVPR 2017)</h2><p><strong>Motivation</strong>:</p>
<ul>
<li>Previous works on image captioning all generate the factual description of the image content while overlooking the style of generated captions. Stylized descriptions can greatly enrich the expressibility and attractiveness of the caption.</li>
<li>Application: people always struggle to come up with an attractive title when uploading images to a social media platform. Stylized captioning can be a helpful solution.</li>
</ul>
<p><img data-src="/notes/images/ImgCpt-StyleNet.png" width="80%"/></p>
<h3 id="Factored-LSTM"><a href="#Factored-LSTM" class="headerlink" title="Factored LSTM"></a>Factored LSTM</h3><p>StyleNet<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Gan, C., Gan, Z., He, X., Gao, J., & Deng, L. (2017). [StyleNet: Generating Attractive Visual Captions with Styles](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8099591). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 955-964.
">[11]</span></a></sup> proposed the <code>Factored LSTM</code> to memorize the languge style pattern, by factorizing the parameters <script type="math/tex">\mathbf{W}_x \in \mathbb{R}^{M \times N}</script> in standard LSTMs into three matrices <script type="math/tex">\mathbf{U}_x \in \mathbb{R}^{M \times E}, \mathbf{S}_x \in \mathbb{R}^{E \times E}, \mathbf{V}_x \in \mathbb{R}^{E \times N}</script>. But it retain the weight parameters of recurrent connections <script type="math/tex">\mathbf{W}_{ih}, \mathbf{W}_{fh}, \mathbf{W}_{oh}, \mathbf{W}_{ch}</script>, which captures the long span syntactic dependency of the text.</p>
<p>The Factored LSTM are defined as:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{i}_t &{}= \sigma (\mathbf{U}_{ix}\mathbf{S}_{ix}\mathbf{V}_{ix} \mathbf{x}_t + \mathbf{W}_{ih}\mathbf{h}_{t-1}) \\
\mathbf{f}_t &{}= \sigma (\mathbf{U}_{fx}\mathbf{S}_{fx}\mathbf{V}_{fx} \mathbf{x}_t + \mathbf{W}_{fh}\mathbf{h}_{t-1}) \\
\mathbf{o}_t &{}= \sigma (\mathbf{U}_{ox}\mathbf{S}_{ox}\mathbf{V}_{ox} \mathbf{x}_t + \mathbf{W}_{oh}\mathbf{h}_{t-1}) \\
\tilde{\mathbf{c}}_t &{}= \tanh (\mathbf{U}_{cx}\mathbf{S}_{cx}\mathbf{V}_{cx} \mathbf{x}_t + \mathbf{W}_{ch}\mathbf{h}_{t-1}) \\
\mathbf{c}_t &{}= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_{t}\\
\mathbf{h}_t &{}= \mathbf{o}_t \odot \mathbf{c}_t \\
\mathbf{p}_{t+1} &{}= \textrm{softmax} (\mathbf{C}\mathbf{h}_t)
\end{align}</script><p>where ${ \mathbf{U}, \mathbf{V}, \mathbf{W} }$ are shared among different styles. But $\mathbf{S}$ is style-specific.</p>
<h3 id="Training-StyleNet"><a href="#Training-StyleNet" class="headerlink" title="Training StyleNet"></a>Training StyleNet</h3><p>Two tasks:</p>
<ol>
<li>Firstly, train the factored LSTM model to generate factual captions given paired images.</li>
<li>Train factored LSTM as language model on stylized language corpus, but only update the style-specific matrix $\mathbf{S}$.</li>
</ol>
<h2 id="SemStyle-CVPR-2018"><a href="#SemStyle-CVPR-2018" class="headerlink" title="SemStyle (CVPR 2018)"></a>SemStyle (CVPR 2018)</h2><p>SemStyle<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mathews, A.P., Xie, L., & He, X. (2018). [SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578994). 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8591-8600.
">[12]</span></a></sup> proposed a <strong>term generator</strong> by generating an ordered term sequence of image semantics, and a <strong>language generator</strong> trained on styled text data.</p>
<p><img data-src="/notes/images/Img-Cpt-SemStyle.png" width="80%"/></p>
<h3 id="Semantic-term"><a href="#Semantic-term" class="headerlink" title="Semantic term"></a>Semantic term</h3><p>Given a setence <script type="math/tex">\mathbf{w} = \{ w_1, w_2, \cdots, w_n \}, w_i \in \mathcal{V}^\text{in}</script>, a set of rules is defined to get ordered semantic terms <script type="math/tex">\mathbf{x} = \{ x_1, x_2, \cdots, x_M \}, x_i \in \mathcal{V}^\text{term}</script>. The rules are as:</p>
<ol>
<li>Filtering non-semantic words</li>
<li>lemmatization and tagging using spaCy.</li>
<li>Verb abstraction. Use semantic role labeling tool SEMAFOR to annotate frames and reduce frame vocabulary.</li>
</ol>
<h3 id="Term-generator"><a href="#Term-generator" class="headerlink" title="Term generator"></a>Term generator</h3><p>Use CNN+GRU to generate semantic terms collected above. The greedy search decoding is used to recover the term sequence from the conditional probabilities. Given input image $I$,</p>
<script type="math/tex; mode=display">x_{i+1} = \arg\max_{j \in \mathcal{V}^\text{term}} P(x_{i=1}=j \vert I, x_i, \cdots, x_1)</script><p>where <script type="math/tex">x_1</script> is the ‘BOS’ token.</p>
<h3 id="Language-generator"><a href="#Language-generator" class="headerlink" title="Language generator"></a>Language generator</h3><p>A bi-GRU is used to encode the semantic terms $x$’s, and concatenate the forward and backward hidden states as outputs: <script type="math/tex">\mathbf{h}_(\text{enc}, i) = [\mathbf{h}_(\text{fw},i); \mathbf{h}_(\text{bw},i)]</script>. The last state is used to initialize the first hidden state of decoder: <script type="math/tex">\mathbf{h}_{(\text{dec}, 0)} = \mathbf{h}_{(\text{enc}, M)}</script>.</p>
<p>The context vector at step $t$ is computed with bi-linear attention:</p>
<script type="math/tex; mode=display">
\begin{align}
v_{t,i} &{}= \mathbf{h}_{\text{enc,i}}^\top \mathbf{W}_a \mathbf{h}_{\text{dec},t} \\
a_{t,i} &{}= \frac{\exp (v_{t,i})}{\sum_{j=1}^M \exp (v_{t,j})}\\
\mathbf{c}_t &{}= \sum_{i=1}^M a_{t,i} \mathbf{h}_{\text{enc},i}
\end{align}</script><p>The output uses a NLP with softmax non-linearity:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{h}^\text{out}_{t} &{}= \mathbf{W}^\text{out}[\mathbf{c}_t, \mathbf{h}^\text{dec}_{t}] + \mathbf{b}^\text{out}\\
p(y_t =k \vert \mathbf{x}) &{}= \frac{\exp(h^\text{out}_{t,k})}{  \sum_{j=1}^{\vert \mathcal{V}^\text{out} \vert} \exp(h^\text{out}_{t,j})}
\end{align}</script><h3 id="Training-2"><a href="#Training-2" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>Train the term generator on factual descriptions, using mean categorical cross entropy over semantic terms:</p>
<script type="math/tex; mode=display">\mathcal{L} = -\frac{1}{M} \sum_{i=1}^M \log p(x_i = \hat{x}_i \vert I,\hat{x}_{i-1}, \cdots,\hat{x}_{1} )</script></li>
<li><p>Train the language generator on both styled and descriptive sentences.</p>
</li>
</ul>
<h2 id="“Factual”-or-“Emotional”-ECCV-2018"><a href="#“Factual”-or-“Emotional”-ECCV-2018" class="headerlink" title="“Factual” or “Emotional” (ECCV 2018)"></a>“Factual” or “Emotional” (ECCV 2018)</h2><h3 id="Style-factual-LSTM"><a href="#Style-factual-LSTM" class="headerlink" title="Style-factual LSTM"></a>Style-factual LSTM</h3><p>Two set of matrices are used in style-factual LSTM:</p>
<script type="math/tex; mode=display">
\begin{align}
i_t &{}= \sigma \big( (g_{xt} S_{xi} + (1- g_{xt})W_{xi})x_t + (g_{ht} S_{hi} + (1-g_{ht})W_{hi})h_{t-1} + b_i \big) \\
f_t &{}= \sigma \big( (g_{xt} S_{xf} + (1- g_{xt})W_{xf})x_t + (g_{ht} S_{hf} + (1-g_{ht})W_{hf})h_{t-1} + b_f \big) \\
o_t &{}= \sigma \big( (g_{xt} S_{xo} + (1- g_{xt})W_{xo})x_t + (g_{ht} S_{ho} + (1-g_{ht})W_{ho})h_{t-1} + b_o \big) \\
\tilde{c}_t &{}= \phi \big( (g_{xt} S_{xc} + (1- g_{xt})W_{xc})x_t + (g_{ht} S_{hc} + (1-g_{ht})W_{hc})h_{t-1} + b_c \big) \\
c_t &{}= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
h_t &{}= o_t \odot \phi(c_t)
\end{align}</script><p>where style-related matrices <script type="math/tex">g_{xt}</script> and <script type="math/tex">g_{ht}</script> controls to predict word based on <script type="math/tex">W_x</script> ($\approx 0$), or a styled word ($\approx 1$) <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, T., Zhang, Z., You, Q., Fang, C., Wang, Z., Jin, H., & Luo, J. (2018). ["Factual" or "Emotional": Stylized Image Captioning with Adaptive Learning and Attention](http://openaccess.thecvf.com/content_ECCV_2018/papers/Tianlang_Chen_Factual_or_Emotional_ECCV_2018_paper.pdf). ECCV.
">[13]</span></a></sup>.</p>
<p><img data-src="/notes/images/Style-factual-LSTM.png" width="60%"/></p>
<h3 id="Training-3"><a href="#Training-3" class="headerlink" title="Training"></a>Training</h3><p>Two stages:</p>
<ol>
<li>At the first stage, fix <script type="math/tex">g_{xt}=g_{h_t}=0</script> and freeze the style-related matrices <script type="math/tex">S_x</script> and <script type="math/tex">S_h</script>. The model is trained using paired factual captioning datasets with MLE loss.</li>
<li>At 2nd stage, train the model on paried stylized captioning datasets, but update <script type="math/tex">S_x</script> and <script type="math/tex">S_h</script> for style-factual LSTM, and fix <script type="math/tex">W_x, W_h</script>. The loss for this stage is designed as:<script type="math/tex; mode=display">
\begin{align}
\mathbb{KL}(P_s^t \Vert P_r^t) &{}= \sum_{w in W} P_s^t(w) \log \frac{P_s^t(w)}{P_r^t(w)} \\
g_{ip}^t &{}= P_s^t \cdot P_r^t \\
\mathcal{L} &{}= \sum_{t=1}^T -(1- g_{ip}^t) \log P_s^t (y_t) + \alpha \cdot \sum_{t=1}^T g_{ip}^t \mathbb{KL}(P_s^t \Vert P_r^t)
\end{align}</script>where <script type="math/tex">P_s^t</script> and <script type="math/tex">P_r^t</script> are predicted word probability distribution by the real model and the reference, <script type="math/tex">g_{ip}^t</script> represents the similarity between word probability distributions <script type="math/tex">P_s^t</script> and <script type="math/tex">P_r^t</script>. The term <script type="math/tex">g_{ip}^t \rightarrow 0</script> when <script type="math/tex">P_s^t</script> has a higher probability to a stylized word.</li>
</ol>
<p><img data-src="/notes/images/ImgCpt-Factual-LSTM-Model.png" width="100%"/></p>
<h1 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h1><h2 id="Show-Adapt-and-Tell-ICCV-2017"><a href="#Show-Adapt-and-Tell-ICCV-2017" class="headerlink" title="Show, Adapt and Tell (ICCV 2017)"></a>Show, Adapt and Tell (ICCV 2017)</h2><p>In the source domain, given a set <script type="math/tex">\mathcal{P} = \{ (\mathbf{x}^n, \hat{\mathbf{y}}^n) \}_n</script> with paired image <script type="math/tex">\mathbf{x}^n</script> and ground truth sentence $\hat{\mathbf{y}}^n$. In the target domain, two separate sets are given: a set of example images <script type="math/tex">\chi = \{ \mathbf{x}^n \}_n</script> and example sentences <script type="math/tex">\hat{\mathcal{Y}} = \{ \hat{\mathbf{y}}^n \}_n</script>.</p>
<p><img data-src="/notes/images/ImgCpt-Show-Adapt-Tell-Model.png" alt="upload successful"></p>
<h3 id="Captioner-as-an-Agent"><a href="#Captioner-as-an-Agent" class="headerlink" title="Captioner as an Agent"></a>Captioner as an Agent</h3><p>Captioner using the standard CNN-RNN architecture is treated as an agent. At time $t$, the captioner takes an action, <em>i.e.</em>, a word <script type="math/tex">y_t</script>, according to a stochastic policy <script type="math/tex">\pi_\theta (y_t \vert \mathbf{x}, \mathbf{y}_{t-1}</script>. The total per-word loss $J(\theta)$ is minimized:</p>
<script type="math/tex; mode=display">
\begin{align}
J(\theta) &{}= \sum_{n=1}^N \sum_{t=1}^{T_n} \mathcal{L}(\pi_\theta(\hat{y}_t^n \vert \mathbf{x}^n, \hat{\mathbf{y}}_{t-1}^n))\\
\mathcal{L}(\pi_\theta(\hat{y}_t^n \vert \mathbf{x}^n, \hat{\mathbf{y}}_{t-1}^n)) &{}= - \log \pi_\theta (\hat{y}_t^n \vert \mathbf{x}^n, \hat{\mathbf{y}}_{t-1}^n)
\end{align}</script><p>where $N$ is the number of images, <script type="math/tex">T_n</script> is the length of the sentence <script type="math/tex">\hat{\mathbf{y}}^n</script>, $\mathcal{L}$ indicates the cross-entropy loss. <script type="math/tex">\hat{\mathbf{y}}_{t-1}^n</script> and <script type="math/tex">\hat{y}_t^n</script> are ground truth partial sentence and word, respectively.</p>
<p>The state-action function <script type="math/tex">Q((\mathbf{x}, \mathbf{y}_{t-1}), y_t) = \mathbb{E}_{\mathbf{y}_{(t+1):T}} [R( [ \mathbf{y}_{t-1}, y_t, \mathbf{y}_{(t+1):T} ] \vert \mathbf{x}, \mathcal{Y}, \mathcal{P} )]</script></p>
<p>The object function:</p>
<script type="math/tex; mode=display">
\begin{align}
J(\theta) &{}= \sum_{n=1}^N J_n (\theta) \\
J_n(\theta) &{}= \sum_{t=1}^{T_n} \mathbb{E}_{\mathbf{y}_t^n} [ \pi_\theta (y_t^n \vert \mathbf{x}^n, \mathbf{y}_{t-1}^n) Q \big((\mathbf{x}^n, \mathbf{y}_{t-1}^n), y_t^n \big) ]
\end{align}</script><p>Since the action sapce of <script type="math/tex">\mathbf{y}_t</script> is huge, $M$ sentences <script type="math/tex">\{ \mathbf{y}^m \}_m</script> is generated and replace expectation with the mean:</p>
<script type="math/tex; mode=display">
\begin{align}
J_n(\theta) &\simeq \frac{1}{M} \sum_{m=1}^M J_{n,m}(\theta) \\
J_{n,m} (\theta) &{}= \sum_{t=1}^{T_m} \pi_\theta (y_t^m \vert \mathbf{x}, \mathbf{y}_{t-1}^m) Q \big( (\mathbf{x}, \mathbf{y}_{t-1}^m), y_t^m \big)
\end{align}</script><p>The policy gradient is:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J_{n,m} (\theta) &{}= \sum_{t=1}^{T_m} \nabla_\theta \pi_\theta (y_t^m \vert \mathbf{x}, \mathbf{y}_{t-1}^m) Q \big( (\mathbf{x}, \mathbf{y}_{t-1}^m), y_t^m \big) \\
&{}= \sum_{t=1}^{T_m} \pi_\theta (y_t^m \vert \mathbf{x}, \mathbf{y}_{t-1}^m) \nabla_\theta \log \pi_\theta (y_t^m \vert \mathbf{x}, \mathbf{y}_{t-1}^m) Q((\mathbf{x}, \mathbf{y}_{t-1}^m), y_t^m) \\
\nabla_\theta J(\theta) &\simeq \frac{1}{M} \sum_{n=1}^N \sum_{m=1}^M \nabla_\theta J_{n,m}(\theta)
\end{align}</script><p>Monte Carlo roolout is used to replace the expectation of Q function:</p>
<script type="math/tex; mode=display">
Q((\mathbf{x}, \mathbf{y}_{t-1}), y_t) \simeq \frac{1}{K} \sum_{k=1}^K R(\big[ \mathbf{y}_{t-1}, y_t, \mathbf{y}_{(t+1):T_k}^k \big] \vert \mathbf{x}, \mathcal{Y}, \mathcal{P})</script><p>where <script type="math/tex">\{\mathbf{y}_{(t+1):T_k}^k \}</script> are generated future words, and $K$ complete sentences are sampled with policy <script type="math/tex">\pi_\theta</script>.</p>
<h3 id="Critics"><a href="#Critics" class="headerlink" title="Critics"></a>Critics</h3><h4 id="Domain-critic"><a href="#Domain-critic" class="headerlink" title="Domain critic"></a>Domain critic</h4><p>Domain critic (DC) model uses an encoder with a classifier. A sentence $\mathbf{y}$ is encoded using TextCNNs with highway connection, the pass to an MLP followed by a softmax to generate probability <script type="math/tex">C_d (l \vert \mathbf{y})</script>, where $l \in$ {source, target, generated}.</p>
<p><strong>Training DC</strong>: the goal is to classify a sentence into source, target, and generated data.</p>
<script type="math/tex; mode=display">\mathcal{L}_d (\phi) = - \sum_{n=1}^N \log C_d (l^n \vert \mathbf{y}^n; \phi)</script><h4 id="Multi-modal-critic"><a href="#Multi-modal-critic" class="headerlink" title="Multi-modal critic"></a>Multi-modal critic</h4><p>Multi-modal critic (MC) classifies $(\mathbf{x}, \mathbf{y})$ as “paired”, “unpaired”, or “generated” data. The model is:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{c} &{}= \textrm{LSTM}(\mathbf{y}) \\
f &{}= \tanh(\mathbf{W}_x \mathbf{x} + \mathbf{b}_x) \odot \tanh (\mathbf{W}_c \mathbf{c} + \mathbf{b}_c) \\
C_m &{}= \textrm{softmax} ( \mathbf{W}_m f + \mathbf{b}_m)
\end{align}</script><p>where <script type="math/tex">\mathbf{W}_x, \mathbf{b}_x, \mathbf{W}_c, \mathbf{b}_c,\mathbf{W}_m, \mathbf{b}_m</script> are learnable parameters, $\odot$ denotes the element-wise product, <script type="math/tex">C_m</script> is the probabilities over three classes: paired, unpaired, and genrated data. <script type="math/tex">C_m</script> indicates how a generated caption $\mathbf{y}$ is relevant to an image $\mathbf{x}$.</p>
<p><strong>Training MC</strong>: the goal is to classify the image-sentence pair into paired, unpaired, and generated data.</p>
<script type="math/tex; mode=display">\mathcal{L}_m (\eta) = - \sum_{n=1}^N \log C_m (l^n \vert \mathbf{x}^n, \mathbf{y}^n; \eta)</script><h4 id="Sentence-reward"><a href="#Sentence-reward" class="headerlink" title="Sentence reward"></a>Sentence reward</h4><p>The sentence reward <script type="math/tex">R(\mathbf{y} \vert \cdot) = C_d(\text{target}\vert \cdot) \cdot C_m(\textrm{paired} \vert \cdot)</script></p>
<h3 id="Training-algorithm"><a href="#Training-algorithm" class="headerlink" title="Training algorithm"></a>Training algorithm</h3><p><strong>Require</strong>: captioner <script type="math/tex">\pi_\theta</script>, domain critic <script type="math/tex">C_d</script>, multi-modal critic <script type="math/tex">C_m</script>, and empty set of generated sentences <script type="math/tex">\mathcal{Y}_{\pi\theta}</script>, and an empty set for paired image-generated-sentence <script type="math/tex">\mathcal{P}_\textrm{gen}</script>.</p>
<p><strong>Input</strong>: sentences <script type="math/tex">\hat{\mathcal{Y}}_\textrm{src}</script>, image-sentence pairs <script type="math/tex">\mathcal{P}_\text{src}</script>, unpaired data <script type="math/tex">\acute{\mathcal{P}}_\textrm{src}</script> in source domain; sentences <script type="math/tex">\hat{\mathcal{Y}}_\textrm{tgt}</script>, images <script type="math/tex">\chi_\textrm{tgt}</script> in target domain.</p>
<p>1, Pretrain <script type="math/tex">\pi_\theta</script> on <script type="math/tex">\mathcal{P}_\text{src}</script>;</p>
<ol>
<li><strong>while</strong> $\theta$ has not converged <strong>do</strong>:<ol>
<li>for $i=0, \cdots, N_c$ do<ol>
<li><script type="math/tex">\mathcal{Y}_{\pi_\theta} \leftarrow \{  \mathbf{y} \}</script>, where <script type="math/tex">\mathbf{y} \sim \pi_\theta (\cdot \vert, \cdot)</script> and <script type="math/tex">\mathbf{x} \sim \chi_\textrm{tgt}</script>;</li>
<li>Compute <script type="math/tex">g_d = \nabla_\phi \mathcal{L}_d (\phi)</script>;</li>
<li><script type="math/tex">\mathcal{Y}_{\pi_\theta} \leftarrow \{  \mathbf{y} \}</script>, where <script type="math/tex">\mathbf{y} \sim \pi_\theta (\cdot \vert, \cdot)</script> and <script type="math/tex">\mathbf{x} \sim \chi_\textrm{src}</script>;</li>
<li><script type="math/tex">\mathcal{P}_\text{gen} \leftarrow \{ \mathbf{x}, \mathbf{y}\}</script>;</li>
<li>Compute <script type="math/tex">g_m = \nabla_\eta \mathcal{L}_m (\eta)</script>;</li>
<li>Adam update for $\eta$ for <script type="math/tex">C_m</script> using <script type="math/tex">g_m</script>;</li>
</ol>
</li>
<li>for <script type="math/tex">i = 0, \cdots, N_g</script> do<ol>
<li><script type="math/tex">\mathcal{Y}_{\pi_\theta} \leftarrow \{  \mathbf{y} \}</script>, where <script type="math/tex">\mathbf{y} \sim \pi_\theta (\cdot \vert, \cdot)</script> and <script type="math/tex">\mathbf{x} \sim \chi_\textrm{tgt}</script>;</li>
<li><script type="math/tex">\mathcal{P}_\text{gen} \leftarrow \{ \mathbf{x}, \mathbf{y}\}</script>;</li>
<li>for $t=1, \cdots, T$ do<ol>
<li>Compute <script type="math/tex">Q((\mathbf{x}, \mathbf{y}_{t-1}), y_t)</script> with Monte Carlo rollouts;</li>
</ol>
</li>
<li>Compute <script type="math/tex">g_\theta = \nabla_\theta J(\theta)</script>;</li>
<li>Adam update of $\theta$ using <script type="math/tex">g_\theta</script>.</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Poetry-generation-ACM-MM-2018"><a href="#Poetry-generation-ACM-MM-2018" class="headerlink" title="Poetry generation (ACM MM 2018)"></a>Poetry generation (ACM MM 2018)</h2><p><sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, B., Fu, J., Kato, M.P., & Yoshikawa, M. (2018). [Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training](https://arxiv.org/pdf/1804.08473.pdf). ArXiv, abs/1804.08473.
">[15]</span></a></sup><br><img data-src="/notes/images/ImgCpt-Poetry-generation.png" alt="upload successful"></p>
<h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><h2 id="Self-Critical-Sequence-Training-CVPR-2017"><a href="#Self-Critical-Sequence-Training-CVPR-2017" class="headerlink" title="Self-Critical Sequence Training (CVPR 2017)"></a>Self-Critical Sequence Training (CVPR 2017)</h2><p><strong>Motivation</strong>: </p>
<ol>
<li>Teacher-Forcing leads to the <strong>mismatch</strong> between training and testing, and <strong>exposure bias</strong>, resulting in error accumulation during generation at test time.</li>
<li>While training with cross-entropy loss, discrete and non-differentiable NLP metrics such as BLEU, ROUGE, METEOR, CIDEr are evaluated at test time.</li>
<li>Ideally, sequence models should be trained to avoid exposure bias and directly optimize metrics for the task at hand.</li>
</ol>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>Reinforcement Learning (RL) can be used to directly optimize NLP metrics and address the exposuire bias issue, such as REINFORCE and Actor-Critic. LSTM can be treated as an agent that interacts with an external environment (state: words and image features, action: predicted words, done: “EOS”). The policy network <script type="math/tex">p_\theta</script> results in an action of next word prediction. After each action, the agent updates its internal state (parameters) until generating the EOS token. The reward $r$ is the NLP metric, like CIDEr score of generated sentence by comparing with ground-truth sequences. The goal is to minimize the negative expected reward:</p>
<script type="math/tex; mode=display">\mathcal{L} (\theta) = - \mathbb{E}_{w^s \sim p_\theta} [r(w^s)]</script><p>where <script type="math/tex">w^s = (w_1^s, \cdots, w_T^s)</script> and <script type="math/tex">w_t^s</script> is the word sampled from the model at the time step $t$. In practive, $ \mathcal{L} (\theta)$ is typically estimated with a single sample from <script type="math/tex">p_\theta</script>:</p>
<script type="math/tex; mode=display">\mathcal{L} (\theta) \approx -r (w^s),\quad w^s \sim p_\theta</script><h4 id="Policy-gradient-with-REINFORCE"><a href="#Policy-gradient-with-REINFORCE" class="headerlink" title="Policy gradient with REINFORCE"></a>Policy gradient with REINFORCE</h4><p>REINFORCE is based on the observation that the expected graident of a non-differentiable reward function:</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{L}(\theta) = - \mathbb{E}_{w^s \sim p_\theta} [r(w^s)\nabla_\theta \log p_\theta (w^s)]</script><p>In practice, a single MC sample <script type="math/tex">w^s = (w_1^s,\cdots,w_T^s)</script> from <script type="math/tex">p_\theta</script>, for each training example in the minibatch:</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{L}(\theta) \approx -r (w^s) \nabla_\theta \log p_\theta (w^s)</script><h4 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h4><p>To reduce the variance of the gradient estimate, it minus a reference reward or baseline $b$:</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{L}(\theta) = - \mathbb{E}_{w^s \sim p_\theta} [(r(w^s) - b) \nabla_\theta \log p_\theta (w^s)]</script><p>The baseline can be arbitrary function, as long as it does not depend on the action $w^s$ because:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{E}_{w^s \sim p_\theta} [b \nabla_\theta \log p_\theta (w^s)] &{}= b \sum_{w_s} \nabla_\theta p_\theta (w^s) \\
&{}= b \nabla_\theta \sum_{w_s} p_\theta (w^s)\\
&{}= b \nabla_\theta 1 = 0
\end{align}</script><p>This shows that the baseline does not change the expected gradient but can reduce the variance.</p>
<p>For each training case, it can be approximated with a single sample <script type="math/tex">w^s \sim p_\theta</script> as:</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{L}(\theta) \approx - (r(w^s) -b) \nabla_\theta \log p_\theta (w^s)</script><p>Note if $b$ is a function of $\theta$ or $t$, this is sill valid.</p>
<p>The gradient is:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_\theta \mathcal{L}(\theta) &{}= \sum_{t=1}^T \frac{\partial \mathcal{L}(\theta)}{\partial s_t} \frac{\partial s_t}{\partial \theta}\\
\frac{\nabla_\theta \mathcal{L}(\theta)}{\partial s_t} &{}\approx (r(w^s)-b) (p_\theta (w_t \vert h_t) - 1_{w_t^s})
\end{align}</script><p>where <script type="math/tex">s_t</script> is the input to the softmax function;</p>
<h3 id="Self-Critical-Sequence-Training-SCST"><a href="#Self-Critical-Sequence-Training-SCST" class="headerlink" title="Self-Critical Sequence Training (SCST)"></a>Self-Critical Sequence Training (SCST)</h3><p>Self-Critical Sequence Training <sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., & Goel, V. (2016). [Self-Critical Sequence Training for Image Captioning](https://arxiv.org/pdf/1612.00563.pdf). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1179-1195.
">[16]</span></a></sup> applies the reward obtained by the current model under the inference mode at test time as the baseline in REINFORCE. The gradient at time step $t$ becomes:</p>
<script type="math/tex; mode=display">
\frac{\nabla_\theta \mathcal{L}(\theta)}{\partial s_t} \approx (r(w^s)- \color{green}{r(\hat{w})}) (p_\theta (w_t \vert h_t) - 1_{w_t^s})</script><p>where <script type="math/tex">r(\hat{w})</script> is the reward obtained by the current model at test time.</p>
<p><img data-src="/notes/images/ImgCpt-SCST.png" width="70%"/></p>
<div class="note info">
            <ol><li>SCST directly optimizes the true, sequence-level, evaluation metric, encouraging train/test time consistency.</li><li>SCST avoids the usual scenario of having to learn a (context-dependent) estimate of expected future rewards as a baseline.</li><li>In practice, it has much lower variance and is more effective on mini-batches using SGD.</li><li>It avoids the training problems with actor-critic methods, where the actor is trained on value functions estimated by a critic rather than actual rewards.</li></ol>
          </div>
<p>It uses the greedy decoding:</p>
<script type="math/tex; mode=display">\hat{w}_t = \arg\max_{w_t} p(w_t \vert h_t)</script><h2 id="RL-with-Embedding-Reward-CVPR-2017"><a href="#RL-with-Embedding-Reward-CVPR-2017" class="headerlink" title="RL with Embedding Reward (CVPR 2017)"></a>RL with Embedding Reward (CVPR 2017)</h2><p>This work<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ren, Z., Wang, X., Zhang, N., Lv, X., & Li, L. (2017). [Deep Reinforcement Learning-Based Image Captioning with Embedding Reward](https://arxiv.org/pdf/1704.03899.pdf). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1151-1159.">[17]</span></a></sup> utilized the Actor-Critic algorithm with the reward of visual-semantic embedding for image captioning. The policy and value network jointly determine the next best word at each time step. The former provides a <strong>local guidance</strong> by predicting the confidence of predicted next words, whereas the latter serves as the <strong>global and lookahead guidance</strong> by evaluating the reward value of the current state.</p>
<p><img data-src="/notes/images/ImgCpt-RL-Embed-Reward.png" width="60%"/></p>
<h3 id="Policy-Network"><a href="#Policy-Network" class="headerlink" title="Policy Network"></a>Policy Network</h3><p>The policy network <script type="math/tex">p_\pi</script> consists of standard CNN-RNN encoder-decoder architecture, with the huge vocabulary size as its action space. </p>
<h3 id="Value-Network"><a href="#Value-Network" class="headerlink" title="Value Network"></a>Value Network</h3><p>The value function <script type="math/tex">v^p</script> is predicted by a value network <script type="math/tex">v_\theta</script>.</p>
<script type="math/tex; mode=display">\begin{align}
v^p (s) &= \mathbb{E} [r \vert s_t =s, a_{t,\cdots, T} \sim p] \\
v_\theta (s) &\approx v^p(s)
\end{align}</script><p>where <script type="math/tex">s_t = \{ \mathbf{I}, w_1, \cdots, w_t \}</script>. </p>
<p>As in the figure, the value network consists of a CNN, an RNN, and an MLP, where CNN encodes the raw image $\mathbf{I}$, RNN encodes the semantic information of partially generated sentence <script type="math/tex">\{ w_1,\cdots, w_t \}</script>. The concatenated representation is projected to a scalar reward from <script type="math/tex">s_t</script> using MLP.</p>
<p><img data-src="/notes/images/ImgCpt-RL-Value-Network.png" width="50%"/></p>
<h3 id="Visual-Semantic-Embedding-Reward"><a href="#Visual-Semantic-Embedding-Reward" class="headerlink" title="Visual-Semantic Embedding Reward"></a>Visual-Semantic Embedding Reward</h3><p>Give an image with feature <script type="math/tex">\mathbf{v}^*</script>, the reward of generated sentence $\hat{S}$ is defined to be the embedding similarity between $\hat{S}$ and $\mathbf{v}^*$:</p>
<script type="math/tex; mode=display">
r = \frac{f_e (\mathbf{v}^*) \cdot \mathbf{h}^\prime_T (\hat{S})}{\Vert \mathbf{v}^* \Vert \Vert \mathbf{h}^\prime_T (\hat{S}) \Vert}</script><p>The bidirectional ranking loss is defined as:</p>
<script type="math/tex; mode=display">\mathcal{L}_e = \sum_\mathbf{v} \sum_{S^-} \max (0, \beta - f_e (\mathbf{v})\cdot \mathbf{h}^\prime_T (S) + f_e (\mathbf{v})\cdot \mathbf{h}^\prime_T (S^-)) +  \sum_{S}\sum_{\mathbf{v}^-} \max (0, \beta - \mathbf{h}^\prime_T (S) \cdot f_e (\mathbf{v}) + \mathbf{h}^\prime_T (S) \cdot f_e (\mathbf{v}^-) )</script><p>where $\beta$ is margin cross-validated, $(\mathbf{v}, S)$ are ground truth image-sentence pair, $S^-$ is a negetive description for image corresponding to $\mathbf{v}$, and vice-versa with $\mathbf{v}^-$.</p>
<h3 id="Training-4"><a href="#Training-4" class="headerlink" title="Training"></a>Training</h3><p>Two steps:</p>
<ol>
<li>Train policy network use cross entropy loss;</li>
<li>Train <script type="math/tex">p_\pi</script> and <script type="math/tex">v_\theta</script> jointly using reinforcement learning and curriculum learning. And the value network <script type="math/tex">v_\theta</script> serves as a moving baseline.<script type="math/tex; mode=display">
\begin{align}
\nabla_\pi J &{}\approx \sum_{t=1}^T \nabla_\pi \log p_\pi (a_t \vert s_t) (r - v_\theta (s_t)) \\
\nabla_\theta J &{}= \nabla_\theta v_\theta (s_t) (r - v_\theta (s_t))
\end{align}</script></li>
</ol>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015). <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">Show and tell: A neural image caption generator</a>. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3156-3164.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S., &amp; Bengio, Y. (2015). <a href="https://arxiv.org/pdf/1502.03044.pdf">Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention</a>. ICML.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Karpathy, A., &amp; Li, F. (2015). <a href="https://doi.org/10.1109/CVPR.2015.7298932">Deep visual-semantic alignments for generating image descriptions</a>. CVPR.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">You, Q., Jin, H., Wang, Z., Fang, C., &amp; Luo, J. (2016). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780872">Image Captioning with Semantic Attention</a>. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4651-4659.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., &amp; Chua, T. (2017). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8100150">SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 6298-6306.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lu, J., Xiong, C., Parikh, D., &amp; Socher, R. (2017). <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099828">Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3242-3250.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Gan, Z., Gan, C., He, X., Pu, Y., Tran, K., Gao, J., Carin, L., &amp; Deng, L. (2017). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099610">Semantic Compositional Networks for Visual Captioning</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1141-1150.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., &amp; Zhang, L. (2017). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8578734">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</a>. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6077-6086.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ren, S., He, K., Girshick, R.B., &amp; Sun, J. (2015). <a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 1137-1149.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780459">Deep Residual Learning for Image Recognition</a>. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Gan, C., Gan, Z., He, X., Gao, J., &amp; Deng, L. (2017). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099591">StyleNet: Generating Attractive Visual Captions with Styles</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 955-964.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mathews, A.P., Xie, L., &amp; He, X. (2018). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8578994">SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text</a>. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8591-8600.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, T., Zhang, Z., You, Q., Fang, C., Wang, Z., Jin, H., &amp; Luo, J. (2018). <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tianlang_Chen_Factual_or_Emotional_ECCV_2018_paper.pdf">&quot;Factual&quot; or &quot;Emotional&quot;: Stylized Image Captioning with Adaptive Learning and Attention</a>. ECCV.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, T., Liao, Y., Chuang, C., Hsu, W.T., Fu, J., &amp; Sun, M. (2017). <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Show_Adapt_and_ICCV_2017_paper.pdf">Show, Adapt, and Tell: Adversarial Training of Cross-Domain Image Captioner</a>. 2017 IEEE International Conference on Computer Vision (ICCV), 521-530.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Liu, B., Fu, J., Kato, M.P., &amp; Yoshikawa, M. (2018). <a href="https://arxiv.org/pdf/1804.08473.pdf">Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training</a>. ArXiv, abs/1804.08473.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., &amp; Goel, V. (2016). <a href="https://arxiv.org/pdf/1612.00563.pdf">Self-Critical Sequence Training for Image Captioning</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1179-1195.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ren, Z., Wang, X., Zhang, N., Lv, X., &amp; Li, L. (2017). <a href="https://arxiv.org/pdf/1704.03899.pdf">Deep Reinforcement Learning-Based Image Captioning with Embedding Reward</a>. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1151-1159.<a href="#fnref:17" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Vision &amp; Language</category>
        <category>Image Captioning</category>
      </categories>
      <tags>
        <tag>Vision &amp; Language</tag>
        <tag>Image Captioning</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention in a Nutshell</title>
    <url>/notes/2019/01/22/NLP/Attention-in-a-nutshell/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Attention mechanism has been widely applied in natural langugage processing and computer vision tasks.<br><span id="more"></span></p>
<h1 id="Attention-mechanism"><a href="#Attention-mechanism" class="headerlink" title="Attention mechanism"></a>Attention mechanism</h1><h2 id="Background-what-is-WRONG-with-seq2seq"><a href="#Background-what-is-WRONG-with-seq2seq" class="headerlink" title="Background: what is WRONG with seq2seq?"></a>Background: what is WRONG with seq2seq?</h2><p><strong>Encoder-decoder</strong> architecture:<br>Encode a source sentence into a fixed-length vector from which a decoder generates a translation.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bahdanau, D., Cho, K., & Bengio, Y. (2014). [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). CoRR, abs/1409.0473.
">[3]</span></a></sup></p>
<ul>
<li><p><strong>Seq2seq</strong> models <strong>encode</strong> an <strong>input sentence of variable length</strong> <script type="math/tex">(x_1,...,x_T)</script> into a <strong>fixed-length vector representation</strong> $c$ (a.k.a sentence embedding, “thought” vector), by apply one LSTM to read the input sentence, one timestep at a time. The representation vector $c$ is expected to well capture the meaning of the source sentence.</p>
</li>
<li><p>Then <strong>decode</strong> the vector representation $v$ to the target sentence <script type="math/tex">(y_1,...,y_{T'})</script> with another LSTM whose initial hidden state is the last hidden state of the encoder (i.e. the representation of the input sentence: $c$). <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sutskever, I., Vinyals, O., & Le, Q. V. (2014). [Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). In Advances in neural information processing systems (pp. 3104-3112).
">[1]</span></a></sup></p>
</li>
</ul>
<script type="math/tex; mode=display">p(y_1,...,y_{T'} | x_1,...,x_T) = \prod_{t=1}^{T'} p(y_t|c,{y_1,...,y_{t-1}})</script><script type="math/tex; mode=display">p(y_t|c,{y_1,...,y_{t-1}}) = g(y_{t-1}, s_t, c)</script><p>where $g$ is a RNN that outputs the probability of <script type="math/tex">y_t</script>, and <script type="math/tex">s_t</script> is the hidden state of the RNN. $c$ is the fixed-length context vector for the input sentence.</p>
<div class="note warning">
            <p><strong>Drawbacks</strong>: The fixed-length context vector $c$ is incapable of remembering long sentences<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Weng L. (2018, Jun 24). Attention? Attention! [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">[2]</span></a></sup>. It will forget the former part when processing the latter sequence. Sutskever et al.(2014)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sutskever, I., Vinyals, O., & Le, Q. V. (2014). [Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). In Advances in neural information processing systems (pp. 3104-3112).">[1]</span></a></sup> proposed a trick that only reversing the order of source sentences rather than target sentences could be of benefit for MT. </p><p>Basic encoder-decoder architecture <strong>compresses all the necessary information of a source sentence into a fixed-length vector</strong>. This may be <strong>incapable of coping with long sentences</strong>, especially those that are longer than the sentences in the training corpus <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bahdanau, D., Cho, K., & Bengio, Y. (2014). [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). CoRR, abs/1409.0473.">[3]</span></a></sup>. The performance of basic encoder-decoder drops rapidly <em>as the length of an input sentence increases</em> <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cho, K., Merrienboer, B.V., Bahdanau, D., & Bengio, Y. (2014). [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf). SSST@EMNLP.">[4]</span></a></sup>.</p><p>Thus attention mechanism is proposed to tackle this problem.</p>
          </div>
<p><img data-src="/notes/images/seq2seq.png" alt="Seq2seq architecture."></p>
<h2 id="Attention-mechanism-1"><a href="#Attention-mechanism-1" class="headerlink" title="Attention mechanism"></a>Attention mechanism</h2><h3 id="NMT-by-jointly-learning-to-align-and-translate-EMNLP-2014"><a href="#NMT-by-jointly-learning-to-align-and-translate-EMNLP-2014" class="headerlink" title="NMT by jointly learning to align and translate (EMNLP 2014)"></a>NMT by jointly learning to align and translate (EMNLP 2014)</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Bi-RNNs, obtain the annotation for each word <script type="math/tex">x_i</script> by concatenating the forward and backward hidden states:</p>
<script type="math/tex; mode=display">h_j = [\overrightarrow{h}_j^T, \overleftarrow{h}_{j}^T]^T</script><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>The conditional probability is :</p>
<script type="math/tex; mode=display">p(y_i|y_1,...,y_{i-1},\mathbf{x}) = g(y_{i-1}, s_i, c_i)</script><p>where $s_i$ is the RNN hidden state for time $i$: </p>
<script type="math/tex; mode=display">s_i = f(s_{i-1}, y_{i-1}, c_i)</script><p>Unlike the basic encoder-decoder architecture, the probability of each output word is conditioned on a distinct context vector <script type="math/tex">c_i</script> for each target word <script type="math/tex">y_i</script>.</p>
<script type="math/tex; mode=display">c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j</script><script type="math/tex; mode=display">\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}</script><script type="math/tex; mode=display">e_{ij} = score(s_{i-1}, h_j)</script><p>where <script type="math/tex">e_{ij}</script> is an alignment model which scores how well the input at the position $j$ and the output at position $i$ match <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bahdanau, D., Cho, K., & Bengio, Y. (2014). [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). CoRR, abs/1409.0473.
">[3]</span></a></sup>. Here $\text{score}$ is a simple FF-NN layer:</p>
<script type="math/tex; mode=display">\text{score}(s_t, \mathbf{h}_i) = \mathbf{v}_a^T \tanh(\mathbf{W}_a[s_t; \mathbf{h}_i])</script><p>where <script type="math/tex">\mathbf{v}_a</script> and <script type="math/tex">\mathbf{W}_a</script> are learnable.</p>
<p><img data-src='/notes/images/additive-attn.png' width='80%'/></p>
<center> Img source: <a href='https://mengxinji.github.io/Blog/2019-03-19/attention/' target='_blank'>M. Ji's blog</a> </center>

<!--![](/notes/images/nmt_attention.png)-->
<h2 id="Attention-zoo"><a href="#Attention-zoo" class="headerlink" title="Attention zoo"></a>Attention zoo</h2><table style="border-collapse:collapse;border-spacing:0;border-color:#aaa" class="tg"><tr><th style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#fff;background-color:#f38630;text-align:center">Content-based location</th><th style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#fff;background-color:#f38630;text-align:center">Alignment score function</th></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center">Concat (additive, FF_NN)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luong, T., Pham, H., & Manning, C.D. (2015). [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.
">[5]</span></a></sup></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;text-align:center">$$\text{score}(\mathbf{h}_t, \overline{\mathbf{h}}_s) = \mathbf{v}_a^T \tanh(\mathbf{W}_a [\mathbf{h}_t; \overline{\mathbf{h}}_s] )$$</td></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top">General <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luong, T., Pham, H., & Manning, C.D. (2015). [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.
">[5]</span></a></sup></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;text-align:center;vertical-align:top">$$\text{score}(\mathbf{h}_t, \overline{\mathbf{h}}_s) = \mathbf{h}_t^T \mathbf{W}_a \overline{\mathbf{h}}_s $$</td></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top">Dot-product <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luong, T., Pham, H., & Manning, C.D. (2015). [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.
">[5]</span></a></sup></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;text-align:center;vertical-align:top">$$\text{score}(\mathbf{h}_t, \overline{\mathbf{h}}_s) = \mathbf{h}_t^T \overline{\mathbf{h}}_s$$</td></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top">Scaled dot-product <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). NIPS.
">[6]</span></a></sup></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;text-align:center;vertical-align:top">$$\text{score}(\mathbf{h}_t, \overline{\mathbf{h}}_s) = \frac{ \mathbf{h}_t^T \overline{\mathbf{h}}_s}{\sqrt{d}}$$  where $d$ is the dimension of the source input hidden states.</td></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top">Self-attention <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cheng, J., Dong, L., & Lapata, M. (2016). [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733.pdf). EMNLP.
">[7]</span></a></sup></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;text-align:center;vertical-align:top">Replace the target sequence with the same input sequence with other attention function.</td></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#ffffff;background-color:#00d2cb;font-weight:bold;text-align:center;vertical-align:top"><span style="font-weight:bold">Location-based attention</span></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#ffffff;background-color:#00d2cb;font-weight:bold;text-align:center;vertical-align:top">Alignment score function</td></tr><tr><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top">Location-based <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luong, T., Pham, H., & Manning, C.D. (2015). [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.
">[5]</span></a></sup></td><td style="font-family:Tahoma, Geneva, sans-serif !important;;font-size:15px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;text-align:center;vertical-align:top">$$ \alpha_{ij} = \text{softmax}(\mathbf{W}_a \mathbf{h}_{t=j})$$</td></tr></table>

<p><img data-src='/notes/images/global_attn.png' target='_blank' width='60%' /></p>
<center> image source:<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luong, T., Pham, H., & Manning, C.D. (2015). [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.
">[5]</span></a></sup> </center>

<p>FF-FC attention(a.k.a additive attention)<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Raffel, C., & Ellis, D.P. (2015). [Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems](https://arxiv.org/pdf/1512.08756.pdf). CoRR, abs/1512.08756.
">[9]</span></a></sup></p>
<p><img data-src='/notes/images/ff-attention.png' width='60%'/></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>End2end memory networks are based on a <strong>recurrent attention mechanism</strong> instead of <strong>sequence-aligned recurrence</strong>. However, sequence-aligned RNNs preclude parallelization.</p>
<h3 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h3><p><strong>Architecture</strong>: stacked self-attention + point-wise FC layer (with residual connection + layer normalization)</p>
<h4 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h4><p>The transformer encoder applies one <strong>multi-head attention</strong> followed by one FC-FF layer, adopting the <strong>residual connection</strong> and <strong>layer normalization</strong> trick: <script type="math/tex">\text{LayerNorm}(x+ \text{Sublayer}(x))</script></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    a residual connection followed by a layer norm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Apply residual connection to any sublayer with the same size&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<p>Layer Normalization:</p>
<script type="math/tex; mode=display">y = \gamma * \frac{x - E[x]}{\sqrt{\text{Var}[x]+\epsilon}} + \beta</script><p>where $\gamma$ and $\beta$ are learnable affine transform parameters.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; layer norm&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.ones(size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(size))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.weight * (x - mean) / (std + self.eps) + self.bias</span><br></pre></td></tr></table></figure>
<ul>
<li>N = 6 stack transformer layers</li>
<li>Output dimension $d_{model}$ = 512</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; produce N identical layers &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Core encoder -&gt; a stack of N layers &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = nn.LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; pass input and mask through each layer in turn&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; encoder consists of a self-attn and ffc&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = utils.clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/transformer-encoder.png" alt="upload successful"></p>
<h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li><p>Same as the encoder, N = 6 identical stacked layers</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; N layer decoder with masking&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = nn.LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; decoder&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublyer = utils.clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublyer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublyer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublyer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Difference</strong><br>The <em>first</em> multi-head attention layer is masked to <strong>prevent positions from attending to subsequent positions</strong>, ensuring that the prediction output at position $i$ only depends on the known outputs at positions less than $i$, <strong>regardless of the future</strong>.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Mask out subsequent positions &quot;&quot;&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/transformer-decoder.png" alt="upload successful"></p>
<h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p>Transformer regarded encoded representation of input sequences as a set of <strong>key-value</strong> pairs (K,V), with dimension of input sequence length $n$. In MT context, encoder hidden states serve as (K, V) pairs. In the decoder the previous output is a query (with dimension $m$)  </p>
<h4 id="Scaled-dot-product-attention"><a href="#Scaled-dot-product-attention" class="headerlink" title="Scaled dot-product attention"></a>Scaled dot-product attention</h4><script type="math/tex; mode=display">\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_{k}}})\mathbf{V}</script><p>where <script type="math/tex">d_k</script> is the dimension of the key. </p>
<div class="note info">
            <p>Q: <strong>Why</strong> dividing <script type="math/tex">\sqrt{d_{k}}</script> in dot-product operation?</p><ul><li>For small values of <script type="math/tex">d_k</script>, the additive attention and dopt product attention perform similarly. </li><li>For large values of <script type="math/tex">d_k</script>, additive attention outperforms dot product attention <strong>without scaling</strong>.</li><li><strong>Interpretation</strong>: The dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Assume $q$ and $k$ are independent random variables with mean 0 and variance 1. Their product <script type="math/tex">q \cdot k = \sum_{i=1}^{d_k} q_i k_i</script> has mean 0 and variance <script type="math/tex">d_k</script>. Thus, scales the dot products by <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>.</li></ul>
          </div>
<p>In conventional attention view:</p>
<script type="math/tex; mode=display">\mathbf{h}_t = Q</script><script type="math/tex; mode=display">\overline{\mathbf{h}}_s = K = V</script><script type="math/tex; mode=display">\text{score}(\mathbf{h}_t, \overline{\mathbf{h}}_s) = \frac{\mathbf{h}_t \overline{\mathbf{h}}_s^T}{\sqrt{d_k}}</script><script type="math/tex; mode=display">\alpha_{ij}= \frac{\exp(e_{ij})}{\sum_{k=1}^{d_k} \exp(e_{ik})}</script><script type="math/tex; mode=display">c_i = \sum_{j=1}^{d_k} \alpha_{ij} \mathbf{h}_j</script><p>Dot-product is faster and more space-efficient compared with additive attention (one FF layer) in practice.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). NIPS.
">[6]</span></a></sup></p>
<p>More precisely, for the input sequence <script type="math/tex">x = (x_1, \cdots, x_n)</script>, dot-product attention outputs the new sequence <script type="math/tex">z = (z_1,\cdots, z_n)</script> of the same length, <script type="math/tex">z_i \in \mathbb{R}^{d_z}</script> </p>
<script type="math/tex; mode=display">
\begin{align}
e_{ij} &= \frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}\\
\alpha_{ij} &= \frac{\exp e_{ij}}{\sum_{k=1}^n \exp e_{ik}} \\
z_i &= \sum_{j=1}^n \alpha_{ij} (x_j W^V) \\

\end{align}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    scaled dot product</span></span><br><span class="line"><span class="string">    ---------------------------</span></span><br><span class="line"><span class="string">    L : target sequence length</span></span><br><span class="line"><span class="string">    S : source sequence length:</span></span><br><span class="line"><span class="string">    N : batch size</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    E : embedding dimension</span></span><br><span class="line"><span class="string">    h : # of attn head</span></span><br><span class="line"><span class="string">    d_k: E // h</span></span><br><span class="line"><span class="string">    ---------------------------</span></span><br><span class="line"><span class="string">    :param query: (N, h, L, d_k)</span></span><br><span class="line"><span class="string">    :param key: (N, h, S, d_k)</span></span><br><span class="line"><span class="string">    :param value: (N, h, S, d_k)</span></span><br><span class="line"><span class="string">    :param mask:</span></span><br><span class="line"><span class="string">    :param dropout: float</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># (nbatch, h, seq_len, d_k) @ (nbatch, h, d_k, seq_len) =&gt; (nbatch, h, seq_len, seq_len)</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="comment"># (nbatch, h, seq_len, seq_len) @ (nbatch, h, seq_len, d_k) = &gt; (nbatch, h, seq_len, d_k)</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/scale_dot_product.png" alt="upload successful"></p>
<h4 id="Multi-head-attention-1"><a href="#Multi-head-attention-1" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><p><strong>Multi-head</strong>: “linear project the $Q$, $K$ and $V$ $h$ times with different, learned linear projections to <script type="math/tex">d_k</script>, <script type="math/tex">d_k</script> and <script type="math/tex">d_v</script> dimensions, respectively.”<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). NIPS.
">[6]</span></a></sup> Then concat all ($h$) the <script type="math/tex">d_v</script> and use a linear layer to project to the final representation values.</p>
<p>Multi-head allows to “jointly attend to information from <strong>different representation subspaces at different positions</strong>“:</p>
<script type="math/tex; mode=display">\begin{align}
\text{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V}) &= \text{concat}(\text{head}_1,...,\text{head}_h) \mathbf{W}^O \\ \text{head}_i &= \text{Attention}(\mathbf{Q} W_i^Q,\mathbf{K} W_i^K,\mathbf{V}W_i^V)
\end{align}</script><p>where <script type="math/tex">W_i^Q \in \mathbb{R}^{d_{model} \times d_k}</script>, <script type="math/tex">W_i^K \in \mathbb{R}^{d_{model} \times d_k}</script>, <script type="math/tex">W_i^V \in \mathbb{R}^{d_{model} \times d_v}</script>, <script type="math/tex">W_i^O \in \mathbb{R}^{hd_{v} \times d_{model}}</script></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        multi-head attention</span></span><br><span class="line"><span class="string">        :param h: nhead</span></span><br><span class="line"><span class="string">        :param d_model: d_model</span></span><br><span class="line"><span class="string">        :param dropout: float</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#  split d_model into h heads</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = utils.clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        L : target sequence length</span></span><br><span class="line"><span class="string">        S : source sequence length:</span></span><br><span class="line"><span class="string">        N : batch size</span></span><br><span class="line"><span class="string">        E : embedding dim</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        :param query: (N,L,E)</span></span><br><span class="line"><span class="string">        :param key: (N,S,E)</span></span><br><span class="line"><span class="string">        :param value: (N,S,E)</span></span><br><span class="line"><span class="string">        :param mask: </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>) <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) split embedding dim to h heads : from d_model =&gt; h * d_k</span></span><br><span class="line">        <span class="comment"># dim: (nbatch, h, seq_length, d_model//h)</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) compute attention</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment"># dim: (nbatch, h, d_model)</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/multi-head.png" alt="Multi-Head Attention"></p>
<p>Transformer attention:</p>
<ul>
<li>Mimic the conventional encoder-decoder attention mechanisms: $Q$ comes from previous decoder, $K$, $V$ come from the decoder output. This allows every position in the decoder to attend over all positions in the <strong>input sequence</strong> (as figure above).</li>
<li>Encoder: K=V=Q, i.e. the output of previous layer. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li>
<li>Decoder: allow each position in the decoder to attend to all positions in the decoder <strong>up to and including that position</strong>.</li>
</ul>
<h3 id="Point-wise-feed-forward-nets"><a href="#Point-wise-feed-forward-nets" class="headerlink" title="Point-wise feed-forward nets"></a>Point-wise feed-forward nets</h3><script type="math/tex; mode=display">\text{FFNN}(x) = \max(0, xW_1 + b_1) W_2 + b_2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; FFN &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="Positional-encoding"><a href="#Positional-encoding" class="headerlink" title="Positional encoding"></a>Positional encoding</h3><div class="note warning">
            <p><strong>Drawbacks</strong>: self-attention cannot capture the order information of input sequences.</p><p>Positional embeddings can be learned or pre-fixed <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="LeCun, Y., Bottou, L., & Bengio, Y. (2006). PROC OF THE IEEE NOVEMBER Gradient Based Learning Applied to Document Recognition.">[8]</span></a></sup>.</p><p>RNNs solution: inherently model the sequential information, but preclude parallelization.</p>
          </div>
<ul>
<li>Residual connections help propagate position information to higher layers.</li>
</ul>
<h4 id="Absolute-Positional-Encoding"><a href="#Absolute-Positional-Encoding" class="headerlink" title="Absolute Positional Encoding"></a>Absolute Positional Encoding</h4><p>Transformer solution: use sinusoidal timing signal as <strong>positional encoding</strong> (PE). </p>
<script type="math/tex; mode=display">PE_{(\text{pos}, 2i)} = \sin(\frac{\text{pos}}{10000^{2i/d_\text{model}}})</script><script type="math/tex; mode=display">PE_{(\text{pos}, 2i+1)} = \cos(\frac{\text{pos}}{10000^{2i/d_\text{model}}})</script><p>where $\text{pos}$ is the position in the sentence and $i$ is the order along the  embedding vector dimension. Assume this allows to learn to attend by relative positions, since for and fixed offset $k$, <script type="math/tex">\text{PE}_{\text{pos}+k}</script> can be represented as the linear function of <script type="math/tex">\text{PE}_{\text{pos}}</script><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., & Polosukhin, I. (2017). [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). NIPS.
">[6]</span></a></sup></p>
<p><img data-src="/notes/images/Transformer-positional-encoding.png" alt="Position encoding"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># positional encoding layer in PyTorch</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0.</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># generate with maximum length</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>, d_model, <span class="number">2</span>) * - (math.log(<span class="number">1e4</span>) / d_model))</span><br><span class="line">        pe[:, ::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        seq_len = x.size(<span class="number">1</span>) <span class="comment"># take the sequence length</span></span><br><span class="line">        x = x + Variable(self.pe[:, :seq_len], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p><strong>Usage</strong>: before stacked encoder/decoder, take the <strong>sum of PE and input embeddings</strong> (as figure below).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        increase the embedding values before addition is to make positional encoding relatively smaller</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model) </span><br></pre></td></tr></table></figure>
<p><img data-src="/notes/images/transformer.png" alt="upload successful"></p>
<h4 id="Relative-Positional-Representation-RPR"><a href="#Relative-Positional-Representation-RPR" class="headerlink" title="Relative Positional Representation(RPR)"></a>Relative Positional Representation(RPR)</h4><ul>
<li><strong>Relation-aware self-attn</strong><br>Consider the pairwise relationships between input elements, which can be seen as a labeled, directed fully-connected graph. Let <script type="math/tex">a_{ij}^V,a_{ij}^K \in \mathbb{R}^{d_a}</script> represent the edge between input elements <script type="math/tex">x_i</script> and <script type="math/tex">x_j</script>.</li>
</ul>
<p>Then add the pairwise information to the sublayer output:</p>
<script type="math/tex; mode=display">\begin{align}
e_{ij} &= \frac{x_i W^Q (x_j W^K \color{red}{+a_{ij}^K})^\top}{\sqrt{d_z}} \\
&= \frac{x_i W^Q (x_jW^K)^\top + \overbrace{\color{green}{\pmb{x_iW^Q(a_{ij}^K)^\top}}}^\text{efficient implementation}}{\sqrt{d_z}} \\
\alpha_{ij} &= \frac{\exp e_{ij}}{\sum_{k=1}^n \exp e_{ik}} \\
z_i &= \sum_{j=1}^n \alpha_{ij}(x_jW^V \color{red}{+ a_{ij}^V} )\\
\end{align}</script><ul>
<li>Clip RPR<br>$k$ denotes the maximum relative position. The relative position information beyond $k$ will be clipped to the maximum value, which generalizes to the unseen sequence lengths during training.<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). [Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155). arXiv preprint arXiv:1803.02155.
">[14]</span></a></sup> In other words, RPR only considers context in a fixed window size $2k+1$, indicating $k$ elements on the l.h.s, and $k$ elements on the r.h.s, as well as itself.</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\text{clip}(x,k) &= \max(-k, \min(k,x)) \\
a_{ij}^K &= w_{\text{clip}(j-i, k)}^K \\
a_{ij}^V &= w_{\text{clip}(j-i, k)}^V
\end{align}</script><p>where rpr <script type="math/tex">w^K = (w_{-k}^K, \cdots, w_k^K) \in \mathbb{R}^{d_a}</script> and <script type="math/tex">w^V = (w_{-k}^V, \cdots, w_k^V) \in \mathbb{R}^{d_a}</script> are learnable.</p>
<p>Trainable param number:</p>
<ul>
<li>MADPA: <script type="math/tex">\overbrace{4 \times \big(\text{d_model} \times \text{d_model} + \text{d_model} \big)}^\text{4 dense layers}</script></li>
<li><p>MADPA with RPR: <script type="math/tex">\underbrace{4 \times \big(\text{d_model} \times \text{d_model} + \text{d_model} \big)}_\text{4 dense layers} + \underbrace{\color{red}{2 \times (\text{seq_len}^2 \times d_k )}}_\text{2 RPR matrices}</script> </p>
</li>
<li><p>PyTorch implementation</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention_RPR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; @ author: Yekun CHAI &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, h, max_relative_position, dropout=<span class="number">.0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        multi-head attention</span></span><br><span class="line"><span class="string">        :param h: nhead</span></span><br><span class="line"><span class="string">        :param d_model: d_model</span></span><br><span class="line"><span class="string">        :param dropout: float</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention_RPR, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#  assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = utils.clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        self.max_relative_position = max_relative_position</span><br><span class="line">        self.vocab_size = max_relative_position * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">        self.embed_K = nn.Embedding(self.vocab_size, self.d_k)</span><br><span class="line">        self.embed_V = nn.Embedding(self.vocab_size, self.d_k)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        L : target sequence length</span></span><br><span class="line"><span class="string">        S : source sequence length:</span></span><br><span class="line"><span class="string">        N : batch size</span></span><br><span class="line"><span class="string">        E : embedding dim</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        :param query: (N,L,E)</span></span><br><span class="line"><span class="string">        :param key: (N,S,E)</span></span><br><span class="line"><span class="string">        :param value: (N,S,E)</span></span><br><span class="line"><span class="string">        :param mask:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)  <span class="comment"># batch size</span></span><br><span class="line">        seq_len = query.size(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1) split embedding dim to h heads : from d_model =&gt; h * d_k</span></span><br><span class="line">        <span class="comment"># dim: (nbatch, h, seq_length, d_model//h)</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) rpr</span></span><br><span class="line">        relation_keys = self.generate_relative_positions_embeddings(seq_len, seq_len, self.embed_K)</span><br><span class="line">        relation_values = self.generate_relative_positions_embeddings(seq_len, seq_len, self.embed_V)</span><br><span class="line">        logits = self._relative_attn_inner(query, key, relation_keys, <span class="literal">True</span>)</span><br><span class="line">        weights = self.dropout(F.softmax(logits, -<span class="number">1</span>))</span><br><span class="line">        x = self._relative_attn_inner(weights, value, relation_values, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment"># dim: (nbatch, h, d_model)</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_relative_positions_matrix</span>(<span class="params">self, len_q, len_k</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        genetate rpr matrix</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        :param len_q: seq_len</span></span><br><span class="line"><span class="string">        :param len_k: seq_len</span></span><br><span class="line"><span class="string">        :return: rpr matrix, dim: (len_q, len_q)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> len_q == len_k</span><br><span class="line">        range_vec_q = range_vec_k = torch.arange(len_q)</span><br><span class="line">        distance_mat = range_vec_k.unsqueeze(<span class="number">0</span>) - range_vec_q.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        disntance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)</span><br><span class="line">        <span class="keyword">return</span> disntance_mat_clipped + self.max_relative_position</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_relative_positions_embeddings</span>(<span class="params">self, len_q, len_k, embedding_table</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        generate relative position embedding</span></span><br><span class="line"><span class="string">        ----------------------</span></span><br><span class="line"><span class="string">        :param len_q:</span></span><br><span class="line"><span class="string">        :param len_k:</span></span><br><span class="line"><span class="string">        :return: rpr embedding, dim: (len_q, len_q, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        relative_position_matrix = self._generate_relative_positions_matrix(len_q, len_k)</span><br><span class="line">        <span class="keyword">return</span> embedding_table(relative_position_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_relative_attn_inner</span>(<span class="params">self, x, y, z, transpose</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        efficient implementation</span></span><br><span class="line"><span class="string">        ------------------------</span></span><br><span class="line"><span class="string">        :param x: </span></span><br><span class="line"><span class="string">        :param y: </span></span><br><span class="line"><span class="string">        :param z: </span></span><br><span class="line"><span class="string">        :param transpose: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nbatches = x.size(<span class="number">0</span>)</span><br><span class="line">        heads = x.size(<span class="number">1</span>)</span><br><span class="line">        seq_len = x.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (N, h, s, s)</span></span><br><span class="line">        xy_matmul = torch.matmul(x, y.transpose(-<span class="number">1</span>, -<span class="number">2</span>) <span class="keyword">if</span> transpose <span class="keyword">else</span> y)</span><br><span class="line">        <span class="comment"># (s, N, h, d) =&gt; (s, N*h, d)</span></span><br><span class="line">        x_t_v = x.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(seq_len, nbatches * heads, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (s, N*h, d) @ (s, d, s) =&gt; (s, N*h, s)</span></span><br><span class="line">        x_tz_matmul = torch.matmul(x_t_v, z.transpose(-<span class="number">1</span>, -<span class="number">2</span>) <span class="keyword">if</span> transpose <span class="keyword">else</span> z)</span><br><span class="line">        <span class="comment"># (N, h, s, s)</span></span><br><span class="line">        x_tz_matmul_v_t = x_tz_matmul.view(seq_len, nbatches, heads, -<span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">return</span> xy_matmul + x_tz_matmul_v_t</span><br></pre></td></tr></table></figure>
</li>
<li><p>Tensorflow implementation: <sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Tensor2Tensor tensorflow code](https://github.com/tensorflow/tensor2tensor/blob/9e0a894034d8090892c238df1bd9bd3180c2b9a3/tensor2tensor/layers/common_attention.py#L1556-L1587)
">[16]</span></a></sup></p>
<div class="note danger">
            <p><strong>Thought</strong>: current attention mechanism is one round, and one dimension (at sequence dimension)</p>
          </div>
</li>
</ul>
<h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2019attn-summary,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Attention <span class="keyword">in</span> a Nutshell&#125;&#125;,</span><br><span class="line">  year = &#123;2019&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;http://cyk1337.github.io/notes/2019/01/22/NLP/Attention-in-a-nutshell/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to sequence learning with neural networks</a>. In Advances in neural information processing systems (pp. 3104-3112).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Weng L. (2018, Jun 24). Attention? Attention! [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a>. CoRR, abs/1409.0473.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cho, K., Merrienboer, B.V., Bahdanau, D., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/pdf/1409.1259.pdf">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</a>. SSST@EMNLP.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Luong, T., Pham, H., &amp; Manning, C.D. (2015). <a href="https://arxiv.org/pdf/1508.04025.pdf">Effective Approaches to Attention-based Neural Machine Translation</a>. EMNLP.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., &amp; Polosukhin, I. (2017). <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>. NIPS.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cheng, J., Dong, L., &amp; Lapata, M. (2016). <a href="https://arxiv.org/pdf/1601.06733.pdf">Long Short-Term Memory-Networks for Machine Reading</a>. EMNLP.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">LeCun, Y., Bottou, L., &amp; Bengio, Y. (2006). PROC OF THE IEEE NOVEMBER Gradient Based Learning Applied to Document Recognition.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raffel, C., &amp; Ellis, D.P. (2015). <a href="https://arxiv.org/pdf/1512.08756.pdf">Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems</a>. CoRR, abs/1512.08756.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec">Towards Data Science: How to code the transformer in PyTorch</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Harvard nlp: the annotated Transformer</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a">Medium: How Self-Attention with Relative Position Representations works</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shaw, P., Uszkoreit, J., &amp; Vaswani, A. (2018). <a href="https://arxiv.org/pdf/1803.02155">Self-attention with relative position representations</a>. arXiv preprint arXiv:1803.02155.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.cnblogs.com/shiyublog/p/11185625.html">RPR blog (in Chinese)</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/tensorflow/tensor2tensor/blob/9e0a894034d8090892c238df1bd9bd3180c2b9a3/tensor2tensor/layers/common_attention.py#L1556-L1587">Tensor2Tensor tensorflow code</a><a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3">Attn: Illustrated Attention</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Attention</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Attention</tag>
        <tag>Survey</tag>
      </tags>
  </entry>
  <entry>
    <title>Active Learning Overview</title>
    <url>/notes/2018/12/02/ML/Active-learning-overview/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><strong>Active learning</strong> (called <em>query learning</em> or <em>optimal experimental design</em> in statistics). The key <strong>hypothesis</strong> is, if the learning algorithm is allowed to choose the data from what it learns, it will perform better with less training.<br><span id="more"></span></p>
<p>Comparison with <strong>passive learning</strong>: typically gathering a large amount of data <em>randomly sampled from the underlying distribution</em> and using this large dataset to train a model.</p>
<h1 id="Why-active-learning"><a href="#Why-active-learning" class="headerlink" title="Why active learning?"></a>Why active learning?</h1><p>Give a simple example shown as below. </p>
<p>Figure (a) illustrates a toy data set of 400 samples, evenly sampled from two class Gaussians. (b) A logistic regression (LR) model trained with 30 sampled instances randomly sampled from overall distribution (70% accuracy). (c) A LR model trained with 30 actively queried instances using uncertainty sampling (90% accuracy).</p>
<p>Fig. (b) illustrates the <strong>traditional supervised learning</strong> approach after random selecting 30 instances for labelling, drawn i.i.d. from the unlabeled pool <script type="math/tex">\mu</script>. The classifier boundary is far away from <script type="math/tex">x=0</script>.</p>
<p>Figure (c) shows the <strong>active learning</strong> approach. The active learner uses <em>uncertainty sampling</em> to focus on instances <em>closest to its decision boundary</em>, assuming it can adequately explain those in other parts of the input sapce characterized by <script type="math/tex">\mu</script>.</p>
<p><img data-src='data:img/jpg;base64,iVBORw0KGgoAAAANSUhEUgAAA/oAAAETCAYAAACGMn3CAAABfGlDQ1BJQ0MgUHJvZmlsZQAAKJFj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'/></p>
<h1 id="Scenarios"><a href="#Scenarios" class="headerlink" title="Scenarios"></a>Scenarios</h1><h2 id="Membership-Query-Synthesis"><a href="#Membership-Query-Synthesis" class="headerlink" title="Membership Query Synthesis"></a>Membership Query Synthesis</h2><p>The learner generates the instance from some underlying natural distribution. E.g. mnist dataset, rotation or excluding some pieces of the digit.</p>
<p><strong>Limitation</strong>: labelling generated arbitrary instances an be awkward if the oracle is a human annotator. Unexpected problems: for image data, generated images are not recognisable; for NLP and speech tasks, synthesised text or speech is gibberish. </p>
<p><img data-src='data:img/jpg;base64,iVBORw0KGgoAAAANSUhEUgAABJIAAACtCAYAAADrjlV0AAAABGdBTUEAALGPC/xhBQAAQABJREFU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'/></p>
<h2 id="Stream-based-selective-sampling"><a href="#Stream-based-selective-sampling" class="headerlink" title="Stream-based selective sampling"></a>Stream-based selective sampling</h2><p>Alternative to synthesizing queries” <strong>selective sampling</strong>. The key assumption: obtaining an unlabeled instance is free.</p>
<p><strong>Stream-based selective sampling</strong> (a.k.a. stream-based or sequential active learning): each unlabeled instance is typically drawn <strong>one at a time</strong> from the data source, and the learner must decide whether to query or discard.</p>
<h3 id="Decision-criteria"><a href="#Decision-criteria" class="headerlink" title="Decision criteria:"></a>Decision criteria:</h3><ul>
<li>informativeness measure (a.k.a. query strategy) See following section</li>
<li>Compute an explicit region of uncertainty, i.e. the part of the instance space that is ambiguous to the learner, and only query instances that fall within it. <ul>
<li>Naive way: set a threshold for informativeness measure and regard those below the threshold as query region;</li>
<li>Hypothesis consistency. Define the region that is unknown to the overall model class, i.e. whether hypothesis between any two models are consistent. That is, any two models (completely expensive; requires keep models after each new query) of the same model class agree on all the labeled data, but disagree on some unlabelled instance, such instance lies within the region of uncertainty.</li>
</ul>
</li>
</ul>
<p><img data-src='data:img/jpg;base64,iVBORw0KGgoAAAANSUhEUgAABJIAAACtCAYAAADrjlV0AAAABGdBTUEAALGPC/xhBQAAQABJREFU
eAHtnQeYFMUWhZsFBBQwAwZ0UUQBUTBnRFSM+MxZEDCgPrPyRFHMOaGiiCjmnBXEiDlnMaEIKNEM
CJLfOc322tvOzs7uzqbZ/37focNUV1f9s8zU3Lp1OwgwCEAAAhCAAAQgAAEIQAACEIAABCAAAQhA
AAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAAC
EIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCA
AAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAE
IAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAA
AQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEI
QAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAA
AhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQ
gAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAA
BCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg
AAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAAB
CEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhA
AAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAAC
EIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCA
AAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAE
IAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAA
AQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEI
QAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAA
AhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEcp5Anaru4c0337y4qtvA/SGQTQJNmjQ577DDDrsg
m3VSFwQgAAEIQAACEIAABCAAAQhAoDoQqFcdGtGlS5fq0AzaAIFyE/jmm2+CmTNnlrseKoAABCAA
AQhAAAIQgAAEIAABCFRHAnnVsVG0CQIQgAAEIAABCEAAAhCAAAQgAAEIQKD6EagWEUnVDwstggAE
IAABCEAAAqUnsHjx4jrDhg3rqCvXXLRoUZPS18AVEIBAgsCiunXr/jxv3rwxffv2nZR4jUMIQAAC
EKgCAjiSqgA6t4QABCAAAQhAILcIyHnUZMGCBf2GDh16/FJLLZXXsGHDxfXr1yfyO7feZnpTBQTk
nF08d+5c+WUXNbj11lu/q1OnzsCjjjrqsSpoCreEAAQgAIECAjiS+FOAAAQgAAEIQAAC5SCgH7cd
Fy5c+NxysjZt2jRo1KhROWrjUghAIBUB+ZOCX375pYPyUd43ZMiQp+SwPeLII4/8O1VZzkEAAhCA
QMUSwJFUsXypHQIQgAAEIACBHCZwyy23rKMfuK+uu+66TVq0aFHlT8PNYdR0rZYTUCRSsPLKKwfy
1zb46quv9vzjjz8e0/+93XWeJ0DX8r8Nug8BCFQ+AUKuK585d4QABCAAAQhAIEcI6EfsYy1btmyM
EylH3lC6Ue0JaMlosP766zfUdgctJT2x2jeYBkIAAhDIQQI4knLwTaVLEIAABCAAAQhUPIHbbrtt
fy1ja5Wfn894quJxcwcIFBLIy8sL2rdv30ARSeePGDGiQeEL7EAAAhCAQKUQYOBTKZi5CQQgAAEI
QAACuUZAyX8PXWONNZbxj1oMAhCoXAJNmzYNGjduHEyePHnnyr0zd4MABCAAAUY+/A1AAAIQgAAE
IACBshHYSvlaynYlV0EAAuUmsOKKKzaZP3/+puWuiAogAAEIQKBUBHAklQoXhSEAAQhAAAIQgMAS
AopIWkFPjgIHBCBQRQT0/y+vbt26q1XR7bktBCAAgVpLAEdSrX3r6TgEIAABCEAAAuUhoPwsdVnW
Vh6CXAuB8hHwk9z0/xBvbvkwcjUEIACBUhPAkVRqZFwAAQhAAAIQgAAEIAABCEAAAhCAAARqJwEc
SbXzfafXEIAABCAAAQhAAAIQgAAEIAABCECg1ARwJJUaGRdAAAIQgAAEIAABCEAAAhCAAAQgAIHa
SQBHUu183+k1BCAAAQhAAAIQgAAEIAABCEAAAhAoNQEcSaVGxgUQgAAEIAABCEAAAhCAAAQgAAEI
QKB2EsCRVDvfd3oNAQhAAAIQgAAEIAABCEAAAhCAAARKTQBHUqmRcQEEIAABCEAAAhCAAAQgAAEI
QAACEKidBHAk1c73nV5DAAIQgAAEIAABCEAAAhCAAAQgAIFSE8CRVGpkXAABCEAAAhCAAAQgAAEI
QAACEIAABGongVrtSPr999+Dbt26BS+99FKFvfuLFy8Opk6dWmH1UzEEIAABCEAAAhCAAAQgAAEI
QAACEKgsArXakbRgwYJgwoQJwV9//VVhvHv37h0MHjy4wuqnYghAAAIQgAAEIAABCEAAAhCoNAJN
K+1ONf9G9jesKjUsQ1ea65pssk7VlmV1j2ZlaFu6S9zfZdIVyIXXarUjqTLewF9++aUybsM9IAAB
CEAAAhCAAAQgAAEIQKBiCPRUta9Iv0l/SlOkkdIOUk21A9Twp6UWFdgBO2kmSbuX4R7v65qBZbiu
uEtSteVyFc7m8iT7V9zf3sU1IlfO18uVjmSjH5MnTw6++uqroHPnzmGk0htvvBH8/fffQceOHYPN
N9/8X7cYO3Zs8OabbwaNGjUKtt5662D11VcvLPPZZ58FP//8czBr1qzA9Xr5XIMGDYJtttmmsMy4
ceOCDz74IJg2bVqw5pprBjvttFNYV1SgtO35448/gnfeeSf45ptvgmWXXTbYbLPNgnbt2kXVhVsv
tXv33XeDzz//PKhbt274usvl5WXmU3R/fL3v0alTp2CjjTYK+xXdxG3+9ttvg+233z74/vvvw/bM
nj076NChQ7DFFltExYpsS+IQFf7000+DTz75JPjzzz+D/Pz8f/FyObM0g/HjxwetWrUKGbRoUZGf
jVHr2EIAAhCAAAQgkC0CHm/MmTMnWHnllUtV5cyZM4N58+YFK664Yqmuixcu673jdWRrvzq1JVt9
oh4I1DACS6u9Q6WDpbsLNF7bdSU7R16QLpXOkWqa/UcN3kPqIJGLpaa9e1Xc3sy8B1XcyMq6/Vtv
vRUcf/zxwf333x/85z//CZ577rng5ptvDnr06BFcfrmdlUtsypQpwa677hrsv//+wSuvvBI8+OCD
oVOjT58+UZGwjoEDB4aODTuLvH/llVeGr0+fPj3Yc889g+7duwePP/544Pv269cv2GOPPYKJEycW
1pFpe3yBnSyu76KLLgrGjBkT3H777cE+++wTHkcVejDi/h1zzDGhA+zRRx8NevbsGfTt2zfwwKsk
Mws7gy677LLg448/Dk455ZTQUWPHTWRu84knnhg8/fTTwUEHHRTyGT16dHDUUUeF5e2YiyxTDvPn
zw/5HXzwwcGzzz4bOrLOOuusYMcddwydVVF9L7/8crD77ruHSwntKBswYECw2267BS+84M93DAIQ
gAAEIACBTAh4Qsg5JD0+KMmcJuCAAw4Iy3uiJ1s2bNiw8Du9tPVdddVV4dimtNfFy5f13vE60u2P
HDkyOPbYY8MJx3Tl/FpFt6Wk+/M6BCAQnCsGdrjsJfWQbpdelm6W7IQ5UTpbOkiqaXaMGuxoCX4s
1bR3rhq0l4ikFG/CfffdFzqRVltttTCi6LzzzguGDx8eDpQc5XLHHXcEdibZqeEyNkfo7L333sFr
r70WbLfddsGll9oxHYTOHUc0XXDBBeGx/7FTypE7I0aMCNZYY43wvCNpPGi77rrrgmuuuaawrHdK
ao9zPHmwt/766weDBg0KGjZcsgT1rrvuCi655JJgq622CnbYYYfQkfX+++8HTz75ZBjR47rtdPJg
xgOv888/36dSmqOdrr/++uCQQw4Jzj3Xn6dB4OgmO5Pc3oceeqjwOs8E3njjjeF9Vl3VS0SXOLp6
9eoVOuZ8jS1TDnfeeWfw8MMPB7fddlvYF1/rwaodZWeccUbw2GOPBT/99FNw8sknh+/R2WefHdSp
Uyd87+z88v2ef/75IGqLr8cgAAEIQAACEEhNYO7cuWFktvNIeuLIEcjF2ahRowJHYdsWLVpUXDHO
xwi8+OKLgSfZ7LArbcRVrBp2IZBTBPS7Ik+/t5rpd8SvmvSeX006l692nCxdJXkJWCq7SSd3kS6T
HpfmSv6B11EaIS2Q4mbn09fSd7GT/kG5g7SO9I30qvSTFFlU37M6kS/tK82SXM9s6Z9ZfR0UWGdt
F0pvRCdSbB262UKqIy2Ovd5e+ztLTiRsJ9MPUkWYo7q2k9z/sZL5uT9Jq6sTW0vu04+Sl6J5mzTX
k45jsnxxx+bRRdpU8vv3sTRaSvUl53J2xv0qPSd5WVutsLxa0ctSdvKwww4rdBA1btw4jDyy0+TL
L78Ma/J+vXr1igyY1l133XDWyEvKSjI7nBwxFDmRXL558+ZBly5dwsii5PUlteeRRx4Jfvvtt8CO
msiJ5DoOPfTQ0Kn13XffhY4vO2N8by8Li6x9+/ahQ8bOGD/Frjhzn23xJXB21thp46gmL1+L2157
7VXEcbPhhhuGbbFzywNUWyYcPCgdOnRoOIi1Qywyc7YTye1xu4cMGRK+H3aouV02v3d2knkJ3913
3x1dyhYCEIAABCAAgQwIeKzj7+105tddDsucgCfuPAHntAgYBCCwhMCtt97aUSsXpug3R+9qxKS7
2lJfsiMpnfn1NSU7Hmw7Sk9KXhaXtEd1Yp/YyT21P0YaIPn6odJX0t5SZFF9R+iEHVAue61kh9JI
aUkUgXYKzA6i5yU7uNLZbnrR7XQfbS0lO6fek9wuRyz5fnaQZNNWUWWfF6intu6fv2y+kNaW4uYf
dsOlG6SNpQskl0v2LROOuqxEa6ISj0vPSDtJvSQ7rp6S4j/0G+j4XmmU5L8Tyw69LaVaYXzzp3ib
d9ml6N/lxhv7bzYIosTZdtg4f5KXTW277bZhTiXnVdpyy8z+bpwnyaHgr7/+eriUzfX6eOrUqYEj
f5JWUnscDbX00kuH+Yri19qBog/l8JRnvuyUWWaZZcJIoXi5pZZaKvDyMTucNt00+vyLlwiC5Zdf
Pjj11FPDiClHXXlZmSOvzMYckpZqcOQ8Uw7n9vK9ddZZJ8wXVRIHM3H0UTy3VHQvc4nYOLeVHWR+
X5LmKLLICZh8jWMIQAACEIAABFIT8HLxZ555JpyMWmUVj/uLmpfVW548crRz0r7++uswosnjnDZt
2oRRxU2aeIxe1ByV/fbbbweTJk0KxxXRuKtoqSWR0OXJ85isz8cVce+Scjp6rOc8mp6kiya/KqIt
TmmQLq9lKh6cg0BVEdDvlnM1yb5IqwwG6P/GEP3fWDKLXVUNWnLfNtr8JP37B1rRdtmxYXP5f/8Y
CV9K+U++zj4k2Xl0kuQ++0PSjqIHpdbSRCmyi7Wzs/SC1FBaRzpO2leyUyOyg7VTXxoWnchwe5rK
2ZnUTppQcM0G2n4k7SrZaZUNu1qV2PHWVvq+oMLVtP1Wukhy+yOz82yQdHjBCffrTukRqZX0s5Qv
lYajihdrzkXTWdpQGltQaiNt7Vi6XDq24Fw/be3s6yq9XnDOfXi4YD/nN/Vyvodl6KCTYsetfn3/
vf4Tsu3BlHMA2TnjHElePuXlb3Z2eElbSWHKDhM/6aSTwkgaL0dbb731iiSsjt/b+yW1Jxqg2SFU
nEVOMOcwcs6mpNmB5CVp6ezoo48Oc0M5d5SdYI7ycdSPl445P0Lc3Kek2dFj+/XXX0NHUiYcXNZm
TukscsY5OitpHrSutNJKydMcQwACEIAABCCQhoDzQXrccO+99wann376v0o6GsmRzXb8xB1JCxcu
DK699towUtsOpGbNmoV1eDLL5zfYwL9LlpgdHc6t6LGOv+u97MsPL1l77aKT0naKnHnmmWGORy+1
syPmiiuuCCfzvDw/lYMqukdx22zf25NyF198cRht5L54/OiIaS/lN6uoT56Qc8oDLwmMxm7ZbovH
pjfddFPgMavHX76/k5e7PcU9/KQ4TpyHQEUT0EqNlppc7tayZcs8/d9uotUIe+qejgCpastXA37M
oBH+wTJHys+gbLxIfx3Ulew8iRxnM7V/sXSCdKIU//C9RMd2Itn+lj6X3pJ6S/dKkfXUzovSD9GJ
DLd1VG6+5DZF9pl2ukmzohNZ2A5XHddLkRPJVU6SnpY290HM7Ci6MHbs9pmDnU19JUcolZajLklp
dqL1kW6UxsZK2JF2h2RH29mSHYunSq9Ir0uRuQ92kv37B2lUIoe2OJLK+GY6jNtROZbto48+Ck47
7bTQqXLPPfcUW6sjcLzcyl/sHixFAwhf4IGG18uX1pz7x09IS2WO0HFb9cEcvnzkkUcWRvGkKl/S
OdfjxNmWB3XOheScSV6mFx+YeIYvvnTP9dpxZPMAMVMOUV6jyKEUVlDwjweRHuA6Msrt8hI3lrDF
CbEPAQhAAAIQKDsBO0L8oAvnKvTDOvyU2sj8Pe/8SHacREvWo9e8bMv5JO00iiKHnc/RuQyPO+64
MA+lJ6L8oI///ve/4fjBzqBoTOQE01FEdVSnH1hS1jyPUR3xbUXcO5OcjvE2RPvZbktp8lpGbWAL
gaokoN8FZ2rMn+ffLHJ8NtFqi4FqT3VwJE1RO9Ytho0jguzMsS0j+QPS5UtjnVTYP/7sqEnaNzqx
UeKkHS1Ju0Un7pTWksZJnn3fWNpfKq1dpQt2lpzPZZT0rDRCeknKpj2vyuyHcL9bS82l+tLq0kpS
3F7TQTI30Rc6Z9bRrERpOcbrj++7PjvRZkqHx1/Q/lypgdRe+lHyMrdUXEbrfK2wvFrRyyx30oOq
5JNMNtpoo6Br165h8urk7aL8Qj5vh4+XavXs2bNwwOTzHmC99FKqv0W/mt48E+hwcD+pLG4e5Lmd
H374YRj15JlAJwhPmhN82xnjNhRnnj3r0KFDkXt4EOgnwNm++ML/n/8xR2wlzWHrdgxFjq9MOPjx
vWuuuWa4JC5Z3y233BIMHDgwHNhusskm4dI8L/OLmyOV7OAaPnx4/DT7EIAABCAAAQhkQMBPYHUe
xCeeeKJIaT8IZLnllgsjleMveMxzww03hA8biZxIft1jEDuS/L3sp+Pa7HCaMWNGODkVOZF83ikE
nDsyMj/gpDx5HqN64tts3zvTnI7xNkT72W5LNO7MNK9l1A62EKgKApqAb6q/2V6aFF7K9/fKDv3t
tpEzedOqaE/innby5Et2IMTNkTtvSEcXnGxTsC36Q6TgZJqNP+hWlHqn0J86N1WK2/T4QcG+l1L9
LvUqOO6prcs9WXBcmo0dJB2kgyRHAp0j+dxIqYWULdtSFY2X3MZDJDMI339tkzYjeaLg2BFDkdOp
tByLqTJ0aPm1HaXke7KDzr0q+W/B7bXFo5aWnFkSuZV0fEWv5dQWR1IZ3k5HIXlJm5dROVLGM0mO
/PEgy86WuLVt2zZ05HgWzbmBHN7tGT7P4jlBts35ffr06VMkUXa8jpL2ncPAyb4ddu6oHy9R8/I1
Rwp54LbffvsFTk7tWUC32wM4RxM5/5BDnf00NC+1c9nizM4q1+EZQTusHL5tp9jVVzt6T584iX6/
8847IQ+3xTNjgwcPDmcSPfPogU1pODg3k5cR2uHlqCMPQh944IGQvxOKO8F47969wy8eO5ac68n3
tePKUWJNmzYNnzZXXN84DwEIFBLw7FqTwqPqv+PZoGbVv5m0EAI1l8AKK6wQ7LnnnmHEb+SgcATS
gw8+GEYrxR1A7qUnsTy+cQ7JpLVr1y7wBFGUt9BR2F6S5uVxcXPeoHjORk8SxfM8ehldJN8/yvMY
r6Ok/Wzfu6Scjh4zOt9kKst2W6K8lnb27bzzzuESQI/L7PhzXkvn1cQgUF0IaCL7WKegiFJ5+P+/
VjU0cs6katDGF9SGutIxibZ4Gdog6SZpN8lLrPzD7gPJNn/JJmhasI02dnzUiw60dQTRr9L2xchO
lpLMUVHDpZ6SnRyHSY5Qitqg3VLZApX2zEEfaQ1pa6md9KCUDXP/n5HsqPI4zvUfL50mvSMlrXPy
hI4dvbSJNF6yZYNjVI+3/oG7fTF6QecnSLatlmyK/OtztcLHEv9DLkKAg+IJeHmYl1X5qRvnnGNH
7RKzs+WSSy6JDsPt4YcfHjpAevToEey0007B9ddfH+ZR8hp1J6T2l7mdKnaEeLmXHTWlNTtmHHHT
v3//cFDnD2AP9lq3bh3mb4pm9TzD5wGX7+G22+yEsYNpn332SXtbO5nsDHJ/999//zA5pO/hAaCf
nuZE2nFz3oJ+/foFAwYMCAd47qfzAfhJbTYPcpxPKhMO3bp1C8Pj7SRyFJLNoa+HHHJIOLvpY0dH
2UH2v//9L9hjjz0K29exY8dwZjQ50PU1GAQgEBJYQf+eKTmEd9XwzJLBkMMX/UHxfcG56ri5XI3y
AKRDdWwcbYJArhA44ogjAj/d1ZNmdhA56thRzI5WSpone2x2WiTN4xOPTaIynmjy0vR45Ex0TTzn
UVTey9nLmucxqjfaZvve0RL8knI6RvePb7PdFtddmryW8bawD4HKJKAJ7nqaAO6n1QdFvJte5vbD
Dz90Va6vfK2uGF+ZbUrc6xMd3yX5B9/L0hdSZD7fWnpIcjTN6VIUPeMoHltXyU6dyE7STtzJ8LqO
XbfHMZ9LkTnCxvfyD8tro5NptkP02qkFZX3t0DRl0730pl60Y6t7rNBb2nfkUK/YufLs2inlsecp
0txYRY21v1fsONrdSDsrSUu+XJac3Uwb/828tuQwzFOUDY6fqr6Z0sHSIwV1R5uLtNNTWk+yE+wr
qYuUtJ2SJ3L1uFY7khw66SeKRObIHSuVxct5wGOHhaNr9GSBMPqlRYsWKZNse0DhiJ+42dFh/fjj
j+GyrHgi6PigLNP2uG47ZpxY0QO7cePGBZ5B9BIyD9oi874dW3bATJgwIXRc5efnF1liF5VNtXVy
TM8AOsR8+vTpYaTPaqutVjiDEL/GA0BHO3lW0oMrJ5hMDhQz5eB6nfDTIfJm5mgqL3dLRlA595Lz
U3nJnKO/nIcqzjbePvYhAIGQgGfKXpSWl5xE8D3J3wu7SHtK3aSNpZ8kDAIQqKUE/AANRwg5B5Ad
Sf5+dzS0o4uSFuVHnDx5cvKlMJfSJ598Eo6B/KLHKXZOeeyS/E6PO4zsbLKVN89jWEnBP9m+d+RI
irbxe8VzOqaKSsp2W6J7m1smeS2j8mwhUNkE9PvqQK0cqO8J4bj5ydP6jVFXvzk80XVc/LUq2O+n
e7aV3pUulOz88QfcdlIbyQ6NBdK9UmQfaMfOBjtL7Fyyw8PRRQdIi6TIrtBOb+lm6WjpO8nhnP2l
PyRHPGVi36qQHV19pdHSWKks9rguclSDnUbPSHb0bC4dIb0vZcMmqZJ5kn90j5TMqaPkvs6WkjZB
Jx6QjpK830myo8x9vkuyZYvj76rrAukS6VjJ7+my0j6S/xYdmTZLsg2QHpYulq6VfN4OOL/Hqfqh
07llebnVncrtjQc9XlLmZV0lPaktVcv8BZ9tR4fb5PbYwRN3IsXv7w/ntdZaK3wUb2kjdVynBzyO
9HEdURhqvP74vh1a66yzzr+cSPEymXLwvT1AdWh8csAZr89L8Mwg22zj92AfAjlCwF+M/kLeQTpH
ekp6TPJgxo6kZpK/IDEIQKCWE3BktZ0+Xi7lpViOUkplXk7uSbRHH330Xy/7yWReFrfVVluFr9k5
5Se8+YElcbNjyQ6nyOzI8vd+WfM8RvXEt9m+d6Y5HeNtiPaz3ZbS5rWM2sEWApVNQCsMBrZq1apJ
qvtqgngpfT70UOL+5VK9Xonnpule20iDpP9Kb0jjpOGSo6LtYPCE2w1SZHYq9JBWlTyucjTN8ZLH
VnY6ReboF9exUBojzZFelBpJHqPZ4ZKp3V5Q0E6Wsto1utAaLLnfdqyMkj6TeknZsF9VSU+plTRV
+lO6Xxom+b5JG6ITH0tfSnZs2aE1WbLDLWKZTY5Xq97TpCslt+1H6VLJDqbhUmSPaucQyc67nyW3
4TJpb2m2lPOGIynn32I6CAEIQKBYAmvrFQ9aUkUceTbNM0CbSUlbVyeOkgZKh0qejYvbTjpoL60o
eeBxluRBWB3J5sHDcZLPbyLFbQ0d7F5woq22x0v9pC4F5zLZrKZCh0v+0nf7VpcwCECgHAR22GGH
cJLqwgsvDPyAC0/qFGdeau9H2zt625FJzltoJ9TZZ58ddOrUKczT42sdaewJOS99t+PITiWXd27E
aFm+y5U3z6PrSFpF3DuTnI7Jdvg4220pbV7LVG3iHAQqmoDSVXRV3tjmnnROZZ6s9kS9cqD5h3pV
23w1wGMWjy+aSZtLHlt4zHKVZIdCd8njqshGasdl1pc87llP+lZqIDmCJrIftOM8QAaxhdRC2kr6
XIrMTiKPoTxmK8487vpNsoMjUxusgq53XsEFi7S1E2UlaUPJY8BVpO0kt7MsZmeR7xFvlx1HHaR1
pDaSx3vu4xCpqRTZGtq5XDpDskPR7TGnHaXpUtwy4ZiqLceqErclssXauUHy/fxF59eWly6S/Frc
HtCBx7oeF7sva0l+31aWBkk5bfVyund0rtIJOPrIs5ROKI5BAALVnsDrauE5kgc0Z0oeKMXt4PiB
9j2YeF7yF+b7kr9QB0gTJDuPvpdswyS/7oHBNMnlHSZsx84r0jPSZ9IG0sXSidKNks2DA4c39y7Y
vqPtMpKvfVw6Uko3kNpTr98teYDh9pwuLZSOkHw9BgEIlIGAl6cfdthhweWXX15sNFJUrZ9ke/fd
dwdnnnlmYAeUI4p9fffu3cPciVHEtM95uZxzLXppvyOmfe7AAw8MnybrvJKRlSfPY1RHfFsR984k
p2O8DdF+ttvi6K3S5LWM2sEWApVJQP/fz8vPz2+c7p5aibC0cqSdqZyqV+lJ0ckxSrpLK/K1n1W5
FTePVexE8pgnbnbQONIoE/tdhTx2Kos10EV9pLukuWWpIHHNLB17nFbRNq4UN3C/Ps6gfHk4xqv3
2PGfHDjxV4rueyz8bdFTtePI3sEqNeX1WdylS5cqbQM3h0C2CPjJMnqK33kabPtHLwaBmkCgvxrp
v9dfpcckO1vs7Ek1YLtP5/eQOkl20tg8O+cv0KekyPE0UfstpV2l56S60tXSSZLvs43kL2c7iFxn
V8kzcB649JJuk76T7FRyXTbP/j0v2eF0tmS7Rdpa6uADWb70lTRU8r385d5EGiCdLLWWovq0i0Gg
fAQYw5TMz09xi3IlplsO7/yHzt/oZNzpyjlqqSx5HtO1NNv39sNI0uV0rKy2uB2Z5LVM157q/pr7
p6WW98jJcHh1byvt+4eAopHayYH6wTbbbNPIjtR0pmjFmUpIf8Kxxx57V7pytfy1Hur/cKm99KWE
QaDCCaT/n1vht+cGEIAABCBQxQQcKdRKsqNnI8mOH8+o2RnjAUnchusgHnnk1yZJT0t29MTNjiLX
ZVsoOUrI9oLk12x/SfdLdii1lSLzJIfLx50+7+p4pGQHUUMpldkpZqfVRZKdSLaZkqOeFkiOfMIg
AIFKJOAlal4Gl8455OY42W779u1LLFeePI/FdTvb9840p2Oq9mSzLW5HafJapmoP5yCQbQIjRoxo
oP/H5+vBOfVLciL53opK8oTQ8Iceeiht9FK221nD6ltK7T1HwolUw964mtzcejW58bQdAhCAAASy
QuBH1eLlbdaq0n+kvtLH0kGSI5Vsjgjy90Y3ydE9zaX60urSSlLcXowfaP/TguO3Eue9ltzm9edx
872S5kipA6W1pVSh4o6U+lZy+5L2jU7YUYZBAAIQgAAEai0BLRPbZtGiRa/L0ThMGqUH77ygJyL+
kW0gus+ychS1VRRhW92ng5xHGytKro0iClv4Xn7acibmHEq6frEiG/8cOnToH9r/Tu3/WPpM+1+p
/V+p/VMzqSuHy3jyD4NApRLAkVSpuLkZBCAAgWpFYAu15hfpu1irJmt/sHSb9IB0r7SCNEfaUnpY
stPoQ+kT6W8plc1PdVLnokih6OXkcXQ+cjxFx97aSWRrJqVyJEWOrd5hqaL//KnD2j7QLEqEIwhA
AAIQqI0EtlGnB0nfybHT4++//75dS80+l1NmlBw/o5Tc/v0DDjjAkcQZ2bBhw1aVs6idHDtt9RS2
jqqno47XVt1LKxJxtnJ25TVp0qTx0ksvXUcKGjVqFOZNy6jygkKdO3cOV9HMmTNnBWkzPdnRmq10
EvN03EBOK+dY+0HFP1dy7o/sYNL+11oON7Y096EsBCCQOQEcSZmzoiQEIACBXCPgZW0eLO6UomPz
dO56aW/JkTxeWvaM5OVmjghy0sPIrtLOBtFBlrarqZ7vE3VtVXA8PnE+OnTSRju5to9OsIUABCAA
AQhAoCgBOVrmyMlyg87eoCVjS/3222/byvHTbcGCBUMU+dNSjqUX9dooOYKeTRftozxxe+upjI/J
UfSH1EBOo0Z2Fjnhe8Fy1mWL3rl8R3ZCWQVPeltatVl+slswe/bsdnIuWXvbyfT7778vJwfTlcqf
dWb57srVEIBAKgI4klJR4RwEIACBLBO47bbbVtBA59f5i5d6tX6deY945q8azJSNVjfPljaUUkUA
tdN523jJ+45MOkWKO5Gcs2AvKdt2qCq8IFHpjjqeUKDES+Hh6/r3HKmDFC2Z8wuOVPpCsuPsWgmD
AAQgAIEcJiBHyM9yjHhiAUsQUORQeEZOoH7ecTL8pInd/jq3v6KVArHcX+OVR5JlfKzzT2p887SW
l3Vt06ZNI41tUhWr8HN+WrQiqULpZkv98MMPfvjNt+qrcyRiEIBABRDIKUfSjBkzAoViho+m3XBD
/y7CIFB6AieffHKY8POoo44q/cVcAYHiCawwP2gw67MF7To3z5u+afN6v1x2481DlWy6zoj6eQuf
VDj4S71793Zi6Mq0a3SzA6QR0vmSHTGzpc2lbaWjJS9tmyR5CZujlPaTRko/Sx2lmyRfk23bQRWO
lx6UnIz7eGk7qZe0ZBSsnYRdoWMva7tZctu/k9yP/pLzP7itWC0mMOiWO/rVX/z3ZYuD+jfXq7vo
af1YelWz1RXx91uLKdN1CFQ9ATk4Vq76VlTPFihK53/67FtOjP7nFjoi6c8//3TepG7SLnIGraHT
L0ijFF309BFHHDHd5VKZyi565ZVX9hk7duyLX3755WZKmN8oVbnKPKcnJi6QpmtctY3G8n9W5r25
FwRqE4GcciTdeeedQcOGDYNcdyL5MbVaDxysvHLVf0e6HXbgNWvWrNTrnavrf7Tdd9896N+/f3DQ
QQcFCtOtrs2kXTWQwKKgzuLJi1YNpKX9DLEmdWYu06zur0essnjK3sst/rPRoJtv+6p+sPBR5ZQc
ue66637SpUsXlapQm6Xad5U8Y2cnS/w7wY6iSyVH8dg8ZdlTslPGuYZ87WTpSql+wVabrNnhquku
aai0lOT7HSPdKRVndsRtLQ2Xxkh2OOVJb0v7SHaEYbWawOLNPl/QPqhfZ8Exqyyeeoj+DzbU/7tP
6+UtfKSunjIop1I8kq1Wk6LzEIBA7hKQI6m1Io3+qx52U0RSZ22/kEbJ+XJMnz593rODKNPee6xy
xx137KLlcW98++237RWZ1DDTa7NdburUqYvGjx//u9q/Za9evTyOwSAAgQoiEP/RUEG3qJxq7cyw
I+m8884rvKHWxobOgDPPPDPo2rVr4fmq3HH4qJ0TCgEtczOU1C649957g/feey+sw2Gnp59+erDV
VlsFhxxySKnq1RdJMG3atKBFixYZXTdy5MjgySefDC688MLQkfXss88G55xzTvDBBx+Ej+7NqJIM
CqVqV3n6mcEtC4v4b8Vrr/33dMIJJxSeL8uOZn2u0uzOaWW5lmtyi4DX7zcI5gd7Nng22TE7OpZ1
Cmo5kRxKuaH+W17wzTfffKT9jZOFK+D4R9V5hGQnjWchl5PsILIWSnG7XwfWWtJf0jQpsiHRjrau
J2l2iqWKef+ymPOeRewirSw1k76SkgPbY3UuaU626UHx8lJraaIUb6cOsdpMYKH8peMX5OeNDVov
W6/OwmClOr9s1rzuzxs0z5t67o0337qwTp16z9XPW/CEvodGyrHEbHZt/mOh7xDITQJvqFuXSn4C
2p3KZXRY/KltZYnI1/V/6/dJF/2meFdJr9eWlf2HThmZ//zzz4vlyJqhhOFbyRn2Uxmr4TIIQCBD
AjnjSLrrrrtCB82uu3pyfYkpYVygR0wGSrgWnarS7bvvvhvogy146aWXwgiebDVm8uTJwYsvvhiu
cS6tI0lLacLHb15wwQUZNcf3GT16dKAP6gqNiErVrvL0M6POFRTSF1CgL8Tg6quvDnr27FkuB5l+
hJyuai2slhNQDoHWs+bX/ej5uV0Lw9wUDREoImnRKnlTZjat80ejRUE9RSQteLRevbwRrVu3/rSS
kc3R/b7J8J7jMiyXjWKeUSzLrOLvuu79bDQg0zo0u3uYnA93Z1qeclVAYPHcoGO9T0Ml7v7PDPri
hQcuXBgc6NdvvfXW9Y8++ugxibIcQgACEKixBDQ2tSMp1cROufrk5fmahN120qRJH2nCfJWWLVs6
WrlSTNFQwVdffTVLjrHO+q3lZe3VzZZWgzxRN0XS1GGlW1Xfv9I7HLuhJ2wdMfGb5DQNWJYI5Iwj
acSIEWHUkUIys4Qm+9Vo/XH4VIFs17zWWmsFjz/+eMZRRfH7//LLL6EjKX4u3f75558f9OjRI9hg
g2w/oKnoXVO1qzz9LFp7yUc777xzMHDgwOCNN94Idtlll5IvoAQEMiCQFyyqt2rdKUHzOtNmK0fS
4rzFC+XlrtIcSRm0miKZElC+iXtU1sKqKYFBt9z+6Jj56+3z48LVwxYWRCQFikj6WxFJC3SsyLm8
55fKW/i4I5LkRCIiqZq+lzQLAhCofgT0G+FXRSZtqeVlHykyaaVVV121bkW30r+vPv/8c+e620mf
2Z9V9P3KWP9Bum6YtKw0o4x1ZHqZnYSrST/FLqjM+8duWy12HdU+SdpPerRatChHGlEvF/qhUMZg
3LhxwSmnnJK2O45okbc66Ny5cxipZCeBl0t17Ngx2HzzzQuvdTlH3Gy//fbB999/H7zzzjt+pGTQ
oUOHYIsttigs550333wzjC5aZ511ipzXB2hguQ49NSBchqYPubCM7+snC6y33nrBaqv5/3nx5mVn
b7/9diDvfrDxxhuHSlXajhcvl1t++eULX3YuJUdBaYlM0KlTp2CjjTaKHsUZfPbZZ4G5uYz76ygp
P6Zzm222CY/NyW33fZ9//vnAj/J0tNMff/wRXuelZ/L6F97LO27r66+/HrgtZuWldvEymbBK1y7f
I1U/ff7rr78OPv744/B1rc0O7x3Pb5Tpe++6bF7apoiQkB+OpCVM+LfcBH6rH8xrtGG9L16rF8x7
uE6gp7b1PXZsuWvNvQoc/XG9RD6j3Htvq0WP6mrFZpt64xa1qDtlZpNgRsOFQV1yJFWLd6Z2NaI6
5bssL/lc6kt5WXC9nnjRu/fk22+/fSsl4P5AT1NbTjldi/5gyCIk/+3pt8Ns/S7prsmcd7NYdU2u
apQa72X+x9TkTtD26k8gJxxJdnrY9KSAtMTfeuutMJ/P2WefHVxxxRXB+uuvHzqW7EzyUqZ+/fqF
17ucl3pdfPHF4dbJu51UetCgQcGOO+4YXHrppWFSbxd2Xd26dQvOOuusIvceNWpUMHjw4ODTTz8N
pkyZEka3zJ07NyzjJVNePuW8RukcSXYCnXjiiaGDx231srLVV1890LrjIvfygfNA7b///sGpp54a
vqZHegY33XRTsMoqqwT5+fmBl/65D8rZEzrD7r///jDaxqGglp1Ndp7YkRRxcv/dPzuRnN/FjqTX
XnstZGKHTzzPk57UEJxxxhmBEgSHjjPf2w67K6+8MmjUqFHYpkxYpWtXqn4u1PqDa6+9NnDeKDuQ
nPTb+aOWWWaZ8HwUORX1qaT3PmxowT/+e4r+tuLn2YdAWQgo1NohtRU2mCpLm6rpNR4IMhispm9O
zW9Wnffa1/u6sxYWPFy3zsKnNdkx+vhj+ngmOyctG3kZcxJMNehUMt9lMgdlNWhixk1I9iXjCymY
swSU6HqclgZvq98Hb+s3TJMVV1wx6331JP9HH300R7lID5ET6aWs36DmVthCTbcjCYNAhRLICUeS
B0q2TD+k7rvvvuC5554LnTj2ZDtB9/Dhw4MDDjggaNWqVVjXvHnzghtvvDFMLK2wzPCcnUL6YAzs
pCkp+im8oOAfOzgcqePIHjuGvAzNDo905iim//73v6HT56qrrip02vjLWh/M6S4No4auv/760PFz
7rnnhmUdQeQ2X3PNNX7MZ+gM8wvdu3cPI7JS5Ui67rrrQgfN1ltvHUROsOJufNlllwXmGjnGHB3k
PEdmaAdTpmYnnS1du+J1uS96UkToNIoih5wT6+STTw6OO+648H1u3Lhx4SWZvPdR4ZVWWslfUNEh
WwhAAAIQqOEETjz2yMvVBSvnraLyMuY8uCrqYGXloKyi7nHbWkjA+eX0m6WrnEmvaKXCMsst5xRB
2TEHAei3xmxNBvSVE+nJ7NRaabX4h6XzZtjh41yOL0pR3qQdtO/JjXekpHXWCT8E5Y3kCwXHm2nr
OptKa0rdpb+l56W4tdZBN8lvyJvSaClpXjLjtnjJjfNnvir9JGVq/vG1k7SB9Iv0tpT8UeXXJ0nO
abWr1FF6QvpUsrmOraQtpD8l9/tDKWkr6ERXqYPk/JijpY+lkqy8fSyp/px/PS8Xeugntil0snDZ
Vkl9OuywwwodHnYyOJLHjhZH1cRtr732CiInks87Mmm77bYLo3tKcqzE6ynLvh0k7pefnBCP/LEj
q3nz5mmrdF9sjnqKzEvMHI3j5NH24GdiSsYX2Ilk87K3dOZ6IyeSy3kpnaObHB3kD/uKMPfzhhtu
CJ1OkRPJ93E0kh1JXgbnCKe4Zfre+5qmTZuG70H8evYhAAEIQAACNYFAReVlrAl9r4ltdA5Kj/2i
cVdN7ANthkCSgJxJdpTs+MUXX/zp3zXZMP8GkxPpL/0O+K9+J92VjTorsY5OupejrveVdpOekZy3
x8mwbT4/UvrnARA+q3gJyQ6hdIlb++r1WyQ7SLYt2L9C27jZgfSytKfUX3pFukyKm19zmoEB0qbS
UMnLf/aWMrENVchOo9skO4F6S/47uF6qL0U2TDvHSuZhBo482F2ynSL9Id0huQ1uq+u4UIrb5jr4
TBokbSz5AUe+t4/TWXn7mK7uWvNavVzoqXPheOmVo4jiTpfi+hZ3OriMcw/Z7HiIW6ovc+dScvjx
xIkTg2RepPi15d13jib3K7lczw6hTTfdNHj66aeLvYXzJHmJmyOKvBTNy/HsAHM/d9vNn1mZWZcu
XTIrqFLxHFPRRc6R5Cgss3JUVrbNOZm8LG/bbf1ZWdTatWsXRqglnYOZvveuzV948WimonfgCAIQ
gAAEIFD9CGSal7Gk3IIl9cx5Fp3D0U/H9XeuxwHOo+iclR5z2DLJjRjdx5NDjqJyPkkl6Q3r3Gyz
zYpMirk+T6atscYaYaS3l5974soTVh4beQIobh7XOZrcZbx8P505mtnL4L3U3+Mo589MjsFS3d9j
LOe8TGeZ5LssLgelI+eLy3cZv6f7+cknnwR2IDqlwU477VSYWiAq5z46YtxlPb7xuNCpE+JWUh8z
6Uu8PvYhIGfPO3py7ST9H3ei6XKbgwf0f76BIpGG+2nYNcyuUXu3kSYUtHtLbe0gOk/qJ9kRdJy0
r3SvFNnB2rETxs6X4uzIghfsWHEE0DEpCp6lc3b0/C75A9OOGjtfbpX8RN586SHJzqOTpMVSE8lO
pQel1tJEqThrqBeelMZLdgD9KdnsIHpE+km6UorMb+BTUmdphtRAWlcyp8HS8ZLN0RG+7mzpPsmO
LUcs+doPJCfSniPZTpTstHpBSvWDOV/ny9NHXY6ZQL1cwBANDrzEzTmBSrJkdI0/kGxaY1vk0lQD
A38523yvinQkeUChx2YWGUCFN9Y/8STS0bnkVjMAwa677hou7fKyurvvvjscNHh5m5fwZWKZLhV0
XakcLhGr33/3Z1X2LXL8pQqVtcPNybKjMtHdM33vXd7vcWkYRPdgCwEIQAACEKgqAiXlZcw0t2C6
9r/33nuFORztbPGyLJtzRg4fPjx8wIiPM8mN6HJ2ljjXo50YdvrYSeVcls616OX90bjH9dlx88EH
H4QPRfF3uiONldg3jER2FHfcnA7AkckeB6UzO9VOOumk0BHmh4X4+/+iiy4K63a7onFiqvv7acGp
xovR/TLNd5kqB2VJ+S59D0+kOqelo5nsFHJbnQ/z8ssvDyPoo7yafl/M1Mv27fhzrktPxvXt2zfs
e9TedH3MtC9RXWwhEBHQ506+c65mw7ziQoEDf+v/91qqz8uiapLZQRI5kdzut6VR0gnSQOlz6S3J
UTz3SpH11I4/aH+ITpRxe4+ui36Y2XFjx9Q+kpeV2ZHkyJ+60kWSnUi2mdLFkttoJ40dT8VZL72w
utRVipxILvusZMfOOdJ10nzJ9qvUU4qWr8zVvpfOHCTZSRSZf6QPlU6V7KCyI8mMmklXSZETSbvB
TVI3qb2UypFU3j6qWswEcsKR1LZt2/DddPRJJo6kTN96z7p41itunsmxOem1zQMID4CS5kFQecxL
6vx0N88eealW3DyAysTsiPKgynIbna/IOZPcp+TT5zKpL12Z999/P9h+++2LFIlYRUvess0qem/8
RLakOezVM3N77LFH8qWMj/335CfdYRCAAAQgAIGaQqCkvIylzS2Y7Lcjnk444YQwAskPD4kiwZ2v
0HkYowdsJK9Ld+wHc3gc8eSTTwbRJNSYMWMCRRyEjiQv+4rs4YcfDnbYYYfgnnvuCSexHI0+fvz4
4JFHHgnHO1G5BQsWBE899VQ4qRY5oqLX4luPF5xT0WMV1xGVHT16dOgscwRUPOoh1f3j9cX3Kzrf
pe915513Bm6TIj7CJ9b6nKOS9tlnnzBH5WOPPRZGiTmX5cEHHxzmBXUZT57asWRnlcdKkcPJr6Xq
Y3n64jqx2ktAD/xppgiivMghmw0S+m20QCsHHAr4XTbqq8Q6XklxLztY9pXWlr6QHJV0p2RHmZ07
60sbS/tL5TXnIIrbawUHdsjYOknfSnbEJO0bnSjph5FfHyt9n7xYxyMlLylzP7+WbC9KkRMpPKF/
fpQc/dRK2lpqKTWWIr/FStq3bSj5R/ibPojZQu3vHjtO7pa3j8n6au1xXi703F/yHnh4piSblmr5
mMO47eSJcifZcfXOO+8Uua2XRDmhdtIcJWNz+HZJ5hBtzxomkz3bsWQHSTrzrJZn1BweHpkjhpzz
yKZ1ytHpcJtJe4pckOLATq+kecbSA7OysHJdJbXLIeyefXv00UeTtw7/Fjw49PK6spijqLy80GH1
GAQgAAEIQCAXCPh7tbS5BZP9tpPB4xxHPkdOJJc54ogjCp9om7wm3bEjqFzn3nvvHY7lorKOdLIz
xI6QeGSzo5DtFIkiod2Ggw46KFxiZ2dUZI5CcmSR82CmM48hpk6dGkb1RE4kl/fkmFMc2NHiqJ/I
Ut0/ei25reh8l3YGDR06NIziio93ll122dCJ5MgNs7Nzz06+6OnEbqdfiyLU4+NFv5aqj+Xpi+vE
ai8B5WZdT3+DSWdBuYDIkdRIf//p15SW6w4VdvGMFDXbcWOLHCQPa99RQ718UtZTmi49KZXXfklU
8M+H25IXmmuzotQ7hRxhNFVKZ77eUUapbEzByRaxF92vpNXVCUdK2Ul4qWSHkcPZkn4L38s/dudJ
pbHy9rE098rpssk3pMZ21rlvXnrppdD5kq1O2EH0xBNPhLmXvNRs8ODB4YyZn6YWJbJ2HoBJkyaF
+YicC8ihwqeddlqQKnwzipzyoMhf2umSULs/6667buCnqdlxZKeSI2+c+6ikZNte8+5BhGf4fB8P
gL7//vvAM4c2O5kic5s+/PDDsF9uf1nMuQw8YPNMovvkmTAPvOx0Kw+rTNrVv3//kPn//ve/kI9n
Ju3Ucmi2w+NLkxMq3nf/LTkSzAnDMQhAAAIQgEAuEChLbsFkvz3JYieOl0fFzWOBVPkS42VS7Tsn
kR0i/s71OCIuO4k8hvnuu3+CDuwwSS5T9zlHKTuiKDJP6HmJe0mRxY58WnPNNf8Vge56PMbzBN6P
P3qCfImlun/0WnJbUr7LZPn4cZTv0k+b3XnnncPoIY9L7eTx2MbjTDvAPOZKNVbxONI8XI8nPX2N
l/s7SstL36699trCibi4o85tSNXH8vQl3i/2ax8BTaSvp//fS2Wz5/r7b6CVDo4sqWnWOUWDo1nv
8QWv2ek2XOopNZAOkxyhlHT66FTWzRFQdgRtX4wO0fl05kikNYsp0KXgvMuks5P0Yi/J92opHSSd
Il0gxW2CDuJOqfhrO+tgh/iJ2H55+xirqnbv5uVK93v06BGubXdy52yZQ37t9LFjxkvBHDZsx45n
zSI7/PDDA39Z33LLLeEX/aGHHhrYOeNt0rwczmHaXq/vGbL4wCRZ1o4qhyvn5+eHM20bbLBBmHvA
y9X85LZ05sGYnV52fvk+vnb33XcPRo0aFc5QxQd6br8jhswvcjSlqzvVax48Ruv4PWBz/WZ1ySWX
BP/5z38KLykNq0zb5fs5/5OX+znU3U/Wc+SVB1XOjRBFgRU2IoMdz9g634LbkEzcmcHlFIFArhFo
pA49Jh1XgR2ro7pXr8D6qRoCEBCBKG+gnRFJKy63YLKc8+rYaePv/qR5Aqy0FrXJia7t+IjLjhNH
aHuSKLIoL2Z07K3b7ugaj3O8lN+OkVdeeaXEaCRf66ilVDz8mh1Rtni6glT3Dwul+Ccb+S7dJ4/l
HE3uVAV2bjk6yOa225IJs8OTsX88GekJNifg9ljPOaHmzJlTbOR3qj6Wty+x5rBbywjoN80Gcj47
oiRr5t86sg2yVmHlVdQtxa2217nx0kQpsiHaWU26VnIEzVApU/PSF4+rymKv66L20j9RB0tqcRt+
luzQSWf+Ie7x3E4pCu2mc19LP6V4LX5qGx3Y2ePlbXFL/rj2cphW0ibxQtpfVXpacj2prLx9TFVn
rTxXL1d67RkXJ1y088BJpm0rr7xy+GUZ9XG//fYLrFTmL9WkOcRZ63rDZIT+svb68SgSKSrbsGHD
MBrJAyHPDDkBdzRTZqdR0vxYeisT88DGIRN72ggAABW6SURBVMseFPmpKB7QRHUnHVWO/ombnUee
1XPI+PTp00OHiJeZRddHZT34sNMnbuk4HXLIIYEVWbysnXi+n0JYg1atWpWLVap2+Z7JfvqcnUlO
9OmZ1uh9SvYz3k5fE7fke+/Bp+vq2bNnvBj7EKitBNZQx+099yBicAVBcKLJH6RjKqh+qoUABEQg
G7kFPfnkpNipcjh6HBS3THIjeoLMduSRR4YTc/HrS7O/7777Btdff33wzDPPBM6PZOfSXnvtVWIV
ZmJnTSqLUiZE3FKVSXfOrCoy32X00JfIoRRvi51fds7Z8eToei/h8/K2eKS2x5eeOMvEstGXTO5D
mdwjoN9OnQocPyV2ztGJyd9aqS5yRJ4TeKd6rRqfW6C27SIdLjl8sqHkSbquUk9pkRTZt9p5Weor
jZbGSpmac6DYibKdNEkqKQJIRQrtCu31lm6Wjpa+k7aV+kt/SE5knc5G6EXrPqmn9JLkJXu+3g6f
3SU7utLZOL3oD2873V6QzMkc+kizpMge0M6Zku/VQ/pQ2lzqJ82UhkmprLx9TFVnrTyXMxFJfvf8
w99fnCXlECrtO+2ZGX9Zp/tg81Mw7PxIOjBKe69U5R1C7lwBpa3bgyh/8fsRtmuttVapr0/VlpLO
OXw6lcMtfl1FsfKSP4fal5ZTvG3e92yoo76Km6FMlucYAjlOwMkVO0ndK7CfxYUmV+AtqRoCuU3A
YwBbPN9gNnILbrLJJuFy+3g+It/Hk0gvv+zfPf9YJnkk/cQz/8h89tln/7mwYO+6664rXF72rxcT
Jzyh6KfG+Tvcy9q8n8n3uPMgefLIzrGkvfrqq+HEWIsWZfuIquh8l36yrJfljRw5Mtn0MFJ+4MCB
YX4kpwpwVHzcieQLUuUC/VdFBSfK05fi6uR87SAg59A6qVJ+JHtvh6iiEOcqZcii+OdWspyPC/Kz
1ddk+AqpXq+m5+xI8ljqXMmOjt8kO0K81OQuKWm3F5woTTSSLxkkObrJH8iXSaUxt2traaHknEZz
pBelRtI+0jypJHO5h6XHpb+kH6U9pJ2l56SS7EIVsKPNsvNqsmTHWGcp7khapOOukqNB/AHutr4m
5Ut7SnaipbJs9DFVvbXuXF4u9diDCH/p23GCQaCsBLws8Pjjjy/r5VwHgcom0Fg39JezByYnSRtL
cVtDBx64OAK1reQyZ0nbS5lacxVsFiu8k/bbS0tLrvt86Sgp1YCuSUGZAdr6C98zS5Ftph1f31Ra
s2DffYlbtvvnNu4vXSA5RNtOslS2mk4eLrncodLqEgaBGkOguLyM5c0t6KhvT655+bqfzuroH0cj
O4dj9JTWCFImeSSd09FPTXM0sJf+O0rGkU2OCHfE9EknnfSvp9dG9Se3TrrtJWDOe1RSku3o2s6d
O4fOKue39JPa/KAO399OGNdlXpFTLrom021l5Ls0d7fbTjcv6XOE/AMPPBA61By97sh5R305n6fz
WTriw8vaHIlkxpn8wHd/y9OXTHlRLvcIaBlmI0UOLZ/uaY52QuuzZJaelvyTcqLtOG7cuA8VDTjL
yynTmeqco7rbpStTjV67XW2xM8ZOj3UkL8ly25eXhkupzOOs36RHU72Y5pwjc3aRPO7zeMfm+3t2
YYYPYjZX+z5/S+zcD9q308bjpS2kFtJW0udSJuY6j5M8fvOYtKXksehLUtx8rl/8RMH+n9oeKK0s
2YHkiKa9penSKtI1UmS/aicaR26u/XypgxRfqjNVx+5jnGN5+6jqMP+BYQkCHiD56SPZfExl4hYc
QgACEMgGgVNUyZXSNOkjyY4Zf/FeLNlxY9tRGiadKF0lvS91kpaWrpZOl0qyu1XgNsmhyTbX97Jk
x5S/1D0w8YDHs0j+0v9Osp0tnSdNlL6V7MTyffeQfH1fyaHLbrO1geSBwvOSLdv98yDDA4m6knl5
1uEa6QbJfCLbUzvus9vyvWRGC6UjJM+wYRCo9gTieRm95MvRJx7fRLkFzzzzzDC3oJ0kjrju3r17
MGDAgBKdJs6NZAeEHS9+nLyP/SPRx15Kf++99xayca5BO3WcR9LyuMpL2Jx70rkVI3MUsCMM/JCQ
888/PzxtB4gdTH5yW6bmaClH6LhPpXnq6o033hg6xk444YQw2srREI5CsrNlyy23zPT2/ypnrs53
ecYZZ4T5Ls3K5w488MDQeeX3pTiL8l2ec845oVPMfXK7nHbB9UX5Lh155cTZdnyZsc1LCp2GIEql
YI529tkp53osX3/PPfdktPzPdZanL74eq50ElFOtjVYKzFbvmyYJ2An9ww8//K2/zQX62z63TZs2
N3Tp0mWBym2mz4e99QCjm+Vobqzca8v48yBpWrFRT07R9XT+jeRrNeD4xxLa2ECv95HukuyYqQr7
XTf1mLGsNk8XflLWi3Xd39JnGV7vSKUPMiwbL1bePsbrqnX79s5VqemDYrE+NKq0DdwcAtki4KfP
zJw58zzl63IUAwaBiiSwrir/WhosHV9wozxtr5TsgLFj5yupl2THj8t6hmqC1EQaIh0o2RlkJ086
s0Ml7kiaqOOWkh0s10l2sthJ86Z0v3S45JmsXyS37wTJ5u+cB6Q1pS2kyDxQeFs6Jjqhbbb711h1
fi95oLGfNEey2YHkX3PdpaelfMnchkp2fC2WzGuAdLLUWnL/MQjYGVKjxzDpcgtm8vbOmDEjfGKq
HVR2ktgxYkfSe++9V+TyVHkkixQoOHBSaOeE9A/M/Pz8aPlKqqIpz7k9joLyE2N79+6dsky6k07q
7afcOsK9rMvZiqs/Vb7L4srGz9t5ZCdQunyXLu9yfoiL72NnWqqcNI62Gj9+fJiCwM6mslpZ+1LW
+6W7zmz0RLl79KAVf+9g1YyAnJsHKkXIrcrdWuhI8t+qnkS9SP/X5upz4z79LfbTQ39+TTZ9xIgR
DRRJd7qi6PrLMV5Xf9d+UlthMf29L5YjatDRRx/t7+Zcsx7q0HDJY7kvJQwC1Y7AP/8bq13TaBAE
IAABCKQh4Bm+g6SnYmUWad8OkFOlTSU7RCK7UTt2ItlmSnYMHSxtJJXkSFKRf9kUnbEDZmHBK+9q
+5bk+9rsNLK5TZHZKWPnTGdpGekvqTjLdv/8q7KZdJUUOZF875ukbpIHa3Yk9ZfqShdJbq/NvBzl
ZYeYHU92oGEQqPEEnFvQKqs551ImTzd1bkSrJLMzyjkdy2pOKG1nVPzpuqWpy1FR0ZLA0lyXSdko
32UmZeNlHD3kfJdWOnO5kpKCO4dkWZ6sl7xvWfuSrIfj3CegSLZ2iqLz931oXn4px98sOTXH6LU+
cvh+Eb2W3CqnlyNxLtaDh26TQ+k6OZ/2UtRSA31m5bmsIiHrqA6PYXLRllKnzpFwIuXiu5sjfcKR
lCNvJN2AAARqHQGHRT8otZK2llpKjrqJPteTv9oe1mtxe10HdpS0iJ8sxf4TKusQ9Li9pgM7W2ye
XbRTxg6ZXSWXHyG9IbndJVm2+7ehbjhLctRU3OwI2z12opP27Vizcylp3+hErg5ak33lGAI1ioBz
/9x3331B165dg1SPr69RnaGxEMgRAnL0bKw8XHWdl0sOpL+U92imIpL0YOtjn8y0i0cdddQ0lT14
yJAhG40dO/Z2RdWtrST9jZ3fS3W1ybSeGlbOk4IYBKo1gegHR7VuJI2DAAQgAIF/EXDUzK1ST2my
ZAeJo4SSzh2dCu3vaKdgO79gG87sJV7L5DBZn6+ZJ8Xru0zHD0n7S15W52ieP6WzpZIGSdnun8Mu
PpfcxnTmcvWl3ikKue1TU5znFAQgIAKdOnUKI4KqAoaX6TlVgnM9YRCAQLUh0F7Oo3lK/bBAUXMX
adnoNQcccEBJ38MpG6/lix/phY5eLvfpp5/eqLqW1jLY5l4CVxC9lPI6TkIAAhVDAEdSxXClVghA
AAIVTcBLxHpJXt4Wj/BxPp/Tpepi49SQywvUVNvzJDvAvpNekYqzbPfPy/raFnOznXXeDriXJbfX
0VzbSxgEIFAKAs5PZFWFrbLKKuET1qri3twTAhBITUBPYcvXUxAf1jK0E/QgI+dbLLcpmulBPQ3u
KTmozlJlA5Qna0VtPaGGQQAClUggPnNcibflVhCAAAQgUE4C2+h6Oz3iTiRXeaj/qQa2q9rgqKUo
Z5KbNEO6xDuyTZZswn+9xC7KqRSdznb/vKTOywDj9/W9VpWcG8n3s70uOV9SBx/EzJFKP0unxM6x
CwEIQAACEIBAMQSU9L25IokOyJYTKbqNoprmaMnbuXoS5IrKs4QTKQLDFgKVSABHUiXC5lYQgAAE
skjATqR8ybl8/Fm+tHSa5EieWVJVmx0yv0tXSHbeeLmYI4K83M32/pJN+O8n+teOHIcyrB2eWeIk
y9d+tvr3gOr6VLpP2lJaStpWcnTUTGmYZHN7vUTwZqmd5HJdJV/3h+Tk3BgEIAABCEAAAiUQyLYD
KXm7Pn36/JY8xzEEIFA5BHAkVQ5n7gIBCEAg2wQuVIWPFMgOjsmSnTGdpergSHIb9pIccm6nkaOT
/PSR/aQzpNFSZIO0M1F6WYocTdnu3yLVbYfQ15LzSc2RXpPypT2lSZLNTqWtJSfhHiO53ItSI2kf
qUy5HXQdBgEIQAACEIAABCAAgZwgQI6knHgb6QQEIFALCTjx84FSQ6mNZCfNAsm2ypJN+O/t+tdK
ZZlOJjRLXLxG4jg6vEA7VmTvaWdDqaXkJWSOUJog2akUtw91sEv8hPYron+/ql5n4vXT7daTvFRt
ouSldXH7QQd2yC0vtZZcZpqEQQACEIAABCAAAQhAoNYTwJFU6/8EAAABCNRwAnbKfFaN+2AnjR0x
VlmsIvrnaKkPMmiMHV/xJXgZXEIRCEAAAhCAAAQgAAEI5DaBvNzuHr2DAAQgAAEIQAACEIAABCAA
AQhAAAIQyBYBHEnZIkk9EIAABCAAAQhAAAIQgAAEIAABCEAgxwngSMrxN5juQQACEIAABCAAAQhA
AAIQgAAEIACBbBHAkZQtktQDAQhAAAIQgAAEIAABCEAAAhCAAARynACOpBx/g+keBCAAAQhAAAIQ
gAAEIAABCEAAAhDIFgEcSdkiST0QgAAEIAABCEAAAhCAAAQgAAEIQCDHCeBIyvE3mO5BAAIQgAAE
IAABCEAAAhCAAAQgAIFsEcCRlC2S1AMBCEAAAhCAAAQgAAEIQAACEIAABHKcAI6kHH+D6R4EIAAB
CEAAAhCAAAQgAAEIQAACEMgWARxJ2SJJPRCAAAQgAAEIQAACEIAABCAAAQhAIMcJ4EjK8TeY7kEA
AhCAAAQgAAEIQAACEIAABCAAgWwRwJGULZLUAwEIQAACEIAABCAAAQhAAAIQgAAEcpwAjqQcf4Pp
HgQgAAEIQAACEIAABCAAAQhAAAIQyBYBHEnZIkk9EIAABCAAAQhAAAIQgAAEIAABCEAgxwngSMrx
N5juQQACEIAABCAAAQhAAAIQgAAEIACBbBHAkZQtktQDAQhAAAIQgAAEIAABCEAAAhCAAARynACO
pBx/g+keBCAAAQhAAAIQgAAEIAABCEAAAhDIFgEcSdkiST0QgAAEIAABCEAAAhCAAAQgAAEIQCDH
CeBIyvE3mO5BAAIQgAAEIFAxBPLy8mbNnz+/YiqnVghAoEQCCxYscJmfSyxIAQhAAAIQyCoBHElZ
xUllEIAABCAAAQjUIgLjZ8+eXYu6S1chUL0I/PXXX/4P+H31ahWtgQAEIJD7BHAk5f57TA8hAAEI
QAACEKgYAo9OmzZtbsVUTa0QgEA6AgsXLgymT59eZ9GiRc+mK8drEIAABCCQfQL1sl9l6Wt89913
S38RV0CgGhKYN29e0KhRo2rYMpoEAQhAAALZJqClbTdPmTKlX4sWLYKmTZtmu3rqgwAE0hAYN27c
Qr38Qt++fcenKcZLEIAABCBQAQSq3JHUpEmTDSqgX1QJgSohYCeS8mX8WCU356YQgAAEIFCpBI46
6qhpQ4YM6T9mzJiLN91000b16lX5sKpS+8/NIFBVBH777bdg8uTJc+TMPbmq2sB9IQABCNRmAnVq
c+fpOwQgAAEIQAACECgvATmTbpATqU/btm0brrDCCuWtjushAIFiCGgZWzBhwoTFEydOnC0n0h5y
5o4upiinIQABCECgAgngSKpAuFQNAQhAAAIQgEDtIHDrrbf2Wrx48TXLLbdcXrNmzZo4QrVu3bq1
o/P0EgIVSED/rwI/nW3GjBkLHYWkyO8xul3PY4455usKvC1VQwACEIBAGgI4ktLA4SUIQAACEIAA
BCCQKYG77rprmblz5+6rH7676Jq16tSpQ+KkTOFRDgLFEND/p0V6aZq2n9avX//JXr16vVpMUU5D
AAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAAC
EIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCA
AAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAE
IAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAA
AQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEI
QAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAA
AhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQ
gAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAA
BCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg
AAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAAB
CEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhA
AAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAAC
EIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCA
AAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAE
IAABCNRyAv8HRK7hcRZ/yy0AAAAASUVORK5CYII='/></p>
<h2 id="Pool-based-sampling"><a href="#Pool-based-sampling" class="headerlink" title="Pool-based sampling"></a>Pool-based sampling</h2><p>Pool-based evaluates and ranks the <strong>entire unlabelled data</strong> collection before selecting the best query, whilst stream-based sampling queries and make decisions for one instance at a time. Pool-based sampling is <strong>more practical</strong> for many real-world problems.</p>
<p><img data-src='data:img/jpg;base64,iVBORw0KGgoAAAANSUhEUgAABLMAAACtCAYAAABLET2aAAAABGdBTUEAALGPC/xhBQAAQABJREFU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'/></p>
<h1 id="Query-strategy"><a href="#Query-strategy" class="headerlink" title="Query-strategy"></a>Query-strategy</h1><p>Active learning involves evaluating the informativeness of unlabelled instances by either <strong>generating</strong> de novo or <strong>sampling</strong> from the given distribution.</p>
<p>Let <script type="math/tex">x_A^*</script> denote the most informative instance (i.e. the best query)</p>
<h2 id="Uncertainty-sampling"><a href="#Uncertainty-sampling" class="headerlink" title="Uncertainty sampling"></a>Uncertainty sampling</h2><h3 id="Least-confident-strategy"><a href="#Least-confident-strategy" class="headerlink" title="Least confident strategy"></a><strong>Least confident strategy</strong></h3><p>The active learner queries <strong>instances about which is least certain how to label</strong>. E.g. for binary classification models, simply query the instance whose posterior probability of being positive is nearest 0.5 .</p>
<p>For three or more class labels, a more general uncertainty sampling method is to query the instance whose prediction is the <em>least confident</em>:</p>
<script type="math/tex; mode=display">x_{LC}^* = \mathop{\arg\max}_x 1-P_{\theta}(\hat{y}|x)</script><p>where </p>
<script type="math/tex; mode=display">\hat{y} = \mathop{\arg \max}_y P_{\theta} (y|x)</script><p>, the class label with the highest posterior probability under the model <script type="math/tex">\theta</script> </p>
<p>One way to interpret this uncertainty measure is the expected 0/1-loss, i.e. the model’s belief that it will mislabel <script type="math/tex">x</script>.</p>
<p><strong>Limitations</strong>: Least confident strategy only considers information about the most probable label. It “throws away” information about the remaining label distribution. Margin sampling incorporates the posterior of the second most likely label to correct this.</p>
<h3 id="Margin-sampling"><a href="#Margin-sampling" class="headerlink" title="Margin sampling"></a>Margin sampling</h3><script type="math/tex; mode=display">x_M^* = \mathop{\arg\min}_x P_{\theta}(\hat{y}_1 |x) - P_{\theta}(\hat{y}_2|x)</script><p>where <script type="math/tex">\hat{y}_1</script> and <script type="math/tex">\hat{y}_2</script> are the first and second most probable class labels, respectively. </p>
<p><strong>Intuitions</strong>:  Instances with <strong>large margins</strong> are easy, since classifiers have little doubt in differentiating between the two most likely class labels. Instances with <strong>small margins</strong> are more <strong>ambiguous</strong>, thus knowing the true label would help discriminate. </p>
<p><strong>Limitations</strong>     Problems with very large label sets, the margin method still ignores much of the output distribution for the remaining classes. solution -&gt; entropy.</p>
<h3 id="Entropy-sampling"><a href="#Entropy-sampling" class="headerlink" title="Entropy sampling"></a>Entropy sampling</h3><script type="math/tex; mode=display">x_H^* =\mathop{\arg\max}_x - \sum_i P_{\theta}(y_i|x) log P_{\theta}(y_i|x)</script><p>where <script type="math/tex">y_i</script> ranges over all possible labelings. Entropy represents the amount of information needed to “encode” a distribution. (Entropy is often thought of as a measure of uncertainty or impurity in machine learning). </p>
<p>Above three uncertainty sampling methods are application-dependent. </p>
<blockquote>
<p>“Intuitively, though, entropy seems appropriate if the objective function is to minimize log-loss, while the other two (particularly margin) are more appropriate if we aim to reduce classification error, since they prefer instances that would help the model better discriminate among specific classes.”</p>
</blockquote>
<h2 id="Query-By-Committee"><a href="#Query-By-Committee" class="headerlink" title="Query-By-Committee"></a>Query-By-Committee</h2><p>Query-by-committee (QBC) algorithm: maintain a committee <script type="math/tex">C = {\theta^{(1)}, ...,\theta^{C}}</script> of models which all trained on the current labeled set <script type="math/tex">L</script>, but represents competing hypotheses. Each committee member is then allowed to vote on the labelings of query candidates. The <strong>most informative query</strong> is considered to be the instance about which they <strong>most disagree</strong>.</p>
<p>Measure the level of disagreement:</p>
<h3 id="vote-entropy"><a href="#vote-entropy" class="headerlink" title="vote entropy:"></a>vote entropy:</h3><script type="math/tex; mode=display">x_{V E}^{*} = \mathop{\arg\max}_x - \sum_i \frac{V(y_i)}{C} \log \frac{V(y_i)}{C}</script><p>where <script type="math/tex">y_i</script> again ranges over all possible labelings, and <script type="math/tex">V(y_i)</script> is the number of “votes” that a label receives from among the committee members’ predictions, and <script type="math/tex">C</script> is the committee size. (It can be thought as a QBC generalisation of entropy-based uncertainty sampling)</p>
<h4 id="Kullback-Leibler-KL-divergence"><a href="#Kullback-Leibler-KL-divergence" class="headerlink" title="Kullback-Leibler (KL) divergence"></a>Kullback-Leibler (KL) divergence</h4><script type="math/tex; mode=display">x_{KL}^* = \mathop{\arg\max}_{x} \frac{1}{C} \sum_{c=1}^C D(P_{\theta^{c}} || P_c)</script><p>where </p>
<script type="math/tex; mode=display">D(P_{\theta}||P_C) = \sum_i P_{\theta}^{c} (y_i|x) \log \frac{P_{\theta^{c}} (y_i | x)}{P_C (y_i | x)}</script><p>Here <script type="math/tex">\theta^{(c)}</script> represents a particular model in the committee, and <script type="math/tex">C</script> represents the committee as a whole, thus </p>
<script type="math/tex; mode=display">P_C(y_i|x) = \frac{1}{C} \sum_{c=1}^C P_{\theta}(y_i|x)</script><p>is the “consensus” probability that <script type="math/tex">y_i</script> is the correct label.</p>
<hr>
<h2 id="Expected-model-change"><a href="#Expected-model-change" class="headerlink" title="Expected model change"></a>Expected model change</h2><p>A <strong>decision-theoretic</strong> approach: selecting the instance that would impart the greatest change to the current model <em>if we knew its label</em>. </p>
<p><strong>Expected gradient length</strong> (EGL): apply to gradient-based training. The “change” imparted to the model can be measured by the <strong>length of the training gradient</strong> (i.e. the vector used to re-estimate parameter values). That is, the learner query the new instances <script type="math/tex">x</script> which if labeled and added to <script type="math/tex">L</script>, would result in the new training gradient of the largest magnitude. Let <script type="math/tex">\nabla l_{\theta} (L \cup <x,y>)</script> be the new gradient that would be obtained by adding the training tuple <script type="math/tex"><x,y></script> to <script type="math/tex">L</script>. Since the query algorithm does not know the label <script type="math/tex">y</script> in advance, we must instead calculate the length as an expectation over the possible labelings:</p>
<script type="math/tex; mode=display">x_{EGL}^* = \mathop{\arg \max}_x \sum_i P_{\theta}(y_i | x) || \nabla l_{\theta} (L \cup <x, y_i>)</script><p>where ||.|| is the Euclidean norm of each resulting gradient vector. Note that, <script type="math/tex">\nabla l_{\theta} (L)</script>  should be nearly zero since <script type="math/tex">l</script> converged at the previous round of training. Thus, we can approximate <script type="math/tex">\nabla l_{\theta} (L \cup <x,y_i>) \approx \nabla l_{\theta} (<x, y_i>)</script> for computational efficiency.</p>
<p><strong>Intuition</strong>: prefer the instance that is likely to most influence the model (i.e. have the greatest impact on its parameters), regardless of the resulting query label.</p>
<p><strong>Limitations</strong>: EGL can lead astray if features are not properly scaled, i.e. the informativeness of a given instance maybe over-estimated simply because one of its future values is usually large, or corresponding parameter estimate is larger. Solution -&gt; parameter regularisation. </p>
<h2 id="Expected-error-reduction"><a href="#Expected-error-reduction" class="headerlink" title="Expected error reduction"></a>Expected error reduction</h2><p><strong>Idea</strong>: estimate the expected future error of a model trained using <script type="math/tex">L \cup <x,y></script> on the training unlabeled instances in <script type="math/tex">U</script> and query the instance with minimal expected future error (a.k.a. risk).</p>
<p>One approach is to minimize the expected 0/1-loss:</p>
<script type="math/tex; mode=display">x_{0/1}^* = \mathop{\arg\min}_x \sum_i P_{theta} (y_i | x) (\sum_{u=1}^U 1-P_{theta^{+<x,y>}}(\hat{y} | x^{(u)} ) )</script><p>where <script type="math/tex">theta^{+<x,y>}</script> is the new model re-trained with the training tuple <script type="math/tex"><x, y_i></script> added to <script type="math/tex">L</script> . </p>
<p>One interpretation is, </p>
<blockquote>
<p>maximizing the expected information gain of the query x, or (equivalently) the mutual information of the output variables over x and U</p>
</blockquote>
<h2 id="Variance-reduction"><a href="#Variance-reduction" class="headerlink" title="Variance reduction"></a>Variance reduction</h2><p>Reduce generalization error indirectly by minimizing output variance. </p>
<p><strong>Regression problem</strong> </p>
<script type="math/tex; mode=display">E_T [(\hat{y} - y)^2 | x] = E[(y-E[y|x])^2] + (E_L [\hat{y}] - E[y|x])^2 + E_L [(\hat{y} - E_L[\hat{y}])^2]</script><p>where <script type="math/tex">E_L[.]</script> is an expectation over the labeled set <script type="math/tex">L</script>, <script type="math/tex">E[.]</script> is an expectation over the conditional density <script type="math/tex">P(y|x)</script> and <script type="math/tex">E_T</script> is an expectation over both.</p>
<h2 id="Density-weighted-methods"><a href="#Density-weighted-methods" class="headerlink" title="Density-weighted methods"></a>Density-weighted methods</h2><p><strong>Challenges</strong>: A central idea of the <em>estimated error</em> and <em>variance reduction</em> frameworks is that they focus on the entire input space rather than individual instances. Thus, they are less prone to querying outliers than simpler query strategies like uncertainty sampling, QBC, and EGL. The least certain instance lies on the classification boundary, but is not “representative” of other instances in the distribution, so knowing its label is unlikely to improve accuracy on the data as a whole.</p>
<p><img data-src='data:img/jpg;base64,iVBORw0KGgoAAAANSUhEUgAAA28AAAGACAYAAAA6QlqCAAABfGlDQ1BJQ0MgUHJvZmlsZQAAKJFj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'/></p>
<blockquote>
<p>informative instances should not only be those which are uncertain, but also those which are “representative” of the underlying distribution (i.e., inhabit dense regions of the input space). </p>
</blockquote>
<script type="math/tex; mode=display">x_{ID}^* = \mathop{\arg \max}_x \phi_A(x) \times (\frac{1}{U} \sum_{u=1}^U sim(x, x^{(u)}))^{\beta}</script><p>Here, <script type="math/tex">\phi_A(x)</script> denotes the informativeness of <script type="math/tex">x</script> according to some “base” query strategy A, such as an uncertainty sampling or QBC method. The second term weights the informativeness of <script type="math/tex">x</script> by its average similarity to all other instances in the input distribution (as approximated by U), s.t. a parameter <script type="math/tex">\beta</script> that controls the relative importance of the density term.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li>[1] Burr Settles. Active Learning Literature Survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison. 2009.</li>
<li>[2] D. Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988.</li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
        <category>Active learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Survey</tag>
        <tag>Active learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic Programming in NLP</title>
    <url>/notes/2019/09/21/NLP/Dynamic-Programming-in-NLP/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><strong>Dynamic Programming</strong> (DP) is ubiquitous in NLP, such as Minimum Edit Distance, Viterbi Decoding, forward/backward algorithm, CKY algorithm, etc.<br><span id="more"></span></p>
<h1 id="Minimum-Edit-Distance"><a href="#Minimum-Edit-Distance" class="headerlink" title="Minimum Edit Distance"></a>Minimum Edit Distance</h1><p>Minimum Edit Distance (Levenshtein distance) is string metric for measuring the difference between two sequences.</p>
<p>Given two strings $a$ and $b$ (with length $|a|$ and $|b|$ respectively), define <script type="math/tex">\text{D}(i,j)</script> as the edit distance between $a[1 \cdots i]$ and $b[1 \cdots j]$, i.e. the first $i$ chars of $a$ and the first $j$ chars of $b$.  The edit distance between two strings is $\text{D}(|a|, |b|)$.</p>
<script type="math/tex; mode=display">
\text{D}[i,j]=\left\{
                \begin{array}{ll}
                  D[i-1, j] + \text{del-cost}(a[i])\\
                  D[i, j-1] + \text{ins-cost}(b[j])\\
                  D[i-1, j-1] + \text{sub-cost}(a[i], b[j])
                  \end{array}
    \right.</script><p>Let the costs of insertion, deletion, substitution are 1,1 and 2, respectively.</p>
<script type="math/tex; mode=display">
\text{D}[i,j]=\min\left\{
                \begin{array}{ll}
                  D[i-1, j] + 1\\
                  D[i, j-1] + 1\\
                  D[i-1, j-1] + \left\{
                            \begin{array}{ll}
                            2 \quad \text{if a[i] }\neq{ b[j]}\\
                            0 \quad \text{if a[i] == b[j]}
                            \end{array}
                        \right.
                \end{array}
    \right.</script><p>That is,</p>
<script type="math/tex; mode=display">
\text{D}[i,j]=\left\{
                \begin{array}{ll}
                D[i-1, j-1] & \quad \text{if a[i] == b[j]} \\
                  \min(D[i-1, j]+1, D[i, j-1]+1, D[i-1, j-1]+2) & \quad \text{if a[i] }\neq\text{ b[j]} 
                \end{array}
    \right.</script><p>Algorithms in Python:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span>(<span class="params">self, word1, word2</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Levenshtein distance</span></span><br><span class="line"><span class="string">        :type word1: str</span></span><br><span class="line"><span class="string">        :type word2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(word1) + <span class="number">1</span>, <span class="built_in">len</span>(word2) + <span class="number">1</span></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp[<span class="number">0</span>][j] = j</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            	<span class="comment"># implementation 1</span></span><br><span class="line">                <span class="comment"># --------------------------</span></span><br><span class="line">            	<span class="comment"># if word1[i - 1] == word2[j - 1]:</span></span><br><span class="line">                <span class="comment">#     sub_cost = 0</span></span><br><span class="line">                <span class="comment"># else:</span></span><br><span class="line">                <span class="comment">#     sub_cost = 2</span></span><br><span class="line">                <span class="comment"># dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + sub_cost)</span></span><br><span class="line">                <span class="comment"># implementation 2</span></span><br><span class="line">                <span class="comment"># ===========================</span></span><br><span class="line">                <span class="keyword">if</span> word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i - <span class="number">1</span>][j] + <span class="number">1</span>, dp[i][j - <span class="number">1</span>] + <span class="number">1</span>, dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[m - <span class="number">1</span>][n - <span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>
<p><img data-src="/notes/images/Min-edit-distance.png" alt="upload successful"></p>
<h1 id="DP-in-HMMs"><a href="#DP-in-HMMs" class="headerlink" title="DP in HMMs"></a>DP in HMMs</h1><p>Hidden Markov Models (HMMs) describe the joint probability of its hidden(unobserved) and observed discrete random variables. It relies on the Markov assumption and output indenpendence.</p>
<ul>
<li>The observation sequence <script type="math/tex">Y = (Y_1=y_1, Y_2=y_2, \cdots, Y_T=y_T)</script></li>
<li>Let <script type="math/tex">X_t</script> be a discrete random vriable with $K$ possible states. The transition matrix <script type="math/tex">A={a_{ij}} = P(X_t=j \vert X_{t-1}=i)</script></li>
<li>The initial state distribution(t=1) is <script type="math/tex">\pi_i = P(X_1 = i)</script></li>
<li>The output/emission matrix <script type="math/tex">B = {b_j(y_i)} = P(Y_t=y_i \vert X_t = j)</script></li>
</ul>
<p>Thus the hidden markov chain can be defined by <script type="math/tex">\theta = (A, B, \pi)</script>.</p>
<p><img data-src="/notes/images/foward-algorithm.png" alt="upload successful"></p>
<h2 id="Forward-algorithm"><a href="#Forward-algorithm" class="headerlink" title="Forward algorithm"></a>Forward algorithm</h2><p>Forward algorithm is used to calculate the <span class="label warning">"belief state"</span>: the probability at time $t$, given the history of evidence.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm)
">[1]</span></a></sup> Such process is also called <strong>filtering</strong>.</p>
<ul>
<li>The goal is to compute the joint probability <script type="math/tex">p(x_t, y_{1:t})</script>, which requires marginalizing over all possible state sequence <script type="math/tex">{x_{1:t-1}}</script>, the # of which grows exponentially with $t$.</li>
<li>The <em>conditional independence</em> of HMM could help to perform calculation recursively.</li>
<li>Time complexity: $O(K^2T)$</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\pmb{\alpha_t(x_t)} &= p(x_t, y_{1:t}) & \\
& = \sum_{x_{t-1}} p(x_t, x_{t-1}, y_{1:t}) & \\
& = \sum_{x_{t-1}} p(y_t \vert x_t, x_{t-1}, y_{1:t-1}) p(x_{t} \vert x_{t-1}, y_{1:t-1}) p(x_{t-1, y_{1:t-1}}) & \text{chain rule} \\
& = \underbrace{p(y_t \vert x_t)}_\text{emission prob.} \sum_{x_{t-1}} \underbrace{ p(x_t \vert x_{t-1})}_\text{transition prob.} \pmb{\alpha_{t-1}(x_{t-1})} & \text{Markov assumption} \\
\end{align}</script><div class="note info">
            <p><strong>Forward algorithm</strong>:</p><ul><li>Init $t=0$, transition probabilities <script type="math/tex">p(x_t \vert x_{t-1})</script>, emission probabilities <script type="math/tex">p(y_j \vert x_i)</script>, observed sequence $y_{1:t}$<br>for $t \leftarrow t+1, \cdots, T$:<script type="math/tex; mode=display">\pmb{\alpha_t(x_t)} \leftarrow p(y_t \vert x_t) \sum_{x_{t-1}} p(x_t \vert x_{t-1}) \pmb{\alpha_{t-1} (x_{t-1})}</script><script type="math/tex; mode=display">p(y_{1:t}) = \alpha_T</script>return <script type="math/tex">\alpha_T</script></li></ul>
          </div>
<h2 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h2><p><strong>Viterbi algorithm</strong> is to finding the most probable sequence of hidden states, i.e. <span class="label info">Viterbi path</span>.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)
">[2]</span></a></sup></p>
<p><strong>Input</strong>:</p>
<ul>
<li>The observation space <script type="math/tex">\pmb{O} = {o_1, o_2, \cdots, o_N }</script></li>
<li>state space <script type="math/tex">S = {s_1, s_2, \cdots, s_K}</script></li>
<li>initial probability array <script type="math/tex">\Pi = (\pi_1, \pi_2, \cdots, \pi_K)</script> such that <script type="math/tex">\pi_i = p(x_1 = s_i)</script></li>
<li>observation sequence <script type="math/tex">Y = {y_1, y_2, \cdots, y_T}</script></li>
<li>transition matrix $A$ with size $K \times K$. <script type="math/tex">A_{ij}</script> stores the transition probability of transition from state <script type="math/tex">s_i</script> to <script type="math/tex">s_j</script></li>
<li>emission matrix $B$ of size $K \times N$, such that <script type="math/tex">B_{ij}</script> stores the probability of observing <script type="math/tex">o_j</script> from state <script type="math/tex">s_i</script></li>
</ul>
<p><strong>Output</strong>: </p>
<ul>
<li>the most likely hidden state sequence <script type="math/tex">X = (x_1, x_2, \cdots, x_T)</script></li>
<li>Time complexity: $O(|S|^2T)$</li>
</ul>
<ol>
<li><strong>function</strong> Viterbi($O$,$S$, $\pi$, $Y$, $A$, $B$): <strong>returns</strong> $X$<ol>
<li>for state i = <script type="math/tex">1,2,\cdots, K</script> do<ul>
<li>Viterbi init step $0 \rightarrow 1$ : <script type="math/tex">T_1[i,1] \leftarrow \pi_i \cdot B_{iy_1}</script></li>
<li>Backtrack init step $0 \rightarrow 1$  : <script type="math/tex">T_2[i,1] \leftarrow 0</script></li>
</ul>
</li>
<li>for observation $j = 2,3,\cdots, T$:<ul>
<li>for state $i = 1,2,\cdots,K$:<ul>
<li>Viterbi records the best prob <script type="math/tex">T_1[i,j] \leftarrow \max_k \big( T_1[k,j-1] \cdot A_{ki} \cdot B_{iy_j} \big)</script></li>
<li>Backpointer saves the best backpointer <script type="math/tex">T_2[i,j] \leftarrow \arg\max_k \big( T_1[k,j-1] \cdot A_{ki} \big)</script></li>
</ul>
</li>
</ul>
</li>
<li>best path prob. <script type="math/tex">Z_T \leftarrow \arg\max_k\big( T_1[k,T] \big)</script></li>
<li>best path pointer <script type="math/tex">x_T \leftarrow s_{z_T}</script></li>
<li>for $j=T, T-1, \cdots, 2$:<ol>
<li>do back tracking $z_{j-1} \leftarrow T_2[z_j,j]$</li>
<li>Prev hidden state <script type="math/tex">x_{j-1} \leftarrow s_{z_{j-1}}</script></li>
</ol>
</li>
<li>return <script type="math/tex">X= (x_1, x_2, \cdots, x_T)</script></li>
</ol>
</li>
</ol>
<h2 id="Baum-Welch-algorithm"><a href="#Baum-Welch-algorithm" class="headerlink" title="Baum-Welch algorithm"></a>Baum-Welch algorithm</h2><p>Baum-Welch algorithm, as a speciqal case of EM algorithms, uses the forward-backward algorithm to find the maximum likelihood estimate of the unknown parameters of a HMM given a set of observed feature vectors. </p>
<p>The local maximum <script type="math/tex">\theta^* = \arg\max_\theta P(Y \vert \theta)</script></p>
<h3 id="Forward-process"><a href="#Forward-process" class="headerlink" title="Forward process"></a>Forward process</h3><p>Let <script type="math/tex">\alpha_i(t) = P(Y_1=y_1, \cdots, Y_t=y_t, X_t=i \vert \theta)</script> represent the probability of the observation of <script type="math/tex">y_1,y_2,\cdots,y_t</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_i(1) &= \pi_i b_i (y_1) \\
\alpha_{i}(t+1) &= b_i(y_{t+1}) \sum_{j=1}^K \alpha_j(t)a_{ji} 
\end{align}</script><h3 id="backward-process"><a href="#backward-process" class="headerlink" title="backward process"></a>backward process</h3><p>Let <script type="math/tex">\beta_i(t)= P(Y_{t+1}=y_{t+1},\cdots,y_T=y_T \vert X_t=i,\theta)</script> denote the probability of the sequence <script type="math/tex">y_{t+1}, \cdots, y_T</script> starting at time $t$.</p>
<script type="math/tex; mode=display">\beta_i(t)=\left\{
                \begin{array}{ll}
                \begin{align}
                  \sum_{j=1}^K \beta_j(t+1) a_{ij}b_j(y_{t+1}) & \text{ if } t\neq T\\
                  1 & \text{ if } t = T
                 \end{align}
                \end{array}
    \right.</script><h3 id="update"><a href="#update" class="headerlink" title="update"></a>update</h3><p>The prob. of being state $i$ at time $t$ given the observation $Y$ and parameters $\theta$</p>
<script type="math/tex; mode=display">\gamma_i(t) = P(X_t=i \vert Y, \theta) = \frac{P(X_t=i, Y \vert \theta)}{P(Y \vert \theta)} = \frac{\alpha_i(t)\beta_i(t)}{\sum_{j=1}^K \alpha_j(t)\beta_j(t)}</script><p>The prob. of being state $i$ and $j$ at times $t$ and $t+1$ respectively given the observation $Y$ and parameter $\theta$</p>
<script type="math/tex; mode=display">
\begin{align}
\xi_{ij}(t) &= P(X_t=i, X_{t+1}=j \vert Y, \theta) \\ 
& = \frac{P(X_t=i, X_{t+1}=j, Y \vert \theta)}{P(Y\vert \theta)} \\
& = \frac{\alpha_{i}(t) a_{ij} \beta_j(t+1) b_j(y_{t+1}) }{ \sum_{i=1}^K \sum_{j-1}^K \alpha_i(t) a_{ij} \beta_j(t+1)b_j(y_{t+1}) }
\end{align}</script><p>Update the HMM $\theta$:</p>
<ul>
<li>$\pi$ at state $i$ at time 1: <script type="math/tex; mode=display">\pi^*_i = \gamma_i(1)</script></li>
<li>the transition matrix $A$. <script type="math/tex; mode=display">
\begin{align}
a_{ij}^* &= \frac{\text{The expected # of transitions from state i to j}}{\text{the expected totoal # of transitions away from state i}}\\
& = \frac{\sum_{t=1}^{T-1}\xi_{ij}(t)}{\sum_{t=1}^{T-1}\gamma_i(t)}
\end{align}</script></li>
<li><p>the emission matrix $B$</p>
<script type="math/tex; mode=display">
\begin{align}
b_i^*(v_k) & = \frac{\text{the expected # of times the output is equal to the symbol }v_k}{\text{the expected total # of times in state }i} \\
&= \frac{\sum_{t=1}^T \mathcal{1}_{y_t=v_k} \gamma_i(t)}{\sum_{t=1}^T \gamma_i(t)}
\end{align}</script><ul>
<li>where  <script type="math/tex; mode=display">\mathcal{1}_{y_t=v_k} = \left\{
          \begin{array}{ll}
            1 \quad \text{ if } y_t=v_k\\
            0 \quad \text{ otherwise}
          \end{array}
\right.</script></li>
</ul>
</li>
</ul>
<h1 id="CKY-algorithm"><a href="#CKY-algorithm" class="headerlink" title="CKY algorithm"></a>CKY algorithm</h1><p>TBD</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Forward_algorithm">Wiki: forward algorithm</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Wiki: Viterbi algorithm</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Wiki: Baum-Welch algorithm</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Dynamic Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>Sequence GANs in a Nutshell</title>
    <url>/notes/2020/08/30/NLG/Sequence-GANs-in-a-Nutshell/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><!--A summary of Generative Adversarial Networks (GANs) for generating discrete sequences, such as language modeling, music generation, *etc*.-->
<p><strong>Background</strong>: Conventional maximum likelihood approaches for sequence generation with teacher forcing algorithms are inherently prone to <em>exposure bias</em> at the inference stage due to the training-testing discrepancy—the generator produces a sequence iteratively conditioned on its previously predicted ones that may be never observed during training—leading to accumulative mismatch with the increment of generated sequences. In other words, the model is only trained on demonstrated behaviors (real data samples) but not free-running mode.<br>Generative Adversarial Networks (GANs) hold the promise of mitigating such issues for generating discrete sequences, such as language modeling, speech/music generation, <em>etc</em>.<br><span id="more"></span></p>
<p>GANs have demonstrated the compelling performance in generating real-valued data such as pixel-based images but have fallen short of discrete data generation primarily resulting from the incapability of gradient propagation passing from the discriminator (denoted as $\mathcal{D}$) to the generator (denoted as $\mathcal{G}$) in the original (image) GAN framework, which is incurred by the non-differential sampling/argmax operation in between.</p>
<p>Existing solutions to discrete sequence generation using GANs could be mainly sorted into different groups by resorting to:</p>
<ol>
<li><strong>Reinforcement Learning</strong> (RL): modeling the sequence generation procedure as a sequential decision-making process <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Lantao, et al. "[Seqgan: Sequence generative adversarial nets with policy gradient.](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489)" Thirty-first AAAI conference on artificial intelligence. (2017).
">[1]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Guo, Jiaxian, et al. "[Long text generation via adversarial training with leaked information.](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16360/16061)" Thirty-Second AAAI Conference on Artificial Intelligence (2018).
">[6]</span></a></sup><sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Che, Tong, et al. "[Maximum-likelihood augmented discrete generative adversarial networks.](https://arxiv.org/pdf/1702.07983.pdf)" arXiv preprint arXiv:1702.07983 (2017).
">[7]</span></a></sup><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lin, Kevin, et al. "[Adversarial ranking for language generation.](http://papers.nips.cc/paper/6908-adversarial-ranking-for-language-generation.pdf)" Advances in Neural Information Processing Systems (2017).
">[8]</span></a></sup>; typically yielding high-variance but unbiased gradient estimates.</li>
<li>RL-free: utilizing soft-argmax operator<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Yizhe, et al. "[Adversarial feature matching for text generation.](https://arxiv.org/pdf/1706.03850)" arXiv preprint arXiv:1706.03850 (2017).
">[2]</span></a></sup>, Gumbel-softmax trick<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kusner, Matt J., and José Miguel Hernández-Lobato. "[Gans for sequences of discrete elements with the gumbel-softmax distribution.](https://arxiv.org/pdf/1611.04051.pdf)" arXiv preprint arXiv:1611.04051 (2016).
">[9]</span></a></sup>, or continuous relaxation<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Gulrajani, Ishaan, et al. "[Improved training of wasserstein gans.](https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf)" Advances in Sneural information processing systems (2017).
">[16]</span></a></sup> to provide the continuous approximation of the discrete distribution on the sequences; yielding low variance but biased estimation.</li>
</ol>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Policy Gradient</th>
<th style="text-align:center">Gumbel-softmax</th>
<th style="text-align:center">Soft-argmax</th>
<th style="text-align:center">Dense Reward</th>
<th style="text-align:center">Internal Feature</th>
<th style="text-align:center">Pretraining</th>
<th style="text-align:center">$\mathcal{G}$</th>
<th style="text-align:center">$\mathcal{D}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SeqGAN  (AAAI’17)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">TextGAN  (ICML’17)</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">MaliGAN  (MILA)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">RankGAN  (NIPS’17)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">LeakGAN  (AAAI’18)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">GSGAN</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">-</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">LSTM</td>
</tr>
<tr>
<td style="text-align:center">FMGAN  (NeurIPS’18)</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">SentiGAN  (IJCAI’18)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">MaskGAN  (ICLR’18)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM (seq2seq)</td>
<td style="text-align:center">LSTM (seq2seq)</td>
</tr>
<tr>
<td style="text-align:center">RelGAN  (ICLR’19)</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">SAN</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">ScratchGAN  (NeurIPS’19)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">LSTM</td>
</tr>
<tr>
<td style="text-align:center">JSDGAN  (AISTATS’19)</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔ / ✘</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">✘</td>
</tr>
<tr>
<td style="text-align:center">CatGAN  (AAAI’20)</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">SAN</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">SALGAN  (ICLR’20)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">LSTM</td>
<td style="text-align:center">CNN</td>
</tr>
<tr>
<td style="text-align:center">ColdGAN (NeurIPS’20)</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">T5 / BART</td>
<td style="text-align:center">N/A</td>
</tr>
</tbody>
</table>
</div>
<h1 id="SeqGAN-AAAI’17"><a href="#SeqGAN-AAAI’17" class="headerlink" title="SeqGAN (AAAI’17)"></a>SeqGAN (AAAI’17)</h1><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>There exist limitations in discrete sequence generation using GANs, such as:</p>
<ol>
<li>The discrete output of the generator $\mathcal{G}$;</li>
<li>$\mathcal{D}$ can only assess the complete sequence, while it is non-trivial to balance the current score and future one for partially generated sequence once the entire sequence has been generated.</li>
</ol>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>SeqGAN<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Lantao, et al. "[Seqgan: Sequence generative adversarial nets with policy gradient.](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489)" Thirty-first AAAI conference on artificial intelligence. (2017).
">[1]</span></a></sup> bypasses the generator differentiation problem by directly performing a policy gradient update, which adopts the judgments of $\mathcal{D}$ on the complete generated sequences as reward signals using Monte Carlo (MC) search.</p>
<p>SeqGAN considers the sequence generation as a sequential decision-making process with a stochastic parameterized policy, in which the generator $\mathcal{G}$ is treated as the actor/agent of RL, the state is previously generated tokens so far and the action is the next token to be generated.</p>
<p><img data-src="/notes/images/SeqGAN.png" width="80%"/></p>
<center> Image source: <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, Lantao, et al. "[Seqgan: Sequence generative adversarial nets with policy gradient.](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489)" Thirty-first AAAI conference on artificial intelligence. (2017).
">[1]</span></a></sup> </center>

<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Given a dataset of real-word structured sequences, train a $\theta$-parameterized generative model <script type="math/tex">G_\theta</script> to produce a sequence <script type="math/tex">Y_{1:T} = (y_1, \cdots, y_t, \cdots, y_T), y_t \in \mathcal{Y}</script>, where $\mathcal{Y}$ is the vocabulary of candidate tokens. The policy <script type="math/tex">G_\theta(y_t \vert Y_{1: t-1})</script> is stochastic: at the $t$-th timestep, the state $s$ is the current partially predicted sequences <script type="math/tex">(y_1, \cdots, y_{t-1})</script>, and the action $a$ is the next token $y_t$ to be selected. </p>
<p>The discriminator <script type="math/tex">D_\phi</script> parameterized by $\phi$ predicts how likely the sampled sequence <script type="math/tex">Y_{1:T}</script> is from real data, providing the guidance (reward) to update the policy <script type="math/tex">G_\theta</script>.</p>
<h3 id="Policy-Gradient-with-MC-Search"><a href="#Policy-Gradient-with-MC-Search" class="headerlink" title="Policy Gradient with MC Search"></a>Policy Gradient with MC Search</h3><p>Let <script type="math/tex">Q_{D_\phi}^{G_\theta} (s, a)</script> be the action-value function of a sequence, <em>i.e.</em>, the expected accumulative reward starting from the state $s$ taking action $a$ with policy <script type="math/tex">G_\theta</script>; <script type="math/tex">R_T</script> be the reward for a complete sequence. The objective of <script type="math/tex">G_\theta(y_t \vert Y_{1:t-1})</script> is to generate a sequence from the start state <script type="math/tex">s_0</script> to maxmize its expected reward at the end of the episode:</p>
<script type="math/tex; mode=display">
J(\theta) = \mathbb{E} [R_T \vert s_0, \theta] = \sum_{y_1 \in \mathcal{Y}} G_\theta (y_1 \vert s_0) \cdot Q_{D_\phi}^{G_\theta} (s_0, y_1).</script><p>SeqGAN adopts the estimated probability of being real by <script type="math/tex">D_\phi (Y_{1:T}^n)</script> as the reward, but $D$ can only provides the reward for a finished sequence. Thus, in order to evaluate the action-value for an intermediate state, Monte Carlo (MC) search with a roll-out policy <script type="math/tex">G_\beta</script> is applied to sample the unknown last $T-t$ tokens. Let an $N$-time Monte Carlo search be <script type="math/tex">\{ Y_{1:T}, \cdots, Y_{1:T}^N \} = \textrm{MC}^{G_\beta}(Y_{1:T}; N)</script>, where <script type="math/tex">Y_{1:t}^n = (y_1, \dots, y_t)</script> and <script type="math/tex">Y_{t+1:T}^n</script> is sampled based on the roll-out policy <script type="math/tex">G_\beta</script> and the current state.</p>
<p>It runs the roll-out policy starting from current state till the end of the  sequence for $N$ times to get a batch of output samples. Thus,</p>
<script type="math/tex; mode=display">
\begin{align} \label{eq1}\tag{1} 
Q_{D_\phi}^{G_\theta} (s=Y_{1:t-1}, a=y_t) = \left\{
\begin{array}{ll}
\frac{1}{N}\sum_{n=1}^N D_\phi(Y_{1:T}^n) & \textrm{for }t<T \\
D_\phi (Y_{1:t}) & \textrm{for }t=T 
\end{array}
\right\},
\end{align}</script><p>where <script type="math/tex">\quad Y_{1:T}^n \in \textrm{MC}^{G_\beta} (Y_{1:t}; N)</script>. The intermediate reward is iteratively defined as the next-state value starting from the state <script type="math/tex">s^\prime = Y_{1:t}</script> and rolling out to the end.</p>
<p>The <script type="math/tex">D_\phi</script> is trained as follows:</p>
<script type="math/tex; mode=display">
\begin{align} \label{eq2} \tag{2}
\min_\phi - \mathbb{E}_{Y \sim p_\textrm{data}} [\log D_\phi (Y)] - \mathbb{E}_{Y \sim G_\theta} [\log (1-D_\phi (Y))]].
\end{align}</script><p>The gradient of objective function $J(\theta)$ w.r.t. policy’s parameter $\theta$ is:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\theta) &{}= \sum_{t=1}^T \mathbb{E}_{Y_{1:t-1} \sim G_\theta} \big[\sum_{y_t \in \mathcal{Y}} \nabla_\theta G_\theta (y_t \vert Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta} (Y_{1:t-1}, y_t) \big] \\
&{}\simeq \sum_{t=1}^T \sum_{y_t \in \mathcal{Y}} \nabla_\theta G_\theta (y_t \vert Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta} (Y_{1:t-1}, y_t) \\
&{}= \sum_{t=1}^T \sum_{y_t \in \mathcal{Y}} G_\theta (y_t \vert Y_{1:t-1}) \nabla_\theta \log G_\theta (y_t \vert Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta} (Y_{1:t-1}, y_t) \\
&{}= \sum_{t=1}^T \mathbb{E}_{y_t \sim G_\theta (y_t \vert Y_{1:t-1})} \big[ \nabla_\theta \log G_\theta (y_t \vert Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta} (Y_{1:t-1}, y_t) \big],
\end{align}</script><p>where <script type="math/tex">Y_{1:t-1}</script> is the obvserved intermediate state sampled from <script type="math/tex">G_\theta</script>.</p>
<h3 id="Training-Algorithm"><a href="#Training-Algorithm" class="headerlink" title="Training Algorithm"></a>Training Algorithm</h3><p><strong>Require</strong>: generator policy <script type="math/tex">G_\theta</script>; roll-out policy <script type="math/tex">G_\beta</script>; discriminator <script type="math/tex">D_\phi</script>; a sequence dataset <script type="math/tex">\mathcal{S}=\{ X_{1:T} \}</script>; learning rate $\alpha$</p>
<ol>
<li>Initialize <script type="math/tex">G_\theta</script>, <script type="math/tex">D_\phi</script> with random weights $\theta$, $\phi$</li>
<li>Pretrain <script type="math/tex">G_\theta</script> using MLE on $\mathcal{S}$</li>
<li>$\beta \leftarrow \theta$</li>
<li>Generate negative samples using <script type="math/tex">G_\theta</script> for training <script type="math/tex">D_\phi</script></li>
<li>Pretrain <script type="math/tex">D_\phi</script> via minimizing the cross entropy</li>
<li><strong>repeat</strong><ul>
<li><strong>for</strong> g-steps <strong>do</strong><ol>
<li>Generate a sequence <script type="math/tex">Y_{1:T} = (y_1, \cdots, y_T) \in G_\theta</script><ol>
<li><strong>for</strong> $t$ in $1:T$ <strong>do</strong><ul>
<li>compute <script type="math/tex">Q(a=y_t; s=Y_{1:t-1})</script> using Eq.(\ref{eq1})</li>
</ul>
</li>
</ol>
</li>
<li>Update generator parameters with policy gradient: <script type="math/tex">\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)</script></li>
</ol>
</li>
<li><strong>for</strong> d-steps <strong>do</strong><ul>
<li>Use current <script type="math/tex">G_\theta</script> to generate negative (synthetic) examples and combine with sampled positive (real) examples $\mathcal{S}$</li>
<li>Train discriminator <script type="math/tex">D_\phi</script> for $k$ epochs using Eq.(\ref{eq2})</li>
</ul>
</li>
<li>$\beta \leftarrow \theta$</li>
</ul>
</li>
<li><strong>until</strong> SeqGAN converges</li>
</ol>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>$G_\theta$: LSTM actor.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{h}_t &{}= \textrm{LSTM}(\mathbf{h}_{t-1}, \mathbf{x}_t), \\
p(y_t \vert x_1, \cdots, x_t) &{}= \textrm{softmax} (\mathbf{c} + \mathbf{Vh}_t),
\end{align}</script><p>where <script type="math/tex">\mathbf{h}_{t-1}</script> represents the $t$-th hidden state of LSTMs, $\mathbf{x}_t$ denotes the input embedding at the time step $t$.</p>
<h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>$D_\phi$: CNN critic.<br>The input word embeddings are:</p>
<script type="math/tex; mode=display">\varepsilon = \mathbf{x}_1 \oplus \mathbf{x}_2 \oplus \cdots \oplus \mathbf{x}_T,</script><p>where <script type="math/tex">\mathbf{x}_t \in \mathbb{R}^k</script> represents the $k$ dimensional embedding, $\oplus$ is the vertical concatenation operator to build the matrix $\varepsilon_{1:T} \in \mathbb{R}^{T \times k}$. Then a kernel $\mathbf{w} \in \mathbb{R}^{n \times k}$ applies a convolutional operation to extract $n$-gram features:</p>
<script type="math/tex; mode=display">c_i = \rho (\mathbf{w} \otimes \varepsilon_{i:i+n-1} + b),</script><p>where $\otimes$ operator is the summation of elementwise product, $b$ is a bias term, $\rho$ is a non-linear function. Then concatenate the output of multi-channel convolutions with various kernel sizes followed by a max-over-time-pooling:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{c} &{}= [c_1, \cdots, c_{T-l+1}], \\
\tilde{c} &{}= \max \{ \mathbf{c} \}.
\end{align}</script><p>Then apply a highway architecture before the final dense layer:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{\tau} &{}= \sigma (\mathbf{W}_T \cdot \tilde{\mathbf{c}} + \mathbf{b}_T), \\
\tilde{\mathbf{C}} &{}= \pmb{\tau} \cdot H(\tilde{\mathbf{c}}, \mathbf{W}_H) + (1-\pmb{\tau}) \cdot \tilde{\mathbf{c}},
\end{align}</script><p>where <script type="math/tex">\mathbf{W}_T</script>, <script type="math/tex">\mathbf{b}_T</script>, <script type="math/tex">\mathbf{W}_H</script> are highway layer weights, $H$ denotes an affine transform with non-linearity, $\pmb{\tau}$ represents the transform gate.</p>
<p>Finally, apply a sigmoid function to get the probability of being real given the input sequences:</p>
<script type="math/tex; mode=display">
\hat{y} = \sigma (\mathbf{W}_o \cdot \tilde{\mathbf{C}} + \mathbf{b}_o),</script><p>where <script type="math/tex">\mathbf{W}_o</script> and <script type="math/tex">\mathbf{b}_o</script> are the weight and bias respectively.</p>
<h1 id="TextGAN-ICML’17"><a href="#TextGAN-ICML’17" class="headerlink" title="TextGAN (ICML’17)"></a>TextGAN (ICML’17)</h1><h2 id="Problems-1"><a href="#Problems-1" class="headerlink" title="Problems"></a>Problems</h2><p>Two fundamental problems of the GAN framework limit their usage in practice:</p>
<ol>
<li><strong>Mode collapse</strong>: $G$ tends to produce a single observation for multiple latent representations.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Metz, Luke, et al. "[Unrolled generative adversarial networks.](https://arxiv.org/pdf/1611.02163.pdf)" arXiv preprint arXiv:1611.02163 (2016).
">[3]</span></a></sup></li>
<li><p><strong>Vanishing gradient</strong>: $G$’s contribution to the <em>learning signal</em> is insubstantial when $D$ is close to its local optimum.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arjovsky, Martin, and Léon Bottou. "[Towards principled methods for training generative adversarial networks.](https://arxiv.org/pdf/1701.04862.pdf)" arXiv preprint arXiv:1701.04862 (2017).
">[4]</span></a></sup></p>
<p> When $D$ is <em>optimal</em>, using standard GAN’s miminax objective is equivalent to minimizing the Jenson-Shannon Divergence (JSD)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arjovsky, Martin, and Léon Bottou. "[Towards principled methods for training generative adversarial networks.](https://arxiv.org/pdf/1701.04862.pdf)" arXiv preprint arXiv:1701.04862 (2017).
">[4]</span></a></sup> between the real data distribution <script type="math/tex">p_x(\cdot)</script> and the synthetic data distribution <script type="math/tex">p_{\tilde{x}}(\cdot) \triangleq p\big( (G(z) )\big)</script>, where <script type="math/tex">z \sim p_z(\cdot)</script>. However, the saddile-point solution of the object is intractable. Thus iteratively updating $D$ and $G$ is required.</p>
<p> However, standard GAN’s objective suffers from unstable weak learning signal when $D$ gets close to its local minimum resulting from the vanishing gradient problem, which comes from that JSD implied by the original GAN objective approaches to a constant when <script type="math/tex">p_x(\cdot)</script> and <script type="math/tex">p_{\tilde{x}}(\cdot)</script> share no support, thus minimizing JSD yields no learning signal. This problem also exists in the distance metric of Total Variance Distance (TVD) of energy-based GAN (EBGAN).</p>
</li>
</ol>
<h2 id="Approach-1"><a href="#Approach-1" class="headerlink" title="Approach"></a>Approach</h2><p>TextGAN<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Yizhe, et al. "[Adversarial feature matching for text generation.](https://arxiv.org/pdf/1706.03850)" arXiv preprint arXiv:1706.03850 (2017).
">[2]</span></a></sup> leverages the kernel-based moment-matching scheme over a Reproducing Kernel Hilbert Space (RKHS) to force the empirical distributions of real and synthetic sentences to have matched moments in latent-feature space, which consequentially ameliorates the mode collapsing issues associated with standard GAN training.</p>
<h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>Given a sentence corpus $\mathcal{S}$, TextGAN proposes the objective:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_D &{}= \mathcal{L}_\textrm{GAN} - \lambda_r \mathcal{L}_\textrm{recon} + \lambda_m \mathcal{L}_{\textrm{MMD}^2}, \tag{3}\label{eq3}\\
\mathcal{L}_G &{}= \mathcal{L}_{\textrm{MMD}^2}, \tag{4}\label{eq4}\\
\mathcal{L}_\textrm{GAN} &{}= \mathbb{E}_{s \sim \mathcal{S}}\log D(s) + \mathbb{E}_{z \sim p_z} \log [1- D(G(z))],\\
\mathcal{L}_\textrm{recon} &{}= \Vert \hat{z} - z \Vert^2,
\end{align}</script><p>where <script type="math/tex">\mathcal{L}_\textrm{recon}</script> is the Euclidean distance between the reconstructed latent code $\hat{z}$ and the original code $z$ drawn from prior distribution <script type="math/tex">p_z(\cdot)</script>; <script type="math/tex">\mathcal{L}_{\textrm{MMD}^2}</script> represents the Maximum Mean Discrepany (MMD) between the emprical distribution of sentence embeddings <script type="math/tex">\tilde{\mathbf{f}}</script> and <script type="math/tex">\mathbf{f}</script> for synthetic and real data respectively.</p>
<p><img data-src="/notes/images/TextGAN.png" width="55%"/></p>
<p>$\mathcal{L}(G)$ attempts to adjust to force the synthetic sentences’ features <script type="math/tex">\tilde{\mathbf{f}}</script> to match the real sentence features <script type="math/tex">\mathbf{f}</script> encoded by $D(\cdot)$, by matching the empirical distributions of <script type="math/tex">\tilde{\mathbf{f}}</script> and <script type="math/tex">\mathbf{f}</script> with a kernel discrepancy metric, MMD.</p>
<h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h4><p>In Eq.(\ref{eq3}), the reconstruction and MMD loss in $D$ serve as the regularizer to the binary classification loss in that $D$ features tend to be more spread out in the feature space.</p>
<p>Thus, $D(\cdot)$ attempts to select informative sentence features, whereas $G(\cdot)$ aims to match these features. Hyperparameters <script type="math/tex">\lambda_r</script> and <script type="math/tex">\lambda_m</script> act as the trade-off.</p>
<p>The original GAN objective is prone to mode collapsing especially when applying $\log D$ alternative for the generator loss, <em>i.e.</em>, replacing the second term of Eq.(\ref{eq3}) with <script type="math/tex">-\mathbb{E}_{z\sim p_z} \log[D(G(z))]</script>. If so, fake samples are more severely penalized than less diverse samples, thus grossly underestimating the variance of latent features<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Metz, Luke, et al. "[Unrolled generative adversarial networks.](https://arxiv.org/pdf/1611.02163.pdf)" arXiv preprint arXiv:1611.02163 (2016).
">[3]</span></a></sup>.</p>
<p>The $G$’s loss in Eq.(\ref{eq4}) forces $G$ to produce highly diverse sentences to match the variations of real data by latent moment matching, thus alleviating the mode-collapsing problem.</p>
<h4 id="Feature-Matching-via-MMD"><a href="#Feature-Matching-via-MMD" class="headerlink" title="Feature Matching via MMD"></a>Feature Matching via MMD</h4><p>MMD measures the mean squared difference between two sets of samples $\mathcal{X}$ andq $\mathcal{Y}$ over a RKHD $\mathcal{H}$ with  kernel function $k(\cdot): \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}$, where <script type="math/tex">\mathcal{X}= \{ x_i \}_{i=1:N_x}, x_i \in \mathbb{R}^d</script>, <script type="math/tex">\mathcal{Y}= \{ y_i \}_{i=1:N_y}, y_i \in \mathbb{R}^d</script>. The kernel can be written as an inner product over $\mathcal{H}$: <script type="math/tex">k(x, x^\prime) = \langle k(x, \cdot), k(x^\prime, \cdot) \rangle_\mathcal{H}</script>, and  <script type="math/tex">\phi(x) \triangleq k(x, \cdot) \in \mathcal{H}</script> denotes the feature mapping. Fomally the MMD between $\mathcal{X}$ and $\mathcal{Y}$ is given by:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_{\text{MMD}^2} &{}= \| \mathbb{E}_{x \sim \mathcal{X}} \phi (x) - \mathbb{E}_{y \sim \mathcal{Y}} \phi(y) \|_\mathcal{H}^2 \\
&{}= \mathbb{E}_{x \sim \mathcal{X}} \mathbb{E}_{x^\prime \sim \mathcal{X}} [k(x, x^\prime)] + \mathbb{E}_{y \sim \mathcal{Y}} \mathbb{E}_{y^\prime \sim \mathcal{Y}}[k(y, y^\prime)] - 2 \mathbb{E}_{x \sim \mathcal{X}}\mathbb{E}_{y \sim \mathcal{Y}} [k(x,y)]
\end{align}</script><p>Here TextGANs adopt a gaussian (rbf) kernel $k(x,y)=\exp\big( - \frac{|x-y|^2}{2 \sigma} \big)$ with brandwidth $\sigma$.</p>
<h3 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><ul>
<li>$G$: LSTM generator.</li>
<li>$D$: CNN discriminator.</li>
</ul>
<h1 id="MaliGAN-MILA"><a href="#MaliGAN-MILA" class="headerlink" title="MaliGAN (MILA)"></a>MaliGAN (MILA)</h1><h2 id="Problems-2"><a href="#Problems-2" class="headerlink" title="Problems"></a>Problems</h2><p>Instability of GAN training: When optimizing $G$ USING $D$’s output as a reward via RL, the policy $G$ has difficulties to get positive and stable reward signals from $D$ even with careful pretraining.</p>
<p>When applying the GAN framework to discrete data, the discontinuity prohibits the update of the generator parameters via standard back-propagation. One way is to employ an RL strategy that directly uses the generator’s output, $D(\cdot)$, or $\log D(\cdot)$ as a reward.</p>
<p>Thus the objective for $G$ is to optimize:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_\textrm{GAN} (\theta) &{}= - \mathbb{E}_{\mathbf{x} \sim p_\theta} [\log D(\mathbf{x})] \\
&{}\approx -\frac{1}{n} \sum_{i=1}^n \log D(\mathbf{x}_i), \quad \mathbf{x}_i \sim p_\theta.
\end{align}</script><p>Define the normalized probability distribution <script type="math/tex">q^\prime (\mathbf{x}) = \frac{1}{Z(D)}D(\mathbf{x})^{1/\tau}</script> in some bounded region to guarantee the integrability ($D$ is an approxmation to <script type="math/tex">\frac{p_d}{p+p_d}</script> if well trained) and also put a maximum-entropy regularizer <script type="math/tex">\mathbb{H}(p_\theta)</script> to encourage diversity, yielding the regularized loss:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_\textrm{GAN} (\theta) &{}= - \mathbb{E}_{\mathbf{x} \sim p_\theta} [\log D(\mathbf{x})] - \tau \mathbb{H}(p_\theta)\\
&{}= \tau \mathbb{KL}(p_\theta \| q^\prime) + c(D),
\end{align}</script><p>where $c(D)$ is a constant only depending on $D$. Hence, optimizing the original GAN is equivalent to minimizing the KL-divergence <script type="math/tex">\mathbb{KL}(p_\theta \| q^\prime)</script>. However, since initially $p$ generates sentences with bad quality, it has little chance of generating good sequences to get a positive reward. Though with dedicated pre-training and variance reduction mechanisms, RL based on the moving reward signals still shows the unstable training and does not work on large scale datasets.</p>
<h2 id="Approach-2"><a href="#Approach-2" class="headerlink" title="Approach"></a>Approach</h2><p><strong>Ma</strong>ximum-<strong>Li</strong>kelihood Augmented Discrete GAN (MaliGAN)<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Che, Tong, et al. "[Maximum-likelihood augmented discrete generative adversarial networks.](https://arxiv.org/pdf/1702.07983.pdf)" arXiv preprint arXiv:1702.07983 (2017).
">[7]</span></a></sup> utilizes the information of $D$ as an additional source of training signals on top of the maximum-likelihood objective, significantly reducing the variance during training.</p>
<h3 id="Basic-MaliGAN"><a href="#Basic-MaliGAN" class="headerlink" title="Basic MaliGAN"></a>Basic MaliGAN</h3><p>MaliGAN keeps a delayed copy $p^\prime(\mathbf{x})$ of $G$ who is less often optimized. We know that the optimal $D$ is: <script type="math/tex">D(\mathbf{x})=\frac{p_d}{p_d + p^\prime}</script>; so we have <script type="math/tex">p_d=\frac{D}{1-D}p^\prime</script>. Thus MaliGAN sets the target distribution $q$ for maximum likelihood training to be $\frac{D}{1-D}p^\prime$.</p>
<p>Let <script type="math/tex">r_D(\mathbf{x}) = \frac{D(\mathbf{x})}{1-D(\mathbf{x})}</script>, we define the augmented target distribution as:</p>
<script type="math/tex; mode=display">
q(\mathbf{x}) = \frac{1}{Z(\theta^\prime)} \frac{D(\mathbf{x})}{1-D(\mathbf{x})} p^\prime (\mathbf{x}) = \frac{1}{Z(\theta^\prime)} r_D(\mathbf{x}) p^\prime (\mathbf{x}).</script><p>Regarding $q$ as a fixed probablity distribution, the target is to optimize:</p>
<script type="math/tex; mode=display">
\mathcal{L}_G(\theta) = \mathbb{KL} (q(\mathbf{x}) \| p_\theta (\mathbf{x})).</script><p>This objective has an attractive prob=perty that $q$ is a “fixed” distribution during training, <em>i.e.</em>, if $D$ is sufficiently trained, then $q$ is always approximately the data generating distribution <script type="math/tex">q_d</script>.<br>Defining the gradient as <script type="math/tex">\nabla \mathcal{L}_G = \mathbb{E}_q [\nabla_\theta \log p_\theta (\mathbf{x})]</script>, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla \mathcal{L}_G &{}= \mathbb{E}_{p^\prime} [\frac{q(\mathbf{x})}{p^\prime(\mathbf{x})} \nabla_\theta (\mathbf{x})] \\
&{}= \frac{1}{Z} \mathbb{E}_{p_\theta} [r_D (\mathbf{x})\nabla_\theta \log p_\theta (\mathbf{x})] ,
\end{align}</script><p>where we assume that <script type="math/tex">p^\prime = p_\theta</script> and the delayed generator is only one step behind the current update in the experiments.</p>
<p>Then $G$ is optimized as:</p>
<script type="math/tex; mode=display">
\nabla \mathcal{L}_G (\theta) \approx \sum_{i=1}^m (\frac{r_D(\mathbf{x}_i)}{\sum_i r_D(\mathbf{x}_i)} - b) \nabla \log p_\theta (\mathbf{x}_i),</script><p>where $b$ is the baseline to reduce variance. In practice, $b$ increases very slowly from 0 to 1 (as $D$).</p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p><img data-src="/notes/images/MaliGAN-Algorithm.png" width="50%"/></p>
<h3 id="MaliGAN-with-Variance-Reduction"><a href="#MaliGAN-with-Variance-Reduction" class="headerlink" title="MaliGAN with Variance Reduction"></a>MaliGAN with Variance Reduction</h3><h4 id="Mixed-MLE-Mali-Training"><a href="#Mixed-MLE-Mali-Training" class="headerlink" title="Mixed MLE-Mali Training"></a>Mixed MLE-Mali Training</h4><p>To alleviate the accumulated variance for long sequence generation, MaliGAN clamps the input using the training data for $N$ time steps&lt; and switch to the free-running mode for the remaining $T-N$ time steps. During training, $N$ slowly moves from $T$ towards 0.</p>
<p>Thus,</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla \mathcal{L}_G &{}= \mathbb{E}_q [\nabla \log p_\theta (\mathbf{x})]\\
&{}= \mathbb{E}_{p_d} [\nabla \log p_\theta (\mathbf{x}_{\leq N})] + \mathbb{E}_q [\nabla \log p_\theta (\mathbf{x}_{>N} \vert \mathbf{x}_{\leq N})] \\
&{}= \mathbb{E}_{p_d} [\nabla \log p_\theta (x_0, x_1, \cdots, x_T)] + \frac{1}{Z} \mathbb{E}_{p_\theta} [\sum_{t=N+1}^L r_D (\mathbf{x} \nabla \log p_\theta (a_t \vert \mathbf{s}_t))] 
\end{align}</script><p>For each sample <script type="math/tex">\mathbf{x}_i</script> from the real data batch, if it has length larger than $N$, we fix the first $N$ words of <script type="math/tex">\mathbf{x}_i</script>, then sample $n$ times from $G$ till the end of the sequence and get $n$ samples <script type="math/tex">\{ \mathbf{x}_{i,j} \}_{j=1}^n</script>. Then for each mini-batch with $0 \leq N \leq T$:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla \mathcal{L}_G^N \approx \sum_{i=1,j=1}^{m,n} \big(\frac{r_D(\mathbf{x}_{i,j})}{\sum_j r_D (\mathbf{x}_{i,j})} -b \big) \nabla \log p_\theta (\mathbf{x}_{>N} \vert \mathbf{x}_{\leq N}) + \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^N p_\theta (a_t^i \vert \mathbf{s}_t^i)
\end{align}</script><h4 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h4><p><img data-src="/notes/images/MaliGAN-Mix-Algorithm.png" width="60%"/></p>
<h1 id="GSGAN-2016"><a href="#GSGAN-2016" class="headerlink" title="GSGAN (2016)"></a>GSGAN (2016)</h1><h2 id="Problems-3"><a href="#Problems-3" class="headerlink" title="Problems"></a>Problems</h2><p>In the standard GAN framework, samples from a distribution on discrete objects such as multinomial are not differentiable w.r.t. the distribution parameters.</p>
<h2 id="Gumbel-softmax-Distribution"><a href="#Gumbel-softmax-Distribution" class="headerlink" title="Gumbel-softmax Distribution"></a>Gumbel-softmax Distribution</h2><p>GSGAN<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kusner, Matt J., and José Miguel Hernández-Lobato. "[Gans for sequences of discrete elements with the gumbel-softmax distribution.](https://arxiv.org/pdf/1611.04051.pdf)" arXiv preprint arXiv:1611.04051 (2016).
">[9]</span></a></sup> uses the Gumbel-softmax distribution parameterized in terms of the softmax function to avoid the non-differential problem in GAN.</p>
<p>The softmax function can be used to parameterize a multinomial distribution on a one-hot-encoding $d$-dimensional vector $\mathbf{y}$ in terms of a continuous $d$-dimensional vector $\mathbf{h}$. Let $\mathbf{p}$ be a $d$-dimensional vector of probabilities specifying the multinomial distribution on $\mathbf{y}$ with <script type="math/tex">p_i = p(y_i=1), i=1,\cdots,d</script>.</p>
<p>Then </p>
<script type="math/tex; mode=display">\mathbf{p} = \textrm{softmax}(\mathbf{h}),</script><p>where <script type="math/tex">[\textrm{softmax}(\mathbf{h})]_i = \frac{\exp(\mathbf{h}_i)}{\sum_{j=1}^K \exp (\mathbf{h}_j)}, \textrm{for }i=1,\cdots,d</script></p>
<p>Sampling $\mathbf{y}$ accoridng to the previous multinomial distribution with probability vector is the same as sampling $\mathbf{y}$ according to</p>
<script type="math/tex; mode=display">\mathbf{y}= \textrm{one_hot}(\arg\max_i (h_i + g_i)),</script><p>where <script type="math/tex">g_i</script> are independent and follow a Gumbel distribution with zero lcoation and unit scale. The sampled result has gradient zero w.r.t. $\mathbf{h}$ because the $\textrm{one_hot}(\arg\max(\cdot))$ is not differentiable. Thus, GSGAN propose to approximate with a differentiable function based on the soft-max transformtion:</p>
<script type="math/tex; mode=display">
\mathbf{y} = \textrm{softmax}(\frac{1}{\tau} (\mathbf{h}+\mathbf{g})),</script><p>where $\tau$ is an inverse temperature parameter. When $\tau \rightarrow 0$, the samples have the same output as argmax versionl when $\tau \rightarrow \infty$, the samples are always the uniform probability vector. GAN on discrete data can be trained with this, starting with soem relatively large $\tau$ and then annealing it to zero during training.</p>
<h1 id="RankGAN-NIPS’17"><a href="#RankGAN-NIPS’17" class="headerlink" title="RankGAN (NIPS’17)"></a>RankGAN (NIPS’17)</h1><h2 id="Problems-4"><a href="#Problems-4" class="headerlink" title="Problems"></a>Problems</h2><p>GANs assume the output of $D$ to be a binary predicate indicating whether the given sequence is from real or fake data, which is too restrictive since the diversity and richness of the sentences are constrained by the degenerated distribution due to binary classification.</p>
<h2 id="Approach-3"><a href="#Approach-3" class="headerlink" title="Approach"></a>Approach</h2><p>RankGAN<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lin, Kevin, et al. "[Adversarial ranking for language generation.](http://papers.nips.cc/paper/6908-adversarial-ranking-for-language-generation.pdf)" Advances in Neural Information Processing Systems (2017).
">[8]</span></a></sup> replaces the original binary classifier discriminator with a ranking model by taking a softmax over the expected cosine distances from the generated sequences to the real data. It relaxes the training of binary discriminator to a learning-to-rank optimization problem, consisting of a generator <script type="math/tex">G_\theta</script> and a ranker <script type="math/tex">R_\phi</script>. Instead of performing binary classification, the ranker is trained to rank the machine-generated sequences lower than the human-generated sequences. </p>
<p><img data-src="/notes/images/RankGAN.png" width="70%"/></p>
<p>$G$ is to confuse the ranker $R$ so that synthetic samples are ranked higher than real samples, while $R$ is to rank the synthetic sample (denoted “G” in the figure) lower than human-written setences (denoted “H” in the figure). Thus, $G$ and $R$ play a minimax game:</p>
<script type="math/tex; mode=display">
\min_\theta \max_\phi \mathcal{L}(G_\theta, R_\phi) = \mathbb{E}_{s \sim \mathcal{P}_h} [\log R_\phi (s \vert U, C^-)] + \mathbb{E}_{s \sim G_\theta} [\log (1-R_\phi (s \vert U, C^+))],</script><p>where <script type="math/tex">\mathcal{P}_h</script> denotes the read data from human-written sentences, $C^+， C^-$ are comparison set w.r.t. different input $s$: when $s$ is the real data, $C^-$ generated data pre-sampled from <script type="math/tex">G_\theta</script>; If $s$ is the synthetic data, $C^+$ is the human written data.</p>
<h3 id="Rank-Score"><a href="#Rank-Score" class="headerlink" title="Rank Score"></a>Rank Score</h3><p>The relevance score of the input sequence $s$ given a reference $u$ is:</p>
<script type="math/tex; mode=display">
\alpha (s \vert u) = \cos (y_s, y_u) = \frac{y_s \cdot y_u}{\Vert y_s \Vert \Vert y_u \Vert},</script><p>where <script type="math/tex">y_u</script> and <script type="math/tex">y_s</script> are embedded feature vectors of the reference and input sequence, respectively.</p>
<p>Then the ranking score for a sequence $s$ is computed given a comparison set $\mathcal{S}$:</p>
<script type="math/tex; mode=display">
P(s \vert u, \mathcal{C}) = \frac{\exp (\gamma \alpha(s\vert u))}{\sum_{s^\prime \in \mathcal{C}^\prime} \exp (\gamma \alpha(s^\prime \vert u)) },</script><p>which is similar to Boltzmann exploration in RL. Lower $\gamma$ results in all setenecs to be nearly equiprobable (uniform), while higher $\gamma$ increases the biases towards the sentence with higher score. <script type="math/tex">\mathcal{C}^\prime = \mathcal{C} \cup \{ s \}</script> denotes the set of input sentences to be ranked.</p>
<h3 id="Training-2"><a href="#Training-2" class="headerlink" title="Training"></a>Training</h3><p>Like SeqGAN, RankGAN employs Monte Carlo rollout methods to simulate the intermediate rewards when a sequence is incomplete. The expected future reward $V$ for partial sequences is computed by:</p>
<script type="math/tex; mode=display">
V_{\theta, \phi} (s_{1:t-1}, U) = \mathbb{E}_{s_r \sim G_\theta} [R_\phi (s_r \vert U, \mathcal{C}^+, s_{1:t-1})],</script><p>where <script type="math/tex">s_r</script> represents the complete setence sampled by rollout methods with given partial sequence <script type="math/tex">s_{1:t-1}</script>. Specifically, the beginning tokens <script type="math/tex">(w_0, w_1, \cdots, w_{t-1})</script> are fixed and the rest tokens are consecutively sampled by <script type="math/tex">G_\theta</script> unitl the last token <script type="math/tex">w_T</script> is generated. It samples $n$ times and take the average ranking score to approximate the expected reward.</p>
<p>The gradient of $G$’s objective is:</p>
<script type="math/tex; mode=display">
\nabla_\theta \mathcal{L} (s_0) = \mathbb{E}_{s_{1:T} \sim G_\theta} \big[ \sum_{t=1}^T \sum_{w_t \in V} \nabla_\theta \pi_\theta (w_t \vert s_{1:t-1}) V_{\theta, \phi}(s_{1:t}, U) \big].</script><p>In practice, minimizing $\log R(\cdot)$ instead of maximizing $\log (1-R(\cdot))$ performs better to train the ranker $R$. Thus, maximize the ranking objective:</p>
<script type="math/tex; mode=display">
\max_\phi \mathcal{L}(G_\theta, R_\phi) = \mathbb{E}_{s \sim \mathcal{P}_h} [\log R_\phi (s \vert U, C^-)] - \mathbb{E}_{s \sim G_\theta} [\log R_\phi (s \vert U, C^+)],</script><div class="note info">
            <p>In a sense, replacing binary predicates with (multi-sentence) ranking scores can relieve the gradient vanishing problem.<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lin, Kevin, et al. "[Adversarial ranking for language generation.](http://papers.nips.cc/paper/6908-adversarial-ranking-for-language-generation.pdf)" Advances in Neural Information Processing Systems (2017).">[8]</span></a></sup></p>
          </div>
<h1 id="LeakGAN-AAAI’18"><a href="#LeakGAN-AAAI’18" class="headerlink" title="LeakGAN (AAAI’18)"></a>LeakGAN (AAAI’18)</h1><h2 id="Problems-5"><a href="#Problems-5" class="headerlink" title="Problems"></a>Problems</h2><ol>
<li>Sparsity: GANs with policy gradient can only get a scalar guiding signal after generating the entire texts and lack intermediate information about text structure during the generation process, which grossly hinders the generation of long texts (&gt;20 words). </li>
<li>Non-informativeness: the scalar guiding signal for a whole text is non-informative as it does not necessarily preserve the picture about the intermediate syntactic and semantics of the text that is being generated for $G$ to sufficiently learn. </li>
</ol>
<h2 id="Approach-4"><a href="#Approach-4" class="headerlink" title="Approach"></a>Approach</h2><p>Inspired by Hierarchical Reinforcement Learning (HRL), LeakGAN<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Guo, Jiaxian, et al. "[Long text generation via adversarial training with leaked information.](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16360/16061)" Thirty-Second AAAI Conference on Artificial Intelligence (2018).
">[6]</span></a></sup> designs a hierarchical generator $G$, consisting of a high-level “MANAGER” module and a low-level “WORKER” module. In each step, “MANAGER” receives $D$’s high-level feature representation to form the guiding goal for the “WORKER” module, which is a leakage of information from $D$. Then the “WORKER” module firstly encodes the currently generated tokens and combines with the goal embedding to take the final action at the current state. As such, the guiding signals from $D$ is available not only at the end but during the generation process.</p>
<div class="note info">
            <p>LeakGAN can implicitly learn sentence structures, such as punctuation, clause structure, and long suffix without any supervision<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Guo, Jiaxian, et al. "[Long text generation via adversarial training with leaked information.](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16360/16061)" Thirty-Second AAAI Conference on Artificial Intelligence (2018).">[6]</span></a></sup>.</p>
          </div>
<p><img data-src="/notes/images/LeakGAN.png" width="60%"/></p>
<h3 id="Feature-Leakage-from-D"><a href="#Feature-Leakage-from-D" class="headerlink" title="Feature Leakage from $D$"></a>Feature Leakage from $D$</h3><p>LeakGAN allows <script type="math/tex">D_\phi</script> to provide additional information, <em>i.e.</em>, feature <script type="math/tex">f_t</script> of the current sequence <script type="math/tex">s_T</script> to generate <script type="math/tex">G_\theta</script>.</p>
<p>Typically, <script type="math/tex">D_\phi</script> can be decomposed into a feature extractor <script type="math/tex">\mathcal{F}(\cdot ;\phi_f)</script> and a final sigmoid classification layer with weight <script type="math/tex">\phi_l</script>. Mathematically, given the input $s$, we have:</p>
<script type="math/tex; mode=display">
D_\phi (s) = \sigma (\phi_l^\top \mathcal{F}(s;\phi_f)) = \sigma (\phi_l^\top f),</script><p>where $f$ is the exxtracted features of CNN after max-over-time pooling.</p>
<p>In each time step $t$, “MANAGER” is an LSTM that takes the extracted feature vector <script type="math/tex">f_t</script> and generates a goal vector <script type="math/tex">g_t</script>, which is then fed into the “WORKER” module to guide the next word’s generation.</p>
<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><p>The “MANAGER” and “WORKER” of LSTMs are all zero-initialized. At each step, the “MANAGER” receives the leaked feature vector <script type="math/tex">f_t</script> from the $D$ to produce the goal vector <script type="math/tex">g_t</script> as:</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{g}, h_t^M &{}= \mathcal{M} (f_t, h_{t-1}^M; \theta_m), \\
g_t &{}= \frac{\hat{g_t}}{\Vert g_t \Vert},
\end{align}</script><p>where <script type="math/tex">\mathcal{M}(\cdot; \theta_m)</script> denotes the LSTM of “MANAGER” with parameters <script type="math/tex">\theta_m</script> and hidden vector <script type="math/tex">h_t^M</script>.</p>
<p>The goal is a linear transformation $\psi$ with weight matrix <script type="math/tex">W_\psi</script> with a summation over recent $c$ goals to produce a $k$-dimensional goal embedding <script type="math/tex">w_t</script> as:</p>
<script type="math/tex; mode=display">
w_t = \psi \big( \sum_{i=1}^c g_{t-i} \big) = W_\psi \big( \sum_{i=1}^c g_{t-i} \big).</script><p>Then the “WORKER” takes the current word <script type="math/tex">x_t</script> and combines the output with the goal embedding <script type="math/tex">w_t</script> with a dot product before softmax:</p>
<script type="math/tex; mode=display">
\begin{align}
O_t, h_t^W &{}= \mathcal{W} (x_t, h_{t-1}^W; \theta_w), \\
G_\theta (\cdot \vert s_t) &{}= \textrm{softmax} (O_t \cdot w_t / \alpha), 
\end{align}</script><p>where <script type="math/tex">\mathcal{W}(\cdot; \theta_w)</script> denotes the LSTM of “WORKER”, $\alpha$ is the temperature to control the generation entropy.</p>
<h4 id="Training-of-G"><a href="#Training-of-G" class="headerlink" title="Training of $G$"></a>Training of $G$</h4><p>“MANAGER” is trained to predict advantageous directions in the discriminative feature space and the “WORKER” is intrinsically rewarded to follow such directions. The gradient of manager is defined as:</p>
<script type="math/tex; mode=display">
\nabla_{\theta_m}^\textrm{adv} g_t = -Q_\mathcal{F} (s_t, g_t) \nabla_{\theta_m} \cos \big( f_{t+c}-f_t, g_t\big)</script><p>where <script type="math/tex">Q_\mathcal{F} (s_t, g_t)= Q (s_t, g_t) = \mathbb{E} [r_t]</script> is the expected reward under the current policy. $\cos(\cdot)$ measures the cosine similarity between the change of feature representation after $c$ step transitions, <em>i.e.</em>, <script type="math/tex">f_{t+c}-f_t</script>, and the goal vector <script type="math/tex">g_t</script>. This loss functin is intuitively force the goal vector to match the transition inArrow the feature space while achieving high reward.</p>
<p>Meanwhile, the “WORKER” is trined to maximize the reaward using the REINFORCE algorithm:</p>
<script type="math/tex; mode=display">
\nabla_{\theta_w} \mathbb{E}_{s_{t-1}\sim G} [\sum_{x_t} r_t^I \mathcal{W} (x_t \vert s_{t-1}; \theta_w)] = \mathbb{E}_{s_{t-1} \sim G, x_t \sim \mathcal{W}(x_t \vert s_{t-1})} [r_t^I \nabla_{\theta_w} \log \mathcal{W}(x_t \vert s_{t-1}; \theta_w)]</script><p>where the intrinsit reward for “WORKER” <script type="math/tex">r_t^I</script> is defined as:</p>
<script type="math/tex; mode=display">
r_t^I = \frac{1}{c} \sum_{i=1}^c \cos \big( f_t - f_{t-i}, g_{t-i} \big).</script><p>To be consistent, in pretraining stage, the gradient of “MANAGER” is:</p>
<script type="math/tex; mode=display">
\nabla_{\theta_m}^\textrm{pre} g_t = - \nabla_{\theta_m} \cos(\hat{f}_{t+c} - \hat{f}_t, g_t)</script><div class="note info">
            <p><strong>Interleaved training of MLE and GAN</strong> instead of full GAN training after pretraining. Blending these two training would help GAN get rid of some local minimum and alleviate mode collapse. Inserting MLE performs an implicit regularization on GAN to prevent it from going too far away from the MLE solution.</p>
          </div>
<h1 id="FM-GAN-NeurIPS’18"><a href="#FM-GAN-NeurIPS’18" class="headerlink" title="FM-GAN (NeurIPS’18)"></a>FM-GAN (NeurIPS’18)</h1><h2 id="Problems-6"><a href="#Problems-6" class="headerlink" title="Problems"></a>Problems</h2><p>TextGAN<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Yizhe, et al. "[Adversarial feature matching for text generation.](https://arxiv.org/pdf/1706.03850)" arXiv preprint arXiv:1706.03850 (2017).
">[2]</span></a></sup> applied feature matching with MMD in the objective, which is difficult to train:</p>
<ol>
<li>Choices of the bandwidth of the RBF kernel;</li>
<li>Kernel methods often suffer from poor scaling;</li>
<li>Empirically, TextGAN tends to generate short sentences.</li>
</ol>
<h2 id="Approach-5"><a href="#Approach-5" class="headerlink" title="Approach"></a>Approach</h2><p>Feature Mover GAN (FM-GAN)<sup><a href="http://papers.nips.cc/paper/7717-adversarial-text-generation-via-feature-movers-distance.pdf">[11]</a></sup> leverages earth-mover’s distance (EMD) in optimal transport (OT), which considers the problem of optimally transporting one set of data points to another. FM-GAN proposes feature-mover’s distance, a variant of EMD between the feature distribution of real and synthetic sentences. In this adversarial setting, $D$ aims to maximize the dissimilarity of the feature distributions based on the FMD, while the generator is trained to minimize the FMD by synthesizing more-realistic data.</p>
<p><img data-src="/notes/images/FM-GAN.png" width="70%"/><br>See <sup><a href="http://papers.nips.cc/paper/7717-adversarial-text-generation-via-feature-movers-distance.pdf">[11]</a></sup> for detailed formula of FMD.</p>
<h1 id="MaskGAN-ICLR’18"><a href="#MaskGAN-ICLR’18" class="headerlink" title="MaskGAN (ICLR’18)"></a>MaskGAN (ICLR’18)</h1><h2 id="Problems-7"><a href="#Problems-7" class="headerlink" title="Problems"></a>Problems</h2><p>Training instability and mode dropping.</p>
<h2 id="Approach-6"><a href="#Approach-6" class="headerlink" title="Approach"></a>Approach</h2><p>MaskGAN<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fedus, William, Ian Goodfellow, and Andrew M. Dai. "[MaskGAN: Better text generation via filling in the \_.](https://arxiv.org/pdf/1801.07736.pdf)" ICLR (2018).
">[10]</span></a></sup> introduces an actor-critic conditional GAN that provides rewards at every time step. It fills in missing text conditioned on the surrounding context including text fill-in-the-blank or in-filling tasks, in which portions of the body of text are deleted or redacted. The goal of the model is to infill the missing portions of the text so that it is indistinguishable from the original data.</p>
<ul>
<li>In-filling text: autoregressively output tokens that have thus far filled in as in standard language modeling while conditioned on the true known context.</li>
<li>If the entire body of the text is redacted, then this reduces to language modeling.</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>Let <script type="math/tex">(x_t, y_t)</script> denote pairs of input and target tokens; <script type="math/tex">\hat{x}_t</script> is the filled-in token. Either real or fake <script type="math/tex">\hat{x}_t</script> will be passed to $D$ during training.</p>
<p>MaskGAN uses seq2seq encoder-decoder architecture. For a discrete sequence <script type="math/tex">\mathbf{x}= (x_1, \cdots, x_T)</script>, a binary mask is generated of the same length <script type="math/tex">\mathbf{m}=(m_1, \cdots, m_T)</script> where <script type="math/tex">m_t \in \{ 0,1 \}</script> determining whether to retain or mask.</p>
<p>The masked sequence <script type="math/tex">\mathbf{m}(\mathbf{x})</script> is fed to the encoder (as below figure), and the decoder fills in missing tokens auto-regressively conditioned on both the masked input and what has filled-in upfront. The generator decomposes the distribution over the sequence into an ordered conditional sequence:</p>
<script type="math/tex; mode=display">
G(x_t) \equiv P(\hat{x}_1, \cdots, \hat{x}_T \vert \mathbf{m(x)}) = \prod_{t=1}^T P(\hat{x}_t \vert \hat{x}_1, \cdots, \hat{x}_{t-1}, \mathbf{m(x)}).</script><p><img data-src="/notes/images/MaskGAN.png" width="100%"/></p>
<center> Generator architecture<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Fedus, William, Ian Goodfellow, and Andrew M. Dai. "[MaskGAN: Better text generation via filling in the \_.](https://arxiv.org/pdf/1801.07736.pdf)" ICLR (2018).
">[10]</span></a></sup> </center>


<p>The discriminator $D$ has the identical architecture to $G$ except the scalar output at each time step, computing the probability of each token $\tilde{x}_t$ being real given the true context of masked sequences $\mathbf{m(x)}$:</p>
<script type="math/tex; mode=display">
D_\phi (\tilde{x}_t \vert \tilde{x}_{0:T}, \mathbf{m(x)}) = P(\tilde{x}_t = x_t^\textrm{real} \vert \tilde{x}_{0:T}, \mathbf{m(x)}).</script><p>The logrithm of the $D$’s estimates are regarded as the reward:</p>
<script type="math/tex; mode=display">r_t \equiv \log D_\phi (\tilde{x}_t \vert \tilde{x}_{0:T}, \mathbf{m(x)}).</script><p>The critic net is an additional head off the discriminator, estimating the value function in RL.</p>
<h2 id="Training-3"><a href="#Training-3" class="headerlink" title="Training"></a>Training</h2><p>MaskGAN employs policy gradient estimation for generator $G$:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_\theta \mathbb{E}_G [R_t] &{}= (R_t - b_t) \nabla_\theta G_\theta (\hat{x}_t) \\
&{}= \mathbb{E}_{\hat{x}_t \sim G} \big[ \sum_{t=1}^T (R_t -b) \nabla_\theta \log G_\theta(\hat{x}_t) \big] \\
&{}= \mathbb{E}_{\hat{x}_t \sim G} \big[ \sum_{t=1}^T (\gamma^s r_s - b_t) \nabla_\theta \log G_\theta (\hat{x}_t) \big],
\end{align}</script><p>where $gamma$ is the discount vector, $b_t$ is the critic.</p>
<p>Finally, $D$ is updated with:</p>
<script type="math/tex; mode=display">
\nabla_\phi \frac{1}{m} \sum_{i=1}^m \big[ \log D(x^{(i)}) + \log (1-D(G(z^{(i)}))) \big]</script><p><strong>Pretraining</strong>:</p>
<ol>
<li>Trin LM using MLE for encoder/decoder.</li>
<li>Then pretrain the seq2seq model on the in-filling task using MLE. Select with holdout set.</li>
<li>Not include critic.</li>
</ol>
<h1 id="SentiGAN-IJCAI’18"><a href="#SentiGAN-IJCAI’18" class="headerlink" title="SentiGAN (IJCAI’18)"></a>SentiGAN (IJCAI’18)</h1><p>SentiGAN<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Ke, and Xiaojun Wan. "[SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks.](https://www.tensorinfinity.com/upload/files/20181227/1545889246130589.pdf)" IJCAI (2018).
">[12]</span></a></sup> employs $k$ generators with $k$ sentiment labels and one multi-class ($k+1$) discriminator.</p>
<p>Let <script type="math/tex">S_t</script> represent the partially generated sequence <script type="math/tex">S_t = \{ X_1, \cdots, X_t \}</script>, where <script type="math/tex">X_t</script> is a token generated at time $t$. It defines the penalty based loss function at step $t$ for $G$:</p>
<script type="math/tex; mode=display">
\mathcal{L}(X) = G_i (X_{t+1} \vert S_t) \cdot V_D^G (S_t, X_{t+1}),</script><p>where <script type="math/tex">V_D^G (S_t, X_{t+1})</script> is generated by $D$.</p>
<p>The objective of $G$ is defined with MC search:</p>
<script type="math/tex; mode=display">
\begin{align}
J_G &= \mathbb{E}_{X \sim P_g} [\mathcal{L}(X)] \\
&= \sum_{t=0}^{t= \vert X \vert -1} G (X_{t+1} \vert S_t) \cdot V_D^G (S_t, X_{t+1})
\end{align}</script><p><img data-src="/notes/images/SentiGAN.png" width="70%"/></p>
<p>$D$ is a CNN-based multi-class discriminator, producing a ${k+1}$-dimensional probability vector. The score at $i$-th ($i \in {1,\cdots,k}$) index represents the probablity of being the $i$-th sentiment, the $(k+1)$-th index denote the probability to be synthetic.</p>
<p>Refer to <sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Ke, and Xiaojun Wan. "[SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks.](https://www.tensorinfinity.com/upload/files/20181227/1545889246130589.pdf)" IJCAI (2018).
">[12]</span></a></sup> for details.</p>
<h1 id="RelGAN-ICLR’19"><a href="#RelGAN-ICLR’19" class="headerlink" title="RelGAN (ICLR’19)"></a>RelGAN (ICLR’19)</h1><h2 id="Problems-8"><a href="#Problems-8" class="headerlink" title="Problems"></a>Problems</h2><p>GANs suffer from mode collapse issue due to either a lack of expressive power in $G$ (not considering many more complex modes in the data distribution), or by a less informative guiding signal in $D$ (constrain the $G$’s update to within certain modes). </p>
<p>The LSTM-based generator might be the bottleneck of GANs with such experimental observations:</p>
<ol>
<li>$D$’s loss value very quickly goes t near minimum after few iterations, which means $D$ may be more powerful than $G$  and can easily distinguish between real/fake samples;</li>
<li>Mode collapse may partly indicate the incapacity of $G$, as it may not be expressive enough to fit all modes of data distribution;</li>
<li>Existing GANs perform poorly at long sentence generation, and LSTM encodes all previous sequences into a fixed hidden vector, potentially limiting its ability to modeling long-distance dependency.</li>
</ol>
<h2 id="Approach-7"><a href="#Approach-7" class="headerlink" title="Approach"></a>Approach</h2><p>RelGAN<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Nie, Weili, Nina Narodytska, and Ankit Patel. "[Relgan: Relational generative adversarial networks for text generation.](https://openreview.net/pdf?id=rJedV3R5tm)" International conference on learning representations (2019).
">[13]</span></a></sup> employs a relational memory based generator; Gumbel-softmax trick; and multi-representations in $D$.</p>
<h3 id="Relational-Memory-based-G"><a href="#Relational-Memory-based-G" class="headerlink" title="Relational Memory based $G$"></a>Relational Memory based $G$</h3><p>As below figure, let each row of the memory <script type="math/tex">M_t</script> denote a memory slot. Given input <script type="math/tex">x_t</script> at time $t$ and $H$ heads, the memory is updated with self-attention mechanisms.<br><img data-src="/notes/images/RelGAN-Generator.png" width="70%"/></p>
<p>For each head, we have query <script type="math/tex">Q_t = M_t W_q</script>, key <script type="math/tex">K_t = [M_t; x_t] W_k</script>, and value <script type="math/tex">V_t = [M_t;x_t]W_v</script>, where $[;]$ denotes row-wise concatenation. Thus, the updated memory <script type="math/tex">\tilde{M}_{t+1}</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
\tilde{M}_{t+1} &{}= [\tilde{M}_{t+1}^{(1)}L\cdots :\tilde{M}_{t+1}^{(H)}],\\
\tilde{M}_{t+1}^{(h)} &{}= \textrm{softmax}\big( d_k^{-1/2} M_t W_q ([M_t;x_t] W_k)^\top \big) [M_t;x_t] W_v,
\end{align}</script><p>where <script type="math/tex">d_k</script> is the column dimension of <script type="math/tex">K_t</script>, $[:]$ denotes column-wise concatenation.</p>
<p>Then the next memory <script type="math/tex">M_{t+1}</script> is computed with skip-connections/MLP/gated operations.</p>
<h3 id="Gumbel-Softmax-Relaxation"><a href="#Gumbel-Softmax-Relaxation" class="headerlink" title="Gumbel-Softmax Relaxation"></a>Gumbel-Softmax Relaxation</h3><p>The multinomial softmax can be parameterized as:</p>
<script type="math/tex; mode=display">
y_{t+1} = \textrm{one_hot} (\arg\max_{1\leq i \leq V} (o_t^{(i)} + g_t^{(i)})),</script><p>where <script type="math/tex">o_t^{(i)}</script> denotes the $i$-th entry of <script type="math/tex">o_t</script> and <script type="math/tex">g_t^{(i)}</script> is from the $i.i.d.$ Gumbel distribution <script type="math/tex">g_t^{(i)} = -\log \big( -\log U_t^{(i)} \big)</script> with <script type="math/tex">U_t^{(i)} \sim \textrm{uniform}(0,1)</script>.</p>
<p>Further, the one-hot with argmax op can be approximated as:</p>
<script type="math/tex; mode=display">
\hat{y}_{t+1} = \textrm{sofmtax} \big( \beta (o_t + g_t) \big),</script><p>where the incerse temperature $\beta \in \mathbb{R}+$ is a tunable parameter. Large $\beta$ encourages exploration for better sample diversity while smaller one does more explitation for bettter sample quality. </p>
<p>Thus it has an exponential policy: <script type="math/tex">\beta_n = \beta_\max^{n/N}</script>, where <script type="math/tex">\beta_\max</script> denotes the maximum inverse temperature, $N$ is the maximum # of training iteration, $n$ denotes current iteration. The increase rate of inverse temperature is from exploitation phrase to exploration phrase.</p>
<h3 id="Multiple-Representaions-in-D"><a href="#Multiple-Representaions-in-D" class="headerlink" title="Multiple Representaions in $D$"></a>Multiple Representaions in $D$</h3><p>RelGAN applies multiple embedded representations for each input with each independently passed through CNN-based classifiers to get the score. Finally, take the average of different representations as the final guiding signal to update $G$. This resembles the use of multiple discriminators in image GANs but keeps a weight-sharing CNN-based classifier to curtail the computational cost.</p>
<p><img data-src="/notes/images/RelGAN-Discriminator.png" width="70%"/></p>
<h3 id="Training-4"><a href="#Training-4" class="headerlink" title="Training"></a>Training</h3><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>RelGAN use the loss of Relativistic GAN (RSGAN), <em>i.e.</em>, <script type="math/tex">f(a,b) = \log \sigma (a-b)</script> for $a,b \in \mathbb{R}$.<br>Thus,</p>
<script type="math/tex; mode=display">\mathcal{L}_D = \frac{1}{S} \sum_{s=1}^S \mathbb{E}_{r_{1:T}\sim P_R; \hat{y}_{1:T}\sim P_\theta} \log \sigma \big( D(\tilde{X}_r^{(s)}) - D(\tilde{X}_y^{(s)}) \big).</script><p>Intuitively, this loss is to directly estimate the average probability that real sentences are more realistic than generated sentences in terms of different embedded representations.</p>
<h1 id="ScratchGAN-NeurIPS’19"><a href="#ScratchGAN-NeurIPS’19" class="headerlink" title="ScratchGAN (NeurIPS’19)"></a>ScratchGAN (NeurIPS’19)</h1><h2 id="Problems-9"><a href="#Problems-9" class="headerlink" title="Problems"></a>Problems</h2><p>Having suffered from challenges with gradient estimation, optimization instability, and mode collapse, existing language GANs resorted to MLE pretraining followed by adversarial fine-tuning with restrictive fine-tuning epochs and a small learning rate.<br>This suggests that “the best-performing GANs tend to stay close to the solution given by MLE training”. Even with pre-training, it shows that discrete GANs do not improve over MLE training.</p>
<h2 id="Learning-Signals"><a href="#Learning-Signals" class="headerlink" title="Learning Signals"></a>Learning Signals</h2><div class="note info">
            <p>The REINFORCE gradient estimator for $G$:</p><script type="math/tex; mode=display">\nabla_\theta \mathbb{E}_{p_\theta (\mathbf{x})} [R(\mathbf{x})] = \mathbb{E}_{p_\theta (\mathbf{x})}[R(\mathbf{x}) \nabla_\theta \log p_\theta (\mathbf{x}) ],</script><p>where $R(\mathbf{x})$ is provided by $D(\cdot)$. When setting <script type="math/tex">R(\mathbf{x})=\frac{p^*(\mathbf{x})}{p_\theta(\mathbf{x})}</script>, it recovers the MLE estimator:</p><script type="math/tex; mode=display">\mathbb{E}_{p_\theta (\mathbf{x})}[\frac{p^*(\mathbf{x})}{p_\theta(\mathbf{x})} \nabla_\theta \log p_\theta (\mathbf{x}) ] = \mathbb{E}_{p^*(\mathbf{x})}[\nabla_\theta \log p_\theta (\mathbf{x})] = \nabla_\theta\mathbb{E}_{p^*(\mathbf{x})}\log p_\theta (\mathbf{x}).</script><p>The gradient updates of MLE can be seen as a special case of the REINFORCE updates in discrete GAN training, whereas the language GANs’ rewards are learned.</p><p>We postulate the learned rewards provide a smoother signal to $G$ than classical MLE loss: $D$ can learn to generalize and provide a meaningful signal over parts of the distribution uncovered by the training data. As the training progresses and the signal from $D$ improves, $G$ also explores other parts of the data space, providing a natural curriculum, whereas MLE training is only exposed to the expert demonstration (real data).</p>
          </div>
<h2 id="Approach-8"><a href="#Approach-8" class="headerlink" title="Approach"></a>Approach</h2><p>ScratchGAN<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="de Masson d'Autume, Cyprien, et al. "[Training language gans from scratch.](http://papers.nips.cc/paper/8682-training-language-gans-from-scratch.pdf)" Advances in Neural Information Processing Systems (2019).
">[14]</span></a></sup> combines existing techniques such as large batch sizes, dense rewards, and discriminator regularization to stabilize and improve the discrete GANs.</p>
<h3 id="Dense-Rewards"><a href="#Dense-Rewards" class="headerlink" title="Dense Rewards"></a>Dense Rewards</h3><p>ScratchGAN emplolys a recurrent discriminator to provide rewards for each generated token. The discriminator learns to distinguish between sentence prefixees coming from real data and sampled sentence prefixes:</p>
<script type="math/tex; mode=display">
\max_\phi \sum_{i=1}^T \mathbb{E}_{p^*(x_t \vert x_1, \cdots, x_{t-1})} [\log D_\phi (x_t \vert x_1, \cdots, x_{t-1})] + \sum_{t=1}^T \mathbb{E}_{p_\theta(x_t \vert x_1, \cdots, x_{t-1})} [1-\log D_\phi (x_t \vert x_1, \cdots, x_{t-1})].</script><p>The recurrent $D$ is much cheaper than Monte Carlo Tree Search (MCTS) to score partial sentences.</p>
<p>FOr the geerated token <script type="math/tex">\hat{x}_t \sim p_\theta (x_t \vert x_1, \cdots, x_{t-1})</script>, the reward at time step $t$ is scaled linearly with $D$’s output:</p>
<script type="math/tex; mode=display">
r_t = 2 D_\phi (\hat{x}_t \vert x_1, \cdots, x_{t-1}) -1.</script><p>The goal of $G$ at timestep $t$ is to maximize the sum of discounted future rewards using a discount factor $\gamma$: </p>
<script type="math/tex; mode=display">R_t = \sum_{s=t}^T \gamma^{s-t} r_s.</script><h3 id="Large-Batch-Size-for-Variance-Reduction"><a href="#Large-Batch-Size-for-Variance-Reduction" class="headerlink" title="Large Batch Size for Variance Reduction"></a>Large Batch Size for Variance Reduction</h3><p>$G$ is updated using MC estimates of policy gradients, where $N$ is the batch size:</p>
<script type="math/tex; mode=display">
\nabla_\theta = \sum_{n=1}^N \sum_{t=1}^T (R_t^n - b_t) \nabla_\theta \log p_\theta (\hat{x}_t^n \vert \hat{x}_1^n , \cdots, \hat{x}_{t-1}^n ), \quad \hat{x}_t^n  \sim  p_\theta (\hat{x}_t^n \vert \hat{x}_1^n , \cdots ,\hat{x}_{t-1}^n )</script><p>ScratchGAN uses a global moving-average of rewards as a baseline <script type="math/tex">b_t</script>.</p>
<h3 id="Training-5"><a href="#Training-5" class="headerlink" title="Training"></a>Training</h3><ul>
<li>$D$ and $G$ both use an embedding layer followed by one or more LSTM layers.</li>
<li>Discriminator regularization: layer normalization, dropout, L<sub>2</sub> weight decay.</li>
<li>Concatenating the fixed sinusoidal position matrices and word embeddings in $D$. </li>
</ul>
<h1 id="JSDGAN-AISTATS’19"><a href="#JSDGAN-AISTATS’19" class="headerlink" title="JSDGAN (AISTATS’19)"></a>JSDGAN (AISTATS’19)</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>MLE is equivalent to minimizing the KL divergence between the empirical data distribution and the model distribution, which tends to favor approximations of model distribtuion that overgeneralize the data distribtuion. Instead the reverse KL divergence favors under-generalization. JSD combines KL and reverse KL, which is symmetric.</p>
<p>GAN is regarded as a two-play minimax game with distinguishability game value function $V(G,D)$:</p>
<script type="math/tex; mode=display">
\min_G \max_D V(G,D) = \mathbb{E}_{x \sim \tilde{p}_\textrm{data}(x)} \log D(x) + \mathbb{E}_{x \sim p_G (x)} \log (1-D(x)),</script><p>where <script type="math/tex">\tilde{p}_\textrm{data}(x)</script> denotes the empirical data distribution over training data <script type="math/tex">\mathcal{C}=\{ x_1, \cdots, x_N \}</script>, and</p>
<script type="math/tex; mode=display">
\tilde{p}_\textrm{data}(x) = \left\{
                \begin{array}{ll}
                  \frac{1}{N} & \textrm{if } x \in \mathcal{C} \\
                  0 & \textrm{otherwise}
                \end{array}
    \right.</script><h2 id="GAN-without-Explicit-D"><a href="#GAN-without-Explicit-D" class="headerlink" title="GAN without Explicit $D$"></a>GAN without Explicit $D$</h2><p><sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Li, Zhongliang, et al. "[Adversarial discrete sequence generation without explicit neuralnetworks as discriminators.](http://proceedings.mlr.press/v89/li19g/li19g.pdf)" The 22nd International Conference on Artificial Intelligence and Statistics (2019).
">[15]</span></a></sup> claimed that optimal $D$ has a closed form solution, and approximation on $D$ with neural networks is unnecessary.<br>It directly optimizes the JSD divergence between the distribution between $G$ and real data without sampling from $G$, which implies an alternative minimax optimizatin procedure.</p>
<p>The optimal discriminator <script type="math/tex">D^*_G (x)</script> is:</p>
<script type="math/tex; mode=display">
D^*_G (x) = \left\{
\begin{array}{ll}
\frac{\tilde{p}_\textrm{data}(x)}{\tilde{p}_\textrm{data}(x) + p_g(x)} & \textrm{if } x \in \mathcal{C} \\
0 & \textrm{otherwise}
\end{array}
\right.</script><p>The value function with optimal <script type="math/tex">D^*_G(x)</script> becomes:</p>
<script type="math/tex; mode=display">
\begin{align}
V(G, D^*_G (x)) &= 2 \textrm{JSD} (\tilde{p}_\textrm{data}(x) \Vert p_G (x)) - \log 4 \\
&= \sum_{x \in \mathcal{C}} \tilde{p}_\textrm{data} \log [ \frac{\tilde{p}_\textrm{data}(x)}{\tilde{p}_\textrm{data}(x) + p_g(x)} ] + \sum_{x \in \mathcal{C}} p_G (x) [\log \frac{p_g(x)}{\tilde{p}_\textrm{data}(x) + p_g(x)} ]
\end{align}</script><div class="note danger">
            <p>This approach is only applicable when <script type="math/tex">p_G(x)</script> has explicit representations.</p>
          </div>
<h1 id="CatGAN-AAAI’20"><a href="#CatGAN-AAAI’20" class="headerlink" title="CatGAN (AAAI’20)"></a>CatGAN (AAAI’20)</h1><p>Category-aware GAN (CatGAN)<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, Zhiyue, Jiahai Wang, and Zhiwei Liang. "[CatGAN: Category-Aware Generative Adversarial Networks with Hierarchical Evolutionary Learning for Category Text Generation.](https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LiuZ.5249.pdf)" AAAI. 2020.
">[18]</span></a></sup> employs such methods to generate sentences of different categories:</p>
<ul>
<li>Gumbel-softmax relaxation (as <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Nie, Weili, Nina Narodytska, and Ankit Patel. "[Relgan: Relational generative adversarial networks for text generation.](https://openreview.net/pdf?id=rJedV3R5tm)" International conference on learning representations (2019).
">[13]</span></a></sup>)</li>
<li>SAN-based relational memory (as <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Nie, Weili, Nina Narodytska, and Ankit Patel. "[Relgan: Relational generative adversarial networks for text generation.](https://openreview.net/pdf?id=rJedV3R5tm)" International conference on learning representations (2019).
">[13]</span></a></sup>)</li>
<li>Category-wise relativistic objective.</li>
<li>Hierarchical evolutionary learning.</li>
</ul>
<p><img data-src="/notes/images/CatGAN.png" width="70%"/></p>
<h1 id="SALGAN-ICLR’20"><a href="#SALGAN-ICLR’20" class="headerlink" title="SALGAN (ICLR’20)"></a>SALGAN (ICLR’20)</h1><h2 id="Problems-10"><a href="#Problems-10" class="headerlink" title="Problems"></a>Problems</h2><ul>
<li>Reward spasity</li>
<li>Mode collapse</li>
</ul>
<h2 id="Comparative-discriminaor"><a href="#Comparative-discriminaor" class="headerlink" title="Comparative discriminaor"></a>Comparative discriminaor</h2><p>SALGAN<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhou, Wangchunshu, et al. "[Self-Adversarial Learning with Comparative Discrimination for Text Generation.](https://arxiv.org/pdf/2001.11691)" arXiv preprint arXiv:2001.11691 (2020).
">[17]</span></a></sup> employs a comparative discriminaor to pairwisely compare the text quality between a pair of samples: better($&gt;$), worse ($&lt;$), or  indistinguishable ($\approx$). Given a training set with $n$ real samples and $n$ generated samples, the comparative discimination can construct $\binom{2n}{2}$ pairwise training examples.<br><img data-src="/notes/images/SALGAN.png" width="70%"/></p>
<h1 id="ColdGAN"><a href="#ColdGAN" class="headerlink" title="ColdGAN"></a>ColdGAN</h1><p>ColdGAN<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Scialom, Thomas, et al. "[ColdGANs: Taming Language GANs with Cautious Sampling Strategies.](https://arxiv.org/pdf/2006.04643)" arXiv preprint arXiv:2006.04643 (2020).">[19]</span></a></sup> adopts such methods on T5 (small) and BART:</p>
<ul>
<li>Importance sampling</li>
<li>PPO Clip</li>
<li>Nucleus sampling</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yu, Lantao, et al. &quot;<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489">Seqgan: Sequence generative adversarial nets with policy gradient.</a>&quot; Thirty-first AAAI conference on artificial intelligence. (2017).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, Yizhe, et al. &quot;<a href="https://arxiv.org/pdf/1706.03850">Adversarial feature matching for text generation.</a>&quot; arXiv preprint arXiv:1706.03850 (2017).<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Metz, Luke, et al. &quot;<a href="https://arxiv.org/pdf/1611.02163.pdf">Unrolled generative adversarial networks.</a>&quot; arXiv preprint arXiv:1611.02163 (2016).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Arjovsky, Martin, and Léon Bottou. &quot;<a href="https://arxiv.org/pdf/1701.04862.pdf">Towards principled methods for training generative adversarial networks.</a>&quot; arXiv preprint arXiv:1701.04862 (2017).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Goodfellow, Ian, et al. &quot;<a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative adversarial nets.</a>&quot; Advances in neural information processing systems (2014).<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Guo, Jiaxian, et al. &quot;<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16360/16061">Long text generation via adversarial training with leaked information.</a>&quot; Thirty-Second AAAI Conference on Artificial Intelligence (2018).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Che, Tong, et al. &quot;<a href="https://arxiv.org/pdf/1702.07983.pdf">Maximum-likelihood augmented discrete generative adversarial networks.</a>&quot; arXiv preprint arXiv:1702.07983 (2017).<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lin, Kevin, et al. &quot;<a href="http://papers.nips.cc/paper/6908-adversarial-ranking-for-language-generation.pdf">Adversarial ranking for language generation.</a>&quot; Advances in Neural Information Processing Systems (2017).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kusner, Matt J., and José Miguel Hernández-Lobato. &quot;<a href="https://arxiv.org/pdf/1611.04051.pdf">Gans for sequences of discrete elements with the gumbel-softmax distribution.</a>&quot; arXiv preprint arXiv:1611.04051 (2016).<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Fedus, William, Ian Goodfellow, and Andrew M. Dai. &quot;<a href="https://arxiv.org/pdf/1801.07736.pdf">MaskGAN: Better text generation via filling in the _.</a>&quot; ICLR (2018).<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, Liqun, et al. &quot;<a href="http://papers.nips.cc/paper/7717-adversarial-text-generation-via-feature-movers-distance.pdf">Adversarial text generation via feature-mover's distance.</a>&quot; Advances in Neural Information Processing Systems (2018).<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, Ke, and Xiaojun Wan. &quot;<a href="https://www.tensorinfinity.com/upload/files/20181227/1545889246130589.pdf">SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks.</a>&quot; IJCAI (2018).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Nie, Weili, Nina Narodytska, and Ankit Patel. &quot;<a href="https://openreview.net/pdf?id=rJedV3R5tm">Relgan: Relational generative adversarial networks for text generation.</a>&quot; International conference on learning representations (2019).<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">de Masson d'Autume, Cyprien, et al. &quot;<a href="http://papers.nips.cc/paper/8682-training-language-gans-from-scratch.pdf">Training language gans from scratch.</a>&quot; Advances in Neural Information Processing Systems (2019).<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Li, Zhongliang, et al. &quot;<a href="http://proceedings.mlr.press/v89/li19g/li19g.pdf">Adversarial discrete sequence generation without explicit neuralnetworks as discriminators.</a>&quot; The 22nd International Conference on Artificial Intelligence and Statistics (2019).<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Gulrajani, Ishaan, et al. &quot;<a href="https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf">Improved training of wasserstein gans.</a>&quot; Advances in Sneural information processing systems (2017).<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhou, Wangchunshu, et al. &quot;<a href="https://arxiv.org/pdf/2001.11691">Self-Adversarial Learning with Comparative Discrimination for Text Generation.</a>&quot; arXiv preprint arXiv:2001.11691 (2020).<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Liu, Zhiyue, Jiahai Wang, and Zhiwei Liang. &quot;<a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LiuZ.5249.pdf">CatGAN: Category-Aware Generative Adversarial Networks with Hierarchical Evolutionary Learning for Category Text Generation.</a>&quot; AAAI. 2020.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Scialom, Thomas, et al. &quot;<a href="https://arxiv.org/pdf/2006.04643">ColdGANs: Taming Language GANs with Cautious Sampling Strategies.</a>&quot; arXiv preprint arXiv:2006.04643 (2020).<a href="#fnref:19" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>NLG</category>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>NLG</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Embedding Pretrained Linguistic Prior in a Nutshell</title>
    <url>/notes/2019/01/06/NLP/Embedding-pretrained-linguistic-prior-in-a-nutshell/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Pretraining on <em>ImageNet</em> followed by domain-specific fine-tuning has illustrated compelling improvements in computer vision research. Similarly, Natual Language Processing (NLP) tasks could borrow ideas from this. </p>
<p>Employing pretrained word representations or even langugage models to <strong>introduce linguistic prior knowledge</strong> has been common sense in amounts of NLP tasks with deep learning.</p>
<p><img data-src="/notes/images/embedding.png" alt="Pretrained word representation topology"></p>
<span id="more"></span>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Language-Model-LM"><a href="#Language-Model-LM" class="headerlink" title="Language Model (LM)"></a>Language Model (LM)</h2><div class="note primary">
            <p>Probability of a sequence of words.<br>   Goal: learn the joint probability function of sequences of words in a language      <script type="math/tex">\prod_{i=1}^N P(w_i | w_{1},..., w_{i-1})</script></p>
          </div>
<div class="note danger">
            <p>Challenge: <strong>the curse of dimensionality</strong></p>
          </div>
<h3 id="Discrete-n-gram"><a href="#Discrete-n-gram" class="headerlink" title="Discrete n-gram"></a>Discrete n-gram</h3><p>For a sentence S of length n: <script type="math/tex">w_1,...,w_m</script>:</p>
<script type="math/tex; mode=display">P(S) = P(w_1,...,w_n)  =  \prod_{i=1}^m P(w_i|w_0,...,w_{i-1}) 
\\ \approx \prod_{i=1}^n \underbrace{P(w_i|w_{i-1},...,w_{i-n+1})}_\textrm{Markov assumption}</script><div class="note success">
            <p><strong>Strong Markov assumption</strong>: for each word i, the probability <em>only</em> depends on previous n-1 words:</p><ul><li>zerogram: uniform distribution</li><li>unigram:  word frenquency</li><li>bigram: <script type="math/tex">x_i</script> depends only on <script type="math/tex">x_{i-1}</script></li><li>trigram: <script type="math/tex">x_i</script> depends only on <script type="math/tex">x_{i-2}, x_{i-1}</script></li></ul>
          </div>
<p>n-gram models:</p>
<script type="math/tex; mode=display">P(w_i|w_1,w_2,...,w_{i-1}) \approx P(w_i | w_{i-(n-1)},...,w_{i-1})</script><p>MLE by counting:</p>
<script type="math/tex; mode=display">P(w_i | w_{i-(n-1)},...,w_{i-1}) = \frac{ \text{count}(w_{i-(n-1)},...,w_{i-1}) }{ \text{count}(w_{i-(n-1)},...,w_{i}) }</script><div class="note danger">
            <p><strong>Problems</strong>: cound-based methods cannot deal with <strong>out-of-vocabulary</strong> (OOV, i.e. unseen words <UNK>)</p>
          </div>
<div class="note warning">
            <p><strong>Solution</strong>: smoothing (discounting)<br>Core idea: reserve part of probability mass for unseen events.</p>
          </div>
<p>Methods:</p>
<ul>
<li>Add-1 smoothing (Laplace smoothing) </li>
<li>Add-$\alpha$ smoothing</li>
<li>Stupid backoff</li>
<li>Interpolation</li>
<li>Kneser-Ney smoothing</li>
</ul>
<h3 id="Continuous-n-gram"><a href="#Continuous-n-gram" class="headerlink" title="Continuous n-gram"></a>Continuous n-gram</h3><div class="note primary">
            <p>Rather than discounting, rely on <code>similarity in internal representation</code> for estimating <code>unseen</code> events.</p>
          </div>
<div class="note info">
            <p><strong>Relationship between LM and word representation</strong> (personal thoughts): neural network(NN) is another way to do <code>matrix factorization but with non-linear transformation</code>. LMs aims to learn the <strong>joint distribution function of word sequences</strong>, which accumulates conditional probability word by word. Before passing the softmax for normalization, the <em>compact</em> projection layer hidden states could provide effective insights to tell the difference among vocabularies (after softmax normalization). As the subsidiary product of LMs, low dimensional projection states could mitigate the curse of dimensionality and serve for NLP transfer learning.</p>
          </div>
<p>The earliest idea using NN for LM is not (Bengio et al., 2003). Previous work e.g. (Miikkulainen and Dyer, 1991).</p>
<h4 id="NNLM-Bengio-et-al-2003"><a href="#NNLM-Bengio-et-al-2003" class="headerlink" title="NNLM (Bengio et al., 2003)"></a>NNLM (Bengio et al., 2003)</h4><p>Employed NN in <strong>statistical n-gram LM</strong>.</p>
<p>Learns simultaneously </p>
<ol>
<li>Distributed representation (see following section for details) for each word;</li>
<li>Joint probability function <script type="math/tex">f(w_t,w_{t-1},...,w_{t-n+1})</script> for word sequences.</li>
</ol>
<div class="note primary">
            <ul><li>Input: n-1 context words</li><li>Output: probability distribution of the next word</li><li>Model: <code>a linear projection layer + a non-linear hidden layer</code>. 1 non-linear hidden layer beyond the word feature mapping (i.e. embedding lookup). Optionally, <em>direct feed lookup word vectors to final layer</em> (<code>Implicitly ResNets!</code>). When the hidden states W is set to 0, there is no direct connection.</li><li>Parameter set: <script type="math/tex">\Theta = (C, \omega)</script>, where C is word vector mapping, $\omega$ denotes parameters.</li><li>Loss function:<br><script type="math/tex">L=\frac{1}{T} \sum_t log f(w_t,w_{t-1},...,w_{t-n+1};\Theta) + R(\Theta)</script>, where $R(\Theta)$ is a regularization term.</li></ul>
          </div>
<p>As below figure, NNLMs decompose the n-gram joint probabilty function <script type="math/tex">f(w_t,...,w_{t-n+1}) = \hat{P}(w_t|w_1^{t-1})</script>:</p>
<ol>
<li>Mapping matrix C with dimension $ |V| \times m $, represents the distributed feature vector associated with each word in the vocabulary (<strong>embedding matrix</strong>, a.k.a embedding loopup table). </li>
<li>probability function over words:<script type="math/tex; mode=display">f(w_t,w_{t-1},...,w_{t-n+1}) = g(i, C(w_{t-1}),...,C(w_{t-n+1}))</script> where C(i) is the i-th word feature vector.</li>
</ol>
<p><img data-src="/notes/images/NNLM.png" width='60%'></p>
<div class="note danger">
            <p>Solved issue: <code>OOV</code></p><p>Drawbacks: </p><ol><li><code>Limited context length (fixed n)</code> that needs to be specified <em>ad hoc</em> before training, only consider previous n-1 words;</li><li>Simple NN architecture;</li><li>Word feature representation (embedding) cannot deal with <code>polysemy</code>, which assign each word a single point in a continuous semantic space. Proposed future solution: assign each word sense with different points. </li></ol>
          </div>
<h4 id="RNNLM-Mikolov-et-al-2010"><a href="#RNNLM-Mikolov-et-al-2010" class="headerlink" title="RNNLM (Mikolov et al., 2010)"></a>RNNLM (Mikolov et al., 2010)</h4><div class="note success">
            <p>NNLMs utilize <code>fixed context length</code> which needs to be pre-specified. RNNLMs encode temporal information implicitly for <code>contexts with arbitrary lengths</code>.</p>
          </div>
<ul>
<li>Motivation: condition on arbitrarily long context $\rightarrow$ no Markov assumption</li>
<li>Input: 1-of-K encoding over the vocabulary with size |V|</li>
<li>Read in one word at a time, and update hidden state incrementally;</li>
<li>Hidden state is initialized as empty vectors at time step 0;</li>
<li>Parameters<ul>
<li>Embedding matrix $E$</li>
<li>Feedforward matrices <script type="math/tex">W_1</script>, <script type="math/tex">W_2</script></li>
<li>Recurrent maxtrix $U$</li>
</ul>
</li>
<li>Training: BPTT</li>
</ul>
<p>Simple RNNs:</p>
<script type="math/tex; mode=display">h_i=\left\{
                \begin{array}{ll}
                  0, \quad if \ i = 0\\
                  \tanh( W_1Ex_i + Uh_{i-1} ), if\  i > 0\quad
                \end{array}
              \right.</script><script type="math/tex; mode=display">y_i = \text{softmax} (W_2 h_{i-1})</script><p>Basic LSTM RNNs:</p>
<script type="math/tex; mode=display">\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  = \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c)</script><script type="math/tex; mode=display">\mathbf{c}^c_j = \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j}</script><script type="math/tex; mode=display">\mathbf{h}_j^c = \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)</script><p>where $\mathbf{i}^c_j$, $\mathbf{f}^c_j$, $\mathbf{o}^c_j$ denotes a set of input, forget and output gates, respectively. $\mathbf{c}^c_j$ denotes the char cell vector, $\mathbf{h}^c_j$ denotes the hidden vector on each char $c_j$, $\mathbf{W}^{c^T}$, $\mathbf{b}^c$ are parameters.</p>
<hr>
<h3 id="Intrinsic-evaluation-of-LM"><a href="#Intrinsic-evaluation-of-LM" class="headerlink" title="Intrinsic evaluation of LM"></a>Intrinsic evaluation of LM</h3><p><strong>Perplexity (PP)</strong>:<br><div class="note warning">
            <p>Intuitional interpretation: <strong>weighted average branching factor</strong></p><script type="math/tex; mode=display">\begin{aligned}PP (S) &{}= P(w_1,...,w_N) = P(w_1,w_2,...,w_N)^{-\frac{1}{N}} \\        &{}=  \sqrt[N]{\frac{1}{P(w_1w_2...w_N)}} \\         &{}= \sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_1...w_{i-1})}}\end{aligned}</script>
          </div></p>
<p>For bigram models:</p>
<script type="math/tex; mode=display">PP (S) = \sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_{i-1})}}</script><hr>
<h2 id="LM-application"><a href="#LM-application" class="headerlink" title="LM application"></a>LM application</h2><div class="note warning">
            <p>LMs serve as a component in various NLP tasks, such as ASR, MT. </p>
          </div>
<h3 id="Automatic-Speech-Recognition"><a href="#Automatic-Speech-Recognition" class="headerlink" title="Automatic Speech Recognition"></a>Automatic Speech Recognition</h3><p><img data-src='/notes/images/asr-intro.png' width="60%" /></p>
<center><small>Image source: Steve Renals, Edinburgh Informatics (INFR11033) </small></center>

<h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>Let T denote a target sentence of length m :<script type="math/tex">x_1, ..., x_m</script>, S denote a source sentence n: <script type="math/tex">y_1,...,y_n</script>. The machine translation can be expressed as:</p>
<script type="math/tex; mode=display">T^* = \arg\max_T P(T|S) \\=  \arg\max_T P(y_1,...,y_n|x_1,...,x_m) \\= \prod_{i=1}^n P(y_i | y_{1}, ...,y_{i-1}, x_1,...,x_m)</script><p>With Bayes’ theorem, we can get:</p>
<script type="math/tex; mode=display">T^* = \arg\max_T P(S|T) \underbrace{P(T)}_{\text{LM}}</script><h2 id="Word-representation"><a href="#Word-representation" class="headerlink" title="Word representation"></a>Word representation</h2><h3 id="Distributional-representation"><a href="#Distributional-representation" class="headerlink" title="Distributional representation"></a>Distributional representation</h3><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>distributional hypothesis</strong>: linguistic items with similar distributions have similar meanings  </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<div class="note warning">
            <ul><li>Statistical (count-based) method;</li><li><code>high-dimensional</code> vector representation obtained from the rows of the <strong>word-context co-occurrence matrix</strong>, whose dimension size equals to the <strong>vocabulary size</strong> of the corpus.</li></ul>
          </div>
<p>Approaches:</p>
<ul>
<li>One-hot encoding (a.k.a 1-of-K encoding)</li>
<li>TF-IDF (Term Frequency - Inverse Document Frequency)<br>N $\rightarrow$ # of ducuments, $df_t$ $\rightarrow$ Term frequency for term $t$, $d$ $\rightarrow$ document.<script type="math/tex; mode=display">\text{tf}_{t,d} = 1 + \log_{10} \text{count} (t,d) \quad \text{if count} (t,d)>0, \text{ else } 0</script><script type="math/tex; mode=display">\text{idf}_{t} = \log (\frac{N}{\text{df}_t})</script><script type="math/tex; mode=display">w_{t,d} = \text{tf}_{t,d} \times \text{idf}_{t}</script></li>
<li>PPMI<br>PMI association between a target word w and a context word c is:<script type="math/tex; mode=display">\text{PMI}(w,c) = \log_2 \frac{P(w,c)}{P(w)P(c)}</script></li>
</ul>
<script type="math/tex; mode=display">\text{PPMI} = \max(PMI(w,c), 0)</script><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>PMI</strong>: The numerator tells us how often we observed the two words together (assuming we compute probability by using the MLE). The denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently, so their probabilities could just be multiplied. Thus, the ratio gives us an estimate of how much more the target and feature co-occur than we expect by chance.</p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<!-- 
**Latent Dirichlet allocation (LDA)**
**Hyperspace Analogue to Language (HAL)**
1. Syntagmatic models
"combinatorial relations between words (i.e., syntagmatic relations), which relate words that co-occur within the
same text region (e.g., sentence, paragraph or document)." ()
a. Build words-by-documents co-occurrence matrix
b. low-rank decomposition
2. Paradigmatic models concern substitutional
<blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p> relations between words (i.e., paradigmatic relations), which relate words that occur in the same<br>context but may not at the same time </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>  -->
<h3 id="Distributed-static-word-representation"><a href="#Distributed-static-word-representation" class="headerlink" title="Distributed (static) word representation"></a>Distributed (static) word representation</h3><p><strong>Distributed representations of words in a vector space</strong> have become an effective way to capturing <strong>fine-grained linguistic regularities</strong>.</p>
<div class="note success">
            <p><strong>low-dimensional</strong>, dense, compact vector representation.</p><ul><li><strong>NN-based</strong> model (such as <strong>word2vec</strong>, Collobert and Weston embeddings, HLBL embeddings) ,</li><li><strong>Matrix factorization based</strong> model on the <strong>word-context co-occurrence matrix</strong> (such as the <em>Glove</em> from Stanford using direct matrix factorization, the <strong>Latent Semantic Analysis</strong> using SVD factorization).</li></ul>
          </div>
<div class="note default">
            <p>You shall know a word by the company it keeps (John Rupert Firth, 1957).</p>
          </div>
<blockquote>
<p>Learning word representations using Language modeling. (Dr. Adam Lopez’s 2018 lecture)</p>
</blockquote>
<div class="note danger">
            <p>Inherent limitation of word representations: </p><ul><li><em>indifference to word order and inability to represent idiomatic phrases</em>.</li><li>cannot tackle <em>polysemy</em></li></ul>
          </div>
<h4 id="Word2Vec-Mikolov-2013-Google"><a href="#Word2Vec-Mikolov-2013-Google" class="headerlink" title="Word2Vec (Mikolov 2013; Google)"></a>Word2Vec (Mikolov 2013; Google)</h4><div class="note danger">
            <p><strong>Issue</strong><br>NLP systems <em>treat words as atomic units</em>: map words to the <strong>indices in the vocabulary</strong>, not considering similarity between words. </p><ul><li>Pros: simplicity, rebustness, simple models</li><li>Cons: require <em>huge amount of data</em>, which is unrealistic in some occasions, e.g. ASR and NMT.</li></ul>
          </div>
<h5 id="Continuous-Bag-of-Words-CBOW"><a href="#Continuous-Bag-of-Words-CBOW" class="headerlink" title="Continuous Bag of Words (CBOW)"></a><strong>Continuous Bag of Words (CBOW)</strong></h5><div class="note default">
            <ul><li>Intuition: predicting the current word based on its context.</li><li>Archtecture: <strong>linear projection layer</strong>. Feedforward NNLM remove the non-linear hidden layer. </li><li>All words get projected into the same position (vectors are averaged). Called bag-of-word model since the <em>word order in the history doesnot influence the projection</em>.</li><li>Same as NNLMs, weights for different positions are shared. </li><li>Computationally much more efficient than NNLMs.</li></ul>
          </div>
<p><img data-src="/notes/images/cbow.png" width='60%'></p>
<center>CBOW <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Weng L. (2017, Oct 15). Learning Word Embedding [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html
">[9]</span></a></sup></center>

<h5 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a><strong>Skip-gram</strong></h5><div class="note primary">
            <ul><li>Intuition: “maximize classification of a word based on another word in the same sentence”, i.e. input each current word to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.</li><li><code>Objective</code>: maximize the <strong>average log probability</strong>, with context size c:<script type="math/tex; mode=display">\frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j}|w_t)</script></li><li><em>Simple vector addition</em> can often produce meaningful results. e.g. vec(“Russia”) + vec(“river”) is close to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). </li><li><strong>Find word representations useful for predicting surrounding words</strong> in a sentence or documentation</li></ul>
          </div>
<p><img data-src="/notes/images/skip-gram.png" width='60%'></p>
<center>SkipGram <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Weng L. (2017, Oct 15). Learning Word Embedding [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html
">[9]</span></a></sup></center>

<ul>
<li><strong>Issue 1</strong>: Inability to represent idiomatic phrases that are not compositions of the individual words. </li>
<li><p><strong>Solution</strong>: Find out the phrases and <em>treat the phrases as individual tokens</em> during training. Typical analogy pair: vec(“Montreal Canadiens”) - vec(“Montreal”) + vec(“Toronto”) is vec(“Toronto Maple Leafs”).</p>
</li>
<li><p><strong>Issue 2</strong>: very large dimension in softmax layer (size equals to vocabulary size |V|)</p>
</li>
<li><strong>Solution</strong>:<br>Hierarchical softmax (<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf">Morin and Bengio, 2005</a>),<br>Negative sampling (NEG) (<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al. 2013</a>)</li>
</ul>
<div class="note danger">
            <p>Basic softmax (impractical due to computing <script type="math/tex">\nabla log p(w_O|w_I)</script> cost $\propto$ W (~ $10^5 - 10^7$ terms) ): </p><script type="math/tex; mode=display">p(w_O|w_I) = \frac{\exp( {v'}_{WO}^T v_{WI} )}{\sum_{w=1}^W \exp({v'}_{W}^T v_{WI})}</script><p>where $v_w$ and ${v’}_w$ are input and output vector representations of w, W is the vocabulary size. </p>
          </div>
<div class="note success">
            <ul><li><p>Solution 1 $\rightarrow$ <code>Hierarchical softmax</code><br>Instead of computing vocabulary size output nodes in NNs, only need to evaluate ~ <script type="math/tex">log_2(W)</script> nodes.<br>(<strong>Binary Huffman tree</strong> structure)</p></li><li><p>Solution 2 $\rightarrow$ <code>Negative sampling</code>: Noise Contrastive Estimation (NCE)<br>Differentiate data from noise by means of <code>logistic regression</code> classifier (<a href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Gutmann and Hyvärinen, 2010</a>).</p></li></ul>
          </div>
<p> Distributed representations capture <code>syntactic</code> and <code>semantic</code> information.</p>
<blockquote>
<p>Additional word2vec limitations:<br>    <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">closed vocabulary assumption</span><br><span class="line">cannot exploit functional relationships in learning </span><br></pre></td></tr></table></figure></p>
</blockquote>
<!--
#### Collobert and Weston embeddings
#### HLBL embeddings -->
<h4 id="FastText-Mikolov-et-al-2017-Facebook"><a href="#FastText-Mikolov-et-al-2017-Facebook" class="headerlink" title="FastText (Mikolov et al. 2017; Facebook)"></a>FastText (Mikolov et al. 2017; Facebook)</h4><div class="note danger">
            <p>Issue: Popular models ignores the mophology of words, by assigning distinct vector to each word. Previous popular models <code>ignore the internal structure of words</code>, which is an important limitation for <strong>morphologically rich languages</strong>, such as Turkish, Finnish.</p>
          </div>
<div class="note success">
            <p>Bag-of-words $\rightarrow$ <code>Bag of features</code></p><p>Fasttext solution: employ <code>the sum of bag of character n-grams</code> as well as itself for each word, as an extension of skip-gram models. Taking into account <code>subword information</code>.</p>
          </div>
<p>Let <code>&lt;</code> and <code>&gt;</code> denote the beginning and ending of tokens to <em>distinguish prefixes and suffixes</em> from other character sequences. Taking <code>&lt;where&gt;</code> for example, we use char trigram(n=3), we can get:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">char trigram: &lt;wh, whe, her, ere, re&gt;</span><br><span class="line">itself: &lt;where&gt;</span><br></pre></td></tr></table></figure>
<p>Fasttext represents a word by the <strong>sum of the vector representations of its n-grams</strong>.<br>Let vector representation $\mathbf{z}_g$ to each n-gram $g$ ($g \in G$) The scoring function:</p>
<script type="math/tex; mode=display">s(w,c) = \sum_{g \in \mathbf{G}} \mathbf{z}_g^T \mathbf{v}_c</script><!--#### LSA (Latent Semantic Analysis)
- TODO
#### LDA (Latent Dirichlet Allocation)
- TODO
Cons: computationally very expensive on large data sets
-->
<h4 id="GloVe-Pennington-2014-Stanford"><a href="#GloVe-Pennington-2014-Stanford" class="headerlink" title="GloVe (Pennington 2014; Stanford)"></a>GloVe (Pennington 2014; Stanford)</h4><p>GloVe<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Pennington, J., Socher, R. & Manning, C. D. (2014). [Glove: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162). EMNLP 2014.
">[8]</span></a></sup> leverages <code>statistical information</code> by training only on the nonzero elements in a <code>word-word co-occurrence matrix</code>.</p>
<div class="note info">
            <p>Define the co-occurence probability as:</p><script type="math/tex; mode=display">p_{\text{co}}(w_k \vert w_i) = \frac{C(w_i, w_k)}{C(w_i)}</script><p><strong>Intuition</strong>: the word meanings are captured by the <code>ratios of co-occurrence probabilities</code> rather than the probabilities themselves. The global vector models the relationship between words i,j towards the thrid context word k:</p><script type="math/tex; mode=display">F(w_i, w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script><p>For words <em>k</em> like water or fashion, that are either related to both <em>ice</em> and <em>steam</em>, or to neither, the <strong>ratio should be close to one</strong>. </p><ul><li>build <code>word-word co-occurrence matrix</code></li><li>do <code>global matrix factorization</code></li></ul>
          </div>
<p><img data-src="/notes/images/GloVe.png" width='60%'></p>
<hr>
<h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><ul>
<li><p><strong>polysemy</strong><br>Frozen representations, can not express <strong>polysemy</strong>.</p>
</li>
<li><p>For languages where tokens are not delimited, such as Chinese and Japanese, NLP pipelines require <code>word segmentation</code> ahead. As we know, error generated by upstream tasks would amplified during the following propagation process. Hence, the performance of Chinese word segmentation also counts.</p>
</li>
</ul>
<h1 id="Pretraining-dynamic-word-representation"><a href="#Pretraining-dynamic-word-representation" class="headerlink" title="Pretraining (dynamic word representation)"></a>Pretraining (dynamic word representation)</h1><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>NLP’s ImageNet moment has arrived</strong><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ruder S. (2018, Jul 08) NLP's ImageNet moment has arrived [Blog post]. Retrieved from https://thegradient.pub/nlp-imagenet/
">[10]</span></a></sup><br>“At the core of the recent advances of ULMFiT, ELMo, and the OpenAI transformer is one key paradigm shift: going from just <strong>initializing the first layer</strong> of our models to <strong>pretraining the entire model with hierarchical representations</strong>. If learning word vectors is like only learning edges, these approaches are like learning the full hierarchy of features, from edges to shapes to high-level semantic concepts.”</p>
<p><strong>CV</strong>: “Interestingly, pretraining <em>entire models</em> to learn both low and high level features has been practiced for years by the computer vision (CV) community.” </p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<h2 id="Feature-based-pretraining-frozen-representation"><a href="#Feature-based-pretraining-frozen-representation" class="headerlink" title="Feature-based pretraining (frozen representation)"></a>Feature-based pretraining (frozen representation)</h2><h3 id="ULMFiT"><a href="#ULMFiT" class="headerlink" title="ULMFiT"></a>ULMFiT</h3><p><strong>ULMFiT</strong>: Universal Language Model Fine-tuning</p>
<p><strong>Problem</strong>: LMs overfit to small datasets and suffered <code>catastrophic forgetting</code> when fine-tuned with a classifier.</p>
<p><strong>Solution</strong></p>
<ul>
<li>Inductive transfer learning.</li>
<li>Model: AWD-LSTM <sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182). arXiv preprint arXiv:1708.02182 .
">[19]</span></a></sup>, a regular LSTM (without attention, shot-cut connections)</li>
</ul>
<p><strong>Three stages</strong>:</p>
<ul>
<li>General domain LM pretraining<br>To capture the general features of the language in different layers;</li>
<li>Target task LM fine-tuning<ul>
<li>Trick: <em>discriminative fine-tuning</em>, <em>slanted triangular learning rates</em></li>
</ul>
</li>
<li>Target task classifier fine-tuning <ul>
<li>Trick: <em>concat pooling</em>, <em>gradual unfreezing</em></li>
</ul>
</li>
</ul>
<p><strong>Sloved issue</strong></p>
<blockquote>
<p>prevent catastrophic forgetting and enable robust transfer learning.</p>
</blockquote>
<p><img data-src="/notes/images/ULMFit.png" width='100%'></p>
<h3 id="ELMo-NAACL-2018-AllenAI"><a href="#ELMo-NAACL-2018-AllenAI" class="headerlink" title="ELMo (NAACL 2018, AllenAI)"></a>ELMo (NAACL 2018, AllenAI)</h3><h4 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h4><p>Some word representations are <code>context-independent</code>, only model complex charateristics of word use (e.g. syntax and semantics), ignoring how these uses vary across linguistic context (i.e. polysemy).</p>
<p><strong>Previous improvement</strong>:</p>
<ul>
<li>Enriching with <strong>subword information</strong> <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wieting, J., Bansal, M., Gimpel, K., & Livescu, K. (2016). [Charagram: Embedding words and sentences via character n-grams](https://arxiv.org/pdf/1607.02789). arXiv preprint arXiv:1607.02789.
">[15]</span></a></sup> <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T. (2017). [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010). Transactions of the Association for Computational Linguistics, 5, 135--146. 
">[7]</span></a></sup></li>
<li>Learning <strong>separate vectors for each word sense</strong> <sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Neelakantan, A., Shankar, J., Passos, A., & McCallum, A. (2015). [Efficient non-parametric estimation of multiple embeddings per word in vector space](https://arxiv.org/pdf/1504.06654). arXiv preprint arXiv:1504.06654.
">[16]</span></a></sup> (as suggested in the conclusion section in the NNLM paper<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). [A neural probabilistic language model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). Journal of machine learning research, 3(Feb), 1137-1155.
">[1]</span></a></sup>)</li>
</ul>
<p><strong>Solved issue</strong>:</p>
<ul>
<li>Seamlessly incorporate <code>multi-sense information</code> into downstream tasks <code>without explicitly training to predefined sense classes</code>.</li>
</ul>
<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><ul>
<li><strong>ELMo</strong> (Embeddings from Language Models) models <strong>polysemy</strong> by extracting <code>context-sensitive features</code>.</li>
<li>Elmo representations are deep $\rightarrow$ <strong>a function of all of the internal layers of the biLM</strong>. </li>
<li>Also incorporate subword information, using char ConvNets in the input and output.</li>
<li>Learn a linear combination of the vectors above each input word. This manner allows for very rich word representations.</li>
<li>Computing on top of two-layer biLMs with <strong>char ConvNets</strong>, as a linear function of internal network states.</li>
<li>After pretrainining the biLM with unlabeled data, ELMo <strong>fixes the weights</strong> and add additional task-specific model.</li>
</ul>
<p><strong>High-level</strong> LSTM states capture <code>context-dependent aspects of word meaning</code> (perform well on WSD tasks), while <strong>lower-level</strong> states model aspects of <code>syntax</code> (POS tagging). Simultaneously exposing <em>all of internal states</em> is highly beneficial, <em>allowing the learned models select the types of semi-supervision that are most useful for each end task</em>.<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365). arXiv preprint arXiv:1802.05365.
">[11]</span></a></sup></p>
<p><strong>BiLM</strong>: Given a sequence of N tokens <script type="math/tex">(t_1, t_2,..., t_N)</script></p>
<ul>
<li>Forward LM models the probability of tokens <script type="math/tex">t_k</script> given the history <script type="math/tex">(t_1,...,t_{k-1})</script>:<script type="math/tex; mode=display">p(t_1, t_2,...,t_N) = \prod_{k=1}^N p(t_k \vert t_1,t_2,...,t_{k-1})</script></li>
<li>Backward LM predicts the previous token given the future context:<script type="math/tex; mode=display">p(t_1,t_2,...,t_N) = \prod_{k=1}^N p(t_k \vert t_{k+1},t_{k+2},...,t_N)</script></li>
<li>biLM combines both of above, by jointly maxmizing the log likelihood of the forward and backward directions:<script type="math/tex; mode=display">\sum_{k=1}^N ( \log p(t_k|t_1,...,t_{k-1}; \Theta_x,  \overrightarrow{\Theta}_{LSTM}, \Theta_s ) + \log p(t_k|t_{k+1},...,t_N; \Theta_x,  \overleftarrow{\Theta}_{LSTM}, \Theta_s )  )</script>where <script type="math/tex">\Theta_x \rightarrow</script> token representation, $\Theta_s \rightarrow$ softmax layer.</li>
</ul>
<h4 id="ELMo-representation"><a href="#ELMo-representation" class="headerlink" title="ELMo representation"></a>ELMo representation</h4><div class="note success">
            <p>ELMo is a <strong>task-specific combination of the intermediate layer representations</strong> in the biLM.</p><p>For each token $t_k$, a L-layer biLM computes a set of 2L+1 representations:</p><script type="math/tex; mode=display">R_k = \{ \mathbf{x}_K^{LM},  \overrightarrow{h}_{k,j}^{LM},  \overleftarrow{h}_{k,j}^{LM} \vert j = 1,...,L \}= \{  \overrightarrow{h}_{k,j}^{LM} \vert j=0,...,L \}</script><p>where <script type="math/tex">\overrightarrow{h}_{k,0}^{LM}</script> is the token layer (j=0) and <script type="math/tex">\overrightarrow{h}_{k,j}^{LM} = [ \overrightarrow{h}_{k,j}^{LM} ; \overleftarrow{h}_{k,j}^{LM} ]</script> for each bi-LSTM layer.</p><p>ELMo collapses all alyers in representation set $\mathbf{R}$ into a single  vector <script type="math/tex">\mathbf{ELMo}_k = E(R_k; \Theta_e)</script>. </p><p><img data-src="/notes/images/ELMo.png" width='80%'></p><blockquote><p>Here previous work like <strong>TagLM</strong><sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Peters, M. E., Ammar, W., Bhagavatula, C., & Power, R. (2017). [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/pdf/1705.00108). arXiv preprint arXiv:1705.00108.">[17]</span></a></sup>, <strong>CoVe</strong><sup id="fnref:18"><a href="#fn:18" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Bryan McCann, James Bradbury, Caiming Xiong, andRichard Socher. 2017. [Learned in translation: Contextualized word vectors](https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf). In NIPS 2017.">[18]</span></a></sup> just selects the top layer.</p></blockquote><p>ELMo computes a task-specific weighting of all BiLM layer representations:</p><script type="math/tex; mode=display">ELMo_{o_k}^{task} = E(R_k ; \Theta^{task}) = \gamma^{task} \sum_{j=0}^L s_j^{task} \mathbf{h}_{k,j}^{LM}</script><p>where s_{task} are <strong>softmax-normalized weight</strong>, and scalar param $y^{task}$ allows the task model to scale the entire ELMo vector.</p>
          </div>
<p><img data-src="/notes/images/ELMo-embedding.png" width='80%'></p>
<center>ELMo embedding<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Alammar J. (2018, Dec 3). The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [Blog post]. Retrieved from https://jalammar.github.io/illustrated-bert/
">[23]</span></a></sup></center>

<h4 id="How-to-utilize-ELMo-into-downstream-supervised-tasks"><a href="#How-to-utilize-ELMo-into-downstream-supervised-tasks" class="headerlink" title="How to utilize ELMo into downstream supervised tasks?"></a>How to utilize ELMo into downstream supervised tasks?</h4><p><strong>Given</strong>: </p>
<ul>
<li>pretrained biLM with residual conection between LSTM layers</li>
<li>a supervised task-specific model</li>
</ul>
<p>Let $\mathbf{x}_k$ denote context-independent token representation (traditional embedding, like w2v, or compositional char cnn embeddings)<br><div class="note warning">
            <p><strong>How to use EMLo?</strong></p><ul><li>Freeze the biLM weights, run the biLM, and record all the layer representations for each word; </li><li>End task model learn previous linear weights $s^{task}$<ul><li>Usage: concat ELMo vector $\mathbf{ELMo}_k^{task}$ with $\mathbf{x}_k$, and  feed concatenated $ [\mathbf{x}_k, \mathbf{ELMo}_k^{task}] $ into the task-specific model.</li><li>Partially empirically practical on SNLI, SQuAD, replace the output $\mathbf{h}_k$ with $ [\mathbf{h}_k, \mathbf{ELMo}_k^{task}]$, followed by  one more domain-specific linear layer.</li></ul></li></ul>
          </div></p>
<h2 id="Fine-tuning-pretraining"><a href="#Fine-tuning-pretraining" class="headerlink" title="Fine-tuning pretraining"></a>Fine-tuning pretraining</h2><h3 id="OpenAI-Transformer-GPT-generative-pre-training"><a href="#OpenAI-Transformer-GPT-generative-pre-training" class="headerlink" title="OpenAI Transformer GPT (generative pre-training)"></a>OpenAI Transformer GPT (generative pre-training)</h3><p><strong>Problems</strong>: NLP with NN suffers from a dearth of annotated resources.</p>
<p><strong>model</strong>: </p>
<ul>
<li>Left-To-Right google Transformer<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). In Advances in Neural Information Processing Systems (pp. 5998-6008).
">[20]</span></a></sup> </li>
<li>Use BPE vocabulary<sup id="fnref:21"><a href="#fn:21" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="R. Sennrich, B. Haddow, and A. Birch. [Neural machine translation of rare words with subword units](https://arxiv.org/pdf/1508.07909). arXiv preprint arXiv:1508.07909, 2015.
">[21]</span></a></sup></li>
<li>Activation function: Gaussian Error Linear Unit (GELU) <sup id="fnref:22"><a href="#fn:22" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="D. Hendrycks and K. Gimpel. [Bridging nonlinearities and stochastic regularizers with gaussian error linear units](https://arxiv.org/pdf/1606.08415). arXiv preprint arXiv:1606.08415, 2016.
">[22]</span></a></sup></li>
</ul>
<p>Two stage:<br><strong>1) Unsupervised training on unannotated data with LM objective</strong></p>
<ul>
<li>Aim: <strong>find a good initialization point</strong>.</li>
<li>LM objective:<script type="math/tex; mode=display">L_1 (U) = \sum_i \log P(u_i \vert u_{i-k},...,u_{i-1};\Theta)</script>where k is the context window size</li>
<li>LM model: a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers:<script type="math/tex; mode=display">h_0 = UW_e + W_p</script><script type="math/tex; mode=display">h_l = \text{transformer block}(h_{l-1}) \quad  \forall \quad i \in [1,n])</script><script type="math/tex; mode=display">P(u) = \text{softmax} (h_nW_e^T)</script>where <script type="math/tex">U= (u_{-k},...,u_{-1})</script> is the context vector of tokens, n is the # of layers, $W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.</li>
</ul>
<p><strong>2) Supervised training on target tasks</strong></p>
<p>Given a labeled dataset C of input tokens <script type="math/tex">x^1,...,x^m</script> along with a label y, and pretrained LM.</p>
<p>Feed the input through our pretrained LM to get the final transformer block’s activation <script type="math/tex">h_l^m</script>. Then pass them to an added linear output layer with parameter <script type="math/tex">W_y</script> to predict y:</p>
<script type="math/tex; mode=display">P(y|x^1,...,x^m) = \text{softmax} (h_l^m W_y)</script><p>The objective is:</p>
<script type="math/tex; mode=display">L_2 (C) = \sum_{x,y} \log P(y \vert x^1,...,x^m)</script><blockquote>
<p>Found including LM as an auxiliary objective can <em>improve generalization </em>of supervised model and <em>accelerate convergence</em>. Hence, the objective is:</p>
<script type="math/tex; mode=display">L_3(C) = L_2(C) + \lambda \times L_1(C)</script></blockquote>
<p><img data-src="/notes/images/OpenAI-GPT.png" alt="OpenAI-GPT"></p>
<p>As the figure shows, Transformer GPT model could be applied in different discriminative NLP tasks. For tasks that contains more than 1 sentence, a delimiter token ($) is added in between.</p>
<ul>
<li>Classification: directly feed text</li>
<li>RTE: concat premise $p$ and hypothesis $h$ with a delimiter token(dollar) in between: [p; $ ; h]</li>
<li>Similarity: no inherent ordering between sentences. We concat both possible sentence orderings (with a delimiter token $ in between)</li>
<li>QA and Commonsense Reasoning: given context document $z$, a question $q$, and a set of possible answer <script type="math/tex">a_k</script>, concatenate the document context and question with each possible answer, adding delimiter in between: <script type="math/tex">[z;q;\$;a_k]</script>.</li>
</ul>
<h3 id="BERT-Google-2018"><a href="#BERT-Google-2018" class="headerlink" title="BERT (Google 2018)"></a>BERT (Google 2018)</h3><p>BERT: <strong>Bidirectional Encoder Representations from Transformers</strong></p>
<div class="note success">
            <ul><li>Model architecture: multi-layer bi-directional Transformer encoder</li><li>Activation functions: GElu (same as OpenAI GPT)</li><li>Most important improvements: <code>MLM</code> pretraining!</li></ul>
          </div>
<h4 id="BERT’s-pretraining-on-2-tasks"><a href="#BERT’s-pretraining-on-2-tasks" class="headerlink" title="BERT’s pretraining on 2 tasks"></a>BERT’s pretraining on 2 tasks</h4><h5 id="Task-1-Masked-LM-MLM"><a href="#Task-1-Masked-LM-MLM" class="headerlink" title="Task#1: Masked LM (MLM)"></a>Task#1: <strong>Masked LM (MLM)</strong></h5><p>Masked 15% of the input tokens <em>at random</em>, and predicting masked tokens.</p>
<p><strong>Potential problems</strong>:</p>
<ol>
<li>Masked tokens are never seen, leading to mismatch between pretraining and fine-tuning.<br> <strong>Solution</strong>: 80% of the time, replace the word with [MASK] token; 10% with random words; the rest 10% unchanged (to bias the representation towards actual observed word).</li>
<li>Masking 15% tokens requires more convergence time in pretraining steps.<br> <strong>Solution</strong>: it deserves compare with empirical improvements.</li>
</ol>
<h5 id="Task-2-Next-Sentence-Prediction"><a href="#Task-2-Next-Sentence-Prediction" class="headerlink" title="Task#2: Next Sentence Prediction"></a>Task#2: Next Sentence Prediction</h5><p><strong>Task</strong>: binarized next sentence prediction (in order to handle relationships between multiple sentences)</p>
<ul>
<li>Downstream tasks like QA and NLI requires understanding the relationship between two ajacent sentences. Specifically, predicting whether sentence A is followed by B or not.</li>
</ul>
<p><img data-src='/notes/images/bert.png' width='40%'/></p>
<h4 id="How-to-employ-BERT-on-downstream-tasks"><a href="#How-to-employ-BERT-on-downstream-tasks" class="headerlink" title="How to employ BERT on downstream tasks?"></a>How to employ BERT on downstream tasks?</h4><p><img data-src="/notes/images/bert-on-downstream-tasks.png" alt="upload successful"></p>
<p>In above figure, $E$ represents the input embedding, <script type="math/tex">T_i</script> represents the contextual representation of token i, [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences.</p>
<ul>
<li>For <strong>sequence-level</strong> classification tasks, take the final hidden state (i.e. the Transformer output)  for the first token in the input. Feed it into a classification FFNN followed by a softmax.</li>
<li>For <strong>span-level</strong> or <strong>token-level</strong> prediction tasks, as shown in the figure.</li>
<li><code>BERT + FFNN + softmax</code></li>
</ul>
<h4 id="Bert-as-a-feature-extractor"><a href="#Bert-as-a-feature-extractor" class="headerlink" title="Bert as a feature extractor"></a>Bert as a feature extractor</h4><p>Like ELMo as a feature-based approach, use pretrained BERT to create <code>ELMo-like contextualized word embeddings</code>, and feed them to a domain-specific model.</p>
<p><img data-src='/notes/images/bert-extractor.png' width='90%'/></p>
<div class="note success">
            <p>Concating the token representations from the top 4 hidden layers of pretrained Transformer, is only 0.3 F1 behind the fine-tuning BERT. It can be seen that <code>BERT is effective for both the finetuning and feature based approaches</code>.</p>
          </div>
<p><img data-src='/notes/images/bert-example.png' width='90%'/></p>
<center>Bert example<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Alammar J. (2018, Dec 3). The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [Blog post]. Retrieved from https://jalammar.github.io/illustrated-bert/
">[23]</span></a></sup></center>


<h4 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h4><p>Investigate the linguitic phonomena that may or may not be captured by BERT.</p>
<h3 id="Comparison-between-ELMo-OpenAI-GPT-and-BERT"><a href="#Comparison-between-ELMo-OpenAI-GPT-and-BERT" class="headerlink" title="Comparison between ELMo, OpenAI GPT and BERT"></a>Comparison between ELMo, OpenAI GPT and BERT</h3><table style="border-collapse:collapse;border-spacing:0;border-color:#bbb" class="tg"><tr><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">Model</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">architecture</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">pretraining task</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#493F3F;background-color:#9DE0AD;text-align:left;vertical-align:top">Usage style</th></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">ELMo</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">bi-LSTM</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">LM</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">feature-based</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">OpenAI GPT</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">left-to-right Transformer</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">LM</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">fine-tuning</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">BERT</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">bi-Transformer</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">MLM, NSP</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#594F4F;background-color:#E0FFEB;text-align:left;vertical-align:top">fine-tuning</td></tr></table>

<p>Training time: Transformer &lt; ConvNets &lt; Simple RNNs &lt; LSTMs .</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A neural probabilistic language model</a>. Journal of machine learning research, 3(Feb), 1137-1155.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">R. Miikkulainen and M.G. Dyer. Natural language processing with modular neural networks and distributed lexicon. Cognitive Science, 15:343–399, 1991.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.quora.com/Whats-the-difference-between-distributed-and-distributional-semantic-representations">What’s the difference between distributed and distributional (semantic) representations?</a> Quora. Retrieved January 7, 2019<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zellig Harris. 1954. <a href="https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520">Distributional structure. Word</a>, 10(23):146–162.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <a href="https://arxiv.org/pdf/1301.3781.pdf?">Efficient estimation of word representations in vector space</a>. arXiv preprint arXiv:1301.3781.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed representations of words and phrases and their compositionality</a>. In Advances in neural information processing systems (pp. 3111-3119).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bojanowski, P., Grave, E., Joulin, A. &amp; Mikolov, T. (2017). <a href="http://aclweb.org/anthology/Q17-1010">Enriching Word Vectors with Subword Information</a>. Transactions of the Association for Computational Linguistics, 5, 135--146.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Pennington, J., Socher, R. &amp; Manning, C. D. (2014). <a href="https://www.aclweb.org/anthology/D14-1162">Glove: Global Vectors for Word Representation</a>. EMNLP 2014.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Weng L. (2017, Oct 15). Learning Word Embedding [Blog post]. Retrieved from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ruder S. (2018, Jul 08) NLP's ImageNet moment has arrived [Blog post]. Retrieved from https://thegradient.pub/nlp-imagenet/<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). <a href="https://arxiv.org/pdf/1802.05365">Deep contextualized word representations</a>. arXiv preprint arXiv:1802.05365.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Howard, J., &amp; Ruder, S. (2018). <a href="http://www.aclweb.org/anthology/P18-1031">Universal language model fine-tuning for text classification</a>. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 328-339).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Improving language understanding by generative pre-training</a>.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <a href="https://arxiv.org/pdf/1810.04805.pdf?fbclid=IwAR3FQiWQzP7stmPWZ4kzrGmiUaN81UpiNeq4GWthrxmwgX0B9f1CvuXJC2E">Bert: Pre-training of deep bidirectional transformers for language understanding</a>. arXiv preprint arXiv:1810.04805.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wieting, J., Bansal, M., Gimpel, K., &amp; Livescu, K. (2016). <a href="https://arxiv.org/pdf/1607.02789">Charagram: Embedding words and sentences via character n-grams</a>. arXiv preprint arXiv:1607.02789.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Neelakantan, A., Shankar, J., Passos, A., &amp; McCallum, A. (2015). <a href="https://arxiv.org/pdf/1504.06654">Efficient non-parametric estimation of multiple embeddings per word in vector space</a>. arXiv preprint arXiv:1504.06654.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). <a href="https://arxiv.org/pdf/1705.00108">Semi-supervised sequence tagging with bidirectional language models</a>. arXiv preprint arXiv:1705.00108.<a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. <a href="https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf">Learned in translation: Contextualized word vectors</a>. In NIPS 2017.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. <a href="https://arxiv.org/pdf/1708.02182">Regularizing and Optimizing LSTM Language Models</a>. arXiv preprint arXiv:1708.02182 .<a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in Neural Information Processing Systems (pp. 5998-6008).<a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">R. Sennrich, B. Haddow, and A. Birch. <a href="https://arxiv.org/pdf/1508.07909">Neural machine translation of rare words with subword units</a>. arXiv preprint arXiv:1508.07909, 2015.<a href="#fnref:21" rev="footnote"> ↩</a></span></li><li id="fn:22"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">D. Hendrycks and K. Gimpel. <a href="https://arxiv.org/pdf/1606.08415">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</a>. arXiv preprint arXiv:1606.08415, 2016.<a href="#fnref:22" rev="footnote"> ↩</a></span></li><li id="fn:23"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Alammar J. (2018, Dec 3). The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [Blog post]. Retrieved from https://jalammar.github.io/illustrated-bert/<a href="#fnref:23" rev="footnote"> ↩</a></span></li><li id="fn:24"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598">Towards data science: FROM Pre-trained Word Embeddings TO Pre-trained Language Models — Focus on BERT</a><a href="#fnref:24" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Language model</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Survey</tag>
        <tag>Language model</tag>
        <tag>Word representation</tag>
      </tags>
  </entry>
  <entry>
    <title>Word Tokenization: How to Handle Out-Of-Vocabulary Vocabularies?</title>
    <url>/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Summary of word tokenization, as well as coping with OOV words. <small> (This is expanded based on my MT course lectured by Dr. Rico Sennrich in Edinburgh Informatics in 2018.) </small><br><span id="more"></span></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="How-to-Represent-Text"><a href="#How-to-Represent-Text" class="headerlink" title="How to Represent Text?"></a>How to Represent Text?</h2><ul>
<li>One-hot encoding<ul>
<li>lookup of word embedding for input</li>
<li>probability distribution over vocabulary for output</li>
</ul>
</li>
<li>Large vocabulary<ul>
<li>increase network size</li>
<li>decrease training and decoding speed</li>
</ul>
</li>
</ul>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p><strong>Open-vocabulary</strong> problems:</p>
<ul>
<li>Many training corpora contain millions of word types</li>
<li>Productive word formation processes (compounding; derivation) allow formation and understanding of unseen words</li>
<li>Names, numbers are morphologically simple, but open word classes</li>
</ul>
<h1 id="Word-based-Tokenization"><a href="#Word-based-Tokenization" class="headerlink" title="Word-based Tokenization"></a>Word-based Tokenization</h1><div class="note info">
            <p>Limits:</p><ul><li>Very similar words have entirely different meanings, such as dog vs dogs.</li><li>The vocabulary can end up very large.</li><li>Large vocabularies result in enormous embedding matrix as the input and output layer.</li></ul>
          </div>
<p>Common methods include:</p>
<ul>
<li>Space and punctuation tokenization;</li>
<li>Rule-based tokenization.</li>
</ul>
<h2 id="Non-solution-Ignoring-Rare-Words"><a href="#Non-solution-Ignoring-Rare-Words" class="headerlink" title="Non-solution: Ignoring Rare Words"></a>Non-solution: Ignoring Rare Words</h2><ul>
<li>Replace OOV words with UNK</li>
<li>A vocabulary of 50,000 words covers 95% of text: <strong> <span class="label danger">95% is not enough !</span> </strong></li>
</ul>
<div class="note warning">
            <p><strong>OOV problem</strong>:</p><ul><li>If two very different words are both OOV, they wil get the same id ([UNK]).</li><li>Large vocabulary will increase the <strong>embedding layer’s parameters</strong>.</li></ul>
          </div>
<h2 id="Approximative-Softmax"><a href="#Approximative-Softmax" class="headerlink" title="Approximative Softmax"></a>Approximative Softmax</h2><p>Compute softmax over “active” subset of vocabulary $\rightarrow$ smaller weight matrix, faster softmax<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[On Using Very Large Target Vocabulary for Neural Machine Translation](https://arxiv.org/pdf/1412.2007)
">[1]</span></a></sup></p>
<ul>
<li>At training time: vocabulary based on words occurring in training set partition</li>
<li>At test time: determine likely target words based on source text (using cheap method like translation dictionary)</li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Allow larger vocabulary, but still not open</li>
<li>Networks may not learn good representation of rare words</li>
</ul>
<h2 id="Back-off-Models"><a href="#Back-off-Models" class="headerlink" title="Back-off Models"></a>Back-off Models</h2><ul>
<li>Replace rare words with UNK at training time <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Addressing the Rare Word Problem in Neural Machine Translation](https://arxiv.org/pdf/1410.8206.pdf)
">[2]</span></a></sup></li>
<li>When system produces UNK, alight UNK to source word, and translate this with back-off method</li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Compounds: hard to model 1-to-many relationships</li>
<li>Morphology: hard to predict inflection with back-off dictionary</li>
<li>Names: if alphabets differ, we need transliteration</li>
<li>Alignment: attention model unreliable</li>
</ul>
<h1 id="Character-based-Tokenization"><a href="#Character-based-Tokenization" class="headerlink" title="Character-based Tokenization"></a>Character-based Tokenization</h1><p><strong>Advantages</strong>:<br>Character tokens solve the OOV problem with following benefits:</p>
<ul>
<li>Vocabularies are slimmer.</li>
<li><span class="label success"> Mostly open-vocabulary</span>: fewer OOV words;</li>
<li>No heuristic or language-specific segmentation;</li>
<li>Neural networks can conceivably learn from raw char sequences.</li>
</ul>
<p><strong>Drawbacks</strong>:<br>Representing the input as a squences of characters can have following problems:</p>
<ul>
<li>Character tokens can <span class="label warning"> increase sequence length</span>, which will slow down the speed of training and decoding (x2 - x4 increase in training time), as well as make it difficult to learn relationships between characters to form meaningful words.</li>
<li>It is harder for the model to learn meaningful input representations. E.g., learn the context-independent reprsentation for a single char “N” is much harder than learning a context-indepdent reprentation for the word “NLP”.</li>
<li>Naive char-level encoder-decoders are currently resource-limited.</li>
</ul>
<div class="note info">
            <p><strong>OPEN QUESTIONS</strong>:</p><ul><li>On which level do we represent meaning?</li><li>On which level do attention operate?</li></ul>
          </div>
<h2 id="Hierarchical-Model-Backoff"><a href="#Hierarchical-Model-Backoff" class="headerlink" title="Hierarchical Model: Backoff"></a>Hierarchical Model: Backoff</h2><ul>
<li>Word-level model produces UNKs <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models](https://arxiv.org/pdf/1604.00788)
">[4]</span></a></sup></li>
<li>For each UNK, char-level model predicts word based on word hidden state</li>
</ul>
<p><strong>Pros</strong>:</p>
<ul>
<li>Prediction is more flexible than dictionary look-up</li>
<li>More efficient than pure char-level translation</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li>Independence assumptions between main model and backoff model</li>
</ul>
<h2 id="Char-level-Output"><a href="#Char-level-Output" class="headerlink" title="Char-level Output"></a>Char-level Output</h2><ul>
<li>No word segmentation on target side <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[A Character-level Decoder without Explicit Segmentation for Neural Machine Translation](https://arxiv.org/pdf/1603.06147)
">[5]</span></a></sup></li>
<li>Encoder is BPE-level</li>
</ul>
<h2 id="Char-level-Input"><a href="#Char-level-Input" class="headerlink" title="Char-level Input"></a>Char-level Input</h2><p>Hierarchical representation: RNN states represent words, but their representation is computed from char-level LSTM <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Character-based Neural Machine Translation](https://arxiv.org/pdf/1511.04586)
">[6]</span></a></sup></p>
<p><img data-src="/notes/images/char-level OOV.png" alt="char-level input"></p>
<h2 id="Fully-Char-level"><a href="#Fully-Char-level" class="headerlink" title="Fully Char-level"></a>Fully Char-level</h2><ul>
<li>Goal: get rid of word boundaries <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Fully Character-Level Neural Machine Translation without Explicit Segmentation](https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00067)
">[7]</span></a></sup></li>
<li>Target side: char-level RNNs</li>
<li>Source side: convolution and max-pooling layers</li>
</ul>
<p><img data-src="/notes/images/fully char-level model.png" alt="Fully char-level model"></p>
<h1 id="Subword-based-Tokenization"><a href="#Subword-based-Tokenization" class="headerlink" title="Subword-based Tokenization"></a>Subword-based Tokenization</h1><p>The <strong>idea</strong> behind subword tokenization is that <span class="label success"> frequently occurring words should be in the vocabulary</span>, whereas <span class="label danger">rare words should be split into frequent sub words.</span></p>
<p><img data-src="/notes/images/word-char-token-cons.png" alt="Image source: Huggingface"></p>
<div class="note primary">
            <p>Subword-based tokenization lies between character and word-based tokenization.</p><ul><li>Frequently used words should not be split into smaller subwords;</li><li>Rare words should be decomposed into meaningful subwords.</li><li>Subwords help identify similar syntactic or semantic situations in texts.</li><li>Subword. tokenization can identify start of word tokens, such as “##” in WordPiece.</li></ul>
          </div>
<h2 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte-Pair Encoding"></a>Byte-Pair Encoding</h2><p>Byte-Pair Encoding (BPE) relies on a pre-tokenizer that splits the training data into words.</p>
<p><img data-src="/notes/images/BPE-slides.png" alt="Dr. Rico Sennrich MT Lecture 2018/07"></p>
<p><strong>Why BPE</strong>? <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rico Sennrich. MT lecture 07. Edinburgh Informatics 2018.
">[13]</span></a></sup></p>
<ul>
<li>Open-vocabulary: operations learned on the training set can be applied to <code>&lt;UNK&gt;</code></li>
<li>Compression of frequent character sequences improves efficiency $\rightarrow$ <span class="label primary">trade-off between text length and vocabulary size</span>.</li>
</ul>
<h3 id="Pre-tokenization"><a href="#Pre-tokenization" class="headerlink" title="Pre-tokenization"></a>Pre-tokenization</h3><p>Pretokenzation splits the texts into words, For example:</p>
<ul>
<li>Space tokenization, e.g. GPT-2, RoBERTa;</li>
<li>Rule-based tokenization (using Moses), e.g., XLM, FlauBERT;</li>
<li>Space and ftfy: GPT.</li>
</ul>
<h3 id="BPE-Training"><a href="#BPE-Training" class="headerlink" title="BPE Training"></a>BPE Training</h3><p>Given pre-tokenized tokens, we can train a BPE tokenizer as follows:<br><div class="note info">
            <p><strong>Bottom-up character merging</strong>: <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[BPE] Neural Machine Translation of Rare Words with Subword UnitsRico](http://www.aclweb.org/anthology/P16-1162.pdf)">[3]</span></a></sup><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rico Sennrich. MT lecture 07. Edinburgh Informatics 2018.">[13]</span></a></sup></p><ul><li>Starting point: char-level representation $\rightarrow$ computationally expensive.</li><li>Compress representation based on information theory $\rightarrow$ byte-pair encoding.</li><li>Repeatedly replace most frequent symbol pair <code>(&#39;A&#39;, &#39;B&#39;)</code> with <code>&#39;AB&#39;</code>.</li><li>Hyperparameter: when to stop $\rightarrow$ controls vocabulary size.</li></ul>
          </div></p>
<p>Step by step:</p>
<ol>
<li>Cut the pre-tokenized corpus into smallest units, usually characters, as a base vocabulary.</li>
<li>Append <code>&lt;/w&gt;</code> at the end of original tokens.</li>
<li>Count the neighbor unit pairs, and merge the pair that occurs most frequently.</li>
<li>Go to 2 until reach the maximum vocabulary size.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span>(<span class="params">vocab</span>):</span></span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>) <span class="comment"># default count 0</span></span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split() <span class="comment"># split to chars</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += freq <span class="comment"># count bigram</span></span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span>(<span class="params">pair, v_in</span>):</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">&quot; &quot;</span>.join(pair)) <span class="comment"># escape</span></span><br><span class="line">    p = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;!\S&gt;)&#x27;</span> + bigram + <span class="string">r&#x27;(?!\S)&#x27;</span>) <span class="comment"># lookaround assertion, equal to [^\f\n\r\t\v]</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">&quot;&quot;</span>.join(pair), word) <span class="comment"># merge the most frequent pair by removing whitespace</span></span><br><span class="line">        v_out[w_out] = v_in[word] <span class="comment"># assign pair count</span></span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    vocab = &#123;<span class="string">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    num_merges = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">        pairs = get_stats(vocab)</span><br><span class="line">        best = <span class="built_in">max</span>(pairs, key=pairs.get)</span><br><span class="line">        vocab = merge_vocab(best, vocab)</span><br><span class="line">        <span class="built_in">print</span>(best)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">----------------------</span></span><br><span class="line"><span class="string">(&#x27;e&#x27;, &#x27;s&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;es&#x27;, &#x27;t&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;est&#x27;, &#x27;&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;l&#x27;, &#x27;o&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;lo&#x27;, &#x27;w&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;n&#x27;, &#x27;e&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;ne&#x27;, &#x27;w&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;new&#x27;, &#x27;est&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;low&#x27;, &#x27;&lt;/w&gt;&#x27;)</span></span><br><span class="line"><span class="string">(&#x27;w&#x27;, &#x27;i&#x27;)</span></span><br><span class="line"><span class="string">----------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="BPE-Encoding"><a href="#BPE-Encoding" class="headerlink" title="BPE Encoding"></a>BPE Encoding</h3><p>BPE encodes the input tokens one by one, progressively merging the pairs according to frequency during training (from high to low). In other words, merge according to the merge order <code>bpe_codes</code> in <em>merge.txt</em> (with decreasing frequency).<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># subword-nmt/apply_bpe.py</span></span><br><span class="line"><span class="comment"># Author: R. Sennrich</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, </span></span></span><br><span class="line"><span class="params"><span class="function">    cache, glossaries_regex=<span class="literal">None</span>, dropout=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Encode word based on list of BPE merge operations,</span></span><br><span class="line"><span class="string">    which are applied consecutively</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> dropout <span class="keyword">and</span> orig <span class="keyword">in</span> cache:</span><br><span class="line">        <span class="keyword">return</span> cache[orig]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> glossaries_regex <span class="keyword">and</span> glossaries_regex.match(orig):</span><br><span class="line">        cache[orig] = (orig,)</span><br><span class="line">        <span class="keyword">return</span> (orig,)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(orig) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> orig</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> version == (<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">        word = <span class="built_in">list</span>(orig) + [<span class="string">&#x27;&lt;/w&gt;&#x27;</span>]</span><br><span class="line">    <span class="keyword">elif</span> version == (<span class="number">0</span>, <span class="number">2</span>): <span class="comment"># more consistent handling of word-final segments</span></span><br><span class="line">        word = <span class="built_in">list</span>(orig[:-<span class="number">1</span>]) + [orig[-<span class="number">1</span>] + <span class="string">&#x27;&lt;/w&gt;&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(word) &gt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get list of symbol pairs; optionally apply dropout</span></span><br><span class="line">        pairs = [(bpe_codes[pair],i,pair) <span class="keyword">for</span> (i,pair) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(word, word[<span class="number">1</span>:])) <span class="keyword">if</span> (<span class="keyword">not</span> dropout <span class="keyword">or</span> random.random() &gt; dropout) <span class="keyword">and</span> pair <span class="keyword">in</span> bpe_codes]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#get first merge operation in list of BPE codes</span></span><br><span class="line">        bigram = <span class="built_in">min</span>(pairs)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find start position of all pairs that we want to merge</span></span><br><span class="line">        positions = [i <span class="keyword">for</span> (rank,i,pair) <span class="keyword">in</span> pairs <span class="keyword">if</span> pair == bigram]</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        new_word = []</span><br><span class="line">        bigram = <span class="string">&#x27;&#x27;</span>.join(bigram)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> positions:</span><br><span class="line">            <span class="comment"># merges are invalid if they start before current position. </span></span><br><span class="line">            <span class="comment"># This can happen if there are overlapping pairs: (x x x -&gt; xx x)</span></span><br><span class="line">            <span class="keyword">if</span> j &lt; i:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            new_word.extend(word[i:j]) <span class="comment"># all symbols before merged pair</span></span><br><span class="line">            new_word.append(bigram) <span class="comment"># merged pair</span></span><br><span class="line">            i = j+<span class="number">2</span> <span class="comment"># continue after merged pair</span></span><br><span class="line">        new_word.extend(word[i:]) <span class="comment"># add all symbols until end of word</span></span><br><span class="line">        word = new_word</span><br><span class="line"></span><br><span class="line">    <span class="comment"># don&#x27;t print end-of-word symbols</span></span><br><span class="line">    <span class="keyword">if</span> word[-<span class="number">1</span>] == <span class="string">&#x27;&lt;/w&gt;&#x27;</span>:</span><br><span class="line">        word = word[:-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> word[-<span class="number">1</span>].endswith(<span class="string">&#x27;&lt;/w&gt;&#x27;</span>):</span><br><span class="line">        word[-<span class="number">1</span>] = word[-<span class="number">1</span>][:-<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    word = <span class="built_in">tuple</span>(word)</span><br><span class="line">    <span class="keyword">if</span> vocab:</span><br><span class="line">        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)</span><br><span class="line"></span><br><span class="line">    cache[orig] = word</span><br><span class="line">    <span class="keyword">return</span> word</span><br></pre></td></tr></table></figure></p>
<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><p>WordPiece was firstly proposed in Google’s Japanese and Korean voice search system<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
">[8]</span></a></sup>, and was used in Google’s machine translation system<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)
">[9]</span></a></sup>. It deals with an infinite vocabulary from large amounts of text automatically and increamentally by running greedy algorithms. This provides a user-specified number of word units which are chosen in a greedy way (without focusing on semantics) to maximize the likelihood on the language model (LM) training data - incidentally the same metric during decoding.</p>
<div class="note info">
            <p>Example:</p><ul><li><strong>Word</strong>: Jet makers feud over seat width with big orders at stake</li><li><strong>Wordpieces</strong>:  _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake</li></ul>
          </div>
<p>Procedure:</p>
<ol>
<li>Initialize the word unit inventory with basic unicode characters.</li>
<li>Build a language model on the triaining data using the inventory from 1.</li>
<li>Generate a new word unit by combinining two units out of the current word inventory to increment the word unit inventory by one. <strong>“Choose the new word unit out of all possible ones that increases the likelihood on the training data the most when added to the model”</strong>.</li>
<li>Goto 2 unitil reach the predefined limit of word units, or the likelihood increase falls below a certain threshold.</li>
</ol>
<div class="note warning">
            <p><strong>How to choose which pair to merge?</strong><br>Suppose we get $z$ after merging neibor subwords $x$ and $y$, the difference between LM loglikelihood is:</p><script type="math/tex; mode=display">\Delta = \log P(z=[x \Vert y]) - \big( \log P(x)+\log P(y) \big) = \log \frac{P(z=[x \Vert y])}{P(x)P(y)}</script><p>This is exactly the mutual infomation (MI) under the trained LM between the subword pair $(x,y)$. Therefore, <strong>WordPiece chooses the subword pair that has the maximum MI value</strong>.</p>
          </div>
<h2 id="Unigram-Language-Model"><a href="#Unigram-Language-Model" class="headerlink" title="Unigram Language Model"></a>Unigram Language Model</h2><div class="note primary">
            <p><strong>Motivation</strong>: </p><ul><li><span class="label primary"> A sentence can be represented into <strong>multiple subword sequences</strong> even with the same vocabulary using BPE</span>. Subword regularization is a regularization method for open-domain NMT.</li></ul>
          </div>
<p><img data-src="/notes/images/SubwordRegularization.png" alt="Subword Regularization"></p>
<p>Subword Regularization based on a unigram language model<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://www.aclweb.org/anthology/P18-1007.pdf)
">[10]</span></a></sup> assumes that each subword occurs independently, and consequently, <span class="label primary"> the probability of a subword sequence $\mathbf{x} = (x_1, \cdots, x_M)$ is formulated as the product of the subword occurence probablities</span> $p(\mathbf{x})$:</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(\mathbf{x}) &{}= \prod_{i=1}^M p(x_i), \quad \forall i x_i \in \mathcal{V}, \sum_{x \in \mathcal{V}} p(x)=1
\end{aligned}</script><p>where $\mathcal{V}$ is a pre-determined vocabulary. The most probable segmentation $\mathbf{x}^*$ for the input $X$ is given by:</p>
<script type="math/tex; mode=display">
\mathbf{x}^* = \arg\max_{\mathbf{x} \in \mathcal{S}(X)} P(\mathbf{x})</script><p>where $\mathcal{S}(X)$ is a set of segmentation candidates built from the input sentence $X$. $\mathbf{x}^*$ is obtained from the <span class="label primary"> Viterbi algorithm</span>.</p>
<!--
If the vocab is given, the subword occurence probablities $p(x_i)$ are estimated via the EM algorithm that maximizes the marginal likelihood $\mathcal{L}$ assuming that $p(x_i)$ are hidden variables：
$$
\mathcal{L} = \sum_{s=1}^{|D|} \log (P(X^{(s)})) = \sum_{s=1}^{|D|} \log \big( \sum_{\mathbf{x} \in \mathcal{S}(X^{(s)})} P(\mathbf{x}) \big)
$$
-->
<p><strong>Procedure:</strong></p>
<ol>
<li>Heuristically make a reasonably big seed vocabulary from the training corpus. Possible choices are:<ul>
<li>All chars and the most frequent substrings.</li>
<li>BPE with sufficient mergence.</li>
</ul>
</li>
<li>Fix the vocab, optimize $p(x)$ with EM algorithm</li>
<li>Compute the loss for each subword $x_i$ using a unigram language model, representing how likely the likelihood $\mathcal{L}$ is reduced when the subword $x_i$ is removed from the current vocabulary inventory.<script type="math/tex; mode=display">
\mathcal{L} = \sum_{s=1}^{|D|} \log (P(X^{(s)})) = \sum_{s=1}^{|D|} \log \big( \sum_{\mathbf{x} \in \mathcal{S}(X^{(s)})} P(\mathbf{x}) \big)</script></li>
<li>Sort the vocabularies by loss and keep top $\eta \%$ ($\eta = 80$) of subwords. (Always keep the single char set to avoid OOV).</li>
<li>Goto 2.</li>
</ol>
<div class="note success">
            <p>Interpretation</p><ul><li><span class="label primary"> Subword regularization with unigram language model</span> can be seen as a probabilistic mixture of characters, subwords, and word segmentations.</li><li>I regard it as a post-regularization approach to subwords techniques, such as <em>BPE units</em>. This means we can use BPE word pieces as the initialization.</li></ul>
          </div>
<h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Subword tokenization</th>
<th style="text-align:center">Merge rules</th>
<th style="text-align:center">Trim rules</th>
<th style="text-align:center">Frequency-based</th>
<th style="text-align:center">Probability-based</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">BPE</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
</tr>
<tr>
<td style="text-align:center">WordPiece</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
</tr>
<tr>
<td style="text-align:center">Unigram</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
</tr>
</tbody>
</table>
</div>
<div class="note danger">
            <p><span class="label danger"> <strong>Comparison</strong> between subword methods:</span></p><ol><li><span class="label info"><strong>BPE</strong> $\Uparrow$</span>: start from <strong>char sets</strong>, <strong>incrementally</strong> merge according to (neighbor) subword pair co-occurrence.</li><li><span class="label info"><strong>WordPiece</strong>$\Uparrow$</span>: start from <strong>char sets</strong>, <strong>incrementally</strong> merge according to the <strong>decreased likelihood (Mutual Information)</strong> after mergence.</li><li><span class="label primary"><strong>Unigram Language Model</strong> (subword regularization)$\Downarrow$</span>: starting from <strong>subword vocabularies</strong>, <strong>reducing</strong> the subword with <strong>likelihood reduction under a unigram LM</strong> after mergence.</li></ol>
          </div>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2019out-of-vocab,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Word Tokenization: How to Handle Out-Of-Vocabulary Words?&#125;&#125;,</span><br><span class="line">  year = &#123;2019&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1412.2007">On Using Very Large Target Vocabulary for Neural Machine Translation</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1410.8206.pdf">Addressing the Rare Word Problem in Neural Machine Translation</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.aclweb.org/anthology/P16-1162.pdf">[BPE] Neural Machine Translation of Rare Words with Subword Units
Rico</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1604.00788">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1603.06147">A Character-level Decoder without Explicit Segmentation for Neural Machine Translation</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1511.04586">Character-based Neural Machine Translation</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00067">Fully Character-Level Neural Machine Translation without Explicit Segmentation</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">[WordPiece] Japanese and Korean voice search (ICASSP 2012, Google)</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1609.08144.pdf">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/P18-1007.pdf">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c">A comprehensive guide to subword tokenisers</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rico Sennrich. MT lecture 07. Edinburgh Informatics 2018.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://huggingface.co/docs/tokenizers/python/latest/quicktour.html">Hugginface tokenizer</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.persagen.com/files/misc/radford2019language.pdf">[GPT-2]Language models are unsupervised multitask learners</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>OOV</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Tokenization</tag>
      </tags>
  </entry>
  <entry>
    <title>BERTology: An Introduction!</title>
    <url>/notes/2019/12/14/NLP/BERTology-an-introduction/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>This is an introduction of recent BERT families.<br><span id="more"></span></p>
<div class="note success">
            <p><strong>Relevant notes</strong>:</p><ol><li><a href="/notes/2019/01/22/NLP/Attention-in-a-nutshell/#Transformer">Transformer detailed explanation</a></li><li><a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/">Transformer variant architectures</a></li><li><a href="/notes/2019/12/14/NLP/BERTology-an-introduction/">BERTology introduction</a></li></ol>
          </div>
<h1 id="LM-pretraining-background"><a href="#LM-pretraining-background" class="headerlink" title="LM pretraining background"></a>LM pretraining background</h1><h2 id="Autoregressive-Language-Modeling"><a href="#Autoregressive-Language-Modeling" class="headerlink" title="Autoregressive Language Modeling"></a>Autoregressive Language Modeling</h2><p>Given a text sequence <script type="math/tex">\pmb{x} = (x_1, \cdots, x_T)</script>, autoregressive (AR) language modeling factorizes the likelihood along a uni direction according to the product rule, either forward:</p>
<script type="math/tex; mode=display">p(\pmb{x})=\prod_{t=1}^T p(x_t \vert x_{<t}))</script><p>or backward:</p>
<script type="math/tex; mode=display">p(\pmb{x})=\prod_{t=T}^1 p(x_t \vert x_{>t}))</script><p>The AR pretraining maximizes the likehood under the forward AR factorization:</p>
<script type="math/tex; mode=display">
\begin{align}
\max_\theta \log p_\theta (\pmb{x}) &= \sum_{t=1}^T \log p_\theta (x_t \vert \pmb{x}_{<t}) \\
& = \sum_{t=1}^T \log \frac{\exp\left(  \overbrace{h_\theta (\pmb{x}_{1:t-1})^T}^\text{context representation } \overbrace{e(x_t)}^\text{ embedding} \right)}{\sum_{x'}\exp\left(  h_\theta (\pmb{x}_{1:t-1})^T e(x') \right)}
\end{align}</script><p>where <script type="math/tex">h_\theta (\pmb{x}_{1:t-1})</script> denotes the context representation by NNs, such as RNNs/Transformers; <script type="math/tex">e(x_t)</script> denotes the embedding of $x$.</p>
<p>It is not effective to model the deep bidirectional contexts.</p>
<h2 id="Autoencoding-based-pretraining"><a href="#Autoencoding-based-pretraining" class="headerlink" title="Autoencoding based pretraining"></a>Autoencoding based pretraining</h2><p>Autoencoding (AE) based pretraining does not perform density estimation, but <strong>recover the original data from corrupted (masked) input</strong>.</p>
<p>Denosing autoencoding based pretraining, such as BERT can <strong>model the bidirectional contexts</strong>. Given a text sequence <script type="math/tex">\pmb{x} = (x_1, \cdots, x_T)</script>, it randomly masks a portion (15%) of tokens $\bar{\pmb{x}}$ in $\pmb{x}$. The training objective is to reconstruct randomly masked token $\bar{\pmb{x}}$ from   corrupted sequence $\hat{\pmb{x}}$:</p>
<script type="math/tex; mode=display">
\begin{align}
\max_\theta \log p_\theta (\bar{\pmb{x}} \vert \hat{\pmb{x}}) & \approx \sum_{t=1}^T m_t \log p_\theta (x_t \vert \hat{\pmb{x}}) \\
& = \sum_{t=1}^T m_t \log \frac{\exp\big( H_\theta (\hat{\pmb{x}})^T_t e(x_t) \big)}{\sum_{x'} \exp\big( H_\theta(\hat{\pmb{x}})_t^T e(x') \big)}
\end{align}</script><p>where </p>
<ul>
<li><script type="math/tex">m_t=1</script> means <script type="math/tex">x_t</script> is masked; </li>
<li>the Transformer <script type="math/tex">H_\theta</script> encodes $\pmb{x}$ into hidden vectors <script type="math/tex">H_\theta(\pmb{x}) = \big[H_\theta(\pmb{x})_1, H_\theta(\pmb{x})_2,  \cdots, H_\theta(\pmb{x})_T \big]</script></li>
</ul>
<p>However, it relies on corrupting the input with masks. The <strong>drawbacks</strong>:</p>
<ol>
<li><strong>Independent assumption</strong>: cannot model joint probability and assume the predicted tokens are independent of each other. It neglects the dependency between the masked positions</li>
<li><strong>pretrain-finetune discrepancy</strong> (input noise): the artificial symbols like [MASK] used by BERT does not exist during the training of downstream tasks.</li>
</ol>
<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p>XLNet<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237). arXiv preprint arXiv:1906.08237.
">[1]</span></a></sup> (CMU &amp; Google brain 2019) leverages both the advatage of AR and AE LM objectives and hinder their drawbacks. </p>
<ul>
<li>The permutation of the factorization order impedes the dependency between masked positions in BERT and still remains the AR-like objectives so as to prevent the pretrain-finetuning discrepancy. </li>
<li>On the other hand, with permutation, it attends to the bi-contextual information as in BERT.</li>
</ul>
<h2 id="Permutation-Language-Model"><a href="#Permutation-Language-Model" class="headerlink" title="Permutation Language Model"></a>Permutation Language Model</h2><p>XLNet<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[GitHub: XLNet](https://github.com/zihangdai/xlnet/blob/master/data_utils.py)">[13]</span></a></sup> applies permutation language model by autoregressively pretraining bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order.</p>
<p><img data-src="/notes/images/XLNet-PLM.png" alt="Permutation Language Model"></p>
<p>For a sequence $\pmb{x}$ of length $T$, there are $T!$ different orders to perform a valid AR factorization.</p>
<script type="math/tex; mode=display">\max_\theta \mathbb{E}_{\pmb{z}\sim Z_T} \left[ \sum_{t=1}^T \log p_\theta(x_{z_t} \vert \pmb{x}_{z<t}) \right]</script><p>Permutation language modeling not only retains the benefits of AR models but also capture the bidirectional contexts as BERT. It only permutes the factorization order, rather than the sequence order.</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><h4 id="XLNet-Implementation"><a href="#XLNet-Implementation" class="headerlink" title="XLNet Implementation"></a>XLNet Implementation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># XLNet implementation (tf)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_local_perm</span>(<span class="params">inputs, targets, is_masked, perm_size, seq_len</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Sample a permutation of the factorization order, and create an</span></span><br><span class="line"><span class="string">  attention mask accordingly.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    inputs: int64 Tensor in shape [seq_len], input ids.</span></span><br><span class="line"><span class="string">    targets: int64 Tensor in shape [seq_len], target ids.</span></span><br><span class="line"><span class="string">    is_masked: bool Tensor in shape [seq_len]. True means being selected</span></span><br><span class="line"><span class="string">      for partial prediction.</span></span><br><span class="line"><span class="string">    perm_size: the length of longest permutation. Could be set to be reuse_len.</span></span><br><span class="line"><span class="string">      Should not be larger than reuse_len or there will be data leaks.</span></span><br><span class="line"><span class="string">    seq_len: int, sequence length.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Generate permutation indices</span></span><br><span class="line">  index = tf.<span class="built_in">range</span>(seq_len, dtype=tf.int64)</span><br><span class="line">  index = tf.transpose(tf.reshape(index, [-<span class="number">1</span>, perm_size]))</span><br><span class="line">  index = tf.random_shuffle(index)</span><br><span class="line">  index = tf.reshape(tf.transpose(index), [-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `perm_mask` and `target_mask`</span></span><br><span class="line">  <span class="comment"># non-functional tokens</span></span><br><span class="line">  non_func_tokens = tf.logical_not(tf.logical_or(</span><br><span class="line">      tf.equal(inputs, SEP_ID),</span><br><span class="line">      tf.equal(inputs, CLS_ID)))</span><br><span class="line"></span><br><span class="line">  non_mask_tokens = tf.logical_and(tf.logical_not(is_masked), non_func_tokens)</span><br><span class="line">  masked_or_func_tokens = tf.logical_not(non_mask_tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Set the permutation indices of non-masked (&amp; non-funcional) tokens to the</span></span><br><span class="line">  <span class="comment"># smallest index (-1):</span></span><br><span class="line">  <span class="comment"># (1) they can be seen by all other positions</span></span><br><span class="line">  <span class="comment"># (2) they cannot see masked positions, so there won&quot;t be information leak</span></span><br><span class="line">  smallest_index = -tf.ones([seq_len], dtype=tf.int64)</span><br><span class="line">  rev_index = tf.where(non_mask_tokens, smallest_index, index)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create `target_mask`: non-funcional and maksed tokens</span></span><br><span class="line">  <span class="comment"># 1: use mask as input and have loss</span></span><br><span class="line">  <span class="comment"># 0: use token (or [SEP], [CLS]) as input and do not have loss</span></span><br><span class="line">  target_tokens = tf.logical_and(masked_or_func_tokens, non_func_tokens)</span><br><span class="line">  target_mask = tf.cast(target_tokens, tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create `perm_mask`</span></span><br><span class="line">  <span class="comment"># `target_tokens` cannot see themselves</span></span><br><span class="line">  self_rev_index = tf.where(target_tokens, rev_index, rev_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1: cannot attend if i &lt;= j and j is not non-masked (masked_or_func_tokens)</span></span><br><span class="line">  <span class="comment"># 0: can attend if i &gt; j or j is non-masked</span></span><br><span class="line">  perm_mask = tf.logical_and(</span><br><span class="line">      self_rev_index[:, <span class="literal">None</span>] &lt;= rev_index[<span class="literal">None</span>, :],</span><br><span class="line">      masked_or_func_tokens)</span><br><span class="line">  perm_mask = tf.cast(perm_mask, tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># new target: [next token] for LM and [curr token] (self) for PLM</span></span><br><span class="line">  new_targets = tf.concat([inputs[<span class="number">0</span>: <span class="number">1</span>], targets[: -<span class="number">1</span>]],</span><br><span class="line">                          axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># construct inputs_k</span></span><br><span class="line">  inputs_k = inputs</span><br><span class="line"></span><br><span class="line">  <span class="comment"># construct inputs_q</span></span><br><span class="line">  inputs_q = target_mask</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> perm_mask, new_targets, target_mask, inputs_k, inputs_q</span><br></pre></td></tr></table></figure>
<h4 id="Huggingface-Implementation"><a href="#Huggingface-Implementation" class="headerlink" title="Huggingface Implementation"></a>Huggingface Implementation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataCollatorForPermutationLanguageModeling</span>(<span class="params">DataCollatorMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Data collator used for permutation language modeling.</span></span><br><span class="line"><span class="string">    - collates batches of tensors, honoring their tokenizer&#x27;s pad_token</span></span><br><span class="line"><span class="string">    - preprocesses batches for permutation language modeling with procedures specific to XLNet</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tokenizer: PreTrainedTokenizerBase</span><br><span class="line">    plm_probability: <span class="built_in">float</span> = <span class="number">1</span> / <span class="number">6</span></span><br><span class="line">    max_span_length: <span class="built_in">int</span> = <span class="number">5</span>  <span class="comment"># maximum length of a span of masked tokens</span></span><br><span class="line">    return_tensors: <span class="built_in">str</span> = <span class="string">&quot;pt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            examples = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        batch = _torch_collate_batch(examples, self.tokenizer)</span><br><span class="line">        inputs, perm_mask, target_mapping, labels = self.torch_mask_tokens(batch)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;perm_mask&quot;</span>: perm_mask, <span class="string">&quot;target_mapping&quot;</span>: target_mapping, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            examples = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        batch = _tf_collate_batch(examples, self.tokenizer)</span><br><span class="line">        inputs, perm_mask, target_mapping, labels = self.tf_mask_tokens(batch)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;perm_mask&quot;</span>: perm_mask, <span class="string">&quot;target_mapping&quot;</span>: target_mapping, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_call</span>(<span class="params">self, examples: <span class="type">List</span>[<span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">Any</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(examples[<span class="number">0</span>], (<span class="built_in">dict</span>, BatchEncoding)):</span><br><span class="line">            examples = [e[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> e <span class="keyword">in</span> examples]</span><br><span class="line">        batch = _numpy_collate_batch(examples, self.tokenizer)</span><br><span class="line">        inputs, perm_mask, target_mapping, labels = self.numpy_mask_tokens(batch)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: inputs, <span class="string">&quot;perm_mask&quot;</span>: perm_mask, <span class="string">&quot;target_mapping&quot;</span>: target_mapping, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">torch_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:</span></span><br><span class="line"><span class="string">            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line"><span class="string">            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line"><span class="string">            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be</span></span><br><span class="line"><span class="string">               masked</span></span><br><span class="line"><span class="string">            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -</span></span><br><span class="line"><span class="string">               span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line"><span class="string">            4. Set `cur_len = cur_len + context_length`. If `cur_len &lt; max_len` (i.e. there are tokens remaining in the</span></span><br><span class="line"><span class="string">               sequence to be processed), repeat from Step 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for permutation language modeling. Please add a mask token if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.size(<span class="number">1</span>) % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        labels = inputs.clone()</span><br><span class="line">        <span class="comment"># Creating the mask and target_mapping tensors</span></span><br><span class="line">        masked_indices = torch.full(labels.shape, <span class="number">0</span>, dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">        target_mapping = torch.zeros((labels.size(<span class="number">0</span>), labels.size(<span class="number">1</span>), labels.size(<span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.size(<span class="number">0</span>)):</span><br><span class="line">            <span class="comment"># Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line">            cur_len = <span class="number">0</span></span><br><span class="line">            max_len = labels.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> cur_len &lt; max_len:</span><br><span class="line">                <span class="comment"># Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line">                span_length = torch.randint(<span class="number">1</span>, self.max_span_length + <span class="number">1</span>, (<span class="number">1</span>,)).item()</span><br><span class="line">                <span class="comment"># Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked</span></span><br><span class="line">                context_length = <span class="built_in">int</span>(span_length / self.plm_probability)</span><br><span class="line">                <span class="comment"># Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line">                start_index = cur_len + torch.randint(context_length - span_length + <span class="number">1</span>, (<span class="number">1</span>,)).item()</span><br><span class="line">                masked_indices[i, start_index : start_index + span_length] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># Set `cur_len = cur_len + context_length`</span></span><br><span class="line">                cur_len += context_length</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Since we&#x27;re replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,</span></span><br><span class="line">            <span class="comment"># the i-th predict corresponds to the i-th token.</span></span><br><span class="line">            target_mapping[i] = torch.eye(labels.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = torch.tensor(</span><br><span class="line">            [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()],</span><br><span class="line">            dtype=torch.<span class="built_in">bool</span>,</span><br><span class="line">        )</span><br><span class="line">        masked_indices.masked_fill_(special_tokens_mask, value=<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels.eq(self.tokenizer.pad_token_id)</span><br><span class="line">            masked_indices.masked_fill_(padding_mask, value=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.</span></span><br><span class="line">        non_func_mask = ~(padding_mask | special_tokens_mask)</span><br><span class="line"></span><br><span class="line">        inputs[masked_indices] = self.tokenizer.mask_token_id</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        perm_mask = torch.zeros((labels.size(<span class="number">0</span>), labels.size(<span class="number">1</span>), labels.size(<span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.size(<span class="number">0</span>)):</span><br><span class="line">            <span class="comment"># Generate permutation indices i.e. sample a random factorisation order for the sequence. This will</span></span><br><span class="line">            <span class="comment"># determine which tokens a given token can attend to (encoded in `perm_mask`).</span></span><br><span class="line">            <span class="comment"># Note: Length of token sequence being permuted has to be less than or equal to reused sequence length</span></span><br><span class="line">            <span class="comment"># (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,</span></span><br><span class="line">            <span class="comment"># we assume that reused length is half of sequence length and permutation length is equal to reused length.</span></span><br><span class="line">            <span class="comment"># This requires that the sequence length be even.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a linear factorisation order</span></span><br><span class="line">            perm_index = torch.arange(labels.size(<span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Split this into two halves, assuming that half the sequence is reused each time</span></span><br><span class="line">            perm_index = perm_index.reshape((-<span class="number">1</span>, labels.size(<span class="number">1</span>) // <span class="number">2</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># Permute the two halves such that they do not cross over</span></span><br><span class="line">            perm_index = perm_index[torch.randperm(labels.size(<span class="number">1</span>) // <span class="number">2</span>)]</span><br><span class="line">            <span class="comment"># Flatten this out into the desired permuted factorisation order</span></span><br><span class="line">            perm_index = torch.flatten(perm_index.transpose(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Set the permutation indices of non-masked (non-functional) tokens to the</span></span><br><span class="line">            <span class="comment"># smallest index (-1) so that:</span></span><br><span class="line">            <span class="comment"># (1) They can be seen by all other positions</span></span><br><span class="line">            <span class="comment"># (2) They cannot see masked positions, so there won&#x27;t be information leak</span></span><br><span class="line">            perm_index.masked_fill_(~masked_indices[i] &amp; non_func_mask[i], -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># The logic for whether the i-th token can attend on the j-th token based on the factorisation order:</span></span><br><span class="line">            <span class="comment"># 0 (can attend): If perm_index[i] &gt; perm_index[j] or j is neither masked nor a functional token</span></span><br><span class="line">            <span class="comment"># 1 (cannot attend): If perm_index[i] &lt;= perm_index[j] and j is either masked or a functional token</span></span><br><span class="line">            perm_mask[i] = (</span><br><span class="line">                perm_index.reshape((labels.size(<span class="number">1</span>), <span class="number">1</span>)) &lt;= perm_index.reshape((<span class="number">1</span>, labels.size(<span class="number">1</span>)))</span><br><span class="line">            ) &amp; masked_indices[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inputs.long(), perm_mask, target_mapping, labels.long()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tf_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:</span></span><br><span class="line"><span class="string">            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line"><span class="string">            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line"><span class="string">            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be</span></span><br><span class="line"><span class="string">               masked</span></span><br><span class="line"><span class="string">            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -</span></span><br><span class="line"><span class="string">               span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line"><span class="string">            4. Set `cur_len = cur_len + context_length`. If `cur_len &lt; max_len` (i.e. there are tokens remaining in the</span></span><br><span class="line"><span class="string">               sequence to be processed), repeat from Step 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for permutation language modeling. Please add a mask token if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tf.shape(inputs)[<span class="number">1</span>] % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        labels = tf.identity(inputs)</span><br><span class="line">        <span class="comment"># Creating the mask and target_mapping tensors</span></span><br><span class="line">        masked_indices = np.full(labels.shape.as_list(), <span class="number">0</span>, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line">        labels_shape = tf.shape(labels)</span><br><span class="line">        target_mapping = np.zeros((labels_shape[<span class="number">0</span>], labels_shape[<span class="number">1</span>], labels_shape[<span class="number">1</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">            <span class="comment"># Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line">            cur_len = <span class="number">0</span></span><br><span class="line">            max_len = tf.shape(labels)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> cur_len &lt; max_len:</span><br><span class="line">                <span class="comment"># Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line">                span_length = randint(<span class="number">1</span>, self.max_span_length + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked</span></span><br><span class="line">                context_length = <span class="built_in">int</span>(span_length / self.plm_probability)</span><br><span class="line">                <span class="comment"># Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line">                start_index = cur_len + randint(<span class="number">0</span>, context_length - span_length + <span class="number">1</span>)</span><br><span class="line">                masked_indices[i, start_index : start_index + span_length] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># Set `cur_len = cur_len + context_length`</span></span><br><span class="line">                cur_len += context_length</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Since we&#x27;re replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,</span></span><br><span class="line">            <span class="comment"># the i-th predict corresponds to the i-th token.</span></span><br><span class="line">            target_mapping[i] = np.eye(labels_shape[<span class="number">1</span>])</span><br><span class="line">        masked_indices = tf.cast(tf.convert_to_tensor(masked_indices), dtype=tf.<span class="built_in">bool</span>)</span><br><span class="line">        target_mapping = tf.convert_to_tensor(target_mapping)</span><br><span class="line">        special_tokens_mask = tf.convert_to_tensor(</span><br><span class="line">            [</span><br><span class="line">                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> val <span class="keyword">in</span> labels.numpy().tolist()</span><br><span class="line">            ],</span><br><span class="line">        )</span><br><span class="line">        special_tokens_mask = tf.cast(special_tokens_mask, dtype=tf.<span class="built_in">bool</span>)</span><br><span class="line">        masked_indices = masked_indices &amp; ~special_tokens_mask</span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices = masked_indices &amp; ~padding_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.</span></span><br><span class="line">        non_func_mask = ~(padding_mask | special_tokens_mask)</span><br><span class="line"></span><br><span class="line">        inputs = tf.where(masked_indices, self.tokenizer.mask_token_id, inputs)</span><br><span class="line">        labels = tf.where(masked_indices, labels, -<span class="number">100</span>)  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        perm_mask = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">            <span class="comment"># Generate permutation indices i.e. sample a random factorisation order for the sequence. This will</span></span><br><span class="line">            <span class="comment"># determine which tokens a given token can attend to (encoded in `perm_mask`).</span></span><br><span class="line">            <span class="comment"># Note: Length of token sequence being permuted has to be less than or equal to reused sequence length</span></span><br><span class="line">            <span class="comment"># (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,</span></span><br><span class="line">            <span class="comment"># we assume that reused length is half of sequence length and permutation length is equal to reused length.</span></span><br><span class="line">            <span class="comment"># This requires that the sequence length be even.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a linear factorisation order</span></span><br><span class="line">            <span class="comment"># tf.range is the equivalent of torch.arange</span></span><br><span class="line">            perm_index = tf.<span class="built_in">range</span>(labels_shape[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># Split this into two halves, assuming that half the sequence is reused each time</span></span><br><span class="line">            perm_index = tf.transpose(tf.reshape(perm_index, (-<span class="number">1</span>, labels_shape[<span class="number">1</span>] // <span class="number">2</span>)))</span><br><span class="line">            <span class="comment"># Permute the two halves such that they do not cross over</span></span><br><span class="line">            perm_index = tf.random.shuffle(perm_index)  <span class="comment"># Shuffles along the first dimension</span></span><br><span class="line">            <span class="comment"># Flatten this out into the desired permuted factorisation order</span></span><br><span class="line">            perm_index = tf.reshape(tf.transpose(perm_index), (-<span class="number">1</span>,))</span><br><span class="line">            <span class="comment"># Set the permutation indices of non-masked (non-functional) tokens to the</span></span><br><span class="line">            <span class="comment"># smallest index (-1) so that:</span></span><br><span class="line">            <span class="comment"># (1) They can be seen by all other positions</span></span><br><span class="line">            <span class="comment"># (2) They cannot see masked positions, so there won&#x27;t be information leak</span></span><br><span class="line">            perm_index = tf.where(~masked_indices[i] &amp; non_func_mask[i], -<span class="number">1</span>, perm_index)</span><br><span class="line">            <span class="comment"># The logic for whether the i-th token can attend on the j-th token based on the factorisation order:</span></span><br><span class="line">            <span class="comment"># 0 (can attend): If perm_index[i] &gt; perm_index[j] or j is neither masked nor a functional token</span></span><br><span class="line">            <span class="comment"># 1 (cannot attend): If perm_index[i] &lt;= perm_index[j] and j is either masked or a functional token</span></span><br><span class="line">            perm_mask.append(</span><br><span class="line">                (tf.reshape(perm_index, (labels_shape[<span class="number">1</span>], <span class="number">1</span>)) &lt;= tf.reshape(perm_index, (<span class="number">1</span>, labels_shape[<span class="number">1</span>])))</span><br><span class="line">                &amp; masked_indices[i]</span><br><span class="line">            )</span><br><span class="line">        perm_mask = tf.stack(perm_mask, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.cast(inputs, tf.int64), tf.cast(perm_mask, tf.float32), target_mapping, tf.cast(labels, tf.int64)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numpy_mask_tokens</span>(<span class="params">self, inputs: <span class="type">Any</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>, <span class="type">Any</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:</span></span><br><span class="line"><span class="string">            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line"><span class="string">            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line"><span class="string">            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be</span></span><br><span class="line"><span class="string">               masked</span></span><br><span class="line"><span class="string">            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -</span></span><br><span class="line"><span class="string">               span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line"><span class="string">            4. Set `cur_len = cur_len + context_length`. If `cur_len &lt; max_len` (i.e. there are tokens remaining in the</span></span><br><span class="line"><span class="string">               sequence to be processed), repeat from Step 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer.mask_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This tokenizer does not have a mask token which is necessary for permutation language modeling. Please add a mask token if you want to use this tokenizer.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs.shape[<span class="number">1</span>] % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        labels = np.copy(inputs)</span><br><span class="line">        <span class="comment"># Creating the mask and target_mapping tensors</span></span><br><span class="line">        masked_indices = np.full(labels.shape, <span class="number">0</span>, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line">        target_mapping = np.zeros((labels.shape[<span class="number">0</span>], labels.shape[<span class="number">1</span>], labels.shape[<span class="number">1</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).</span></span><br><span class="line">            cur_len = <span class="number">0</span></span><br><span class="line">            max_len = labels.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> cur_len &lt; max_len:</span><br><span class="line">                <span class="comment"># Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)</span></span><br><span class="line">                span_length = randint(<span class="number">1</span>, self.max_span_length + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked</span></span><br><span class="line">                context_length = <span class="built_in">int</span>(span_length / self.plm_probability)</span><br><span class="line">                <span class="comment"># Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`</span></span><br><span class="line">                start_index = cur_len + randint(<span class="number">0</span>, context_length - span_length + <span class="number">1</span>)</span><br><span class="line">                masked_indices[i, start_index : start_index + span_length] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># Set `cur_len = cur_len + context_length`</span></span><br><span class="line">                cur_len += context_length</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Since we&#x27;re replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,</span></span><br><span class="line">            <span class="comment"># the i-th predict corresponds to the i-th token.</span></span><br><span class="line">            target_mapping[i] = np.eye(labels.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        special_tokens_mask = np.array(</span><br><span class="line">            [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=<span class="literal">True</span>) <span class="keyword">for</span> val <span class="keyword">in</span> labels.tolist()],</span><br><span class="line">            dtype=np.<span class="built_in">bool</span>,</span><br><span class="line">        )</span><br><span class="line">        masked_indices[special_tokens_mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer._pad_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            padding_mask = labels == self.tokenizer.pad_token_id</span><br><span class="line">            masked_indices[padding_mask] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.</span></span><br><span class="line">        non_func_mask = ~(padding_mask | special_tokens_mask)</span><br><span class="line"></span><br><span class="line">        inputs[masked_indices] = self.tokenizer.mask_token_id</span><br><span class="line">        labels[~masked_indices] = -<span class="number">100</span>  <span class="comment"># We only compute loss on masked tokens</span></span><br><span class="line"></span><br><span class="line">        perm_mask = np.zeros((labels.shape[<span class="number">0</span>], labels.shape[<span class="number">1</span>], labels.shape[<span class="number">1</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labels.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># Generate permutation indices i.e. sample a random factorisation order for the sequence. This will</span></span><br><span class="line">            <span class="comment"># determine which tokens a given token can attend to (encoded in `perm_mask`).</span></span><br><span class="line">            <span class="comment"># Note: Length of token sequence being permuted has to be less than or equal to reused sequence length</span></span><br><span class="line">            <span class="comment"># (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,</span></span><br><span class="line">            <span class="comment"># we assume that reused length is half of sequence length and permutation length is equal to reused length.</span></span><br><span class="line">            <span class="comment"># This requires that the sequence length be even.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a linear factorisation order</span></span><br><span class="line">            perm_index = np.arange(labels.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># Split this into two halves, assuming that half the sequence is reused each time</span></span><br><span class="line">            perm_index = perm_index.reshape((-<span class="number">1</span>, labels.shape[<span class="number">1</span>] // <span class="number">2</span>)).T</span><br><span class="line">            <span class="comment"># Permute the two halves such that they do not cross over</span></span><br><span class="line">            np.random.shuffle(perm_index)</span><br><span class="line">            <span class="comment"># Flatten this out into the desired permuted factorisation order</span></span><br><span class="line">            perm_index = perm_index.T.flatten()</span><br><span class="line">            <span class="comment"># Set the permutation indices of non-masked (non-functional) tokens to the</span></span><br><span class="line">            <span class="comment"># smallest index (-1) so that:</span></span><br><span class="line">            <span class="comment"># (1) They can be seen by all other positions</span></span><br><span class="line">            <span class="comment"># (2) They cannot see masked positions, so there won&#x27;t be information leak</span></span><br><span class="line">            perm_index[~masked_indices[i] &amp; non_func_mask[i]] = -<span class="number">1</span></span><br><span class="line">            <span class="comment"># The logic for whether the i-th token can attend on the j-th token based on the factorisation order:</span></span><br><span class="line">            <span class="comment"># 0 (can attend): If perm_index[i] &gt; perm_index[j] or j is neither masked nor a functional token</span></span><br><span class="line">            <span class="comment"># 1 (cannot attend): If perm_index[i] &lt;= perm_index[j] and j is either masked or a functional token</span></span><br><span class="line">            perm_mask[i] = (</span><br><span class="line">                perm_index.reshape((labels.shape[<span class="number">1</span>], <span class="number">1</span>)) &lt;= perm_index.reshape((<span class="number">1</span>, labels.shape[<span class="number">1</span>]))</span><br><span class="line">            ) &amp; masked_indices[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inputs.astype(np.int64), perm_mask, target_mapping, labels.astype(np.int64)</span><br></pre></td></tr></table></figure>
<h3 id="Two-stream-self-attention"><a href="#Two-stream-self-attention" class="headerlink" title="Two-stream self-attention"></a>Two-stream self-attention</h3><p>The next-token distribution with the standard softmax formulation:</p>
<script type="math/tex; mode=display">p_\theta(X_{z_t}=x \vert \pmb{x}_{\pmb{z}<t}) = \frac{\exp \big( e(x)^T h_\theta (\pmb{x}_{\pmb{z}<t}) \big)}{\sum_{x'}\exp \big( e(x')^T h_\theta (\pmb{x}_{\pmb{z}<t}) \big)}</script><p>where <script type="math/tex">h_\theta(\pmb{z}_{\pmb{z}<t})</script>, abbr. <script type="math/tex">h_{z_t}</script>, denotes <strong>content representation</strong>, which encodes both context and <script type="math/tex">x_{z_t}</script> itself, as hidden states in Transformer, i.e. standard self-attention, see below figure(a).</p>
<p><img data-src='/notes/images/content-stream-attention.png' width='50%'/></p>
<p>However, the previous $t-1$ sequence cannot implies the unique predicted target since different target words might have the same previous sequence in the permutated AR factorization. Hence, XLNet also consider the target position information:</p>
<script type="math/tex; mode=display">p_\theta(X_{z_t}=x \vert \pmb{x}_{z<t}) = \frac{\exp\left( e(x)^T g_\theta(\pmb{x}_{\pmb{z}<t}, \color{red}{z_t}) \right)}{\sum_{x'} \exp\left( e(x')^T g_\theta(\pmb{x}_{\pmb{z}<t}, \color{red}{z_t}) \right)}</script><p>where <script type="math/tex">g_\theta(\pmb{x}_{\pmb{z}<t}, z_t)</script>, abbr. <script type="math/tex">g_{z_t}</script> denotes a <strong>query representation</strong>, only using the position <script type="math/tex">\color{red}{z_t}</script> and not the context <script type="math/tex">\mathbf{x_{z_t}}</script>, as below figure(b).</p>
<p><img data-src='/notes/images/query-stream-attention.png' width='50%'/></p>
<ul>
<li><strong>query stream</strong> uses <script type="math/tex">z_t</script> but cannot see <script type="math/tex">x_{z_t}</script>:<script type="math/tex; mode=display">g_{z_t}^m \leftarrow \text{attention}\big(\pmb{Q}=g_{z_t}^{(m-1)}, \pmb{KV} = \pmb{h}_{\pmb{z}<t}^{(m-1)} ;\theta \big)</script></li>
<li><strong>content stream</strong> uses both <script type="math/tex">z_t</script> and <script type="math/tex">x_{z_t}</script>:<script type="math/tex; mode=display">h_{z_t}^m \leftarrow \text{attention}\big(\pmb{Q}=h_{z_t}^{(m-1)}, \pmb{KV} = \pmb{h}_{\pmb{z}\leq t}^{(m-1)} ;\theta \big)</script></li>
</ul>
<p>Here $\pmb{Q}$,$\pmb{K}$,$\pmb{V}$ denot the query, key, value in an attention op.<br><div class="note info">
            <ul><li>During finetuning, we can simply <strong>drop the query stream</strong> and <strong>use the content stream</strong> as a normal Transformer(-XL).</li><li>The permutation implementation is relying on the <strong>attention mask</strong>, as shown in the figure, which does not affect the original sequecen orders.</li></ul>
          </div></p>
<p><img data-src="/notes/images/XLNet-two-stream-attention.png" alt="upload successful"></p>
<h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h2><p>Borrow <strong>relative positional encoding</strong> and <strong>segment recurrence mechanism</strong> from Transformer-XL<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). [Transformer-xl: Attentive language models beyond a fixed-length context](https://arxiv.org/pdf/1901.02860). arXiv preprint arXiv:1901.02860.
">[2]</span></a></sup>.<br>The next segment with memory is:</p>
<script type="math/tex; mode=display">h_{z_t}^{(m)} \leftarrow \text{Att}( \mathbf{Q}= h_{z_t}^{(m-1)}, \, \mathbf{KV}= \big[ \mathbf{\tilde{h}}^{(m-1)}, \color{green}{\mathbf{h}_{z \leq t}^{(m-1)}} \big] ; \theta )</script><h2 id="Relative-segment-encoding"><a href="#Relative-segment-encoding" class="headerlink" title="Relative segment encoding"></a>Relative segment encoding</h2><p>XLnet only considers ‘’whether the two positions in segments are <strong>within the same segment</strong> as opposed to considering <em>which specific segments they are from</em>‘’. </p>
<p>The idea of relative encodings is only modeling the relationships between positions <script type="math/tex">s_{ij}</script>, denoting the segment encoding beween position i to j.<br>The attention weight <script type="math/tex">a_{ij} = (\mathbf{q}_i + \mathbf{b})^\top s_{ij}</script>, where <script type="math/tex">\mathbf{q}_i</script> is the query vector in std attention and $\mathbf{b}$ is a learnable head-specific bias vector. Finally add <script type="math/tex">a_{ij}</script> to the normal attention weight.</p>
<p>The advantage of relative segment encodings:</p>
<ol>
<li>to introduce inductive biases to improve generalization;</li>
<li>to allow for the multiple input segments in finetuning on tasks.</li>
</ol>
<h1 id="RoBERTa-“BERT-is-undertrained”"><a href="#RoBERTa-“BERT-is-undertrained”" class="headerlink" title="RoBERTa: “BERT is undertrained”!"></a>RoBERTa: “BERT is undertrained”!</h1><p>RoBERTa<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). [Roberta: A robustly optimized bert pretraining approach](https://arxiv.org/pdf/1907.11692). arXiv preprint arXiv:1907.11692.
">[5]</span></a></sup> (Fair &amp; UW 2019) (<u><b>R</b></u>obustly <u><b>o</b></u>ptimized <u><b>BERT</b></u> <u><b>a</b></u>pproach) redesigned the BERT experiments<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91). arXiv preprint arXiv:1810.04805.
">[6]</span></a></sup>, illustrating <strong>BERT is underfitted</strong>. It showed that BERT pretraining with a larger batch size over more data for more training steps could lead to a better pretraining results. </p>
<div class="note info">
            <p>Recent works<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237). arXiv preprint arXiv:1906.08237.">[1]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). [Roberta: A robustly optimized bert pretraining approach](https://arxiv.org/pdf/1907.11692). arXiv preprint arXiv:1907.11692.">[5]</span></a></sup> questioned the effectiveness of Next Sentence Prediction (NSP) pretraining task proposed by BERT<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91). arXiv preprint arXiv:1810.04805.">[6]</span></a></sup>.</p>
          </div>
<h1 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h1><p>SpanBERT<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., & Levy, O. (2019). [Spanbert: Improving pre-training by representing and predicting spans](https://arxiv.org/pdf/1907.10529). arXiv preprint arXiv:1907.10529.
">[7]</span></a></sup> (UW &amp; Fair) proposed a span-level pretraining approach by <strong>masking contiguous random spans</strong> rather than individual tokens as in BERT. It consistently surpass BERT and substantially outweights on span selection tasks involving question answering and coreference resolution. The NSP auxiliary objective is removed.</p>
<div class="note warning">
            <p>In comparison, </p><ul><li>The concurrent work <strong>ERNIE</strong><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., ... & Wu, H. (2019). [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223). arXiv preprint arXiv:1904.09223.">[8]</span></a></sup> (Baidu 2019) that masked linguistically-informed spans in Chinese, i.e. masking phrase and named entity, achieve improvements on Chinese NLP tasks.</li></ul>
          </div>
<h2 id="Span-masking"><a href="#Span-masking" class="headerlink" title="Span masking"></a>Span masking</h2><p>At each iteration, the span’s length is samplled from a geometric distribution $\mathscr{l} \sim Geo(p) = (1-p)^{(k-1)} p$; the starting point of spans are uniformly random selected from the sequence. (In SpanBERT, p=0.2, and clip <script type="math/tex">l_\max = 10</script>.) </p>
<p>15% of tokens <strong>in span-level</strong> are masked: of which masking 80%, replacing 10% with noise, keeping the rest 10%.</p>
<p><img data-src="/notes/images/SpanBERT.png" alt="upload successful"></p>
<h2 id="Span-boundary-objective-SBO"><a href="#Span-boundary-objective-SBO" class="headerlink" title="Span boundary objective (SBO)"></a>Span boundary objective (SBO)</h2><p>Given a masked span <script type="math/tex">(x_s, \cdots, x_e) \in Y</script>, where (s,e) denotes the start and ending positions. Each token <script type="math/tex">x_i</script> in the span are represented using the encodings of the outside boundary tokens <script type="math/tex">x_{s-1}</script> and <script type="math/tex">x_{e+1}</script> (i.e., <script type="math/tex">x_4</script> and <script type="math/tex">x_9</script> in the figure) and the target positional embedding of target token <script type="math/tex">\mathbf{p}_i</script>, that is:</p>
<script type="math/tex; mode=display">\mathbf{y}_i = f( \mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_i)</script><p>where $f(\cdot)$ indicates the 2-layer FFNN with layer normalizations and Gelu activations.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbf{h} & = \text{LayerNorm} (\text{Gelu} (W_q \cdot [\mathbf{x}_{s-1}; \mathbf{x}_{e+1}; \mathbf{p}_{i}] )) \\
f(\cdot) & =  \text{LayerNorm} (\text{Gelu} (W_2 \cdot \mathbf{h}))
\end{align}</script><p>The representions of span tokens <script type="math/tex">\mathbf{y}_i</script> is used to predict <script type="math/tex">\mathbf{x})i</script> and compute the corss entropy loss like MLM objective in BERT.</p>
<h1 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h1><p>ALBERT<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). [Albert: A lite bert for self-supervised learning of language representations](https://arxiv.org/pdf/1909.11942). arXiv preprint arXiv:1909.11942.
">[9]</span></a></sup> (<strong>A</strong> <strong>L</strong>ite <strong>BERT</strong>) (Google 2019) adopted <strong>factorized embedding parameterization</strong> and <strong>cross-layer parameter sharing</strong> techiniques to reuduce the memory cost of BERT architecture.</p>
<h2 id="Factorized-embedding-parameterization"><a href="#Factorized-embedding-parameterization" class="headerlink" title="Factorized embedding parameterization"></a>Factorized embedding parameterization</h2><p>ALBERT decomposed the embedding parameters with higher dimension to smaller matrices, by firstly projecting the inputs into a lower dimensional embedding of size E, followed by the second projection to the hidden space. The embedding paprameters are reduced from $O(V \times H)$ to $O(V \times E + E \times H)$, which is obvious when $H \gg E$</p>
<h2 id="Cross-layer-parameter-sharing"><a href="#Cross-layer-parameter-sharing" class="headerlink" title="Cross-layer parameter sharing"></a>Cross-layer parameter sharing</h2><p>All parameters across layers on both self-attentions and FFNs are shared. It is empirically showed that the L2 distance and cosine similarity between the input and output are oscillating rather than converging, which is different than that in Deep Equilibrium Model (DEQ)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Deep Equilibrium Models](https://arxiv.org/pdf/1909.01377.pdf). arXiv 2019
">[10]</span></a></sup>.</p>
<h2 id="Sentence-order-prediction-SOP"><a href="#Sentence-order-prediction-SOP" class="headerlink" title="Sentence-order prediction (SOP)"></a>Sentence-order prediction (SOP)</h2><p>ALBERT use two consecutive setences as positive samples as in NSP, and <strong>swap the order of the same ajacent segments directly as the negative samples</strong>, consistently showing a better results for multi-sentence encoding tasks.</p>
<h1 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h1><p>ELECTRA<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ELECTRA: Pre-training Text Encoders
as Discriminators rather than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)
">[11]</span></a></sup> (<strong>E</strong>fficiently <strong>L</strong>earning an <strong>E</strong>ncoder that <strong>C</strong>lassifies <strong>T</strong>oken <strong>R</strong>eplacements <strong>A</strong>ccurately) (Standford NLP) proposed a more sample-efficient pre-training approach, <strong>replaced token detection</strong> to efficiently boost the pretraining efficiency, which solves the pretraining-finetuning discrepancy led by [MASK] symbols.</p>
<p><img data-src="/notes/images/ELECTRA-replaced-token-detection.png" alt="upload successful"></p>
<h2 id="Replaced-token-detection"><a href="#Replaced-token-detection" class="headerlink" title="Replaced token detection"></a>Replaced token detection</h2><ul>
<li>Rather than randomly masked tokens with the probability 15% as in BERT, <strong>replaced token detection</strong> replaces tokens with plausible alternatives that sampled from the output of a small generator network.</li>
<li>Then adopt a discriminator to predict whether each token was corrupted with a sampled replacement.</li>
</ul>
<p><img data-src='/notes/images/ELECTRA-approach.png' width='80%'/></p>
<p>ELECTRA trains two NNs, a generator $G$ and a discriminator $D$. Each one primarily consists of an encoder that maps a sequence on input tokens <script type="math/tex">\mathbf{x}= [x_1,\cdots,x_n]</script> into the contextual representation <script type="math/tex">h(\mathbf{x}) = [h_1, \cdots, h_n]</script>. </p>
<ol>
<li><p>The generator is used to to do Masked Language Model (MLM) as in BERT<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91). arXiv preprint arXiv:1810.04805.
">[6]</span></a></sup>. For the position $t$, the generator outputs the distribution of <script type="math/tex">\mathbf{x}_t</script> via a softmax layer:</p>
<script type="math/tex; mode=display">p_G (x_t \vert \mathbf{x}) = \frac{\exp (e(x_t)\top h_G(\mathbf{x})_t)}{\sum_{x^\prime} \exp(e(x^\prime)^\top h_G(\mathbf{x})_t)}</script><p>where $e$ is the word embeddings.</p>
</li>
<li><p>For the discriminator $\mathscr{D}$, it discriminates whether the token <script type="math/tex">x_t</script> at position $t$ is replaced.</p>
<script type="math/tex; mode=display">\mathscr{D}(\mathbf{x},t) = \sigma (w\top h_D(\mathbf{x})_t)</script></li>
</ol>
<div class="note success">
            <ul><li>MLM of BERT first randomly selects the  positions to mask <script type="math/tex">\mathbf{m} = [m_1, \cdots, m_k]</script>, wherein tokens at masked positions are replaced with a [MASK] token:<script type="math/tex; mode=display">\begin{align}m_i  &\sim \text{Uniform}\{1,n\} \\\mathbf{x}^\text{masked} &= \text{Replace}(\mathbf{x}, \mathbf{m}, \text{[MASK]})\end{align}</script></li><li>In contrast, the replaced token detection uses the generator G to learn the MLE of masked tokens whilst the discriminator D is applied to detect the fakeness.<script type="math/tex; mode=display">\begin{align}\color{red}{\hat{x}_i}  &\sim p_G(x_i \vert \mathbf{x}^\text{masked}) \; \text{for}\, i\in \mathbf{m} \\\mathbf{x}^\text{corrupted} &= \text{Replace}(\mathbf{x}, \mathbf{m}, \color{red}{\hat{\mathbf{x}}})\end{align}</script></li></ul>
          </div>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>The loss functions are:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}_\text{MLM}(\mathbf{x}, \theta_G) &= \mathbb{E} \bigg( \sum_{i \in \mathbf{m}} - \log p_G (x_i \vert \mathbf{x}^\text{masked}) \bigg) \\
\mathcal{L}_\text{D} (\mathbf{x}, \theta_D) &= \mathbb{E} \bigg( \sum_{t=1}^n \mathbb{I} (x_t^\text{corrupt} = x_t) \log D(\mathbf{x}^\text{corrupt}, t) + \mathbb{I} (x_t^\text{corrupt} \neq x_t) \log ( 1- D\big(\mathbf{x}^\text{corrupt}, t\big) ) \bigg)
\end{align}</script><p>The combined loss is minimized:</p>
<script type="math/tex; mode=display">\min_{\theta_G, \theta_D} \sum_{\mathbb{x} \in \chi} \mathcal{L}_\text{MLM} (\mathbf{x}, \theta_G) + \lambda \mathcal{L}_\text{D}(\mathbf{x}, \theta_D)</script><p>where $\chi$ denotes the corpus.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><h4 id="Weight-sharing"><a href="#Weight-sharing" class="headerlink" title="Weight sharing"></a>Weight sharing</h4><ul>
<li>Share the embeddings (both token embeddings and position embeddings) of the generator and discriminator.</li>
<li>Weight tying strategy<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., & Wolf, L. (2016). [Using the output embedding to improve language models](https://arxiv.org/pdf/1608.05859). arXiv preprint arXiv:1608.05859.
">[12]</span></a></sup> -&gt; only tied embeddings.</li>
</ul>
<h4 id="Two-stage-training"><a href="#Two-stage-training" class="headerlink" title="Two-stage training"></a>Two-stage training</h4><ol>
<li>Train only the geenrator with <script type="math/tex">\mathcal{L}_\text{MLM}</script> for $n$ steps</li>
<li>Initialize the weights of the $D$ with $G$ and train $D$ with <script type="math/tex">\mathcal{L}_\text{Disc}</script> for $n$ steps, keeping the generator’s weight frozen.</li>
</ol>
<p>After pretraining, throw away the generator and fine-tune the discriminator on downstream tasks.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., &amp; Le, Q. V. (2019). <a href="https://arxiv.org/pdf/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>. arXiv preprint arXiv:1906.08237.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., &amp; Salakhutdinov, R. (2019). <a href="https://arxiv.org/pdf/1901.02860">Transformer-xl: Attentive language models beyond a fixed-length context</a>. arXiv preprint arXiv:1901.02860.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shaw, P., Uszkoreit, J., &amp; Vaswani, A. (2018). <a href="https://arxiv.org/pdf/1803.02155">Self-attention with relative position representations</a>. arXiv preprint arXiv:1803.02155.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... &amp; Stoyanov, V. (2019). <a href="https://arxiv.org/pdf/1907.11692">Roberta: A robustly optimized bert pretraining approach</a>. arXiv preprint arXiv:1907.11692.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <a href="https://arxiv.org/pdf/1810.04805.pdf%E3%80%91">Bert: Pre-training of deep bidirectional transformers for language understanding</a>. arXiv preprint arXiv:1810.04805.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., &amp; Levy, O. (2019). <a href="https://arxiv.org/pdf/1907.10529">Spanbert: Improving pre-training by representing and predicting spans</a>. arXiv preprint arXiv:1907.10529.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., ... &amp; Wu, H. (2019). <a href="https://arxiv.org/pdf/1904.09223">ERNIE: Enhanced Representation through Knowledge Integration</a>. arXiv preprint arXiv:1904.09223.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019). <a href="https://arxiv.org/pdf/1909.11942">Albert: A lite bert for self-supervised learning of language representations</a>. arXiv preprint arXiv:1909.11942.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1909.01377.pdf">Deep Equilibrium Models</a>. arXiv 2019<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://openreview.net/pdf?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders
as Discriminators rather than Generators</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, O., &amp; Wolf, L. (2016). <a href="https://arxiv.org/pdf/1608.05859">Using the output embedding to improve language models</a>. arXiv preprint arXiv:1608.05859.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/zihangdai/xlnet/blob/master/data_utils.py">GitHub: XLNet</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Language model</category>
        <category>BERT</category>
      </categories>
      <tags>
        <tag>Pre-training</tag>
        <tag>NLP</tag>
        <tag>Language model</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>POS Tagging with HMMs</title>
    <url>/notes/2019/03/04/NLP/PoS-tagging-with-HMMs/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>An introduction of Part-of-Speech tagging using <code>Hidden Markov Model</code> (HMMs).<br><span id="more"></span></p>
<h2 id="Markov-chains"><a href="#Markov-chains" class="headerlink" title="Markov chains"></a>Markov chains</h2><p>Consider a sequence of state variables <script type="math/tex">q_1, q_2, ..., q_i</script>. A first-order Markov model instantiate two simplifying assumptions.</p>
<ol>
<li>The probability of a state depends only the previous state.<script type="math/tex; mode=display">\text{Markov Assumption:}\quad P(q_i=a \vert q_1 ... q_{i-1}) = P(q_i = a \vert q_{i-1})</script></li>
<li>The probability of an output observation <script type="math/tex">o_i</script> depends only on the current state that produced the observation <script type="math/tex">q_i</script>:<script type="math/tex; mode=display">\text{Output Independence:}\quad P(o_i \vert q_1 \cdots q_i, \cdots, q_T, o_1, \cdots, o_i, \cdots,o_T) = P(o_i \vert q_i)</script></li>
</ol>
<h2 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a>Hidden Markov Model</h2><div class="note info">
            <ul><li>A set of $N$ <strong>states</strong>: <script type="math/tex">Q = q_1 q_2 \cdots q_N</script> </li><li>A <strong>transition probability matrix</strong> <script type="math/tex">A</script>, each <script type="math/tex">a_{ij}</script> representing the probability of moving from state $i$ to state $j$, s.t. <script type="math/tex">\sum_{j=1}^N a_{ij} =1</script>:<script type="math/tex; mode=display">A=a_{11} \cdots a_{ij} \cdots a_{NN}</script></li><li>A sequence of $T$ <strong>observations</strong>, each one drawn from a vocabulary <script type="math/tex">V = v_1,v_2,\cdots,v_V</script>:<script type="math/tex; mode=display">O=o_1 o_2 \cdots o_T</script></li><li>A sequence of <strong>observation likelihoods</strong>, a.k.a. <strong>emission probabilities</strong>, each expressing the probability of an observation <script type="math/tex">o_t</script>, being generated from a state $i$:<script type="math/tex; mode=display">B=b_i(o_t)</script></li><li>An <strong>initial probability distribution</strong> over states. <script type="math/tex">\pi_i</script> is the prob. that the Markov chain will start in state $i$. Some state $j$ may have <script type="math/tex">\pi_j = 0</script>, meaning that they cannot be initial states. Also, <script type="math/tex">\sum_{i=1}^n \pi_i = 1</script>:<script type="math/tex; mode=display">\pi = \pi_1 ,\pi_2,\cdots,\pi_N</script></li></ul>
          </div>
<h2 id="Training-with-MLE"><a href="#Training-with-MLE" class="headerlink" title="Training with MLE"></a>Training with MLE</h2><ul>
<li><p>The initial probability $\pi$:</p>
<script type="math/tex; mode=display">\pi(t) = \frac{C(\text{the tag occurs in the sentence beginning})}{\text{count of sentences}}</script></li>
<li><p>The transition probability matrix $A$ contains <script type="math/tex">P(t_i \vert t_{i-1})</script>:</p>
<script type="math/tex; mode=display">P(t_i \vert t_{i-1}) = \frac{C(t_{i-1}, t_i)}{C(t_{i-1})}</script><p>where <script type="math/tex">C(t_{i-1})</script> means the count of the <span class="label info">first</span> word’s pos tag in bigram tuples. </p>
</li>
<li><p>The emission probability matrix $B$ contains <script type="math/tex">P(w_i \vert t_i)</script>:</p>
<script type="math/tex; mode=display">P(w_i \vert t_i) = \frac{C(t_i, w_i)}{C(t_i)}</script></li>
</ul>
<h3 id="Handling-OOV-problems"><a href="#Handling-OOV-problems" class="headerlink" title="Handling OOV problems"></a>Handling OOV problems</h3><h2 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h2><p>Given as input an HMM $\lambda = (A, B)$ and a sequence of observations <script type="math/tex">O = o_1, o_2,...,o_T</script>, find the most probable sequence of states <script type="math/tex">Q=q_1q_2q_3...q_T</script></p>
<script type="math/tex; mode=display">\hat{t}_1^n = \arg\max_{t_1^n} P(t_1^n \vert w_1^n) \approx \arg\max_{t_1^n} P(w_1^n \vert t_1^n) P(t_1^n) \\ \approx \arg\max_{t_1^n} \prod_{i=1}^n \overbrace{P(t_i \vert t_{i-1})}^{\text{transition}} \overbrace{P(w_i \vert t_i)}^{\text{emission}}</script><h3 id="Viterbi-algorithm"><a href="#Viterbi-algorithm" class="headerlink" title="Viterbi algorithm"></a>Viterbi algorithm</h3><p><img data-src="/notes/images/viterbi-decoding.png" width="100%"/></p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>Sequence labeling</category>
        <category>POS tagging</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>POS tagging</tag>
      </tags>
  </entry>
  <entry>
    <title>Text Classification: An Overview</title>
    <url>/notes/2019/01/18/NLP/Text-classification-overview/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Text classification is one of the most important fundamental NLP tasks. Its goal is to <strong>assign labels to texts</strong>, including sentiment analysis, spam detection, topic labeling, Twitter hashtag prediction, domain detection, etc.<br><span id="more"></span></p>
<h1 id="Sentence-classfication"><a href="#Sentence-classfication" class="headerlink" title="Sentence classfication"></a>Sentence classfication</h1><h2 id="Fasttext-Facebook-2016"><a href="#Fasttext-Facebook-2016" class="headerlink" title="Fasttext (Facebook 2016)"></a>Fasttext (Facebook 2016)</h2><p>A simple and efficient <code>baseline</code> for text classification.</p>
<ul>
<li>Model: average word representations into a text representation, and then feed into a linear classifier (the model architecture is similar to the <em>CBOW model</em>, by replacing the middle word with a label).</li>
<li>Hierarchical softmax</li>
<li>N-gram features: Besides bag-of-word features, <code>bag of n-grams</code> as additional features to <strong>capture partial information about the local word order</strong>.<br>  Hashing trick</li>
</ul>
<p><img data-src="/notes/images/fasttext-model.png" alt="upload successful"></p>
<h2 id="TextCNN-Kim-2014"><a href="#TextCNN-Kim-2014" class="headerlink" title="TextCNN (Kim 2014)"></a>TextCNN (Kim 2014)</h2><p><strong>Background</strong></p>
<ul>
<li>sparse, 1-of-V encoding <script type="math/tex">\rightarrow</script> low dimensional vector space</li>
</ul>
<div class="note default">
            <p>CNNs utilize layers with <strong>convolving filters</strong> that are applied to <strong>loca l features</strong>. (Lecun et al., 1998) <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kim, Y. (2014). [Convolutional neural networks for sentence classification](https://arxiv.org/pdf/1408.5882). arXiv preprint arXiv:1408.5882.">[3]</span></a></sup></p>
          </div>
<p>Let <script type="math/tex">\mathbf{x}_i \in \mathbb{R}^k</script> be the $k$-dimensional word vectors w.r.t. $i$-th word in the sentence. A sentence with length $n$ (padded if necessary) is:</p>
<script type="math/tex; mode=display">\mathbf{x}_{1:n} = \mathbf{x}_{1} \oplus \mathbf{x}_{2} \oplus ... \oplus \mathbf{x}_{n}</script><p>Convolution op applies filters $\mathbf{w} \in \mathbb{R}^{hk}$ to a window of $h$ words to extract new features:</p>
<script type="math/tex; mode=display">c_i=f(\mathbf{w} \cdot \mathbf{x}_{i:i+h-1} + b)</script><p>where $b \in \mathbb{R} $ is a bias term and $f$ is a non-linear function, e.g. hyperbolic tangent.</p>
<p>Thus, for a sentence <script type="math/tex">\{\mathbf{x}_{1:h},\mathbf{x}_{2:h+1},...,\mathbf{x}_{n-h+1:n}\}</script>, generate a feature map $\mathbf{c} \in \mathbb{R}^{n-h+1} $:</p>
<script type="math/tex; mode=display">\mathbf{c} = [c_1, c_2,...,c_{n-h+1}]</script><p>Then apply a <strong>max-over-time pooling</strong> op over the feature map and takes the maximum value <script type="math/tex">\hat{c} = \max \mathbf{c}</script> as the feature w.r.t. this filter. </p>
<div class="note primary">
            <p><strong>max-over-time pooling</strong> op is to capture the most important feature - with maximum value - for each feature map.</p><p>One feature is extracted from $one$ filter.</p>
          </div>
<p>TextCNNs use multiple filters with varying window sizes $h$ to get multiple features. Then pass these features to a FC-softmax layer, and output the probability distribution over labels.</p>
<div class="note primary">
            <p><strong>Tricks</strong>:</p><ul><li>Dropout regularization: prevent co-adaptation of hidden units by randomly dropping out a proportion $p$ of the hidden units during the training process.<br>Given $\mathbf{c}$, conventional FC layer is:<script type="math/tex; mode=display">y = \mathbf{w} \cdot \mathbf{z} + b</script>While dropout is:<script type="math/tex; mode=display">y = \mathbf{w} \cdot (\mathbf{z} \circ \mathbf{r}) + b</script>where $ \circ $ is the element-wise multiplication op and $\mathbf{r} \in \mathbb{R}^m$ is a masking vector of Bernoulli random variables with probability $p$ to keep. </li></ul><p>At training time, after masking $(1-p)\%$ hidden units, backprop only goes though unmasked units.</p><p>At test time, the learned weight $\mathbf{w}$ is <strong>scaled</strong> by $p$: $\hat{\mathbf{w}} = p \mathbf{w}$. Then $\hat{\mathbf{w}}$ is used at <strong>test time without dropout</strong> op.</p><ul><li>Weight decay (L2-norm)</li></ul>
          </div>
<div class="note success">
            <p><strong>TextCNN variants</strong>:</p><ol><li><strong>CNN-rand</strong>: randomly initialize word vectors during training.</li><li><strong>CNN-static</strong>: use pretrained embeddings and keep them <strong>static</strong> during training, i.e. only train parameters not in embedding layers.</li><li><strong>CNN-non-static</strong>: use pretrained embeddings and <strong>fine tune</strong>, a.k.a. domain-specific training.</li><li><strong>CNN-multichannel</strong>: combine aforementioned two scenarios, i.e. use two channels of word embeddings followed by convolution op, but the <strong>gradients only backprop through the fine-tuned channel</strong>.</li></ol><p><strong>Intuition</strong>: multichannel CNNs could pervent overfitting (preventing the shift of fine-tuned embeddings by considering original static embeddings in the other channel at the same time), especially on small-scale datasets.<br>(But results are mixed; further work: on regularizing the fine-tuning process, e.g. use extra dimensions for fine-tune channel)</p>
          </div>
<p><strong>Static v.s. Non-static representations</strong>:</p>
<ul>
<li><p>Static: W2V<br>“good” is similar to “bad” using word2vec!<br>Because of syntactically equivalence</p>
</li>
<li><p>Non-static: e.g. good ~ nice<br>Fine-tuning can learn more meaningful representations</p>
</li>
</ul>
<p><img data-src="/notes/images/textcnn-model.png" alt="upload successful"></p>
<div class="note danger">
            <p><strong>Empirical findings on textCNNs</strong>:</p><ul><li>Dense pretrained word representations are better than 1-of-V encodings, although different representations perform variously on different tasks.</li><li>Filter rigion size and # of feature maps have a large impact on performance.</li><li>Regularization (dropout or $l2$-norm) has relatively little effect on performance.</li></ul>
          </div>
<p><strong>Fine-tuning on textCNN suggestions</strong>:</p>
<ul>
<li>Use non-static word representations instead of one-hot.</li>
<li>Line-search over the <strong>single region size</strong> (rather than combined region sizes, e.g. [3,4,5]) to find the best one (e.g. 1~10).</li>
<li>Alter the <strong># of feature maps</strong> for each filter size (from 100~600), with small dropout rate (0-0.5) and large l2-norm constraint.</li>
<li>Consider different activation functions if possible (ReLU, tanh)</li>
</ul>
<p>Give a detailed figure of a binary classification task<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Y., & Wallace, B. (2015). [A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification](https://arxiv.org/pdf/1510.03820). arXiv preprint arXiv:1510.03820.
">[5]</span></a></sup></p>
<p><img data-src="/notes/images/Textcnn1D-conv.png" alt="upload successful"></p>
<h2 id="RCNN-AAAI-2015"><a href="#RCNN-AAAI-2015" class="headerlink" title="RCNN (AAAI 2015)"></a>RCNN (AAAI 2015)</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a><strong>Background</strong></h3><ul>
<li>Recurrent NNs are better to capture the contextual information without the limitation of window size. But it is a <strong>biased</strong> model since the latter words play more important roles than former contexts.</li>
<li>ConvNets are unbiased since it can fairly tackle the words in a fixed context with a max-pooling layer. However, it is limited by the pre-defined filter region size. Higher order window size or n-gram could lead to sparse problems.</li>
</ul>
<p>Recurrent ConvNets(RCNN) combines both of them.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lai, S., Xu, L., Liu, K., & Zhao, J. (2015, January). [Recurrent Convolutional Neural Networks for Text Classification](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552). In AAAI (Vol. 333, pp. 2267-2273).
">[6]</span></a></sup></p>
<h3 id="Two-steps"><a href="#Two-steps" class="headerlink" title="Two steps"></a><strong>Two steps</strong></h3><h4 id="a-Word-representation-with-recurrent-connection"><a href="#a-Word-representation-with-recurrent-connection" class="headerlink" title="a) Word representation with recurrent connection"></a>a) <strong>Word representation with recurrent connection</strong></h4><p>Combine current word embedding <script type="math/tex">\mathbf{e}(w_i)</script> and its left and right context (i.e. <script type="math/tex">\mathbf{c}_l(w_i), \mathbf{c}_r(w_i)</script>) to represent $i$-th word <script type="math/tex">\mathbf{w}_i</script>: </p>
<script type="math/tex; mode=display">\mathbf{x}_i = [ \mathbf{c}_l(w_i), \mathbf{e}(w_i), \mathbf{c}_r(w_i) ]</script><p>Here compute the left and right context representation recursively:</p>
<script type="math/tex; mode=display">\mathbf{c}_l(w_i) = f(W^{(l)} \mathbf{c}_l(w_{i-1}) + W^{sl} \mathbf{e}(w_{i-1}) )</script><script type="math/tex; mode=display">\mathbf{c}_r(w_i) = f(W^{(r)} \mathbf{c}_r(w_{i-1}) + W^{sr} \mathbf{e}(w_{i-1}) )</script><p>where $f$ is a non-linear activation function.</p>
<p>Afterwards, go through a FC layer with $\tanh$ activation function.</p>
<script type="math/tex; mode=display">\mathbf{y}_i^{(2)} = \tanh( W^{(2)} \mathbf{x}_i + \mathbf{b}^{(2)})</script><p>where <script type="math/tex">\mathbf{y}_i^{(2)}</script> is a learned word representation, i.e. latent semantic vector.</p>
<h4 id="b-Text-representation-learning"><a href="#b-Text-representation-learning" class="headerlink" title="b) Text representation learning"></a>b) <strong>Text representation learning</strong></h4><p>CNNs are used for text representation. Previous step can be seen as a recurrent convolution op.</p>
<p>Then apply a max-pooling layer:</p>
<script type="math/tex; mode=display">\mathbf{y}^{(3)} = \max_{i=1}^n \mathbf{y}_i^{(2)}</script><p>where $k$-th element of $\mathbf{y}^{(3)}$ is the maximum of the $k$-th elements of $\mathbf{y}_i^{(2)}$.</p>
<p>Finally, go to a FC-softmax layer.</p>
<p><img data-src="/notes/images/RCNN.png" alt="upload successful"></p>
<h2 id="DMN-ICML-2016"><a href="#DMN-ICML-2016" class="headerlink" title="DMN (ICML 2016)"></a>DMN (ICML 2016)</h2><div class="note primary">
            <p><strong>DMN (Dynamic Memory Network)</strong></p><p><strong>Inituition</strong>: Most NLP tasks can be <strong>cast as question-answering</strong> problems, (e.g. machine translation, sequence modeliing, classification problems) using raw <strong>input-question-answer triplets</strong>: firstly obtain representations for inputs and the question. The question representation will trigger the <em>iterative attention process</em> by searching at inputs and relevant facts. Then the memory module produces a vector representation of all relevant information to answer the module.</p>
          </div>
<p><img data-src="/notes/images/DMN.png" alt="upload successful"></p>
<p><strong>Input module</strong></p>
<ul>
<li>Encode <em>raw texts</em> into distributed representations:<script type="math/tex; mode=display">h_t = \text{GRU}(E(w_t), h_{t-1})</script>where $E$ is the embedding lookup table, <script type="math/tex">w_t</script> is the word index of $t$-th word of the input sentence.</li>
</ul>
<p><strong>Question module</strong></p>
<ul>
<li>Encode <em>question</em> into distributed representations with GRU. Unlike input module, output the last hiddden states $q$.</li>
</ul>
<p><strong>Episodic memory module</strong></p>
<ul>
<li>During each iteration, the attention mechanism attends over all the fact representation $c$ with gated function, whilst taking into account the question representation $q$ and the previous memory $m^{i-1}$ to produce the episode $e^i$.</li>
<li><p>Use <strong>gating function</strong> as the attention for each pass $i$: $G_i^t = G(c_t, m^{i-1}, q)$.<br>The scoring function $G$ takes (candidate fact $c$, previous memory $m$, question $q$)  as the input feature and output a scala score: </p>
<script type="math/tex; mode=display">z = [c,m,q, c \circ q, c \circ m, |c-q|, |c-m|, c^TW^{(b)}q, c^TW^{(b)}m]</script><p>where $\circ$ is an element-wise product.<br>The scoring function is a two-layer FC layer:</p>
<script type="math/tex; mode=display">G(c,m,q) = \sigma(W^{(2)} \tanh (W^{(1)} z(c,m,q) + b^{(1)}) + b^{(2)})</script></li>
<li><p>Memory update: for pass $i$, given a sentence of <script type="math/tex">{c_1, ...,c_{T_c}}</script>, the hidden states at time $t$ and episode $e^i$:</p>
<script type="math/tex; mode=display">
\begin{align}
h_i^t &= g_t^i \text{GRU} (c_t, h_{t-1}^i) + (1-g_t^i) h_{t-1}^i \\
e^i &= h^i_{T_C}
\end{align}</script></li>
</ul>
<p><strong>Answer module</strong></p>
<ul>
<li>Employ another GRU whose initial state is last memory: <script type="math/tex">a_0 = m^{T_M}</script>. At each time, considering the question $q$, last hidden state <script type="math/tex">a_{t-1}</script>, as well as previous predicted output <script type="math/tex">y_{t-1}</script>.<script type="math/tex; mode=display">y_t = \text{softmax} (W^{(a)}a_t)</script><script type="math/tex; mode=display">a_t = \text{GRU} ([y_{t-1},q],a_{t-1})</script>where concat last generated output and question vector <script type="math/tex">[y_{t-1},q]</script>.</li>
</ul>
<h2 id="BERT-Google-2018"><a href="#BERT-Google-2018" class="headerlink" title="BERT (Google 2018)"></a>BERT (Google 2018)</h2><p><strong>B</strong>i-directional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers</p>
<ul>
<li>Model: bi-transformer</li>
<li>Pretraining: <ol>
<li>Masked Language Models</li>
<li>Next Sentence Prediction</li>
</ol>
</li>
<li>Fine-tuning</li>
</ul>
<p>My solution: <a href="https://github.com/cyk1337/BERT-classification">github</a></p>
<p><img data-src="/notes/images/BERT-classification.png" alt="upload successful"></p>
<h1 id="Document-classification"><a href="#Document-classification" class="headerlink" title="Document classification"></a>Document classification</h1><h2 id="HAN-NAACL-2016"><a href="#HAN-NAACL-2016" class="headerlink" title="HAN (NAACL 2016)"></a>HAN (NAACL 2016)</h2><p>HAN(Hierarchical Attention Net) models the attention mechanism in two levels: <strong>word and sentence-level</strong>. <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). [Hierarchical attention networks for document classification](http://www.aclweb.org/anthology/N16-1174). In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489).
">[4]</span></a></sup></p>
<div class="note danger">
            <ul><li><strong>Intuition</strong>: <strong>incorporating knowledge of document structure</strong> in the model architecture.<br>Because <strong>not all parts of documents are equally relevant</strong>, and determing the relevant parts includes <strong>modeling the interaction of the words</strong>, not just their presence in isolation.</li><li><strong>Hierarchical structure</strong>: words form sentences, sentences form a document.</li><li>Different words and sentences in a document are <strong>differently informative</strong>. The importance of the informative words and sentences are <strong>highly context-dependent</strong>.</li><li>Attention mechanism could provide insight into which words and sentences contribute more or less to the decision<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). [Hierarchical attention networks for document classification](http://www.aclweb.org/anthology/N16-1174). In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489).">[4]</span></a></sup> (by plotting hotmap I think;) )</li></ul>
          </div>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>Overall: </p>
<ol>
<li><strong>Word-level</strong>: a word encoder + word-level attention layer;</li>
<li><strong>Sentence-level</strong>: a sentence encoder + sentence-level attention layer.</li>
</ol>
<p><strong>Sequence encoder</strong>: GRU</p>
<h4 id="Hierarchical-attention"><a href="#Hierarchical-attention" class="headerlink" title="Hierarchical attention"></a><strong>Hierarchical attention</strong></h4><h5 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a><strong>Word Encoder</strong></h5><p>Get word representations from characters using bi-GRU.</p>
<p>Given a sentence with words $w_{it}, t \in [1,T]$, firstly map the words to vectors through an embedding matrix <script type="math/tex">W_e</script>: </p>
<script type="math/tex; mode=display">x_{ij} = W_e w_{ij}</script><p>Then concat the bi-GRU representation:</p>
<script type="math/tex; mode=display">\overrightarrow{h}_{it} =  \overrightarrow{GRU}(x_{it})</script><script type="math/tex; mode=display">\overleftarrow{h}_{it} =  \overleftarrow{GRU}(x_{it})</script><script type="math/tex; mode=display">h_{it} = [\overrightarrow{h}_{it}, \overleftarrow{h}_{it}]</script><p>HAN<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). [Hierarchical attention networks for document classification](http://www.aclweb.org/anthology/N16-1174). In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489).
">[4]</span></a></sup> directly applied word embeddings for simplification.</p>
<h5 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a><strong>Word Attention</strong></h5><p><strong>Intuition</strong>: not all words contribute equally to the sentence representation. Hence employ attention to extract the important words that are most informative and aggregate all the words according to their informativeness (attention vector distribution) to obtain the sentence vector <script type="math/tex">s_i</script>.</p>
<script type="math/tex; mode=display">u_{it}= \tanh (W_wh_{it}+b_w)</script><script type="math/tex; mode=display">\alpha_{it}=\frac{\exp(u_{it}^T u_w)}{ \sum_t \exp( u_{it}^T u_w ) }</script><script type="math/tex; mode=display">s_i = \sum_t \alpha_{it} h_{it}</script><p><strong>Interpretation</strong>:  firstly feed the word representation into a FC layer to get a hidden representation of <script type="math/tex">u_{it}</script>. Then measure the (cosine) similarity between the current representation <script type="math/tex">u_{it}</script> and randomly initialized context <script type="math/tex">u_w</script>, followed by a softmax to obtain the normalized attention weights <script type="math/tex">\alpha{it}</script>. Finally, aggregate all the word representations <script type="math/tex">h_{it}</script> according to the weight vector.<br><div class="note primary">
            <p>Here, the word context vector <script type="math/tex">u_w</script> is <strong>randomly initialized</strong> and <strong>joint learned</strong> during the training process<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). [Hierarchical attention networks for document classification](http://www.aclweb.org/anthology/N16-1174). In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489).">[4]</span></a></sup>. “The context vector <script type="math/tex">u_w</script> can be seen as a high level representation of a fixed query “what is the informative word” over the words like that used in memory networks.”</p>
          </div></p>
<h5 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h5><p>Given sentene vector <script type="math/tex">s_i</script>, we get the document vector <script type="math/tex">h_i</script> with bi-GRU (same as word encoder):</p>
<script type="math/tex; mode=display">\overrightarrow{h}_{i} =  \overrightarrow{\text{GRU}}(s_{i})</script><script type="math/tex; mode=display">\overleftarrow{h}_{i} =  \overleftarrow{\text{GRU}}(s_{i})</script><script type="math/tex; mode=display">h_{i} = [\overrightarrow{h}_{i}, \overleftarrow{h}_{i}]</script><h5 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h5><p>Same as word attention. Obtain the document vector $v$:</p>
<script type="math/tex; mode=display">u_{i}=\tanh(W_s h_{i}+b_s)</script><script type="math/tex; mode=display">\alpha_{i}=\frac{\exp(u_{i}^T u_s)}{\sum_t \exp(u_{t}^T u_s) }</script><script type="math/tex; mode=display">v = \sum_t \alpha_{i} h_{i}</script><p>where <script type="math/tex">u_s</script> is sentence-level randomly-initialized sentence context vector, and is joinly learned during training.</p>
<h5 id="Document-classification-1"><a href="#Document-classification-1" class="headerlink" title="Document classification"></a>Document classification</h5><p>Feed high-level document representation $v$ into a FC-softmax layer.</p>
<script type="math/tex; mode=display">p=\text{softmax}(W_c v + b_c)</script><p>The loss function is NLL(negative log likelihood):</p>
<script type="math/tex; mode=display">L = -\sum_d log p_{dj}</script><p>where $j$ is the label of the document $d$.</p>
<p><img data-src="/notes/images/HAN.png" alt="upload successful"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Joulin, A., Grave, E., Bojanowski, P., &amp; Mikolov, T. (2016). <a href="https://arxiv.org/pdf/1607.01759">Bag of tricks for efficient text classification</a>. arXiv preprint arXiv:1607.01759.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <a href="https://arxiv.org/pdf/1810.04805.pdf?fbclid=IwAR3FQiWQzP7stmPWZ4kzrGmiUaN81UpiNeq4GWthrxmwgX0B9f1CvuXJC2E">Bert: Pre-training of deep bidirectional transformers for language understanding</a>. arXiv preprint arXiv:1810.04805.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kim, Y. (2014). <a href="https://arxiv.org/pdf/1408.5882">Convolutional neural networks for sentence classification</a>. arXiv preprint arXiv:1408.5882.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., &amp; Hovy, E. (2016). <a href="http://www.aclweb.org/anthology/N16-1174">Hierarchical attention networks for document classification</a>. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1480-1489).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, Y., &amp; Wallace, B. (2015). <a href="https://arxiv.org/pdf/1510.03820">A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification</a>. arXiv preprint arXiv:1510.03820.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lai, S., Xu, L., Liu, K., &amp; Zhao, J. (2015, January). <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552">Recurrent Convolutional Neural Networks for Text Classification</a>. In AAAI (Vol. 333, pp. 2267-2273).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ondruska, P., Gulrajani, I., &amp; Socher, R. (2016). <a href="https://arxiv.org/pdf/1506.07285.pdf">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a>. ICML.<a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Text classification</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Survey</tag>
        <tag>Text classification</tag>
      </tags>
  </entry>
  <entry>
    <title>An Introduction to Activation Functions</title>
    <url>/notes/2019/05/10/NN/Activation-functions-introduction/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Activation functions lead to non-linearity in neural networks. Most common types are Sigmoid, Tanh, Relu, <em>etc</em>.<br><span id="more"></span></p>
<h1 id="Commonly-used-Activations"><a href="#Commonly-used-Activations" class="headerlink" title="Commonly-used Activations"></a>Commonly-used Activations</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><p>Sigmoid function takes a real-valued number and ‘squashes’ it into the range (0,1).<br><div class="note info">
            <p><strong>Drawbacks</strong>:</p><ol><li>Sigmoids <code>saturate and kill gradients</code>.  When at either the tail of 0 or 1, the gradient is almost zero. Take care of the weight initialization: if too large most neurons would saturate soon and the networks will barely learn.</li><li>Outputs are not zero-centered.</li></ol>
          </div><br><img data-src="/notes/images/activation-function-sigmoid.png" alt="upload successful"></p>
<h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><script type="math/tex; mode=display">
\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} = 2\sigma(2x) -1</script><p>Tanh squashes the real number input into the range [-1,1]. </p>
<div class="note info">
            <ul><li>Like sigmoid, its activations saturate; </li><li>but the output of tanh is zero-centered. Therefore, <code>tanh</code> non-linearity is always preferred to the <strong>sigmoid</strong> non-linearity.</li><li>$\tanh$ is simply a <code>scaled sigmoid</code> neuron: <script type="math/tex; mode=display">\tanh(x) = 2 \sigma(2x) -1</script></li></ul>
          </div>
<p><img data-src="/notes/images/activation-function-tanh.png" alt="upload successful"></p>
<h2 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU (Rectified Linear Unit)"></a>ReLU (Rectified Linear Unit)</h2><script type="math/tex; mode=display">\text{Relu}(x) = \max(0, x)</script><p>ReLu is simply thresholded at zero.<br><div class="note info">
            <p><strong>Pros</strong>:</p><ul><li>6x accecerate the convergence of SGD compared to the tanh functions.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Nair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) (pp. 807-814).">[2]</span></a></sup></li><li>No expensive operations (e.g. exponential)<br><strong>Problems</strong>:</li><li>Fragile during training and can “die”: ReLU units can irreversibly die during training.</li></ul>
          </div></p>
<p><img data-src="/notes/images/activation-function-comparison.png" alt="upload successful"></p>
<h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><script type="math/tex; mode=display">f(x)=\left\{
                \begin{array}{ll}
                \begin{align}
                  x \quad & x>=0\\
                  \alpha x \quad & x<0\\
                \end{align}
                \end{array}
              \right.</script><p>Leaky ReLU attempts to fix the <code>dying ReLU</code> problem, by setting a small nagative slope when $x&lt;0$. However, the consistency of the benefits across tasks is unclear.</p>
<h2 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h2><p>Parametric ReLU</p>
<script type="math/tex; mode=display">f(x) = \max(\alpha x, x)</script><p>where $\alpha$ is learnable.</p>
<h2 id="ELU-Exponential-Linear-Units"><a href="#ELU-Exponential-Linear-Units" class="headerlink" title="ELU (Exponential Linear Units)"></a>ELU (Exponential Linear Units)</h2><p>ELU <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Clevert, D. A., Unterthiner, T., & Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (ELUs). arXiv preprint arXiv:1511.07289.">[4]</span></a></sup></p>
<script type="math/tex; mode=display">f(x)=\left\{
                \begin{array}{ll}
                \begin{align}
                  x \quad & x>=0\\
                  \alpha (\exp(x)-1) \quad & x<0\\
                \end{align}
                \end{array}
              \right.</script><h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><p>See Maxout Networks(Goodfellow et.al 2013)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., & Bengio, Y. (2013). Maxout networks. arXiv preprint arXiv:1302.4389.">[3]</span></a></sup> :</p>
<script type="math/tex; mode=display">f(x) = \max(w_1^Tx+b_1, w_2^Tx + b_2)</script><h2 id="GELU-Gaussian-Error-Linear-Units"><a href="#GELU-Gaussian-Error-Linear-Units" class="headerlink" title="GELU (Gaussian Error Linear Units)"></a>GELU (Gaussian Error Linear Units)</h2><div class="note info">
            <p><strong>Motivation</strong>: </p><ul><li>combine the properties of dropout, zoneout, and ReLUs.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hendrycks, D., & Gimpel, K. (2016). Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415.">[5]</span></a></sup></li><li>multiplying the input by zero or one, but the values of this zero-one mask are stochastically determined while also dependent upon the input. </li></ul>
          </div>
<p>Specically, multiply the neuron input $x$ by $m \sim \text{Benoulli}(\Phi(x))$, where $\Phi(x)=P(X \leq x)$, $X \sim \mathcal{N}(0,1)$.</p>
<p>The non-linearity is the expected transformation of the stochastic regularizer on an input $x$:<br>$\Phi(x) \times I x + (1-\Phi(x)) \times 0x = x \Phi(x)$</p>
<p>Then define the <code>Gaussian Error Linear Unit (GELU)</code> as:</p>
<script type="math/tex; mode=display">
\begin{align}
\text{GELU}(x) &= x P(X \leq x) = x \Phi(x) \\
& \approx 0.5x (1+ \tanh [\sqrt{\frac{2}{\pi}} (x+ 0.044715x^3)]) \\ &= x \sigma(1.702x)
\end{align}</script><p>BERT implementation:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span>(<span class="params">x</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Gaussian Error Linear Unit.</span></span><br><span class="line"><span class="string">  This is a smoother version of the RELU.</span></span><br><span class="line"><span class="string">  Original paper: https://arxiv.org/abs/1606.08415</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    x: float Tensor to perform activation.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    `x` with the GELU activation applied.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + tf.tanh(</span><br><span class="line">      (np.sqrt(<span class="number">2</span> / np.pi) * (x + <span class="number">0.044715</span> * tf.<span class="built_in">pow</span>(x, <span class="number">3</span>)))))</span><br><span class="line">  <span class="keyword">return</span> x * cdf</span><br></pre></td></tr></table></figure></p>
<p><img data-src="/notes/images/activation-function-GELU.png" alt="GELU"></p>
<h2 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h2><p>Swish has the property of one-sided boundaries at zero, smoothness and  non-monotonicity. Swish is shown to outperform ReLU on many tasks.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Ramachandran, P., Zoph, B., & Le, Q. V. (2017). [Swish: a self-gated activation function](https://pdfs.semanticscholar.org/4f57/f486adea0bf95c252620a4e8af39232ef8bc.pdf). arXiv preprint arXiv:1710.05941.">[6]</span></a></sup></p>
<script type="math/tex; mode=display">f(x) = x \cdot \sigma (x)</script><p><img data-src="/notes/images/Swish.png" alt="Swish"></p>
<p>Personally, this idea is borrowed from the work of (Dauphin et. al, 2017)<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dauphin, Y. N., Fan, A., Auli, M., & Grangier, D. (2017, August). [Language modeling with gated convolutional networks](https://arxiv.org/pdf/1612.08083). In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 933-941). JMLR. org.">[7]</span></a></sup> at FAIR in 2017, <strong>Gated Linear Unit(GLU)</strong> in gated CNNs, which is used to capture the sequential information after temporal convolutions: </p>
<script type="math/tex; mode=display">H_l(\pmb{X}) = (\pmb{X} * \pmb{W} + \pmb{b}) \otimes \sigma ((\pmb{X} * \pmb{V} + \pmb{b}))</script><p><img data-src="/notes/images/GLU.png" alt="Image source: &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Dauphin, Y. N., Fan, A., Auli, M., &amp; Grangier, D. (2017, August). [Language modeling with gated convolutional networks](https://arxiv.org/pdf/1612.08083). In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 933-941). JMLR. org.&quot;&gt;[7]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<p>Relu can be seen as a simplication of <strong>GLU</strong>, where the activation of the gate depends on the sign of the input:</p>
<script type="math/tex; mode=display">\text{Relu}(\pmb{X}) = \pmb{X} \otimes (\pmb{X} > 0)</script><p>The gradient of LSTM-style gating of Gated Tanh Unit (GTU) is gradually <strong>vanishing</strong> because of the downscaling factors $\color{salmon}{\tanh’(\pmb{X})}$ and $\color{salmon}{\sigma’(\pmb{X})}$:</p>
<script type="math/tex; mode=display">\nabla [ \tanh (\pmb{X}) \otimes \sigma(\pmb{X})] = \color{salmon}{\tanh'(\pmb{X})} \nabla \pmb{X} \otimes \sigma (\pmb{X}) + \color{salmon}{\sigma'(\pmb{X})} \nabla \pmb{X} \otimes \tanh(\pmb{X})</script><p><strong>GLU</strong> has the path $\color{green}{\nabla \pmb{X} \otimes \sigma(\pmb{X})}$, which does not downscale the activated gating unit. This can be thought as a <strong>multiplicative skip connection</strong>.</p>
<script type="math/tex; mode=display">\nabla[\pmb{X} \otimes \sigma(\pmb{X})] = \color{green}{\nabla \pmb{X} \otimes \sigma(\pmb{X})} + \pmb{X} \otimes \sigma'(\pmb{X}) \nabla \pmb{X}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Swish</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x*F.sigmoid(x)</span><br></pre></td></tr></table></figure>
<h2 id="Mish"><a href="#Mish" class="headerlink" title="Mish"></a>Mish</h2><p>Mish is a non-monotonic, self-gated/regularized, smoothing activation function. It is shown to outperform Swish and ReLU on various tasks.<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Misra, D. (2019). [Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/pdf/1908.08681). arXiv preprint arXiv:1908.08681.">[8]</span></a></sup></p>
<script type="math/tex; mode=display">
f(x) = x \cdot \tanh \big( \underbrace{\ln (1+ \exp(x))}_\text{Softplus} \big)</script><p><img data-src="/notes/images/Mish.png" alt="Mish"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cs231n.github.io/neural-networks-1/">http://cs231n.github.io/neural-networks-1/</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) (pp. 807-814).<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., &amp; Bengio, Y. (2013). Maxout networks. arXiv preprint arXiv:1302.4389.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Clevert, D. A., Unterthiner, T., &amp; Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (ELUs). arXiv preprint arXiv:1511.07289.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hendrycks, D., &amp; Gimpel, K. (2016). Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Ramachandran, P., Zoph, B., &amp; Le, Q. V. (2017). <a href="https://pdfs.semanticscholar.org/4f57/f486adea0bf95c252620a4e8af39232ef8bc.pdf">Swish: a self-gated activation function</a>. arXiv preprint arXiv:1710.05941.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dauphin, Y. N., Fan, A., Auli, M., &amp; Grangier, D. (2017, August). <a href="https://arxiv.org/pdf/1612.08083">Language modeling with gated convolutional networks</a>. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 933-941). JMLR. org.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Misra, D. (2019). <a href="https://arxiv.org/pdf/1908.08681">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a>. arXiv preprint arXiv:1908.08681.<a href="#fnref:8" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title>An Introduction to Capsules</title>
    <url>/notes/2020/04/23/NN/An-Introduction-to-Capsules/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A capsule is defined as a group of neuron instantiations whose parameters represent specific properties of a specific type of entity. Here is a brief note of Capsule networks<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sabour, S., Frosst, N. and Hinton, G.E., 2017. [Dynamic routing between capsules](http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf). In Advances in neural information processing systems (pp. 3856-3866).
">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hinton, G.E., Sabour, S., and Frosst, N., 2018. [Matrix capsules with EM routing](https://openreview.net/pdf?id=HJWLfGWRb).
">[2]</span></a></sup>.</p>
<span id="more"></span>
<p>Convolutional Neural Networks (CNNs) extract local-region features with fixed strides, followed by max-pooling, which may only retain the remarkable features but ignore the fine-grain features such as overlapping entities in images. Capsules are regarded as a better solution to handle this.</p>
<h1 id="Capsules-with-dynamic-routing"><a href="#Capsules-with-dynamic-routing" class="headerlink" title="Capsules with dynamic routing"></a>Capsules with dynamic routing</h1><p>Standard inputs and outputs of neural layers are scalar features, while Capsule Networks leverage capsules of vectors to represent features of entities. </p>
<h2 id="Capsule"><a href="#Capsule" class="headerlink" title="Capsule"></a>Capsule</h2><p>For each capsule, the input <script type="math/tex">\mathbf{u}_i</script> and the output <script type="math/tex">\mathbf{v}_j</script> are vectors.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sabour, S., Frosst, N. and Hinton, G.E., 2017. [Dynamic routing between capsules](http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf). In Advances in neural information processing systems (pp. 3856-3866).
">[1]</span></a></sup><br><img data-src="/notes/images/capsule-layer.png"></p>
<p>For all except the first layer of capsules, the input to a capsule <script type="math/tex">\mathbf{s}_j</script> is a weighted sum over all “predicted vector” <script type="math/tex">\hat{\mathbf{u}}_{j \vert i}</script>, which is linearly transformed with a learnable weight matrix <script type="math/tex">\mathbf{W}_{ij}</script>.</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\mathbf{u}}_{j \vert i} &= \mathbf{W}_{ij} \mathbf{u}_i \\
\mathbf{s}_j &= \sum_i c_{ij} \hat{\mathbf{u}}_{j \vert i} 
\end{align}</script><p>where <strong>coupling coefficients</strong> <script type="math/tex">c_{ij}</script> between capsule $i$ and all all capsules in the layer above are scaled with a “routing softmax” to sum to 1. We will introduce this in the next section.</p>
<p>A <strong>squashing activation function</strong> rather than ReLU is used on <script type="math/tex">\mathbf{s}_j</script>, ensuring that the short and long vectors to approach to the length of almost zero and slighly below 1, respectively.</p>
<script type="math/tex; mode=display">
\mathbf{v}_j = \frac{\Vert \mathbf{s}_j \Vert^2}{1 + \Vert \mathbf{s}_j \Vert^2} \frac{\mathbf{s}_j}{\Vert \mathbf{s}_j \Vert}</script><p>Here <script type="math/tex">\mathbf{v}_j \rightarrow \Vert \mathbf{s}_j \Vert \mathbf{s}_j</script> for <script type="math/tex">\mathbf{s}_j</script> is too small, while <script type="math/tex">\mathbf{v}_j \rightarrow \frac{\mathbf{s}_j}{\Vert \mathbf{s}_j \Vert}</script> when that is large.</p>
<p>The weight <script type="math/tex">c_{ij}</script> is computed with iteration:</p>
<script type="math/tex; mode=display">
c_{ij} = \frac{\exp (b_{ij})}{\sum_{k} \exp(b_{ik})}</script><p>where the log probabilities <script type="math/tex">b_{ij}</script> measure the probability that capsule $i$ should be coupled to capsule $j$ above, and are iteratively updated with:</p>
<script type="math/tex; mode=display">
\begin{align}
a_{ij} &= \hat{\mathbf{u}}_{j \vert i} \cdot \mathbf{v}_j \\
b_{ij} &\leftarrow b_{ij} + a_{ij}
\end{align}</script><p>where <script type="math/tex">a_{ij}</script> measures the agreement between the current output <script type="math/tex">\mathbf{v}_j</script> of capsule $j$ in above layer and <script type="math/tex">\hat{\mathbf{u}}_{j \vert i}</script> in capsule $i$.</p>
<h2 id="Dynamic-routing"><a href="#Dynamic-routing" class="headerlink" title="Dynamic routing"></a>Dynamic routing</h2><p><img data-src="/notes/images/capsule-dynamic-routing.png" width="90%"/></p>
<h2 id="Margin-loss"><a href="#Margin-loss" class="headerlink" title="Margin loss"></a>Margin loss</h2><p>For each capsule $k$, seperate margin loss <script type="math/tex">L_k</script> is:</p>
<script type="math/tex; mode=display">
L_k = T_k \max(0, m^+ - \Vert \mathbf{v}_k \Vert^2) + \lambda (1-T_k) \max(0, \Vert \mathbf{v}_k \Vert - m^-)^2</script><p>where <script type="math/tex">T_k=1</script> iff class $k$ is present, $m^+ =0.9, m^-=0.1, \lambda = 0.5$. The total loss just simply sum all capsules.</p>
<h1 id="Matrix-capsules-with-EM-routing"><a href="#Matrix-capsules-with-EM-routing" class="headerlink" title="Matrix capsules with EM routing"></a>Matrix capsules with EM routing</h1><p>Capsules consist of a pose matrix $\mathbf{M} \in \mathbb{R}^{4 \times 4}$ and an activation probability $a \in \mathbb{R}$. Expectation-Maximization algorithm is used to iteratively update the cluster of capsules with similar votes.</p>
<p>Let we denote the capsules in layer $L$ as <script type="math/tex">\Omega_L</script>, trainable weights between each capsule $i$ in layer $i$ and each capsule $j$ in layer $L+1$ as $\mathbf{W}_{ij} \in \mathbb{R}^{4 \times 4}$.</p>
<script type="math/tex; mode=display">\mathbf{V}_{ij}= \mathbf{M}_i \mathbf{W}_{ij}</script><p>The iterative update of pose matrix and activations of all capsules in layer $L+1$ leverages a non-linera routing procedure to get input <script type="math/tex">\mathbf{V}_{ij}</script> and <script type="math/tex">a_i</script> for all <script type="math/tex">i \in \Omega_L, j \in \Omega_{L+1}</script>.</p>
<p><img data-src="/notes/images/matrix-capsule-EM-routing.png" width="90%"/></p>
<h2 id="Spread-loss"><a href="#Spread-loss" class="headerlink" title="Spread loss"></a>Spread loss</h2><p>Spread loss directly maximizes the gap between the activation of the target class <script type="math/tex">a_t</script> and the activation of other classes.</p>
<script type="math/tex; mode=display">
\begin{align}
L_i &= (\max(0, m- (a_t - a_i)))^2 \\
L &= \sum_{i \neq t} L_i
\end{align}</script><p>where $m = 0.2$ initially and linearly increases during training to 0.9, equal to squared Hinge loss with $m=1$.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sabour, S., Frosst, N. and Hinton, G.E., 2017. <a href="http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf">Dynamic routing between capsules</a>. In Advances in neural information processing systems (pp. 3856-3866).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hinton, G.E., Sabour, S., and Frosst, N., 2018. <a href="https://openreview.net/pdf?id=HJWLfGWRb">Matrix capsules with EM routing</a>.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/">Blog: Understanding Dynamic Routing between Capsules</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/">Blog: Understanding Matrix capsules with EM Routing</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.freecodecamp.org/news/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc/">FreeCodeCamp: Understanding Capsule Networks</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/naturomics/CapsNet-Tensorflow">TensorFlow code</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
        <category>Capsule</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>Capsule</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Reading Comprehension: a Survey!</title>
    <url>/notes/2019/04/05/NLP/Machine-Reading-Comprehension-a-Survey/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Machine reading comprehension aims to answer questions given a passage or document.<br><span id="more"></span></p>
<h1 id="Symbol-matching-models"><a href="#Symbol-matching-models" class="headerlink" title="Symbol matching models"></a>Symbol matching models</h1><h2 id="Frame-Semantic-parsing"><a href="#Frame-Semantic-parsing" class="headerlink" title="Frame-Semantic parsing"></a>Frame-Semantic parsing</h2><p>Frame-semantic parsing identifies predicates and their arguments, i.e. “who did what to whom”.</p>
<h2 id="Word-Distance"><a href="#Word-Distance" class="headerlink" title="Word Distance"></a>Word Distance</h2><p>Sum the distances of every word in $q$ to their nearest aligned word in $d$</p>
<h1 id="Teaching-Machines-to-Read-and-Comprehend"><a href="#Teaching-Machines-to-Read-and-Comprehend" class="headerlink" title="Teaching Machines to Read and Comprehend"></a>Teaching Machines to Read and Comprehend</h1><h2 id="Deep-LSTM-Reader"><a href="#Deep-LSTM-Reader" class="headerlink" title="Deep LSTM Reader"></a>Deep LSTM Reader</h2><p>In NMT, deep LSTMs have shown a remarkable ability to embed long sequences into a vector representation, which contains enough information to generate a full translation in another language.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hermann, K.M., Kociský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. (2015). [Teaching Machines to Read and Comprehend](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf). NIPS.
">[2]</span></a></sup> </p>
<p><img data-src="/notes/images/deep-lstm-reader.png" alt="upload successful"></p>
<p>Deep LSTMs feed out documents one word at a time into a <strong>Deep LSTM encoder</strong>, after a delimiter, followed by a query ($d \oplus |||  \oplus q$,  or $q \oplus |||  \oplus d$  ). The network predicts which token in the document answers the query.</p>
<h2 id="Attentive-Reader"><a href="#Attentive-Reader" class="headerlink" title="Attentive Reader"></a>Attentive Reader</h2><div class="note danger">
            <p><strong>Limitations of the Deep LSTM Reader</strong>:</p><ul><li>fixed width hidden vector</li></ul>
          </div>
<ul>
<li><strong>Solution</strong>: the Attentive Reader employs a finer grained token level attention mechanism, where the tokens are embedded given their entire future and past context in the input documents.</li>
</ul>
<p>Attentive Reader encodes the document $d$ and the query $q$ with two separate 1-layer bi-LSTMs.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hermann, K.M., Kociský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. (2015). [Teaching Machines to Read and Comprehend](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf). NIPS.
">[2]</span></a></sup></p>
<p><img data-src="/notes/images/atten-reader.png" alt="upload successful"></p>
<p>When encoding the query $q$, the encoding $u$ of a query with length $|q|$ is the concatenation of the final forward and backward outputs:</p>
<script type="math/tex; mode=display">u = \overrightarrow{y_q}(|q|) || \overleftarrow{y_q}(1)</script><p>When encoding the document $d$, each token at position $t$ is:</p>
<script type="math/tex; mode=display">y_d(t) = \overrightarrow{y_d}(t) || \overleftarrow{y_d}(t)</script><p>The representation $r$ of $d$ is a weighted sum of these output vectors. The weights can be interpreted as the degree to which the network attends to a particular token in the document $d$ when answering the query:</p>
<script type="math/tex; mode=display">m(t) = tanh(W_{ym} y_d(t)) + W_{um}u</script><script type="math/tex; mode=display">s(t) \approx exp(w^T_{ms} m(t))</script><script type="math/tex; mode=display">r = y_d s</script><p>Finally, the joint document and query embedding is:</p>
<script type="math/tex; mode=display">g^{AR}(d,q) = \text{tanh}(W_{rg}r + W_{ug}u)</script><h2 id="Impatient-Reader"><a href="#Impatient-Reader" class="headerlink" title="Impatient Reader"></a>Impatient Reader</h2><p>The Attentive Reader focuses on the passage of a context document that are most likely to inform the answer to the query.</p>
<p><strong>Impatient Reader</strong> can <code>reread</code> from the document as each query token is read.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hermann, K.M., Kociský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. (2015). [Teaching Machines to Read and Comprehend](https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf). NIPS.
">[2]</span></a></sup></p>
<p><img data-src="/notes/images/impatient-reader.png" alt="upload successful"></p>
<p>At each token $i$ of the query $q$, the model computes the document representation vector $r(i)$ with the bidirectional embedding <script type="math/tex">y_q(i) = \overrightarrow{y_q}(i) || \overleftarrow{y_q}(i)</script>:</p>
<script type="math/tex; mode=display">m(i, t) = \text{tanh}(W_{dm}y_d(t) + W_{rm} r(i-1) + W_{qm} y_q(i)), \quad 1 \leq i \leq |q|</script><script type="math/tex; mode=display">s(i,t) \propto \text{exp}(W_{ms}^T m(i,t))</script><script type="math/tex; mode=display">r(0)= \pmb{r_0}, \quad r(i) = y_d^T s(i) + \pmb{\text{tanh}(W_{rr}r(i-1))} \quad 1 \leq i \leq |q|</script><p>The attention mechanism allows the model to recurrently accumulate information from the document as it sees each query token, ultimately outputting a final joint document query representation for the answer prediction</p>
<script type="math/tex; mode=display">g^{IR}(d,q) = \text{tanh}(W_{rg}r(|q|) + W_{qg} u)</script><h1 id="Attention-Sum-Reader"><a href="#Attention-Sum-Reader" class="headerlink" title="Attention Sum Reader"></a>Attention Sum Reader</h1><ul>
<li>For <code>cloze-style</code> QA. <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kadlec, R., Schmid, M., Bajgar, O., & Kleindienst, J. (2016). [Text Understanding with the Attention Sum Reader Network](https://www.aclweb.org/anthology/P16-1086). CoRR, abs/1603.01547.
">[6]</span></a></sup></li>
</ul>
<p><img data-src="/notes/images/MRC-Attn-sum-Reader.png" alt="upload successful"></p>
<ol>
<li>Compute the vector embedding for the query.<script type="math/tex; mode=display">g(\pmb{q}) = \overrightarrow{g_{|\pmb{q}|}}(\pmb{q}) || \overleftarrow{g_1}(\pmb{q})</script></li>
<li>Compute the vector embedding of each individual word in the context of the whole document. The word embedding is a look-up table $V$.<script type="math/tex; mode=display">f_i(\pmb{d}) = \overrightarrow{f_i} (\pmb{d}) || \overleftarrow{f_i}(\pmb{d})</script></li>
<li>Dot product between the question embedding and the contextual embedding. Select the most likely answer.</li>
</ol>
<h1 id="EpiReader"><a href="#EpiReader" class="headerlink" title="EpiReader"></a>EpiReader</h1><h2 id="Pointer-Nets"><a href="#Pointer-Nets" class="headerlink" title="Pointer Nets"></a>Pointer Nets</h2><div class="note danger">
            <p><strong>Problems</strong>:</p><ul><li>Conventional seq2seq architecture can only applies softmax distribution over a <strong>fixed-sized</strong> output dictionary. It cannot handle problems where the size of the output dictionary is equal to the <strong>length of the input sequence</strong>.<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vinyals, O., Fortunato, M., & Jaitly, N. (2015). [Pointer Networks](http://papers.nips.cc/paper/5866-pointer-networks.pdf). NIPS.">[8]</span></a></sup></li></ul><script type="math/tex; mode=display">p(\mathcal{C} \vert \mathcal{P}; \theta) = \prod_{i=1}^{m(\mathcal{P})} p(C_i \vert C_1, \cdots, c_{i-1}, \mathcal{P}; \theta)</script><p>where <script type="math/tex">\mathcal{P}=\{ P_1, \cdots, P_n \}</script> is a sequence of $n$ vectors and <script type="math/tex">\mathcal{C}^{\mathcal{P}} = \{ C_1, \cdots, C_{m(\mathcal{P})} \}</script> is a sequence of $m(\mathcal{P})$ indices.</p><p>The parameters are learnt by maximizing the conditional probabilities of the training set:</p><script type="math/tex; mode=display">\theta^* = \arg\max_\theta \sum_{\mathcal{P}, \mathcal{C}^{\mathcal{P}}} \log p(\mathcal{C}^{\mathcal{P}} \vert \mathcal{P}; \theta)</script>
          </div>
<p><img data-src="/notes/images/Pointer-nets.png" alt="upload successful"></p>
<p>Solution: <code>Pointer Net</code>.</p>
<ul>
<li><p>Applies the attention mechanism:</p>
<script type="math/tex; mode=display">u_j^i = v^T \tanh (W_1 e_j + W_2 d_i) \quad j \in (1,\cdots,n)</script><script type="math/tex; mode=display">p(C_i \vert C_1, \cdots, C_{i-1}, \mathcal{P}) = \text{softmax}(u^i)</script><p>  where softmax normalizes the vector $u^i$ (of length $n$) to be an output distribution over the dictionary of inputs. And $v$,<script type="math/tex">W_1</script>, <script type="math/tex">W_2</script> are learnable parameters of the output model.</p>
<p>  Here, we do not blend the encoder state <script type="math/tex">e_j</script> to propagate extra information to the decoder. Instead, we use <script type="math/tex">u_j^i</script> as <code>pointers to the input elements</code>. </p>
</li>
</ul>
<div class="note info">
            <ul><li><code>Ptr Nets</code> can be seen as an application of <strong><code>content-based attention mechanisms</code></strong>.</li></ul>
          </div>
<h2 id="EpiReader-1"><a href="#EpiReader-1" class="headerlink" title="EpiReader"></a>EpiReader</h2><p><img data-src="/notes/images/MRC_EpiReader.png" alt="upload successful"></p>
<h3 id="Extractor-Pointer-Nets"><a href="#Extractor-Pointer-Nets" class="headerlink" title="Extractor: Pointer Nets"></a>Extractor: Pointer Nets</h3><ol>
<li>Use bi-RNNs to encode passage <script type="math/tex">f(\theta_T, \pmb{T})</script> and question <script type="math/tex">g(\theta_Q, \pmb{Q})</script>, where <script type="math/tex">\theta_T</script> and <script type="math/tex">\theta_Q</script> represents the parameters of the text and question encoders, <script type="math/tex">\pmb{T} \in \mathbb{R}^{D \times N}</script> and <script type="math/tex">\pmb{Q} \in \mathbb{R}^{D \times N_Q}</script> are matrix representations of the texts and questions (comprising $N$ words and <script type="math/tex">N_Q</script> words separately) . Concatenate the last hidden states of forward and backward GRU, denoted <script type="math/tex">g(\pmb{Q}) \in \mathbb{R}^{2d}</script></li>
<li><p>Take the inner product of text and question representations, followed by a softmax. The probability that the $i$-th word in text $\tau$ answers <script type="math/tex">\mathcal{Q}</script>:</p>
<script type="math/tex; mode=display">s_i \propto \exp (f(\pmb{t}_i) \cdot g(\pmb{Q}))</script></li>
<li><p>Compute the total probability that word $w$ is the correct answer:</p>
<script type="math/tex; mode=display">P(w \vert \tau, \mathcal{Q}) = \sum_{i: t_i=w} s_i</script></li>
<li>The extractor take the $K$ highest word probabilities with the corresponding $K$ most probable answer words <script type="math/tex">\{\hat{a}_1,\cdots,\hat{a}_K \}</script></li>
</ol>
<h3 id="Reasoner"><a href="#Reasoner" class="headerlink" title="Reasoner"></a>Reasoner</h3><ol>
<li>Insert the answer candidates into the question sequence $\mathcal{Q}$ at the placeholder location, which forms $K$ hypotheses ${ \mathcal{H}_1, \cdots, \mathcal{H}_K }$</li>
<li>For each hypothesis and each sentence of the text: <script type="math/tex">\pmb{S}_i \in \mathbb{R}^{D \times |\mathcal{S}_i|}</script> whose columns are embedding vectors for each word of sentence <script type="math/tex">\mathcal{S}_i</script>, <script type="math/tex">\pmb{H}_k \in \mathbb{R}^{D \times |\mathcal{H}_k|}</script> whose columns are the embedding vectors for each word in the hypothesis <script type="math/tex">\mathcal{H}_k</script></li>
<li>Augment <script type="math/tex">\pmb{S}_i</script> with <code>word-matching features</code> <script type="math/tex">\pmb{M} \in \mathbb{R}^{2 \times |\mathcal{S}_i|}</script>. The first row is the <strong>inner product</strong> of each word embedding in the sentence with the candidate answer embedding; the second row is the <strong>maximum inner product</strong> of each sentence word embedding with any word embedding in the question.</li>
<li>Then the augmented <script type="math/tex">\pmb{S}_i</script> and <script type="math/tex">\pmb{H}_k</script> are fed into two different ConvNets, with filters <script type="math/tex">\pmb{F}^S \in \mathbb{R}^{(D+2) \times m}</script> and <script type="math/tex">\pmb{F}^H \in \mathbb{R}^{D \times m}</script>, where $m$ is the filter width. After ReLU and maxpooling op, we can obtain the representations of the text sentence and the hypothesis: <script type="math/tex">\pmb{r}_{\mathcal{S}_i} \in \mathbb{R}^{N_F}</script>, <script type="math/tex">\pmb{r}_{\mathcal{H}_k} \in \mathbb{R}^{N_F}</script>, where <script type="math/tex">N_F</script> is the number of filters.</li>
<li><p>Then compute a scalar similarity score representations using bilinear form:</p>
<script type="math/tex; mode=display">\zeta = r_{\mathcal{S}_i}^T \pmb{R} \pmb{r}_{\mathcal{H}_k}</script><p>where <script type="math/tex">\pmb{R} \in \mathbb{R}^{N_F \times N_F}</script> is a trainable parameter.</p>
</li>
<li><p>Concat the similarity score with the sentence and hypothesis representations to get: <script type="math/tex">\pmb{x}_{ik} = [\zeta; \pmb{r}_{\mathcal{S}_i}; \pmb{r}_{\mathcal{H}_k}]^T</script></p>
</li>
<li>Pass <script type="math/tex">\pmb{x}_{ik}</script> to a GRU, and the final hidden state is given to an FC layer, followed by a softmax op.</li>
</ol>
<p>Finally, combine the output of the Reasoner and the Extractor at the same time when minimizing the loss function. (See the original paper<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Trischler, A., Ye, Z., Yuan, X., Bachman, P., Sordoni, A., & Suleman, K. (2016). [Natural Language Comprehension with the EpiReader](https://arxiv.org/pdf/1606.02270.pdf). EMNLP.
">[9]</span></a></sup> for details)</p>
<h1 id="Bi-Directional-Attention-Flow-BiDAF"><a href="#Bi-Directional-Attention-Flow-BiDAF" class="headerlink" title="Bi-Directional Attention Flow (BiDAF)"></a>Bi-Directional Attention Flow (BiDAF)</h1><h2 id="Highway-Networks"><a href="#Highway-Networks" class="headerlink" title="Highway Networks"></a>Highway Networks</h2><ul>
<li>A plain feedforward NN consists of $L$ layers where the $l^{th}$ layer $(l \in { 1,2,\cdots,L})$ applies a non-linear transformation $H$ (with parameter <script type="math/tex">\pmb{H,l}</script>) on its input $\pmb{x}$ to the output $\pmb{y}$.<script type="math/tex; mode=display">y = H(\pmb{x}, \pmb{W_H})</script></li>
</ul>
<p>$H$ is usually a affine transformation followed by a non-linear activation function.</p>
<ul>
<li><p><strong>Highway Network</strong>:</p>
<ul>
<li>Additionally define $T$ as the <code>transform gate</code>, $C$ as the <code>carry gate</code>. Intuitionally, these gates express how much of the output is produced by transforming the input and carrying it.<script type="math/tex; mode=display">\pmb{y} = \underbrace{H(\pmb{x}, \pmb{W_H})}_\text{FFNN output} \cdot \underbrace{T(\pmb{x}, \pmb{W_T})}_\text{transform gate} + \pmb{x} \cdot  \underbrace{C(\pmb{x}, \pmb{W_C})}_\text{carry gate}</script></li>
<li><p>For simplicity we set $C = 1 - T$, giving</p>
<script type="math/tex; mode=display">\pmb{y} =  H(\pmb{x}, \pmb{W_H}) \cdot T(\pmb{x}, \pmb{W_T}) + \pmb{x} \cdot ( 1 - T(\pmb{x}, \pmb{W_T}) )</script></li>
<li><p>In particular,</p>
<script type="math/tex; mode=display">\pmb{y}=\left\{
          \begin{array}{ll}
            \pmb{x} \quad \text{if } T(\pmb{x}, \pmb{W_T}) = \pmb{0}, \\
           H(\pmb{x}, \pmb{W_H}) \quad \text{if } T(\pmb{x}, \pmb{W_T}) = \pmb{1}
          \end{array}
        \right.</script></li>
</ul>
</li>
</ul>
<h2 id="BiDAF"><a href="#BiDAF" class="headerlink" title="BiDAF"></a>BiDAF</h2><p>Problems:</p>
<ul>
<li><p>Previous models summarized the context paragraph into a fixed-size vector, which could lead to the information loss.</p>
</li>
<li><p>Solution: the attention is computed at each time step, and the attended vector at each time step, along with the representations from previous  layers, is allowed to <em>flow</em> through to the subsequent modeling layer.</p>
</li>
</ul>
<p><img data-src="/notes/images/MRC-BIDAF.png" alt="upload successful"></p>
<h3 id="Char-embedding-layer"><a href="#Char-embedding-layer" class="headerlink" title="Char embedding layer"></a>Char embedding layer</h3><p>Let <script type="math/tex">\pmb{x}_1, \cdots, \pmb{x}_T</script> and <script type="math/tex">\pmb{q}_1, \cdots, \pmb{q}_J</script> represent the words in the input context paragraph and query. Use TextCNNs to encode the char-level inputs, followed by a max-pooling over the entire width to obtain a fixed-size vector for each word.</p>
<h3 id="Word-embedding-layer"><a href="#Word-embedding-layer" class="headerlink" title="Word embedding layer"></a>Word embedding layer</h3><p>Applied pretrained word embeddings, GloVe.</p>
<p>Then concatenate the char and word embedding vectors, feed them into a 2-layer Highway Network. The outputs are $\pmb{X} \in \mathbb{R}^{2d \times T}$ for the context, and $\pmb{Q} \in \mathbb{R}^{d \times J}$ for the query.</p>
<h3 id="Contextual-embedding-layer"><a href="#Contextual-embedding-layer" class="headerlink" title="Contextual embedding layer"></a>Contextual embedding layer</h3><p>Use bi-LSTMs to encode the context and query representations, by concatenating the last hidden states of each direction. We obtain $\pmb{H} \in \mathbb{R}^{2d \times T}$ from the context word vectors $\pmb{X}$, $\pmb{U} \in \mathbb{R}^{2d \times J}$ from query word vectors $\pmb{Q}$</p>
<div class="note info">
            <p>The first three layers are used to <strong><code>extract features form the query and context at different levels of granularity</code></strong>, akin to mlti-stage feature computation of CNNs in computer vision field.</p>
          </div>
<h3 id="Attention-flow-layer"><a href="#Attention-flow-layer" class="headerlink" title="Attention flow layer"></a>Attention flow layer</h3><ul>
<li>Inputs: the context $\pmb{H}$ and the query $\pmb{U}$.</li>
<li>Outputs: query-aware vector representation of context words, $\pmb{G}$, along with previous contextual embedding</li>
<li>Similarity matrix <script type="math/tex">\pmb{S} \in \mathbb{R}^{T \times J}</script> between the contextual embeddings of the context($\pmb{H}$) and the query ($\pmb{U}$), where $\pmb{S}_{tj}$ indicates the similarity between the $t$-th context word and $j$-th query word:<script type="math/tex; mode=display">\pmb{S}_{tj} = \alpha(\pmb{H}_{:t}, \pmb{U}_{:j}) \in \mathbb{R}</script><script type="math/tex; mode=display">\alpha(\pmb{h},\pmb{u}) = \pmb{w}_{(\pmb{S})}^T [\pmb{h};\pmb{u};\pmb{h} \odot \pmb{u}]</script>where $\alpha$ is a trainable scalar function that encodes the similarity between its input vectors, <script type="math/tex">\pmb{H}_{:t}</script> is $t$-th column vector of $\pmb{H}$ and <script type="math/tex">\pmb{U}_{:j}</script> is $j$-th column vector of $\pmb{U}$.</li>
</ul>
<p>Then use $\pmb{S}$ to obtain the attentions and the attended vectors in both directions.</p>
<ul>
<li><strong>Context-to-query Attention</strong>: context-to-query(C2Q) attention signifies which query words are most relevant to each context word. Let <script type="math/tex">\pmb{a}_t \in \mathbb{R}^J</script> represent the attention weights on the query words by $t$-th context word, <script type="math/tex">\sum_j \pmb{a}_{tj} = 1</script> for each $t$. The attention weight:<script type="math/tex; mode=display">\pmb{a}_t = \text{softmax}(\pmb{S}_{t:}) \in \mathbb{R}^J</script>  Each attended query vector:<script type="math/tex; mode=display">\tilde{\pmb{U}}_{:t} = \sum_j \pmb{a}_{tj} \pmb{U}_{:j}</script>  Here $\tilde{\pmb{U}}$ is a 2$d$-by-$T matrix.</li>
<li><strong>Query-to-context Attention</strong>: query-to-context(Q2C) attention signifies which context words have the closest similarity to one query word and hence crucial for answering. The attention weights on the context words:<script type="math/tex; mode=display">\pmb{b} = \text{softmax}(\max_{col} (\pmb{S})) \in \mathbb{R}^T</script>  where the maximum function (<script type="math/tex">\max_{col}</script>) is performed across the column.<br>  The attended context vector is <script type="math/tex">\tilde{\pmb{h}} = \sum_t \pmb{b}_t \pmb{H}_{:t} \in \mathbb{R}^{2d}</script></li>
</ul>
<p>Finally, concatenate the contextual embeddings and attention vectors:</p>
<script type="math/tex; mode=display">\pmb{G}_{:t} = \beta(\pmb{H}_{:t}, \tilde{U}_{:t}, \tilde{H}_{:t}) \in \mathbb{R}^{d_{G}}</script><p>where <script type="math/tex">\pmb{G}_{:t}</script> is the $t$-th column vector, $\beta$ is a trainable vector function that fuses three input vectors. In the experiments, <script type="math/tex">\\pmb{\beta(h, \tilde{u}, \tilde{h}) = [h; \tilde{u}; h \odot \tilde{u}; h \odot \tilde{h} ] } \in \mathbb{R}^{D_G \times T}</script></p>
<h3 id="Modeling-layer"><a href="#Modeling-layer" class="headerlink" title="Modeling layer"></a>Modeling layer</h3><ul>
<li>Use bi-LSTMs to encode, obtaining a matrix <script type="math/tex">\pmb{M} \in \mathbb{R}^{2d \times T}</script></li>
</ul>
<h3 id="Output-layer"><a href="#Output-layer" class="headerlink" title="Output layer"></a>Output layer</h3><ul>
<li>Application-specific</li>
<li><p>For QA-tasks, find the sub-phrase of the paragraph to answer the query. We obtain the start index over the entire paragraph by:</p>
<script type="math/tex; mode=display">\pmb{p}^1 = \text{softmax}(\pmb{w}^T_{(p^1)} [\pmb{G};\pmb{M}] )</script><p>  For the end index of the answer phrase, we pass $\pmb{M}$ into another bi-LSTM and obtain $\pmb{M}^2 \in \mathbb{R}^{2d \times T}$</p>
<script type="math/tex; mode=display">\pmb{p}^2 = \text{softmax}(\pmb{w}^T_{(p^2)} [\pmb{G};\pmb{M}^2] )</script></li>
<li><p><strong>Training</strong>: minimize the sum of the negative log probabilities of the true start and end indices by the predicted distributions, averaged over all examples:</p>
<script type="math/tex; mode=display">L(\theta) = -\frac{1}{N} \sum_i^N \log(\pmb{i}_{y_i^1}^1) + \log(\pmb{p}_{y_i^w}^2)</script></li>
</ul>
<h1 id="Match-LSTM-and-Answer-pointer"><a href="#Match-LSTM-and-Answer-pointer" class="headerlink" title="Match-LSTM and Answer pointer"></a>Match-LSTM and Answer pointer</h1><h2 id="Match-LSTM"><a href="#Match-LSTM" class="headerlink" title="Match-LSTM"></a>Match-LSTM</h2><ul>
<li>It is used for textual entailment (RTE). In RTE, given two sentences, one <em>premise</em> and another <em>hypothesis</em>, predict where the premise entails the hypothesis. </li>
<li>Match-LSTMs go through the hypothesis sequentially. At each position of the hypothesis, apply attention mechanism to obtain a weighted vector representation of the premise. This weighted vector is combined with current token representation of the hypothesis, then fed to an LSTM.</li>
<li>Match-LSTMs sequentially aggregates the matching of the attention-weighted premise to each token of the hypothesis.</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><ul>
<li>Given the matrix of passage $\pmb{P} \in \mathbb{R}^{d \times P}$, question $\pmb{Q} \in \mathbb{R}^{d \times Q}$, where the $P$ and $Q$ os the length (# of tokens) of the passage and question, $d$ is the dimension of word embeddings. </li>
<li>The answer is a sequence of inteegers <script type="math/tex">\pmb{a} = (a_1,a_2,\cdots)</script>, where each <script type="math/tex">a_i</script> is an integer between 1 and $P$, indicating the certain region in the passage. Or select only the start and end index from input passages, represented as <script type="math/tex">\pmb{a} = (a_s, a_e)</script>, where <script type="math/tex">a_s</script> and <script type="math/tex">a_e</script> are integers between 1 and $P$.</li>
<li>Overall, given <script type="math/tex">\{ \pmb{P}_n, \pmb{Q}_n, \pmb{a}_n \}_{n=1}^N</script>.</li>
<li>Goal: identify a subsequence from the passage as the answer to the question.</li>
</ul>
<p><img data-src="/notes/images/MRC-match-LSTM.png" alt="upload successful"></p>
<h3 id="LSTM-Preprocessing-layer"><a href="#LSTM-Preprocessing-layer" class="headerlink" title="LSTM Preprocessing layer"></a>LSTM Preprocessing layer</h3><ul>
<li>In order to incorporate contextual information to the representation of each token, apply one-dimensional LSTM to process the passage and the question separately.<script type="math/tex; mode=display">\pmb{H}^p = \overleftarrow{\text{LSTM}}(\pmb{P})</script><script type="math/tex; mode=display">\pmb{H}^q = \overrightarrow{\text{LSTM}}(\pmb{Q})</script>  The output <script type="math/tex">\pmb{H}^p \in \mathbb{R}^{l \times P}</script> and <script type="math/tex">\pmb{H}^q \in \mathbb{R}^{l \times Q}</script> are hidden representations of the passage and the question, where $l$ is the hidden dimension.</li>
</ul>
<h3 id="Match-LSTM-layer"><a href="#Match-LSTM-layer" class="headerlink" title="Match-LSTM layer"></a>Match-LSTM layer</h3><ul>
<li>Apply match-LSTM model by sequentially goes through the <strong>passage</strong>, obtaining the weighted representation of question.</li>
<li><p>At position $i$ of the passage, it first uses the standard word-by-word attention mechanism to obtain attention weight <script type="math/tex">\overrightarrow{\alpha}_i \in \mathbb{R}^{Q}</script>:</p>
<script type="math/tex; mode=display">\overrightarrow{\pmb{G}}_i = \tanh \big(\pmb{W}^q \pmb{H}^q + (\pmb{W}^p \pmb{h}_i^p + \pmb{W}^r \overrightarrow{\pmb{h}_{i-1}^r} + \pmb{b}^p ) \otimes \pmb{e}_Q \big)</script><script type="math/tex; mode=display">\overrightarrow{\alpha}_i = \text{softmax} (\pmb{w}^T \overrightarrow{\pmb{G}}_i + b \otimes \pmb{e}_Q)</script><p>  where <script type="math/tex">\pmb{W}^q</script>, <script type="math/tex">\pmb{W}^p</script>, <script type="math/tex">\pmb{W}^r \in \mathbb{R}^{l \times l}</script>, $\pmb{b}^p, \pmb{w} \in \mathbb{R}$ are learnable,  <script type="math/tex">\overrightarrow{\pmb{h}_{i-1}^r} \in \mathbb{R}^l</script> is the hidden vector of the one-directional match-LSTM at previous position. The outer product (<script type="math/tex">\cdot \otimes \pmb{e}_Q</script>) generates a matrix or row vector by repeating the vector or scalar on the left for $Q$ times.</p>
</li>
<li><p>Then combine the weighted vector with original representations:</p>
<script type="math/tex; mode=display">\overrightarrow{\pmb{z}}_i =  \begin{bmatrix} \pmb{h}_i^p  \\ \pmb{h}^q \overrightarrow{\alpha}_i^T \end{bmatrix}</script></li>
</ul>
<p>The vector <script type="math/tex">\overrightarrow{\pmb{z}}_i</script> is fed to a one-directional LSTM, so-called <code>match-LSTM</code>:</p>
<script type="math/tex; mode=display">\overrightarrow{\pmb{h}}_i^r = \overrightarrow{\text{LSTM}}(\overrightarrow{\pmb{z}}_i, \overrightarrow{\pmb{h}}_{i-1}^r)</script><p>where <script type="math/tex">\overrightarrow{\pmb{h}}_i^r \in \mathbb{R}^l</script></p>
<ul>
<li><p>Further apply a match-LSTM in the reverse direction.</p>
<script type="math/tex; mode=display">\overleftarrow{\pmb{G}}_i = \tanh \big(\pmb{W}^q \pmb{H}^q + (\pmb{W}^p \pmb{h}_i^p + \pmb{W}^r \overleftarrow{\pmb{h}_{i-1}^r} + \pmb{b}^p ) \otimes \pmb{e}_Q \big)</script><script type="math/tex; mode=display">\overleftarrow{\alpha}_i = \text{softmax} (\pmb{w}^T \overleftarrow{\pmb{G}}_i + b \otimes \pmb{e}_Q)</script></li>
<li><p>Let <script type="math/tex">\overrightarrow{\pmb{H}^r} \in \mathbb{R}^{l \times P}</script> represent the hidden states <script type="math/tex">[\overrightarrow{\pmb{h}^r_1}, \overrightarrow{\pmb{h}^r_2, \cdots, \overrightarrow{\pmb{h}^r_P}}]</script> and <script type="math/tex">\overleftarrow{\pmb{H}^r} \in \mathbb{R}^{l \times P}</script> represent <script type="math/tex">[\overleftarrow{\pmb{h}^r_1}, \overleftarrow{\pmb{h}^r_2}, \cdots, \overleftarrow{\pmb{h}^r_P}]</script>.</p>
</li>
<li><p>Define <script type="math/tex">\pmb{H}^r \in \mathbb{R}^{2l \times P}</script> as the concatenation:</p>
<script type="math/tex; mode=display">\pmb{H}^r = \begin{bmatrix}
 \overrightarrow{\pmb{H}^r} \\
  \overleftarrow{\pmb{H}^r}
\end{bmatrix}</script></li>
</ul>
<h3 id="Answer-pointer-layer"><a href="#Answer-pointer-layer" class="headerlink" title="Answer pointer layer"></a>Answer pointer layer</h3><h3 id="The-sequence-model"><a href="#The-sequence-model" class="headerlink" title="The sequence model"></a>The sequence model</h3><ul>
<li>Compute the attention weight vector <script type="math/tex">\beta_k \in \mathbb{R}^{(P+1)}</script>:<script type="math/tex; mode=display">\pmb{F}_k = \tanh (\pmb{V} \tilde{H}^r + (\pmb{W}^a \pmb{h}_{k-1}^a + \pmb{b}^a) \otimes \pmb{e}_{(P+1)})</script><script type="math/tex; mode=display">\beta_k = \text{softmax}(\pmb{v}^T \pmb{F}_k + \pmb{c} \otimes \pmb{e}_{(P+1)})</script>where <script type="math/tex">\tilde{H}^r \in \mathbb{R}^{2l \times (P+1)}</script> is the concatenation of $\pmb{H}^r$ with a zero vector, defined as <script type="math/tex">\tilde{H}^r = [\pmb{H}^r; \pmb{0}]</script></li>
</ul>
<script type="math/tex; mode=display">\pmb{h}_k^a = \overrightarrow{\text{LSTM}} (\tilde{\pmb{H}}^r \beta_k^T, \pmb{h}_{k-1}^a)</script><p>Then model the probability of generating the answer sequence as:</p>
<script type="math/tex; mode=display">p(\pmb{a} \vert \pmb{H}^r) = \prod_k p(a_k \vert a_1, a_2, \cdots, a_{k-1}, \pmb{H}^r)</script><script type="math/tex; mode=display">p(a_k = j \vert a_1, a_2, \cdots, a_{k-1}, \pmb{H}^r) = \beta_{k,j}</script><ul>
<li>Minimize the loss:<script type="math/tex; mode=display">J(\theta) = -\sum_{n=1}^N \log p(\pmb{a}_n \vert \pmb{P}_n, \pmb{Q}_n)</script></li>
</ul>
<h3 id="The-boundary-model"><a href="#The-boundary-model" class="headerlink" title="The boundary model"></a>The boundary model</h3><ul>
<li>Predict the start and end index from input sequences. The probability is modeled as:<script type="math/tex; mode=display">p(\pmb{a} \vert \pmb{H}^r) = p(a_s \vert \pmb{H}^r) p(a_e \vert a_s, \pmb{H}^r)</script></li>
</ul>
<h1 id="Gated-self-matching-networks"><a href="#Gated-self-matching-networks" class="headerlink" title="Gated self-matching networks"></a>Gated self-matching networks</h1><ul>
<li>Firstly, apply bi-RNNs to process the question and passage separately; then match the question and passage with gated attention-based RNNs, obtaining question-aware representation for the passage. On top of that, apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation, which is then fed to the output layer to predict the boundary of the answer span.</li>
</ul>
<h2 id="Question-and-passage-encoder"><a href="#Question-and-passage-encoder" class="headerlink" title="Question and passage encoder"></a>Question and passage encoder</h2><ul>
<li>Given question <script type="math/tex">\mathcal{Q} = \{ w_t^Q\}_{t=1}^m</script> and passage <script type="math/tex">\mathcal{P} = \{ w_t^P\}_{t=1}^n</script>.</li>
<li>Concatenate the respective word-level embeddings (<script type="math/tex">\{ e_t^Q\}_{t=1}^m</script> and <script type="math/tex">\{ e_t^P\}_{t=1}^n</script>) and char-level embeddings (<script type="math/tex">\{ c_t^Q\}_{t=1}^m</script> and <script type="math/tex">\{ c_t^P\}_{t=1}^n</script>). The char-level embedding is generated by concatenating the final hidden state of bi-directional RNNs, which is helpful to handel OOV words.</li>
<li>Then use a bi-RNN to produce the new representation of all words in the question and passage respectively:<script type="math/tex; mode=display">u_t^Q = \text{BiRNN}_Q (u_{t-1}^Q, [e_t^Q, c_t^Q])</script><script type="math/tex; mode=display">u_t^P = \text{BiRNN}_P (u_{t-1}^P, [e_t^P, c_t^P])</script></li>
</ul>
<p><img data-src="/notes/images/MRC-gated-self-matching-net.png" alt="upload successful"></p>
<h2 id="Gated-attention-based-RNNs"><a href="#Gated-attention-based-RNNs" class="headerlink" title="Gated attention-based RNNs"></a>Gated attention-based RNNs</h2><ul>
<li>Incorporate an additional gate to determine the importance of information in the passage regarding a question. </li>
<li><p>Rocktäschel et al.(2015)<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rocktäschel, T., Grefenstette, E., Hermann, K.M., Kociský, T., & Blunsom, P. (2016). Reasoning about Entailment with Neural Attention. CoRR, abs/1509.06664.
">[15]</span></a></sup> proposed generating sentence-pair representation <script type="math/tex">\{ v_t^P \}_{t=1}^n</script> via soft-alignment of words in the question and passage:</p>
<script type="math/tex; mode=display">v_t^P = \text{RNN}(v_{t-1}^P, c_t)</script><p>where <script type="math/tex">c_t = \text{att}(u^Q, [u_t^P, v_{t-1}^P])</script> is an attention-pooling vector of the whole question $u^Q$:</p>
<script type="math/tex; mode=display">s_j^t = v^T \tanh (w_u^Q u_j^Q + W_u^P u_t^P + W_v^P v_{t-1}^P)</script><script type="math/tex; mode=display">a_i^t = \frac{\exp (s_i^t)}{\sum_{j=1}^m \exp(s_j^t)}</script><script type="math/tex; mode=display">c_t = \sum_{i=1}^m a_i^t u_i^Q</script></li>
<li><p>Match-LSTM(Wang and Jiang, 2016) takes <script type="math/tex">u_t^P</script> as an additional input into the recurrent network:</p>
<script type="math/tex; mode=display">v_t^P = \text{RNN}(v_{t-1}^P, [u_t^P, c_t])</script></li>
<li><p>To determine the importance of passage parts and attend to the ones relevant to the question, add another gate to the input <script type="math/tex">[u_t^p, c_t]</script> of RNNs:</p>
<script type="math/tex; mode=display">g_t = \text{sigmoid} (W_g [u_t^P, c_t])</script><script type="math/tex; mode=display">[u_t^P, c_t]^* = g_t \odot [u_t^P, c_t]</script></li>
</ul>
<h2 id="Self-matching-attention"><a href="#Self-matching-attention" class="headerlink" title="Self-matching attention"></a>Self-matching attention</h2><ul>
<li><p>Match the question-aware passage representation against itself.</p>
<script type="math/tex; mode=display">h_t^P = \text{BiRNN}(h_{t-1}^P, [v_t^P, c_t])</script><p>where <script type="math/tex">c_t=\text{att}(v^P, v_t^P)</script> is an attention pooling vector of the whole passage $v^P$:</p>
<script type="math/tex; mode=display">s_j^t = v^T \tanh(W_v^P v_j^P + W_v^{\tilde{P}}v_t^P)</script><script type="math/tex; mode=display">a_i^t = \frac{\exp(s_i^t)}{\sum_{j=1}^n \exp(s_j^t)}</script><script type="math/tex; mode=display">c_t = \sum_{i=1}^n a_i^t v_i^P</script></li>
<li><p>An additional gate as in gated attention-based RNNs is applied to <script type="math/tex">[v_t^P, c_t]</script> to adaptively control the input of RNNs.</p>
</li>
</ul>
<h2 id="Output-layer-1"><a href="#Output-layer-1" class="headerlink" title="Output layer"></a>Output layer</h2><ul>
<li>Use pointer net to select the start position ($p^1$) and end position ($p^2$) from the passage:<script type="math/tex; mode=display">s_j^t = v^T \tanh (W_h^P h_j^P + W_h^a h_{t-1}^a)</script><script type="math/tex; mode=display">a_i^t = \frac{\exp(s_i^t)}{\sum_{j=1}^n \exp(s_j^t)}</script><script type="math/tex; mode=display">p^t = \arg \max (a_1^t, \cdots, a_n^t)</script></li>
<li>Utilize the question vector $r^Q$ as the initial state of the answer RNNs: <script type="math/tex">r^Q \text{att}(u^Q, v_r^Q)</script></li>
</ul>
<h1 id="Attention-over-Attention-Reader"><a href="#Attention-over-Attention-Reader" class="headerlink" title="Attention-over-Attention Reader"></a>Attention-over-Attention Reader</h1><ul>
<li><p><strong>Contextual embedding</strong> for document $\mathcal{D}$ and query $\mathcal{Q}$ using bi-GRUs: <script type="math/tex">h_{doc} \in \mathbb{R}^{|\mathcal{D}|*2d}</script>, <script type="math/tex">h_{query} \in \mathbb{R}^{|\mathcal{Q}|*2d}</script></p>
<script type="math/tex; mode=display">e(x) = W_e \cdot x, \text{where } x \in \mathcal{D}, \mathcal{Q}</script><script type="math/tex; mode=display">\overrightarrow{h_s(x)} = \overrightarrow{\text{GRU}}(e(x))</script><script type="math/tex; mode=display">\overleftarrow{h_s(x)} = \overleftarrow{\text{GRU}}(e(x))</script><script type="math/tex; mode=display">h_s(x) = [\overrightarrow{h_s(x)}; \overleftarrow{h_s(x)} ]</script></li>
<li><p><strong>Pair-wise matching score</strong>:<br>  Given $i$-th word of the document and $j$-th word of query, we compute a matching score by dot product, forming a matrix $M \in \mathbb{R}^{\mathcal{D}*\mathcal{Q}} $, where the value of $i$-th row and $j$-th column is filled by $M(i,j)$:</p>
<script type="math/tex; mode=display">M(i,j) = h_{\text{doc}}(i)^T \cdot h_{\text{query}}(j)</script></li>
<li><p><strong>Individual document-level attentions</strong><br>  Apply a <code>column-wise softmax</code> function to get distribution of each column, where each column is an individual <strong>document-level attention</strong> considering a single query word (one element in rows). Let $\alpha(t) \in \mathbb{R}^{|\mathcal{D}|}$ is a <code>query-to-document attention</code> at time $t$:</p>
<script type="math/tex; mode=display">\alpha(t) = \text{softmax} (M(1,t), \cdots, M(|\mathcal{D}|,t))</script><script type="math/tex; mode=display">\alpha = [\alpha(1), \alpha(2), \cdots, \alpha(\mathcal{Q})]</script></li>
</ul>
<p><img data-src="/notes/images/MRC-attn-over-attn.png" alt="upload successful"></p>
<ul>
<li><p><strong>Attention-over-Attention</strong> <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Cui, Y., Chen, Z., Wei, S., Wang, S., & Liu, T. (2017). [Attention-over-Attention Neural Networks for Reading Comprehension](https://aclweb.org/anthology/P17-1055). ACL.
">[7]</span></a></sup></p>
<ol>
<li>First, for each document word at time $t$, compute the “importance” distribution on the query, indicating which query words are most important given a single document word.</li>
<li>Apply <code>row-wise softmax</code> function to the pair-wise matching matrix $M$ to get query-level attentions. The document-to-query attention $\beta(t) \in \mathbb{R}^{|\mathcal{Q}|}$ is；<script type="math/tex; mode=display">\beta(t) = \text{softmax}\big(M(t,1), \cdots, M(t_m, |\mathcal{Q}|)\big)</script></li>
<li>We average the attention for each query word:<script type="math/tex; mode=display">\beta = \frac{1}{n} \sum_{t=1}^{|\mathcal{D}|} \beta(t)</script></li>
<li>Calculate the dot product of $\alpha$ and $\beta$ to get the <code>attended document-level attention</code>:<script type="math/tex; mode=display">s = \alpha^T \beta</script></li>
</ol>
</li>
<li><p>Predictions<br>  The final output is mapped to the vocabulary space $V$, rather than document-level attention $|\mathcal{D}|$:</p>
<script type="math/tex; mode=display">p(W \vert \mathcal{D}, \mathcal{Q}) = \sum_{i \in I(w, \mathcal{D})}  s_i, w \in V</script><p>  where $I(w, \mathcal{D})$ indicate the positions that word $w$ appears in the document $\mathcal{D}$. </p>
<p>  The training objective is to maximize the log-likelihood of the correct answer:</p>
<script type="math/tex; mode=display">\mathcal{L} = \sum_i \log{(p(x))}, x \in \mathcal{A}</script></li>
</ul>
<h1 id="R-Net"><a href="#R-Net" class="headerlink" title="R-Net"></a>R-Net</h1><p><strong>Overview</strong>:</p>
<ol>
<li>First, the question $Q$ and passage $P$ are processed by a bi-RNNs separately.</li>
<li>Then, match the $Q$ and $P$ with gated attention-based RNNs, obtaining question-aware representation for the passage $P$</li>
<li>Apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation.</li>
<li>Feed into the output layer to predict the boundary of the answer span.</li>
</ol>
<p><img data-src="/notes/images/R-Net.png" alt="upload successful"></p>
<h2 id="Question-and-passage-encoder-1"><a href="#Question-and-passage-encoder-1" class="headerlink" title="Question and passage encoder"></a>Question and passage encoder</h2><p>Consider a question <script type="math/tex">Q = \{ w_t^Q \}_{t=1}^m</script> and a passage <script type="math/tex">P=\{ w_t^P \}^n_{t=1}</script>. </p>
<ul>
<li>First convert words to word-level embeddings <script type="math/tex">\{ e_t^Q\}_{t=1}^m</script> and <script type="math/tex">\{ e_t^P \}_{t=1}^n</script> and char-level embeddings <script type="math/tex">\{ c_t^Q\}_{t=1}^m</script> and <script type="math/tex">\{ c_t^P \}_{t=1}^n</script> (generated by the final hidden states of bi-RNNs, which benefits for OOV tokens)</li>
<li>Then use a bi-RNN to encode the question and passage respectively:<script type="math/tex; mode=display">u_t^Q = \text{bi-RNN}_Q (u_{t-1}^Q, [e_t^Q, c_t^Q])</script><script type="math/tex; mode=display">u_t^P = \text{bi-RNN}_P (u_{t-1}^P, [e_t^P, c_t^P])</script></li>
</ul>
<h2 id="Gated-attention-based-RNNs-1"><a href="#Gated-attention-based-RNNs-1" class="headerlink" title="Gated attention-based RNNs"></a>Gated attention-based RNNs</h2><p>Given question representation <script type="math/tex">\{u_t^Q\}_{t=1}^m</script> and passage representation <script type="math/tex">\{ u_t^P \}_{t=1}^n</script>.</p>
<ul>
<li>Generate sentence-pair representation <script type="math/tex">\{v_t^P\}_{t=1}^n</script> with soft-alignment of words in the question and passage:<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rocktäschel, T., Grefenstette, E., Hermann, K.M., Kociský, T., & Blunsom, P. (2016). [Reasoning about Entailment with Neural Attention](https://arxiv.org/pdf/1509.06664.pdf). CoRR, abs/1509.06664.
">[4]</span></a></sup><script type="math/tex; mode=display">\pmb{v_t^P} = \text{RNN} (v_{t-1}^P, c_t)</script>where <script type="math/tex">c_t = \text{att}(u^Q, [u_t^P, v_{t-1}^P])</script> is an attention-pooling vector of the whole question $(u^Q)$:<script type="math/tex; mode=display">s_j^t = v^T \text{tanh}(W_u^Q u_j^Q + W_u^P u_t^P + W_V^P v_{t-1}^P)</script><script type="math/tex; mode=display">a_i^t = \frac{\exp(s_i^t)}{\sum_{j=1}^m \exp{(s_j^t)} }</script><script type="math/tex; mode=display">c_t = \sum_{i=1}^m a_i^t u_i^Q</script></li>
</ul>
<p>Each passage representation <script type="math/tex">v_t^P</script> dynamically incorporates aggregated matching information from the whole question.</p>
<p>or</p>
<ul>
<li><strong>match-LSTM</strong><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, S., & Jiang, J. (2016). [Learning Natural Language Inference with LSTM](https://arxiv.org/pdf/1512.08849.pdf). HLT-NAACL.
">[5]</span></a></sup>. Take <script type="math/tex">u_t^P</script> as an additional input into the RNNs:<script type="math/tex; mode=display">\pmb{v_t^P} = \text{RNN}(v_{t-1}^P, [u_t^P, c_t])</script>To determine the importance of passage parts and attend to the ones relevant to the question, add another gate <script type="math/tex">g_t</script> to the input <script type="math/tex">([u_t^P, c_t])</script> of RNN:<script type="math/tex; mode=display">g_t = \text{sigmoid}(W_g [u_t^P, c_t])</script><script type="math/tex; mode=display">[u_t^P, c_t]^* = g_t \odot [u_t^P, c_t]</script></li>
</ul>
<h2 id="Self-matching-attention-1"><a href="#Self-matching-attention-1" class="headerlink" title="Self-matching attention"></a>Self-matching attention</h2><p>Given question-aware passage representation <script type="math/tex">\{ v_t^P \}_{t=1}^n</script>. One problem is that, it has very limited knowledge of context,</p>
<p><strong>Solution</strong>: match the question-aware passage representation against itself.</p>
<script type="math/tex; mode=display">\pmb{h_t^P} = \text{bi-RNN}(h_{t-1}^P, [v_t^P, c_t])</script><p>where <script type="math/tex">c_t = \text{att}(v^P, v_t^P)</script> is an attention-pooling vector of the whole passage $(v^P)$:</p>
<script type="math/tex; mode=display">s_j^t = v^T \text{tanh}(W_u^P v_j^P + W_v^{\tilde{P}} v_t^P)</script><script type="math/tex; mode=display">a_i^t = \frac{\exp{(s_i^t)}}{\sum_{j=1}^n \exp{(s_j^t)}}</script><script type="math/tex; mode=display">c_t = \sum_{i=1}^n a_i^t v_i^P</script><p>An additional gate as in gated attention-based RNNs is applied to <script type="math/tex">[v_t^P, c_t]</script> to adaptively control the input of RNNs.</p>
<h2 id="Output-layer-2"><a href="#Output-layer-2" class="headerlink" title="Output layer"></a>Output layer</h2><p>Given the passage representation <script type="math/tex">\{ h_t^P \}_{t=1}^n</script></p>
<ul>
<li><p>Use pointer networks to predict the start and the end position of the answer.</p>
</li>
<li><p>Attention mechanism is utilized as the pointer to select the start position $(p^1)$ and end position $(p^2)$:</p>
<script type="math/tex; mode=display">s_j^t = v^T \text{tanh}(W_h^P h_j^P + W_h^a h_{t-1}^a)</script><script type="math/tex; mode=display">a_i^t = \frac{\exp{(s_i^t)}}{\sum_{j=1}^n \exp{(s_j^t)}}</script><script type="math/tex; mode=display">o^t = \arg\max{a_1^t, \cdots, a_n^t}</script><p>here <script type="math/tex">h_{t-1}^a</script> represents the last hidden state of the answer RNNs(pointer net).</p>
</li>
</ul>
<p>The input of the answer RNN is the attention-pooling vector:</p>
<script type="math/tex; mode=display">c_t= \sum_{i=1}^n a_i^t h_i^P</script><script type="math/tex; mode=display">h_t^a = \text{RNN}(h_{t-1}^a, c_t)</script><p>When predicting the <strong>start position</strong>, <script type="math/tex">h_{t-1}^a</script> represents the initial hidden state of the answer RNN. We use the question vector $r^Q$ as the initial state of the answer RNN. <script type="math/tex">r^Q = \text{att}(u^Q, V_r^Q)</script> is an attention-pooling vector of the question based on the parameter <script type="math/tex">V_r^Q</script>:</p>
<script type="math/tex; mode=display">s_j v^T \text{tanh}(W_u^Q u_j^Q + W_v^Q V_r^Q)</script><script type="math/tex; mode=display">a_i = \frac{\exp{(s_i)}}{\sum_{j=1}^m \exp{(s_j)}}</script><script type="math/tex; mode=display">r^Q = \sum_{i=1}^m a_i u_i^Q</script><ul>
<li><strong>Loss</strong>: the sum of negative log probabilities of the label start and end position by the predicted distributions.</li>
</ul>
<h1 id="Reasoning-Network-ReasoNet"><a href="#Reasoning-Network-ReasoNet" class="headerlink" title="Reasoning Network (ReasoNet)"></a>Reasoning Network (ReasoNet)</h1><ul>
<li>ReasoNet mimics the inference process of human readers by introducing a termination state in the inference with reinforcement learning. The state can decide whether to continue the inference to the next turn after digesting intermediate information, or to terminate the whole inference when it concludes that existing information is sufficient to yield an answer.</li>
</ul>
<p><img data-src="/notes/images/MRC-ReasoNet.png" alt="upload successful"></p>
<ul>
<li>The stochastic inference process can be seen as a POMDP. The state sequence <script type="math/tex">s_{1:T}</script> is controlled by an RNN sequence model. The ReasoNet performs an answer action <script type="math/tex">a_T</script> at $T$-th step, which implies that the termination gate variables <script type="math/tex">t_{1:T} = (t_1=0, t_2=0, \cdots, t_{t-1}=0, t_T=1)</script>. </li>
<li>The ReasoNet learns a stochastic policy <script type="math/tex">\pi((t_t, a_t) \vert s_t; \theta)</script> with parameters $\theta$ to get a distribution of termination actions if the model decides to stop at the current step.</li>
</ul>
<p><img data-src="/notes/images/MRC-ReasoNet-algorithm.png" alt="upload successful"></p>
<ul>
<li><p>The expected reward for an instance is:</p>
<script type="math/tex; mode=display">J(\theta) = \mathbb{E}_{\pi(t_{1:T}, a_T, \theta)} \big[ \sum_{t=1}^T r_t \big]</script><ul>
<li><p>The reward can only be received at the final termination step when an asnwer action <script type="math/tex">a_T</script> is performed.</p>
<script type="math/tex; mode=display">r_T=\left\{
          \begin{array}{ll}
            \begin{align}
            1 & \text{ if } t_T=1 \text{ and the answer is correct}\\
            0 & \text{ otherwise}
            \end{align}
          \end{array}
        \right.</script><ul>
<li><p>$J$ can be maximized by directly applying gradient based optimization methods:</p>
<script type="math/tex; mode=display">\nabla_\theta J(\theta) =\mathbb{E}_{\pi(t_{1:T}, a_T; \theta)} \big[ \nabla_\theta \log \pi(t_{1:T}, a_T; \theta) r_t \big]</script></li>
<li><p>Motivated by REINFORCE algorithm, we compute <script type="math/tex">\nabla_\theta J(\theta)</script>:</p>
<script type="math/tex; mode=display">\mathbb{E}_\pi(t_{1:T, a_T; \theta}) \big[ \nabla_\theta \log \pi(t_{1:T}, a_T; \theta) r_t \big] = \sum_{(t_{1:T}, a_T) \in \mathbb{A}} \pi(t_{1:T}, a_T; \theta) \big[ \nabla_\theta \log \pi(t_{1:T}, a_T; \theta) (r_T - b_T) \big]</script><p>where <script type="math/tex">b_T = \mathbb{E}[r_T]</script> and can be updated via online moving average approach: <script type="math/tex">b_T = \lambda b_T + (1- \lambda) b_t</script></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Cross-passage-answer-verification"><a href="#Cross-passage-answer-verification" class="headerlink" title="Cross-passage answer verification"></a>Cross-passage answer verification</h1><ol>
<li>Compute the question-aware representation for each passage. Employ a Pointer network    to predict the start and end position of the answer in the module of <strong>answer boundary prediction</strong>.</li>
<li>Meanwhile, with the <strong>answer content module</strong>, we estimate whether each word should be included in the answer.</li>
<li>In the <strong>answer verification module</strong>, each answer candidate can attend to the other answer candidates to collect supportive information and compute one score for each candidate to indicate whether it is correct or not according to the verification.</li>
</ol>
<p><img data-src="/notes/images/MRC-cross-passage-verification.png" alt="upload successful"></p>
<h2 id="Question-and-passage-modeling"><a href="#Question-and-passage-modeling" class="headerlink" title="Question and passage modeling"></a>Question and passage modeling</h2><ul>
<li><p><strong>Encoding</strong>: map each word into the vector space by concatenating the word embedding and sum of its char-embeddings. Then employ bi-LSTM to encode the question $\pmb{Q}$ and passages <script type="math/tex">\{ \pmb{P}_i\}</script>:</p>
<script type="math/tex; mode=display">\pmb{u}_t^Q = \text{biLSTM}_Q(\pmb{u}_{t-1}^Q, [\pmb{e}_t^Q, \pmb{c}_t^Q])</script><script type="math/tex; mode=display">\pmb{u}_t^{P_i} = \text{biLSTM}_P(\pmb{u}_{t-1}^{P_i}, [\pmb{e}_t^{P_i}, \pmb{c}_t^{P_i}])</script><p>where <script type="math/tex">\pmb{e}_t^Q</script>, <script type="math/tex">\pmb{c}_t^Q</script> are word-level and char-level embeddings of the $t$-th word.</p>
</li>
<li><p><strong>Q-P Matching</strong>: use the attention flow layer to conduct Q-P matching in two directions. The similarity between the $t$-th word in the question and $k$-th word in passage $i$ is:</p>
<script type="math/tex; mode=display">\pmb{S}_{t,k} = \pmb{u}_t^{QT} \cdot \pmb{u}_k^{P_i}</script></li>
</ul>
<p>Then the context-to-question attention and question-to-context attention is applied as aforementioned BiDAF to obtain the question-aware passage representation <script type="math/tex">\{ \pmb{\tilde{u}}_t^{P_i}\}</script>.</p>
<p>The match output:</p>
<script type="math/tex; mode=display">\pmb{v}_t^{P_i} = \text{BiLSTM}_M (\pmb{v}_{t-1}^{P_i}, \pmb{\tilde{u}}_t^{P_i})</script><h2 id="Answer-boundary-prediction"><a href="#Answer-boundary-prediction" class="headerlink" title="Answer boundary prediction"></a>Answer boundary prediction</h2><ul>
<li><p>Employ Pointer net to compute the probability of each word to be the start or end position of the span:</p>
<script type="math/tex; mode=display">g_k^t = {\pmb{w}_1^{\alpha}}^T \tanh(\pmb{W}_2^\alpha [\pmb{v}_k^P, \pmb{h}_{t-1}^a])</script><script type="math/tex; mode=display">\alpha_k^t = \frac{ \exp (g_k^t)}{\sum_{j=1}^{\pmb{|P|}} \exp(g_j^t)}</script><script type="math/tex; mode=display">\pmb{c}_t = \sum_{k=1}^{|\pmb{P}|} \alpha_k^t \pmb{v}_k^P</script><script type="math/tex; mode=display">\pmb{h}_t^a = \text{LSTM}(\pmb{h}_{t-1}^a, \pmb{c}_t)</script></li>
<li><p>The probability of $k$-th word in the passage to be the start and end position of the answer is obtained as <script type="math/tex">\alpha_k^1</script> and <script type="math/tex">\alpha_k^2</script></p>
</li>
<li>Minimize the negative log probabilities of the true start and end indices:<script type="math/tex; mode=display">\mathcal{L}_{\text{boundaries}} = -\frac{1}{N} \sum_{i=1}^N (\log \alpha_{Y_i^1}^1 + \log \alpha_{y_i^2}^2)</script>where $N$ is the # of samples in the dataset and <script type="math/tex">y_i^1</script>, <script type="math/tex">y_i^2</script> are the gold start and end positions.</li>
</ul>
<h2 id="Answer-content-modeling"><a href="#Answer-content-modeling" class="headerlink" title="Answer content modeling"></a>Answer content modeling</h2><ul>
<li><p>We predict whether each word should be included in the context of the answer. The content probability of the $k$-th word is computed as:</p>
<script type="math/tex; mode=display">p_k^c = \text{sigmoid}({\pmb{w}_1^c}^T \text{ReLU}(\pmb{W}_2^c \pmb{v}_k ^{P_i}))</script></li>
<li><p>Words within the answer span are labeled as 1 and the other 0. The loss is averaged cross entropy:</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text{content}} = -\frac{1}{N} \frac{1}{|P|} \sum_{i=1}^N \sum_{j=1}^{|P|} [y_k^c \log p_k^c + (1-y_k^c)\log(1-p_k^c)]</script></li>
<li><p>The content probabilities provide another view to measure the quality of the answer in addition to the boundary. Moreover, with these probabilities, we can represent the answer from passage $i$ as a weighted sum of all word embeddings:</p>
<script type="math/tex; mode=display">\pmb{r}^{A_i} = \frac{1}{|\pmb{P}_i|} \sum_{k=1}^{|\pmb{P}_i|} p_k^c [\pmb{e}_k^{P_i}, \pmb{c}_k^{P_i}]</script></li>
</ul>
<h2 id="Cross-passage-answer-verification-1"><a href="#Cross-passage-answer-verification-1" class="headerlink" title="Cross-passage answer verification"></a>Cross-passage answer verification</h2><ul>
<li>The boundary and content model focus on modeling within <strong>a single passage</strong>, with little consideration of the cross-passage information.</li>
<li>Given the representation of the answer candidates from all passages <script type="math/tex">\{ \pmb{r}^{A_i}\}</script>, each answer candidate then attends to other candidates to collect supportive information via attention mechanism:<script type="math/tex; mode=display">s_{i,j} = \left\{ 
  \begin{array}{ll}
  0 & \text{ if } i=j \\
  {\pmb{r}^{A_i}}^T \cdot \pmb{r}^{A_j} & otherwise
     \end{array}
  \right.</script></li>
</ul>
<script type="math/tex; mode=display">\alpha_{i,j} = \frac{\exp(s_{i,j})}{\sum_{k=1}^n \exp(s_{i,k})}</script><script type="math/tex; mode=display">\tilde{\pmb{r}}^{A_i} =\sum_{j=1}^n \alpha_{i,j} \pmb{r}^{A_j}</script><p>Here <script type="math/tex">\tilde{\pmb{r}}^{A_i}</script> is the collected verification information from other passages with attention weights. Then we pass it together with the original <script type="math/tex">\pmb{r}^{A_i}</script> to a FC layer:</p>
<script type="math/tex; mode=display">g_i^v = {\pmb{w}^v}^T [\pmb{r}^{A_i}, \tilde{\pmb{r}}^{A_i}, \pmb{r}^{A_i} \odot \tilde{\pmb{r}}^{A_i} ]</script><p>Then normalize the score:</p>
<script type="math/tex; mode=display">p_i^v = \frac{\exp(g_i^v)}{\sum_{j=1}^n \exp(g_j^v)}</script><p>The loss function:</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text{verify}} = -\frac{1}{N} \sum_{i=1}^N \log p_{y_i^v}^v</script><p>where <script type="math/tex">y_i^v</script> is the index of the correct answer in all the answer candidates of the $i$-th instance.</p>
<h2 id="Joint-training"><a href="#Joint-training" class="headerlink" title="Joint training"></a>Joint training</h2><p>The joint objective function:</p>
<script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{boundary} + \mathcal{L}_{content} + \mathcal{L}_{verify}</script><h1 id="QANet"><a href="#QANet" class="headerlink" title="QANet"></a>QANet</h1><p>Previous models relied on Recurrent neural nets, slowing down the training and inference speed. QANet<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Yu, A.W., Dohan, D., Luong, M., Zhao, R., Chen, K., Norouzi, M., & Le, Q.V. (2018). [QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541). CoRR, abs/1804.09541.">[17]</span></a></sup> applied exclusively convolutions and self-attentions to speed up the training process.</p>
<h2 id="QANet-architecture"><a href="#QANet-architecture" class="headerlink" title="QANet architecture"></a>QANet architecture</h2><h3 id="Input-embedding-layer"><a href="#Input-embedding-layer" class="headerlink" title="Input embedding layer"></a>Input embedding layer</h3><ul>
<li>Concatenate the pretrained word embedding <script type="math/tex">x_w</script> and char embedding <script type="math/tex">x_c</script>: <script type="math/tex">[x_w; x_c] \in \pmb{R}^{p_1+p_2}</script>. Also adopt a two-layer high-way network on top of the representation.</li>
</ul>
<h3 id="Embedding-encoder-layer"><a href="#Embedding-encoder-layer" class="headerlink" title="Embedding encoder layer"></a>Embedding encoder layer</h3><p><img data-src="/notes/images/MRC-QANet.png" alt="upload successful"></p>
<ul>
<li>A stack of the building block: [convolution-layer $\times$ # + self-attention layer + feed-forword layer]</li>
</ul>
<h3 id="Context-query-attention-layer"><a href="#Context-query-attention-layer" class="headerlink" title="Context-query attention layer"></a>Context-query attention layer</h3><ul>
<li><p>Firstly compute the similarity matrix $S$ between each word pair of context $C$ and query $Q$, i.e. $S \in \pmb{R}^{n \times m}$. We then normalize each row of $S$ by applying the softmax function, getting a matrix $\bar{S}$. Then the context to query attention is computed as: $A = \bar{S} \cdot Q^T \in \pmb{R}^{n \times d}$.</p>
</li>
<li><p>The similarity function is the trilinear function:</p>
<script type="math/tex; mode=display">f(q,c) = W_0 [q,c, q \odot c]</script><p>where $\odot$ is the element-wise multiplication and <script type="math/tex">W_0</script> is a trainable variable.</p>
</li>
</ul>
<p>The query-to-context attention is:</p>
<script type="math/tex; mode=display">B = \bar{S} \odot {\overline{\overline{S}}}^T \odot C^T</script><p>where $\overline{\overline{S}}$ is normalized matrix of $S$ along column with softmax function.</p>
<h3 id="Model-encoder-layer"><a href="#Model-encoder-layer" class="headerlink" title="Model encoder layer"></a>Model encoder layer</h3><p>The input at each position is $[c,a, c \odot a, c \odot b]$, where $a$ and $b$ are respectively a row of attention matrix $A$ and $B$.</p>
<h3 id="Output-layer-3"><a href="#Output-layer-3" class="headerlink" title="Output layer"></a>Output layer</h3><ul>
<li><p>Predict the probability of each position in the context being the start and end of an answer span</p>
<script type="math/tex; mode=display">p^1 = \text{softmax}(W_1[M_0;M_1])</script><script type="math/tex; mode=display">p^2 = \text{softmax}(W_2[M_0;M_2])</script><p>where <script type="math/tex">W_1</script>, <script type="math/tex">W_2</script> are two trainable variables and <script type="math/tex">M_0</script>, <script type="math/tex">M_1</script>, <script type="math/tex">M_2</script> are the outputs of the three model encoders, from bottom to top.</p>
</li>
<li><p>Loss function</p>
<script type="math/tex; mode=display">\mathcal{L}(\theta) = -\frac{1}{N}\sum_{i}^{N} [\log (p_{y_i^1}) + \log (p^2_{y_i^2})]</script></li>
</ul>
<h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><ul>
<li>Data augmentation with back-translation</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jason Weston, Sumit Chopra, and Antoine Bordes (2014). <a href="https://arxiv.org/pdf/1410.3916">Memory networks</a>. arXiv preprint arXiv:1410.3916.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hermann, K.M., Kociský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp; Blunsom, P. (2015). <a href="https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf">Teaching Machines to Read and Comprehend</a>. NIPS.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kadlec, R., Schmid, M., Bajgar, O., &amp; Kleindienst, J. (2016). <a href="https://arxiv.org/pdf/1603.01547">Text understanding with the attention sum reader network</a>. CoRR, abs/1603.01547.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rocktäschel, T., Grefenstette, E., Hermann, K.M., Kociský, T., &amp; Blunsom, P. (2016). <a href="https://arxiv.org/pdf/1509.06664.pdf">Reasoning about Entailment with Neural Attention</a>. CoRR, abs/1509.06664.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, S., &amp; Jiang, J. (2016). <a href="https://arxiv.org/pdf/1512.08849.pdf">Learning Natural Language Inference with LSTM</a>. HLT-NAACL.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kadlec, R., Schmid, M., Bajgar, O., &amp; Kleindienst, J. (2016). <a href="https://www.aclweb.org/anthology/P16-1086">Text Understanding with the Attention Sum Reader Network</a>. CoRR, abs/1603.01547.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cui, Y., Chen, Z., Wei, S., Wang, S., &amp; Liu, T. (2017). <a href="https://aclweb.org/anthology/P17-1055">Attention-over-Attention Neural Networks for Reading Comprehension</a>. ACL.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Fortunato, M., &amp; Jaitly, N. (2015). <a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf">Pointer Networks</a>. NIPS.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Trischler, A., Ye, Z., Yuan, X., Bachman, P., Sordoni, A., &amp; Suleman, K. (2016). <a href="https://arxiv.org/pdf/1606.02270.pdf">Natural Language Comprehension with the EpiReader</a>. EMNLP.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Seo, M.J., Kembhavi, A., Farhadi, A., &amp; Hajishirzi, H. (2017). <a href="https://arxiv.org/pdf/1611.01603.pdf">Bidirectional Attention Flow for Machine Comprehension</a>. CoRR, abs/1611.01603.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, S., &amp; Jiang, J. (2017). <a href="https://arxiv.org/pdf/1608.07905.pdf">Machine Comprehension Using Match-LSTM and Answer Pointer</a>. CoRR, abs/1608.07905.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, W., Yang, N., Wei, F., Chang, B., &amp; Zhou, M. (2017). <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf">R-NET: Machine reading comprehension with self-matching networks</a>. Natural Lang. Comput. Group, Microsoft Res. Asia, Beijing, China, Tech. Rep, 5.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shen, Y., Huang, P., Gao, J., &amp; Chen, W. (2016). <a href="http://dl.acm.org/citation.cfm?id=3098177">ReasoNet: Learning to Stop Reading in Machine Comprehension</a>. CoCo@NIPS.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, W., Yang, N., Wei, F., Chang, B., &amp; Zhou, M. (2017). <a href="https://pdfs.semanticscholar.org/b798/cfd967e1a9ca5e7bc995d33a907bf65d1c7f.pdf?_ga=2.85729691.304907061.1555921866-1863904407.1553653768">Gated Self-Matching Networks for Reading Comprehension and Question Answering</a>. ACL.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rocktäschel, T., Grefenstette, E., Hermann, K.M., Kociský, T., &amp; Blunsom, P. (2016). Reasoning about Entailment with Neural Attention. CoRR, abs/1509.06664.<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, Y., Liu, K., Liu, J., He, W., Lyu, Y., Wu, H., Li, S., &amp; Wang, H. (2018). <a href="https://arxiv.org/pdf/1805.02220.pdf">Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification</a>. ACL.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yu, A.W., Dohan, D., Luong, M., Zhao, R., Chen, K., Norouzi, M., &amp; Le, Q.V. (2018). <a href="https://arxiv.org/abs/1804.09541">QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a>. CoRR, abs/1804.09541.<a href="#fnref:17" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Machine Reading Comprehension</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Counting the Number of Parameters in Deep Learning</title>
    <url>/notes/2019/10/11/NN/Counting-the-number-of-parameters-in-deep-learning/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A guide to calculate the number of trainable parameters by hand.</p>
<span id="more"></span>
<h1 id="Feed-forward-NN"><a href="#Feed-forward-NN" class="headerlink" title="Feed-forward NN"></a>Feed-forward NN</h1><p>FFNN</p>
<script type="math/tex; mode=display">y = X \cdot w^T + b</script><p>Given</p>
<ul>
<li>$\pmb{i}$ input size</li>
<li>$\pmb{o}$ output size</li>
</ul>
<p>For each FFNN layer</p>
<script type="math/tex; mode=display">\begin{align}
\text{the # of params} &= \text{connections between layers + biases of the layer} \\ &= (\pmb{i} \times  \pmb{o}) + (\pmb{o})
\end{align}</script><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>Given</p>
<ul>
<li>$\pmb{n}$ # of FFNN in each unit<ul>
<li>RNN: 1</li>
<li>GRU: 3</li>
<li>LSTM: 4</li>
</ul>
</li>
<li>$\pmb{i}$ input size</li>
<li>$\pmb{h}$ hidden size</li>
</ul>
<p>For each FFNN, the input state and previous hidden state are concatenated, thus each FFFN has $\pmb{(h+i) \times h} + \pmb{h}$ parameters. </p>
<p>The total # of params is</p>
<script type="math/tex; mode=display">\pmb{n} \times \left[ \pmb{(h+i) \times h} + \pmb{h} \right]</script><ul>
<li>LSTM:  <script type="math/tex">4 \times \left[ \pmb{(h+i) \times h} + \pmb{h} \right]</script></li>
<li>GRU:  <script type="math/tex">3 \times \left[ \pmb{(h+i) \times h} + \pmb{h} \right]</script></li>
<li>RNN:  <script type="math/tex">1 \times \left[ \pmb{(h+i) \times h} + \pmb{h} \right]</script></li>
</ul>
<h2 id="LSTMs"><a href="#LSTMs" class="headerlink" title="LSTMs"></a>LSTMs</h2><script type="math/tex; mode=display">\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  = \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c)</script><p><img data-src="/notes/images/lstms.png" alt="Image source:&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;[Towards data science: Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
&quot;&gt;[2]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>Given</p>
<ul>
<li>$\pmb{i}$ input channel </li>
<li>$\pmb{f}$ filter size</li>
<li>$\pmb{o}$ output channel (i.e. # of filters)</li>
</ul>
<script type="math/tex; mode=display">\# \text{ of params} =  [\pmb{i} \times (\pmb{f} \times \pmb{f}) \times \pmb{o}] + \pmb{o}</script><p><img data-src='/notes/images/conv_param_example.png' width="70%"/></p>
<center> Image source: <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Towards data science: Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
">[2]</span></a></sup></center>

<h1 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h1><p>Given</p>
<ul>
<li>$\pmb{x}$ denotes the embedding dim == model dimension == output dimension</li>
</ul>
<h2 id="MHDPA"><a href="#MHDPA" class="headerlink" title="MHDPA"></a>MHDPA</h2><p><img data-src="/notes/images/multi-head.png" alt="Image source: &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). In Advances in neural information processing systems (pp. 5998-6008).&quot;&gt;[3]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<ul>
<li>Scaled dot product <script type="math/tex; mode=display">\text{Attention}(\mathbf{Q}W_Q^\top, \mathbf{K}W_K^\top, \mathbf{V}W_V^\top) = \text{softmax}(\frac{(\mathbf{Q}W_Q^\top) (\mathbf{K}W_K^\top)^\top}{\sqrt{d_{k}}})(\mathbf{V}W_V^\top)</script></li>
<li><p>Multi-head dot product attention (MHDPA)</p>
<script type="math/tex; mode=display">\text{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{concat}(\text{head}_1,...,\text{head}_h) \mathbf{W}^O</script></li>
<li><p>Overall, MHDPA has 4 linear connections (i.e., K, V, Q, output after concat). There are $4 \times \left[ (\pmb{x} \times \pmb{x}) + \pmb{x} \right]$ trainable parameters.</p>
</li>
</ul>
<h2 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h2><p><img data-src="/notes/images/transformer-encoder.png" alt="Image source: &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). In Advances in neural information processing systems (pp. 5998-6008).&quot;&gt;[3]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;"></p>
<p>Given </p>
<ul>
<li><p>$\pmb{m}$ is # of encoder stacks</p>
</li>
<li><p>Layer normalization</p>
<script type="math/tex; mode=display">y = \gamma * \frac{x - E[x]}{\sqrt{\text{Var}[x]+\epsilon}} + \beta</script><p>  the param number of single layer norm is sum the count of weights $\gamma$ and biases $\beta$: $\pmb{x}+\pmb{x}$</p>
</li>
<li><p>FFNN: param number of two linear layers = $(\pmb{x} \times \pmb{4x} + \pmb{4x}) + (\pmb{4x} \times \pmb{x} + \pmb{x})$</p>
</li>
</ul>
<p>Thus the total number of transformer encoder is: sum the number of 1 MHDPA, 2 Layer norm, 1 two-layer FFNN, times the stack number $\pmb{m}$:</p>
<script type="math/tex; mode=display">\pmb{m} \times [1 \times \underbrace{\big(4 \times \left[ (\pmb{x} \times \pmb{x}) + \pmb{x} \right]\big)}_\text{MHDPA} + 2 \times \underbrace{\big( \pmb{x}+\pmb{x} \big)}_\text{Layer Norm} + \underbrace{\big( 2\times 4\pmb{x} \times \pmb{x} + 5\pmb{x} \big)}_\text{FFNN}]</script><h2 id="Transformer-Decoder"><a href="#Transformer-Decoder" class="headerlink" title="Transformer Decoder"></a>Transformer Decoder</h2><p>Given </p>
<ul>
<li>$\pmb{n}$ is # of decoder stacks</li>
</ul>
<p><img data-src="/notes/images/transformer-decoder.png" alt=""></p>
<p>The total number of transformer decoder is: sum the number of 2 MHDPA, 3 Layer norm, 1 two-layer FFNN, times the stack number $\pmb{n}$:</p>
<script type="math/tex; mode=display">\pmb{n} \times [2 \times \underbrace{\big(4 \times \left[ (\pmb{x} \times \pmb{x}) + \pmb{x} \right]\big)}_\text{MHDPA} + 3 \times \underbrace{\big( \pmb{x}+\pmb{x} \big)}_\text{Layer Norm} + \underbrace{\big( 2\times 4\pmb{x} \times \pmb{x} + 5\pmb{x} \big)}_\text{FFNN}]</script><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">Towards data science: Counting No. of Parameters in Deep Learning Models by Hand</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45">Towards data science: Animated RNN, LSTM and GRU</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).<a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title>Efficient Softmax Explained</title>
    <url>/notes/2019/12/13/NN/Efficient-Softmax-Explained/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><code>Softmax</code> encounters large computing cost when the output vocabulary size is very large. Some feasible approaches will be explained under the circumstance of skip-gram pretraining task.<br><img data-src='/notes/images/sg-softmax.png' width='50%' /></p>
<span id="more"></span>
<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><p>The training objective of Skip-gram model is predicting the surrounding words in the sequences. Given a sequence of words w<sub>1</sub>,w<sub>2</sub>, …, w<sub>T</sub>. The final objective is to minimize the average log probability:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c,\, j\neq 0} \log p(w_{t+j \vert w_t})</script><p>The output of skip-gram formula is:</p>
<script type="math/tex; mode=display">p(w_O \vert w_I) = \frac{\exp({v'}_{wO})^\top v_{wI}}{\sum_{w=1}^W\exp({v'_w}^\top v_{wI})}</script><p>where $v_w$ and $v’_w$ are the “input” vector representation (i.e., the hidden layer output h<sub>i</sub> in previous fig) and the “output” vectors (i.e., the weights of W’<sub>N x V</sub>).</p>
<h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>Hierarchical Softmax (HSM)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rong, X. (2014). [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf). ArXiv, abs/1411.2738.
">[1]</span></a></sup> reduce the computing cost from $O(V)$ to $O(\log V)$, where V denotes the output vocab size. The V leaf nodes in the binary tree represent the vocabulay words wilst the (V-1) inner nodes denote the probability mass. </p>
<p><img data-src='/notes/images/hierachical-softmax-tree.png' width='50%'/></p>
<p>For each leaf node, there exits a unique path from the root node to that unit, on which path the probability is estimated along the nodes in the path. As the figure below shown, the highlighted path is from root to the leaf node w<sub>2</sub>, denoted $n(w,4)$, where $n(w,j)$ denotes the j<sup>th</sup> node on the path from root to the word $w$.</p>
<script type="math/tex; mode=display">p(w=w_O) = \prod_{j=1}^{L(w)-1} \sigma \bigg( \mathbb{I} \big[ n(w, j+1) = ch(n(w,j)) \big] {v'}_{n(w,j)}^\top h \bigg)</script><p>where $ch(n)$ is the left child of node $n$, $v’_{n(w,j)}$ denote the vector representation of the inner node $n(w,j)$, $\mathbf{h}$ is the output of hidden layer:</p>
<script type="math/tex; mode=display">
    \mathbf{h}=\left\{
                \begin{array}{ll}
                  \mathbf{v}_{wI} \quad \text{in skip-gram}\\
                  \frac{1}{C} \sum_{c=1}^C \mathbf{v}_{wc} \quad \text{in CBOW}
                \end{array}
              \right.</script><p>$\mathbb{I} \big[ \cdot \big]$ is defined as:</p>
<script type="math/tex; mode=display">
    \mathbb{I} \big[ \cdot \big]=\left\{
                \begin{array}{ll}
                  1 \quad \text{if x is true}\\
                  -1 \quad \text{otherwise}
                \end{array}
              \right.</script><p>The marked path can be computed as:</p>
<script type="math/tex; mode=display">
\begin{align}
p(w_2 = w_O)  &= p(n(w_2, 1), \text{left}) \cdot p(n(w_2, 2), \text{left}) \cdot p(n(w_2, 3), \text{right}) \\
             &= \sigma \big( {v}_{n(w_2,1)}^{\prime \top} h \big) \cdot \sigma \big( {v}_{n(w_2,2)}^{\prime \top} h \big) \cdot \sigma \big( -{v'}_{n(w_2,3)}^\top h \big)
\end{align}</script><p>It is obvious that</p>
<script type="math/tex; mode=display">\sum_{i=1}^V p(w_i = w_O) = 1</script><h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p>Negative Sampling(NEG)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., & Dean, J. (2013). [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf). NIPS.
">[2]</span></a></sup> selects a subset of negative targets as well as the target to approximate the softmax normalization.<br>The objective is defined as:</p>
<script type="math/tex; mode=display">E = \log \sigma (\mathbf{v}^\prime_{wO} \mathbf{h}) - \sum_{w_j \in W_\text{neg}} \log \sigma (-\mathbf{v}^{\prime\top}_{w_j} \mathbf{h})</script><p>where w<sub>O</sub> is the output word, <script type="math/tex">\mathbf{v}^\prime_{wO}</script> is the output vector. $h$ is the hidden layer output, and the values on Skip-gram and CBOW are as descibed in the previous section. </p>
<h2 id="Adaptive-Softmax"><a href="#Adaptive-Softmax" class="headerlink" title="Adaptive Softmax"></a>Adaptive Softmax</h2><p>Adaptive Softmax partition the vocab into clusters, V<sub>h</sub> and V<sub>t</sub>, representing the <i>head</i> of distribution consisting of the most frequent words and the <i>tail</i> denoting rare words. Each cluster is regarded as independent, and the capacity of clusters are adaptive. <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., & Dean, J. (2013). [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf). NIPS.
">[2]</span></a></sup> found that the small number of clusters can obtain comparable results as the original softmax on the very large vocabulary size.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rong, X. (2014). <a href="https://arxiv.org/pdf/1411.2738.pdf">word2vec Parameter Learning Explained</a>. ArXiv, abs/1411.2738.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., &amp; Dean, J. (2013). <a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>. NIPS.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mohammed, A.A., &amp; Umaashankar, V. (2018). <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8554637">Effectiveness of Hierarchical Softmax in Large Scale Classification Tasks</a>. 2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI), 1090-1094.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Grave, E., Joulin, A., Cissé, M., Grangier, D., &amp; Jégou, H. (2016). <a href="https://arxiv.org/pdf/1609.04309.pdf">Efficient softmax approximation for GPUs</a>. ArXiv, abs/1609.04309.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Baevski, A., &amp; Auli, M. (2018). <a href="https://arxiv.org/pdf/1809.10853">Adaptive input representations for neural language modeling</a>. arXiv preprint arXiv:1809.10853.<a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NLP</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title>FLOPs Computation</title>
    <url>/notes/2019/10/12/NN/FLOPs-Computation/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>FLOPs is a measure of model complexity in deep learning.</p>
<span id="more"></span>
<h2 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h2><p>Floating point operations (FLOPs) measures the complexity of neural models.</p>
<p>Assume convolution is implemented as a sliding window and that the nonlinearity function is computed for free.</p>
<p>For convolutional kernels, we have:</p>
<script type="math/tex; mode=display">
 \textrm{FLOPs} = 2 H * W (C_\textrm{in} K^2 +1) C_\textrm{out}</script><p>where <script type="math/tex">H</script>, <script type="math/tex">W</script>, and <script type="math/tex">C_\textrm{in}</script> are height, width, and number of channels of the input feature map, <script type="math/tex">K</script> is the kernel width (assumed to be symmetric), and <script type="math/tex">C_\textrm{out}</script> is the number of output channels.</p>
<p>For MLP layers, we have:</p>
<script type="math/tex; mode=display">
 \textrm{FLOPs} = (2I-1)O</script><p>where <script type="math/tex">I</script> is the input dimensionality and <script type="math/tex">O</script> is the output dimensionality.</p>
<div class="note info">
            <ul><li><strong>FLOPs</strong> is abbreviation of <strong>floating operations</strong> which includes mul/add/div,…,etc.</li><li><code>MACs</code> stands for <strong>multiply-accumulate operation</strong> that performs <script type="math/tex">a \leftarrow a + (b \times c)</script>. <script type="math/tex; mode=display">\text{FLOPs} \approx 2 * \textrm{MACs}</script></li></ul>
          </div>
<h3 id="Transformer-Training-Compute"><a href="#Transformer-Training-Compute" class="headerlink" title="Transformer Training Compute"></a>Transformer Training Compute</h3><p>The FLOPS cost of a transformer is:</p>
<script type="math/tex; mode=display">
C \approx \tau T = 6 P D</script><p>where $C$ is the training compute of floating point operations, $\tau$ is the aggregate throughput of the hardware setup $\tau = \text{gpu_num} \times \text{FLOPS per GPU}$, $T$ is the training time spent (in seconds), $P$ is the parameter count, $D$ is the training data size (in tokens).<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[ransformer Math 101. Eleuther AI. 2023](https://blog.eleuther.ai/transformer-math/)">[2]</span></a></sup></p>
<script type="math/tex; mode=display">C = C_\text{forward} + C_\text{backward}</script><p>where <script type="math/tex">C_\text{forward} \approx 2PD</script> , <script type="math/tex">C_\text{forward} \approx 4PD</script>. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Computes the flops needed for training/running transformer networks.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="comment"># We checked this code with TensorFlow&quot;s FLOPs counting, although we had to</span></span><br><span class="line"><span class="comment"># correct for this issue: https://github.com/tensorflow/tensorflow/issues/22071</span></span><br><span class="line"><span class="comment"># Assumptions going into the FLOPs counting</span></span><br><span class="line"><span class="comment">#   - An &quot;operation&quot; is a mathematical operation, not a machine instruction. So</span></span><br><span class="line"><span class="comment">#     an &quot;exp&quot; takes one opp like and add, even though in practice an exp</span></span><br><span class="line"><span class="comment">#     might be slower. This is not too bad an assumption because</span></span><br><span class="line"><span class="comment">#     matrix-multiplies dominate the compute for most models, so minor details</span></span><br><span class="line"><span class="comment">#     about activation functions don&quot;t matter too much. Similarly, we count</span></span><br><span class="line"><span class="comment">#     matrix-multiplies as 2*m*n flops instead of m*n, as one might if</span></span><br><span class="line"><span class="comment">#     if considering fused multiply-add ops.</span></span><br><span class="line"><span class="comment">#   - Backward pass takes the same number of FLOPs as forward pass. No exactly</span></span><br><span class="line"><span class="comment">#     right (e.g., for softmax cross entropy loss the backward pass is faster).</span></span><br><span class="line"><span class="comment">#     Importantly, it really is the same for matrix-multiplies, which is most of</span></span><br><span class="line"><span class="comment">#     the compute anyway.</span></span><br><span class="line"><span class="comment">#   - We assume &quot;dense&quot; embedding lookups (i.e., multiplication by a one-hot</span></span><br><span class="line"><span class="comment">#     vector). On some hardware accelerators, these dense operations are</span></span><br><span class="line"><span class="comment">#     actually faster than sparse lookups.</span></span><br><span class="line"><span class="comment"># Please open a github issue if you spot a problem with this code!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># I am not sure if the below constants are 100% right, but they are only applied</span></span><br><span class="line"><span class="comment"># to O(hidden_size) activations, which is generally a lot less compute than the</span></span><br><span class="line"><span class="comment"># matrix-multiplies, which are O(hidden_size^2), so they don&#x27;t affect the total</span></span><br><span class="line"><span class="comment"># number of FLOPs much.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># random number, &gt;=, multiply activations by dropout mask, multiply activations</span></span><br><span class="line"><span class="comment"># by correction (1 / (1 - dropout_rate))</span></span><br><span class="line">DROPOUT_FLOPS = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute mean activation (sum), computate variance of activation</span></span><br><span class="line"><span class="comment"># (square and sum), bias (add), scale (multiply)</span></span><br><span class="line">LAYER_NORM_FLOPS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GELU: 0.5 * x * (1 + tanh(sqrt(2 / np.pi) * (x + 0.044715 * pow(x, 3))))</span></span><br><span class="line">ACTIVATION_FLOPS = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># max/substract (for stability), exp, sum, divide</span></span><br><span class="line">SOFTMAX_FLOPS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerHparams</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Computes the train/inference FLOPs for transformers.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, l, s=<span class="number">512</span>, v=<span class="number">30522</span>, e=<span class="literal">None</span>, i=<span class="literal">None</span>, heads=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">      head_size=<span class="literal">None</span>, output_frac=<span class="number">0.15625</span>, sparse_embed_lookup=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">      decoder=<span class="literal">False</span></span>):</span></span><br><span class="line">    self.h = h  <span class="comment"># hidden size</span></span><br><span class="line">    self.l = l  <span class="comment"># number of layers</span></span><br><span class="line">    self.s = s  <span class="comment"># sequence length</span></span><br><span class="line">    self.v = v  <span class="comment"># vocab size</span></span><br><span class="line">    self.e = h <span class="keyword">if</span> e <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> e  <span class="comment"># embedding size</span></span><br><span class="line">    self.i = h * <span class="number">4</span> <span class="keyword">if</span> i <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> i  <span class="comment"># intermediate size</span></span><br><span class="line">    self.kqv = h <span class="keyword">if</span> head_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> head_size * heads  <span class="comment"># attn proj sizes</span></span><br><span class="line">    self.heads = <span class="built_in">max</span>(h // <span class="number">64</span>, <span class="number">1</span>) <span class="keyword">if</span> heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> heads  <span class="comment"># attention heads</span></span><br><span class="line">    self.output_frac = output_frac  <span class="comment"># percent of tokens using an output softmax</span></span><br><span class="line">    self.sparse_embed_lookup = sparse_embed_lookup  <span class="comment"># sparse embedding lookups</span></span><br><span class="line">    self.decoder = decoder  <span class="comment"># decoder has extra attn to encoder states</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_block_flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get the forward-pass FLOPs for a single transformer block.&quot;&quot;&quot;</span></span><br><span class="line">    attn_mul = <span class="number">2</span> <span class="keyword">if</span> self.decoder <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">    block_flops = <span class="built_in">dict</span>(</span><br><span class="line">        kqv=<span class="number">3</span> * <span class="number">2</span> * self.h * self.kqv * attn_mul,</span><br><span class="line">        kqv_bias=<span class="number">3</span> * self.kqv * attn_mul,</span><br><span class="line">        attention_scores=<span class="number">2</span> * self.kqv * self.s * attn_mul,</span><br><span class="line">        attn_softmax=SOFTMAX_FLOPS * self.s * self.heads * attn_mul,</span><br><span class="line">        attention_dropout=DROPOUT_FLOPS * self.s * self.heads * attn_mul,</span><br><span class="line">        attention_scale=self.s * self.heads * attn_mul,</span><br><span class="line">        attention_weighted_avg_values=<span class="number">2</span> * self.h * self.s * attn_mul,</span><br><span class="line">        attn_output=<span class="number">2</span> * self.h * self.h * attn_mul,</span><br><span class="line">        attn_output_bias=self.h * attn_mul,</span><br><span class="line">        attn_output_dropout=DROPOUT_FLOPS * self.h * attn_mul,</span><br><span class="line">        attn_output_residual=self.h * attn_mul,</span><br><span class="line">        attn_output_layer_norm=LAYER_NORM_FLOPS * attn_mul,</span><br><span class="line">        intermediate=<span class="number">2</span> * self.h * self.i,</span><br><span class="line">        intermediate_act=ACTIVATION_FLOPS * self.i,</span><br><span class="line">        intermediate_bias=self.i,</span><br><span class="line">        output=<span class="number">2</span> * self.h * self.i,</span><br><span class="line">        output_bias=self.h,</span><br><span class="line">        output_dropout=DROPOUT_FLOPS * self.h,</span><br><span class="line">        output_residual=self.h,</span><br><span class="line">        output_layer_norm=LAYER_NORM_FLOPS * self.h,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(block_flops.values()) * self.s</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_flops</span>(<span class="params">self, output=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get the forward-pass FLOPs the transformer inputs or output softmax.&quot;&quot;&quot;</span></span><br><span class="line">    embedding_flops = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> output <span class="keyword">or</span> (<span class="keyword">not</span> self.sparse_embed_lookup):</span><br><span class="line">      embedding_flops[<span class="string">&quot;main_multiply&quot;</span>] = <span class="number">2</span> * self.e * self.v</span><br><span class="line">    <span class="comment"># input embedding post-processing</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> output:</span><br><span class="line">      embedding_flops.update(<span class="built_in">dict</span>(</span><br><span class="line">          tok_type_and_position=<span class="number">2</span> * self.e * (self.s + <span class="number">2</span>),</span><br><span class="line">          add_tok_type_and_position=<span class="number">2</span> * self.e,</span><br><span class="line">          emb_layer_norm=LAYER_NORM_FLOPS * self.e,</span><br><span class="line">          emb_dropout=DROPOUT_FLOPS * self.e</span><br><span class="line">      ))</span><br><span class="line">    <span class="comment"># projection layer if e != h</span></span><br><span class="line">    <span class="keyword">if</span> self.e != self.h <span class="keyword">or</span> output:</span><br><span class="line">      embedding_flops.update(<span class="built_in">dict</span>(</span><br><span class="line">          hidden_kernel=<span class="number">2</span> * self.h * self.e,</span><br><span class="line">          hidden_bias=self.e <span class="keyword">if</span> output <span class="keyword">else</span> self.h</span><br><span class="line">      ))</span><br><span class="line">      <span class="comment"># extra hidden layer and output softmax</span></span><br><span class="line">      <span class="keyword">if</span> output:</span><br><span class="line">        embedding_flops.update(<span class="built_in">dict</span>(</span><br><span class="line">            hidden_activation=ACTIVATION_FLOPS * self.e,</span><br><span class="line">            hidden_layernorm=LAYER_NORM_FLOPS * self.e,</span><br><span class="line">            output_softmax=SOFTMAX_FLOPS * self.v,</span><br><span class="line">            output_target_word=<span class="number">2</span> * self.v</span><br><span class="line">        ))</span><br><span class="line">        <span class="keyword">return</span> self.output_frac * <span class="built_in">sum</span>(embedding_flops.values()) * self.s</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(embedding_flops.values()) * self.s</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_binary_classification_flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    classification_flops = <span class="built_in">dict</span>(</span><br><span class="line">        hidden=<span class="number">2</span> * self.h * self.h,</span><br><span class="line">        hidden_bias=self.h,</span><br><span class="line">        hidden_act=ACTIVATION_FLOPS * self.h,</span><br><span class="line">        logits=<span class="number">2</span> * self.h</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(classification_flops.values()) * self.s</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_train_flops</span>(<span class="params">self, batch_size, train_steps, discriminator=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get the FLOPs for pre-training the transformer.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 2* for forward/backward pass</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * batch_size * train_steps * (</span><br><span class="line">        (self.l * self.get_block_flops()) +</span><br><span class="line">        self.get_embedding_flops(output=<span class="literal">False</span>) +</span><br><span class="line">        (self.get_binary_classification_flops() <span class="keyword">if</span> discriminator <span class="keyword">else</span></span><br><span class="line">         self.get_embedding_flops(output=<span class="literal">True</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_infer_flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get the FLOPs for running inference with the transformer on a</span></span><br><span class="line"><span class="string">    classification task.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> ((self.l * self.get_block_flops()) +</span><br><span class="line">            self.get_embedding_flops(output=<span class="literal">False</span>) +</span><br><span class="line">            self.get_binary_classification_flops())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_electra_train_flops</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    h_d, l_d, h_g, l_g, batch_size, train_steps, tied_embeddings,</span></span></span><br><span class="line"><span class="params"><span class="function">    e=<span class="literal">None</span>, s=<span class="number">512</span>, output_frac=<span class="number">0.15625</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Get the FLOPs needed for  pre-training ELECTRA.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> e <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    e = h_d</span><br><span class="line">  disc = TransformerHparams(</span><br><span class="line">      h_d, l_d, s=s, e=e,</span><br><span class="line">      output_frac=output_frac).get_train_flops(batch_size, train_steps, <span class="literal">True</span>)</span><br><span class="line">  gen = TransformerHparams(</span><br><span class="line">      h_g, l_g, s=s, e=e <span class="keyword">if</span> tied_embeddings <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">      output_frac=output_frac).get_train_flops(batch_size, train_steps)</span><br><span class="line">  <span class="keyword">return</span> disc + gen</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MODEL_FLOPS = collections.OrderedDict([</span><br><span class="line">    <span class="comment"># These runtimes were computed with tensorflow FLOPs counting instead of the</span></span><br><span class="line">    <span class="comment"># script, as the neural architectures are quite different.</span></span><br><span class="line">    <span class="comment"># 768648884 words in LM1b benchmark, 10 epochs with batch size 20,</span></span><br><span class="line">    <span class="comment"># seq length 128, 568093262680 FLOPs per example.</span></span><br><span class="line">    (<span class="string">&quot;elmo&quot;</span>, <span class="number">2</span> * <span class="number">10</span> * <span class="number">768648884</span> * <span class="number">568093262680</span> / (<span class="number">20.0</span> * <span class="number">128</span>)),</span><br><span class="line">    <span class="comment"># 15064773691518 is FLOPs for forward pass on 32 examples.</span></span><br><span class="line">    <span class="comment"># Therefore 2 * steps * batch_size * 15064773691518 / 32 is XLNet compute</span></span><br><span class="line">    (<span class="string">&quot;xlnet&quot;</span>, <span class="number">2</span> * <span class="number">500000</span> * <span class="number">8192</span> * <span class="number">15064773691518</span> / <span class="number">32.0</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Runtimes computed with the script</span></span><br><span class="line">    (<span class="string">&quot;gpt&quot;</span>, TransformerHparams(<span class="number">768</span>, <span class="number">12</span>, v=<span class="number">40000</span>, output_frac=<span class="number">1.0</span>).get_train_flops(</span><br><span class="line">        <span class="number">128</span>, <span class="number">960800</span>)),</span><br><span class="line">    (<span class="string">&quot;bert_small&quot;</span>, TransformerHparams(<span class="number">256</span>, <span class="number">12</span>, e=<span class="number">128</span>, s=<span class="number">128</span>).get_train_flops(<span class="number">128</span>, <span class="number">1.45e6</span>)),</span><br><span class="line">    (<span class="string">&quot;bert_base&quot;</span>, TransformerHparams(<span class="number">768</span>, <span class="number">12</span>).get_train_flops(<span class="number">256</span>, <span class="number">1e6</span>)),</span><br><span class="line">    (<span class="string">&quot;bert_large&quot;</span>, TransformerHparams(<span class="number">1024</span>, <span class="number">24</span>).get_train_flops(<span class="number">256</span>, <span class="number">1e6</span>)),</span><br><span class="line">    (<span class="string">&quot;electra_small&quot;</span>, get_electra_train_flops(<span class="number">256</span>, <span class="number">12</span>, <span class="number">64</span>, <span class="number">12</span>, <span class="number">128</span>, <span class="number">1e6</span>, <span class="literal">True</span>, s=<span class="number">128</span>, e=<span class="number">128</span>)),</span><br><span class="line">    (<span class="string">&quot;electra_base&quot;</span>, get_electra_train_flops(<span class="number">768</span>, <span class="number">12</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">256</span>, <span class="number">766000</span>, <span class="literal">True</span>)),</span><br><span class="line">    (<span class="string">&quot;electra_400k&quot;</span>, get_electra_train_flops(<span class="number">1024</span>, <span class="number">24</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">2048</span>, <span class="number">400000</span>, <span class="literal">True</span>)),</span><br><span class="line">    (<span class="string">&quot;electra_1.75M&quot;</span>, get_electra_train_flops(<span class="number">1024</span>, <span class="number">24</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">2048</span>, <span class="number">1750000</span>, <span class="literal">True</span>)),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RoBERTa, ALBERT, and T5 have  minor architectural differences from</span></span><br><span class="line">    <span class="comment"># BERT/ELECTRA, but I believe they don&#x27;t significantly effect the runtime,</span></span><br><span class="line">    <span class="comment"># so we use this script for those models as well.</span></span><br><span class="line">    (<span class="string">&quot;roberta&quot;</span>, TransformerHparams(<span class="number">1024</span>, <span class="number">24</span>, v=<span class="number">50265</span>).get_train_flops(<span class="number">8000</span>, <span class="number">500000</span>)),</span><br><span class="line">    (<span class="string">&quot;albert&quot;</span>, TransformerHparams(<span class="number">4096</span>, <span class="number">12</span>, v=<span class="number">30000</span>, e=<span class="number">128</span>).get_train_flops(</span><br><span class="line">        <span class="number">4096</span>, <span class="number">1.5e6</span>)),</span><br><span class="line">    (<span class="string">&quot;t5_11b&quot;</span>, TransformerHparams(</span><br><span class="line">        <span class="number">1024</span>,  <span class="comment"># hidden size</span></span><br><span class="line">        <span class="number">24</span>,  <span class="comment"># layers</span></span><br><span class="line">        v=<span class="number">32000</span>,  <span class="comment"># vocab size</span></span><br><span class="line">        i=<span class="number">65536</span>,  <span class="comment"># ff intermediate hidden size</span></span><br><span class="line">        heads=<span class="number">128</span>, head_size=<span class="number">128</span>,  <span class="comment"># heads/head size</span></span><br><span class="line">        output_frac=<span class="number">0.0</span>  <span class="comment"># encoder has no output softmax</span></span><br><span class="line">    ).get_train_flops(<span class="number">2048</span>, <span class="number">1e6</span>) +  <span class="comment"># 1M steps with batch size 2048</span></span><br><span class="line">     TransformerHparams(</span><br><span class="line">         <span class="number">1024</span>,</span><br><span class="line">         <span class="number">24</span>,</span><br><span class="line">         v=<span class="number">32000</span>,</span><br><span class="line">         i=<span class="number">65536</span>,</span><br><span class="line">         heads=<span class="number">128</span>, head_size=<span class="number">128</span>,</span><br><span class="line">         output_frac=<span class="number">1.0</span>,  <span class="comment"># decoder has output softmax for all positions</span></span><br><span class="line">         decoder=<span class="literal">True</span></span><br><span class="line">     ).get_train_flops(<span class="number">2048</span>, <span class="number">1e6</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">  <span class="keyword">for</span> k, v <span class="keyword">in</span> MODEL_FLOPS.items():</span><br><span class="line">    <span class="built_in">print</span>(k, v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Molchanov, Pavlo, et al. &quot;<a href="https://arxiv.org/pdf/1611.06440.pdf">Pruning convolutional neural networks for resource efficient inference</a>.&quot; arXiv preprint arXiv:1611.06440 (2016).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.eleuther.ai/transformer-math/">ransformer Math 101. Eleuther AI. 2023</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title>An Introduction to Graph Neural Networks</title>
    <url>/notes/2020/03/16/NN/Graph-Neural-Networks/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Graph Neural Networks (GNNs) has demonstrated efficacy on non-Euclidean data, such as social media, bioinformatics, etc.<br><img data-src="/notes/images/GNN-variants.png" alt="upload successful"></p>
<center> Image source: <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., & Sun, M. (2018). [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf). ArXiv, abs/1812.08434.
">[1]</span></a></sup></center>

<span id="more"></span>
<h1 id="Related-background"><a href="#Related-background" class="headerlink" title="Related background"></a>Related background</h1><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>Denoting $g$ as the filter on $f$, the convolution:</p>
<script type="math/tex; mode=display">(f \star g)(t) = \int_\mathbb{R} f(x) g(t-x) dx</script><h2 id="Fourier-transformation"><a href="#Fourier-transformation" class="headerlink" title="Fourier transformation"></a>Fourier transformation</h2><ul>
<li>Fourier transformation:<script type="math/tex; mode=display">\mathcal{F}\{f\}(v) = \int_\mathbb{R} f(x)e^{-2 \pi i x \cdot v} dx</script></li>
<li><p>Inverse Fourier transformation:</p>
<script type="math/tex; mode=display">\mathcal{F}^{-1}\{f\}(x) = \int_\mathbb{R} f(v)e^{-2 \pi i x \cdot v} dv</script></li>
<li><p>The convolution can be rewritten as:</p>
<script type="math/tex; mode=display">f \star g = \mathcal{F}^{-1}\{\mathcal{F}\{f\} \cdot \mathcal{F}\{g\} \}</script></li>
</ul>
<!--## todo: Laplacian-->
<h1 id="Spectral-methods"><a href="#Spectral-methods" class="headerlink" title="Spectral methods"></a>Spectral methods</h1><h2 id="Graph-Convolutional-Networks-GCN"><a href="#Graph-Convolutional-Networks-GCN" class="headerlink" title="Graph Convolutional Networks (GCN)"></a>Graph Convolutional Networks (GCN)</h2><p>GCN<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kipf, T., & Welling, M. (2016). [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf). ArXiv, abs/1609.02907.
">[2]</span></a></sup> applies convolutions in the Fourier domain by computing the eigendecomposition of the graph Lapacian <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Laplacian matrix wiki](https://en.wikipedia.org/wiki/Laplacian_matrix)
">[3]</span></a></sup> using the first-order approximation. When applying convolution on the signal $x$ with the filter <script type="math/tex">g_\theta = \text{diag}(\theta)</script> parameterized by $\theta \in \mathbb{R}^N$ in the Fourier domain, i.e.,</p>
<script type="math/tex; mode=display">
\begin{align}
f \star g  &= \mathcal{F}^{-1}\{\mathcal{F}\{f\} \cdot \mathcal{F}\{g\} \} & \\
g \star x &= U ( \underbrace{ U^\top g}_{g_\theta(\Lambda)} \cdot U^\top x ) & \\
& = U g_{\theta^\prime}(\Lambda) U^\top x  & \\
& \approx \theta (I_N + D^{-1/2} A D^{-1/2}) x & \text{Chebyshev polynomials (K-order 1)}
\end{align}</script><p>where $U$ is the matrix of eigenvectors of the normalized graph Laplacian <script type="math/tex">L = I_N - D^{-1/2} A D^{-1/2} = U \Lambda U^\top</script></p>
<p>Let <script type="math/tex">\tilde{A} = A + I_N, \tilde{D}_{ii} = \sum_{j} \tilde{A}_{ii}</script>, we get:</p>
<script type="math/tex; mode=display">H^{l+1} = \sigma( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)})</script><p><img data-src="/notes/images/GCN.png" width="70%" /></p>
<p><a href="https://github.com/tkipf/gcn">Source code (TensorFlow)</a><br><a href="https://github.com/tkipf/pygcn">Source code (PyTorch)</a></p>
<div class="note warning">
            <ul><li>spectral convolution can only be applied on <strong>undirected graphs</strong>, assuring that $L$ is a symmetric matrix.</li></ul>
          </div>
<h1 id="Non-spectral-methods"><a href="#Non-spectral-methods" class="headerlink" title="Non-spectral methods"></a>Non-spectral methods</h1><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p><img data-src="/notes/images/GraphSAGE.png" width="80%" /></p>
<p>GraphSAGE is one of the archetype of non-spatial approaches. It samples from the neighborhood in the graph and aggregates the feature information from its local neighborhood.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hamilton, W.L., Ying, Z., & Leskovec, J. (2017). [Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216.pdf). NIPS.
">[4]</span></a></sup></p>
<p><img data-src="/notes/images/GraphSAGE algorithm.png" width="90%" /></p>
<h2 id="Graph-Attention-Networks-GAT"><a href="#Graph-Attention-Networks-GAT" class="headerlink" title="Graph Attention Networks (GAT)"></a>Graph Attention Networks (GAT)</h2><p>Notation:</p>
<ul>
<li>Input (node features): <script type="math/tex">\mathbf{h}= \{ \overrightarrow{h}_1, \overrightarrow{h}_2, \cdots, \overrightarrow{h}_N\}, \overrightarrow{h}_i \in \mathbb{R}^F</script>, where $N$ is the # of nodes, $F$ is the # of features in each node.</li>
<li>Output: <script type="math/tex">\mathbf{h}^\prime= \{ \overrightarrow{h}_1, \overrightarrow{h}_2, \cdots, \overrightarrow{h}_N\}, \overrightarrow{h}_i \in \mathbb{R}^{F^\prime}</script>, where $F^\prime$ is the output dimension.</li>
</ul>
<ol>
<li>Pass inputs $\mathbf{h}$ into a learnable shared linear transformation, parameterized by a weight matrix $\mathbf{W} \in \mathbb{R}^{F^\prime \times F}$.<br><img data-src="/notes/images/GAT-attn-coefficient.png" width="30%" /></li>
<li>Perform self-attention on the nodes<ul>
<li>compute the attention coefficients using a shared attention mechanism $a: \mathbf{R}^{F^\prime \times F} \rightarrow \mathbf{R}$:<script type="math/tex; mode=display">e_{ij} = a (\mathbf{W}\overrightarrow{h}_i, \mathbf{W}\overrightarrow{h}_j) = \text{LeakyReLU} \big( \overrightarrow{\mathbf{a}}^\top [\mathbf{W}\overrightarrow{h}_i \Vert \mathbf{W}\overrightarrow{h}_j] \big)</script></li>
<li>Compute <script type="math/tex">e_{ij}</script> for nodes <script type="math/tex">j \in \mathcal{N}_i</script>,where <script type="math/tex">\mathcal{N}_i</script> is the neighborhood of node $i$ (including $i$) in the graph. Let $\Vert$ denote concatenation,<script type="math/tex; mode=display">\alpha_{ij} = \text{softmax}_j (e_{ij}) = \frac{e_{ij}}{\sum_{k \in \mathcal{N}_i} \exp (e_{ik}) }</script></li>
</ul>
</li>
<li>Multi-head attention. The final output dimension is <script type="math/tex">\mathbf{R}^{KF^\prime}</script>, where $k$ is the attention number.<script type="math/tex; mode=display">\overrightarrow{h}^{\prime}_i = \Vert_{k=1}^K \sigma \bigg( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^k \mathbf{W}^k \overrightarrow{h}_j \bigg)</script></li>
<li>For the <strong>final layer</strong>, GAT aggregates neighbor nodes by <strong>averaging</strong> different node representations:<script type="math/tex; mode=display">\overrightarrow{h}^{\prime}_i =  \sigma \bigg( \frac{1}{K} \sum_{k=1}^K \sum_{j \in \mathcal{N}_i} \alpha_{ij}^k \mathbf{W}^k \overrightarrow{h}_j \bigg)</script></li>
</ol>
<p><img data-src="/notes/images/GAT.png" width="50%" /></p>
<p><a href="https://github.com/PetarV-/GAT">Source code (TensorFlow)</a></p>
<h1 id="Graph-Transformer-Networks-GTN"><a href="#Graph-Transformer-Networks-GTN" class="headerlink" title="Graph Transformer Networks (GTN)"></a>Graph Transformer Networks (GTN)</h1><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., &amp; Sun, M. (2018). <a href="https://arxiv.org/pdf/1812.08434.pdf">Graph Neural Networks: A Review of Methods and Applications</a>. ArXiv, abs/1812.08434.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kipf, T., &amp; Welling, M. (2016). <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-Supervised Classification with Graph Convolutional Networks</a>. ArXiv, abs/1609.02907.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Laplacian_matrix">Laplacian matrix wiki</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hamilton, W.L., Ying, Z., &amp; Leskovec, J. (2017). <a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive Representation Learning on Large Graphs</a>. NIPS.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., &amp; Bengio, Y. (2018). <a href="https://arxiv.org/pdf/1710.10903.pdf">Graph Attention Networks</a>. ICLR<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Yun, S., Jeong, M., Kim, R., Kang, J., &amp; Kim, H.J. (2019). Graph Transformer Networks. NeurIPS.<a href="#fnref:6" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Graph Neural Networks</category>
      </categories>
      <tags>
        <tag>Graph Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network Tricks</title>
    <url>/notes/2019/12/15/NN/Neural-Network-Tricks/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Techniques of NN training. Keep updating.<br><span id="more"></span></p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><h2 id="Weight-tying"><a href="#Weight-tying" class="headerlink" title="Weight tying"></a>Weight tying</h2><p>Weight tying<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Press, O., & Wolf, L. (2016). [Using the output embedding to improve language models](https://arxiv.org/pdf/1608.05859). arXiv preprint arXiv:1608.05859.">[1]</span></a></sup> is tying the input word embedding matrix $U$ (called input embedding) with topmost weight matrix $V$ (called output embedding) of neural network language models (NNLM), i.e., setting $U=V$. This technique can reduce the parameter size and therefore lead to less overfitting.</p>
<p><em>Press et. al (2016)</em> showed that the weight typing</p>
<p><img data-src='/notes/images/weight-tying.png' width='50%' /></p>
<h3 id="Untied-NNLM"><a href="#Untied-NNLM" class="headerlink" title="Untied NNLM"></a>Untied NNLM</h3><p>Give the word sequence <script type="math/tex">i_{1:t} = [i_1, \cdots, i_t]</script> at timestep t, and current output target word <script type="math/tex">o_t</script>, the NLL loss is:</p>
<script type="math/tex; mode=display">\mathcal{L}_t = - \log p_t (o_t \vert i_{1:t})</script><p>where <script type="math/tex">p_t (o_t \vert i_{1:t}) = \frac{\exp(\mathbf{V}^{\top}_{o_t} h_2^{(t)} )}{ \sum_{x=1}^C \exp(\mathbf{V}_x^\top h_t^{(t)} ) }</script>, <script type="math/tex">\mathbf{U}_k / \mathbf{V}_k</script> is the $k$-th row of $\mathbf{U}/\mathbf{V}$, $k$ is the corresponding word index, <script type="math/tex">h_2^{(t)}</script> is the vector of activations of the topmost LSTM layers’ output at time t.</p>
<p>The update for row $k$ of input embedding $\mathbf{U}$ is:</p>
<script type="math/tex; mode=display">
 \frac{\partial \mathcal{L}_t}{\partial \mathbf{U}_t } =\left\{
                \begin{array}{ll}
                  (\sum_{x=1}^C p_t(x \vert i_{1:t} \cdot \mathbf{V}_x^\top - \mathbf{V}_{o_t}^\top) \frac{\partial h_2^{(t)}}{\partial U_{i_t}}) & k=i_t\\
                  0 & k \neq i_t
                \end{array}
              \right.</script><p>For the output embedding $\mathbf{V}$, the $k$-th row update is:</p>
<script type="math/tex; mode=display">
 \frac{\partial \mathcal{L}_t}{\partial \mathbf{V}_t } =\left\{
                \begin{array}{ll}
                  (p_t(o_t \vert i_{1:t})-1) h_2^{(t)} & k=o_t\\
                  p_t(o_t \vert i_{1:t}) \cdot h_2^{(t)} & k \neq o_t
                \end{array}
              \right.</script><p>Therefore, in the untied NNLM,</p>
<ul>
<li>the input embedding $\mathbf{U}$ only updates the current input word at $k$-th row, which denotes that the update times is correlated with its occurrence and thus rare words would be updated few times;</li>
<li>the output embedding $\mathbf{V}$ updates every row at each timestep.</li>
</ul>
<h3 id="Tied-NNLM"><a href="#Tied-NNLM" class="headerlink" title="Tied NNLM"></a>Tied NNLM</h3><p>With weight tying, we set $\color{red}{\mathbf{U}=\mathbf{V}=S}$. Thus $S$ serves as the role of both the input and output embeddings, whose update of each row in $S$ would conducted through both of them.</p>
<div class="note info">
            <ol><li>It can be seen that the update is mostly affected by the output embeddings and the tied weights perform similarly to output embedding $\mathbf{V}$ rather than input embedding $\mathbf{U}$ in the untied model.</li><li><em>Projection regularization</em> is used at large models, by inserting the projection matrix $\color{red}{P}$ before the output embedding $\mathbf{V}$: <script type="math/tex; mode=display">h_3 = \mathbf{V} \color{red}{P} h_2</script>Then add the regularization term <script type="math/tex">\lambda \| \color{red}{P}\|_2</script> to the loss. $\lambda=0.15$ in our experiments.</li></ol>
          </div>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Press, O., &amp; Wolf, L. (2016). <a href="https://arxiv.org/pdf/1608.05859">Using the output embedding to improve language models</a>. arXiv preprint arXiv:1608.05859.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
        <category>NN tricks</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NN tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>Normalization in Neural Networks: A Summary !</title>
    <url>/notes/2019/02/28/NN/Normalization-in-Neural-Networks-a-Summary/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A survey of the <code>normalization</code> tricks in Neural Networks.<br><span id="more"></span></p>
<h1 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h1><h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><ul>
<li><p><strong>Normalization</strong>: subtract the mean of the input data from every feature, and scale by its std deviation.</p>
<script type="math/tex; mode=display">\hat{x}_i^n = \frac{x_i^n - \text{mean}(x_i)}{\text{std}(x_i)}</script></li>
<li><p><strong>PCA</strong> (Principal Components Analysis)</p>
<ul>
<li>Decorrelate the data by projecting onto the principal components</li>
<li>Also possible to reduce dimensionality by only projecting onto the top $P$ principal components.</li>
</ul>
</li>
<li><p><strong>Whitening</strong></p>
<ul>
<li>Decorrelate by PCA</li>
<li>Scale each dimension</li>
</ul>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><div class="note danger">
            <p><strong>Problems</strong>:</p><ul><li><code>Internal covariate shift</code>: the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring <span class="label default">lower learning rates</span> and <span class="label default">careful parameter initialization</span>， and make it notoriously hard to train models with <code>saturating nonlinearities</code>.</li></ul>
          </div>
<ul>
<li><strong>Intuition</strong> : To reduce the <code>internal covariate shift</code>, by fixing the distribution of the layer inputs $x$.</li>
<li>Idea: The NN converges faster if the inputs is whitened, i.e. linearly transformed to have zero mean and unit variance, and decorrelated.</li>
</ul>
<p><strong>Solution</strong>: <code>batch normalization</code> (BN) <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.html)
">[1]</span></a></sup>. </p>
<ul>
<li>Use mini-batch statistics to <strong>normalize activations of each layer</strong>.</li>
<li>Parameter $\gamma$ and $\beta$ can scale and shift (a.k.a. bias) the normalized activations. </li>
<li>BatchNorm depends on the current training example - and on examples in mini-batch (for computing mean and variance)</li>
<li><p>Training</p>
<ul>
<li>Set parameters $\gamma$ and $\beta$ by gradient descent - require gradients $\frac{\partial E}{\partial \gamma}$ and $\frac{\partial E}{\partial \beta}$</li>
<li>TO backpropagate gradients through the batchNorm layer aliso require: $\frac{\partial E}{\partial \hat{u}}$,  $\frac{\partial E}{\partial \sigma^2}$,  $\frac{\partial E}{\partial \mu}$,  $\frac{\partial E}{\partial u_i}$</li>
</ul>
</li>
<li><p>Runtime: use the sample mean and variance computed over the complete training data as the mean and variance parameters for each layer - fixed transform:</p>
</li>
</ul>
<script type="math/tex; mode=display">\hat{u}_i = \frac{u_i - \text{mean}(u_i)}{\sqrt{\text{Var}(u_i) + \epsilon}}</script><ul>
<li>Backprop: see <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[What does the gradient flowing through batch normalization looks like ?](http://cthorey.github.io/backpropagation/)
">[2]</span></a></sup></li>
</ul>
<p><img data-src="/notes/images/BN-Steve.png" alt="Batch Norm"></p>
<ul>
<li><strong>Input</strong>: values of $x$ over a mini-batch: <script type="math/tex">\beta = {x_{1 \cdots m}}</script></li>
<li><p><strong>Outputs</strong>: <script type="math/tex">{y_i = BN_{\gamma, \beta} (x_i)}</script></p>
<p>  mini-batch mean:</p>
<script type="math/tex; mode=display">\mu_{\beta} \leftarrow \sum_{i=1}^m x_i</script><p>  mini-batch variance:</p>
<script type="math/tex; mode=display">\sigma_{\beta}^2 \leftarrow \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2</script><p>  normalize:</p>
<script type="math/tex; mode=display">\hat{x} \leftarrow \frac{x_i - \mu_{\beta}}{\sqrt{\sigma_{\beta}^2 + \epsilon}}</script><p>  scale and shift:</p>
<script type="math/tex; mode=display">y_i \leftarrow \gamma \hat{x_i} + \beta \equiv \text{BN}_{\gamma, \beta}(x_i)</script></li>
<li><p><strong>Parameters</strong>: $\gamma$ and $\beta$ are trainable parameters with size $C$ (where $C$ is the channel size). By default, the elements of $\gamma$ are set to 1s and the elements of $\beta$ are set to 0s. </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># `num_features`: the number of outputs for a fully-connected layer</span></span><br><span class="line">    <span class="comment"># or the number of output channels for a convolutional layer. `num_dims`:</span></span><br><span class="line">    <span class="comment"># 2 for a fully-connected layer and 4 for a convolutional layer</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># The scale parameter and the shift parameter (model parameters) are</span></span><br><span class="line">        <span class="comment"># initialized to 1 and 0, respectively</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># The variables that are not model parameters are initialized to 0 and 1</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># If `X` is not on the main memory, copy `moving_mean` and</span></span><br><span class="line">        <span class="comment"># `moving_var` to the device where `X` is located</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># Save the updated `moving_mean` and `moving_var`</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean, self.moving_var,</span><br><span class="line">            eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Benefits</strong><ul>
<li>Make training many-layered networks easier.<ul>
<li>allow <code>higher learning rates</code></li>
<li><code>weight initialization</code> less cruc</li>
</ul>
</li>
<li>Can act like a regularizer: can reduce need for techniques like dropout</li>
</ul>
</li>
</ul>
<div class="note success">
            <p><strong>Pros</strong>: </p><ul><li>Prevent small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients, e.g. it prevents the training from getting stuck in the saturated regimes of nonlinearities.</li><li>More resilient to the the <strong>parameter scale</strong>. Large learning rates may increase the scale of layer parameters, which then amplify the gradient during back-propagation and lead to the model explosion.</li></ul>
          </div>
<p><strong>Drawbacks</strong>:</p>
<ul>
<li>BN performs different in training and test time.</li>
<li>It is not legitimate at <code>inference time</code>, so the mean and variance are <strong>pre-computed from the training set</strong>, often by running <code>average</code>.</li>
</ul>
<div class="note danger">
            <h3 id="Different-opinion-at-NIPS-2018-lt-span-class-”hint—top-hint—error-hint—medium-hint—rounded-hint—bounce”-aria-label-”How-Does-Batch-Normalization-Help-Optimization"><a href="#Different-opinion-at-NIPS-2018-lt-span-class-”hint—top-hint—error-hint—medium-hint—rounded-hint—bounce”-aria-label-”How-Does-Batch-Normalization-Help-Optimization" class="headerlink" title="Different opinion at NIPS 2018: &lt;span class=”hint—top hint—error hint—medium hint—rounded hint—bounce” aria-label=”How Does Batch Normalization Help Optimization?"></a>Different opinion at <strong>NIPS 2018</strong>: <sup id="fnref:7"><a href="#fn:7" rel="footnote">&lt;span class=”hint—top hint—error hint—medium hint—rounded hint—bounce” aria-label=”<a href="https://arxiv.org/pdf/1805.11604.pdf">How Does Batch Normalization Help Optimization?</a></h3><p>“&gt;[7]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;</p><ul><li>Argument: BatchNorm cannot handle <code>internal covariate shift</code>, i.e. no link between the performance gain of BatchNorm and the reduction of internal covariate shift. It makes the <code>optimization landscape significantly smoother</code>.</li><li>Experiment: inject random noise with a severe covariate shift after batch normalization, which still performs better when training.</li></ul><p><img data-src="/notes/images/bn-experiment.png" alt="upload successful"></p><ul><li>BatchNorm makes the landscape significantly more smooth: improvement in the Lipschitzness of the loss function. i.e. the loss exhibits a significantly better “effective” $\beta$-smoothness.<ol><li>Reparametrization make it more stable (in the sense of loss Lipschitzness)</li><li>more smooth (in the sense of “effective” $\beta$-smoothness of the loss) </li></ol></li></ul>
          </div>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><div class="note danger">
            <p><strong>Problems</strong>:</p><ul><li>Batch normalization is dependent on the mini-batch size, i.e. cannot apply on extremely small minibatches.</li><li>Not obvious how to apply on RNNs. It can easily applied on FFNN because of the fixed length of inputs.</li></ul>
          </div>
<p><strong>Solution</strong>: <code>layer normalization</code> (LN)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Layer normalization](https://arxiv.org/pdf/1607.06450v1.pdf)
">[3]</span></a></sup>.</p>
<ul>
<li>Compute the layer normalization statistics over all <em>hidden units</em> in the same layer:<script type="math/tex; mode=display">\mu^l = \frac{1}{H} \sum_{i=1}^{H} \alpha_i^l</script></li>
</ul>
<script type="math/tex; mode=display">\sigma^l = \sqrt{\frac{1}{H} \sum_{i=1}^H (a_i^l - \mu^l)^2 }</script><p>where $H$ denotes the # of hidden units in a layer. Under layer norm, all hidden states in a layer share the same normalization terms $\mu$ and $\sigma$. Furthermore, it does not impose any constraint on the size of a mini-batch, and it can be used in the pure online regime with batch size 1.</p>
<script type="math/tex; mode=display">
y = \frac{x-\mathbb{E}[x]}{\sqrt{\textrm{Var}[x] + \epsilon}} * \gamma + \beta</script><p>where the mean and standard deviation are calculated seperately over the last certain number dimensions which have to be of the shape specified by the last dim. $\gamma$ and $\beta$ are learnable affine transform parameters. Denoting the hidden dim as $D$, the parameter count of LN is $2*D$.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; layer norm&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.weight * (x - mean) / (std + self.eps) + self.bias</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="Layer-Normalization-on-RNNs"><a href="#Layer-Normalization-on-RNNs" class="headerlink" title="Layer Normalization on RNNs"></a>Layer Normalization on RNNs</h3><p>In a std RNN, $h^{t-1}$ denotes previous hidden states, $x^t$ represents the current input vector:</p>
<script type="math/tex; mode=display">\pmb{a}^t = W_{hh}\pmb{h}^{t-1} + W_{xh}\pmb{x}^t</script><p>Do layer normalization:</p>
<script type="math/tex; mode=display">\pmb{h}^t = f(\frac{\pmb{g}}{\sigma^t} \odot(\pmb{a}^t - \mu^t) + \pmb{b})</script><script type="math/tex; mode=display">\mu^t = \frac{1}{H} \sum_{i=1}^H a_i^t</script><script type="math/tex; mode=display">\sigma^t = \sqrt{\frac{1}{H} \sum_{i=1}^H (a_i^t - \mu^t)^2}</script><p>where <script type="math/tex">W_{hh}</script> is the recurrent hidden to hidden weights and <script type="math/tex">W_{xh}</script> are the bottom up input to hidden weights, $\odot$ is element-wise multiplication between to vectors.</p>
<div class="note warn">
            <p><strong>Differences</strong> between batch normalization and layer normalization:</p><ul><li>LN: <span class="label info">neurons in the same layer</span> have the same mean and variance; different input samples have different mean and variance.</li><li>BN: <span class="label info">input samples in the same batch</span> have the same mean and variance; different neurons.</li></ul><p>Unlike BN, layer norm performs exactly <code>the same</code> computation at <code>training</code> and <code>test</code> times.</p>
          </div>
<h2 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h2><p>Root Mean Square Normalization (RMSNorm)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Biao, and Rico Sennrich. "Root mean square layer normalization." Advances in Neural Information Processing Systems 32 (2019).">[8]</span></a></sup> hypothesize that the rescaling invariance is the reason for success of LayerNorm, rather than re-centering invariance. RMSNorm rescales invariance and regularizes the summed inputs using root mean square (RMS) statistic:</p>
<script type="math/tex; mode=display">
\begin{equation}
    \bar{a}_i=\frac{a_i}{\mathrm{RMS}(\mathbf{a})}g_i,\quad\text{where RMS}(\mathbf{a})=\sqrt{\frac1n\sum_{i=1}^na_i^2}.
\end{equation}</script><p>RMSNorm simplifies LayerNorm by totally removing the mean statistic at the cost of sacrificing the invariance that mean normalization affords. When the mean of summed inputs is zero, RMSNorm is exactly equal to LayerNorm.</p>
<p>Implementation:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Code source: https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d, p=-<span class="number">1.</span>, eps=<span class="number">1e-8</span>, bias=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            Root Mean Square Layer Normalization</span></span><br><span class="line"><span class="string">        :param d: model size</span></span><br><span class="line"><span class="string">        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)</span></span><br><span class="line"><span class="string">        :param eps:  epsilon value, default 1e-8</span></span><br><span class="line"><span class="string">        :param bias: whether use bias term for RMSNorm, disabled by</span></span><br><span class="line"><span class="string">            default because RMSNorm doesn&#x27;t enforce re-centering invariance.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(RMSNorm, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.d = d</span><br><span class="line">        self.p = p</span><br><span class="line">        self.bias = bias</span><br><span class="line"></span><br><span class="line">        self.scale = nn.Parameter(torch.ones(d))</span><br><span class="line">        self.register_parameter(<span class="string">&quot;scale&quot;</span>, self.scale)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bias:</span><br><span class="line">            self.offset = nn.Parameter(torch.zeros(d))</span><br><span class="line">            self.register_parameter(<span class="string">&quot;offset&quot;</span>, self.offset)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.p &lt; <span class="number">0.</span> <span class="keyword">or</span> self.p &gt; <span class="number">1.</span>:</span><br><span class="line">            norm_x = x.norm(<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            d_x = self.d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            partial_size = <span class="built_in">int</span>(self.d * self.p)</span><br><span class="line">            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            norm_x = partial_x.norm(<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            d_x = partial_size</span><br><span class="line"></span><br><span class="line">        rms_x = norm_x * d_x ** (-<span class="number">1.</span> / <span class="number">2</span>)</span><br><span class="line">        x_normed = x / (rms_x + self.eps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bias:</span><br><span class="line">            <span class="keyword">return</span> self.scale * x_normed + self.offset</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.scale * x_normed</span><br></pre></td></tr></table></figure></p>
<h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><div class="note danger">
            <p><strong>Image style transfer</strong></p><ul><li>Transfer a style from one image to another, which relies more on a specific instance rather than a batch.</li></ul>
          </div>
<p><strong>Solution</strong>:</p>
<ul>
<li><strong>Instance normalization</strong> (IN) <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/pdf/1607.08022.pdf)
">[4]</span></a></sup> , a.k.a. contrast normalization,  do instance-specific normalization rather than batch normalization.</li>
<li>It performs the same at training and test time.</li>
</ul>
<script type="math/tex; mode=display">y_{tijk} = \frac{x_{tijk} - \mu_{ti}}{\sqrt{\sigma^2_{ti}+ \epsilon}}</script><script type="math/tex; mode=display">\mu_{ti} = \frac{1}{HW} \sum_{l=1}^W \sum_{m=1}^H x_{tilm}</script><script type="math/tex; mode=display">\sigma^2_{ti} = \frac{1}{HW} \sum_{l=1}^W \sum_{m=1}^H (x_{tilm} - mu_{ti})^2</script><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><div class="note danger">
            <p><strong>Problems</strong>:</p><ul><li>BN’s error increases rapidly when the <code>batch size</code> becomes <strong>smaller</strong> because of <code>inaccurate batch statistics estimation</code>.</li></ul>
          </div>
<p><strong>Solution</strong>: <code>Group Normalization</code> (GN)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)
">[5]</span></a></sup>. GN <strong>divides the <code>channels</code> into groups</strong> and computes within each group the mean and variance for normalization. GN’s computation is <strong>independent of batch sizes</strong>, and its accuracy is stable in a wide range of batch sizes.</p>
<p>GN divides the set <script type="math/tex">S_i</script> as:</p>
<script type="math/tex; mode=display">S_i = \{k \vert k_N = i_N, \lfloor \frac{k_C}{C/G} \rfloor = \lfloor \frac{i_C}{c/G} \rfloor \}</script><p>where $G$ is the number of groups, $C/G$ is the number of channels per group, $k$, $i$ is the index. GN compute the $\mu$ and $\sigma$ along the (H,W) axes and along a group by $\frac{C}{G}$ channels.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GroupNorm</span>(<span class="params">x, gamma, beta, G, eps=<span class="number">1e-5</span></span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    : param x: input features with shape [N,C,H,W]</span></span><br><span class="line"><span class="string">    : param gamma, beta: scale and offset, with shape [1,C,1,1]</span></span><br><span class="line"><span class="string">    : param G: number of groups for GN</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">	</span><br><span class="line">    N,C,H,W = x.shape</span><br><span class="line">    x = tf.reshape(x, [N, G, C//G, H, W]</span><br><span class="line">    </span><br><span class="line">    mean, var = tf.nn.moments(x, [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], keep_dims=<span class="literal">True</span>)</span><br><span class="line">    x = (x - mean) / tf.sqrt(var + eps)</span><br><span class="line">    </span><br><span class="line">    x = tf.reshape(x, [N, C, H, W])</span><br><span class="line">    <span class="keyword">return</span> x* gamma + beta</span><br></pre></td></tr></table></figure>
<h2 id="Switchable-Normalization"><a href="#Switchable-Normalization" class="headerlink" title="Switchable Normalization"></a>Switchable Normalization</h2><div class="note danger">
            <p><strong>Problems</strong>:</p><ul><li>Existing BN, IN, LN employed the same normalizer in all normalization layers of an entire network, rendering suboptimal performance.</li><li>Different normalizers are used to solve different tasks, making model design cumbersome.</li></ul>
          </div>
<p><strong>Solution</strong>: </p>
<ul>
<li>Switchable Normalization (SN) <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Differentiable learning-to-normalize via Switchable Normalization](https://arxiv.org/pdf/1806.10779.pdf)
">[6]</span></a></sup>. It combines three distinct scopes to compute statistics (i.e. mean and variance): channel-wise, layer-wise and minibatch-wise, by using IN, LN and BN respectively. SN switches them by learning their importance weights end-to-end.</li>
</ul>
<p>Given a 4D tensor [N,C,H,W], denoting the # of samples, # of channels, heights and weights. Let <script type="math/tex">h_{ncij}</script> and  <script type="math/tex">\hat{h_{ncij}}</script> be a pixel before and after the normalization, where $n \in [1,N]$, $c \in [1,C]$, $i \in [1,H]$ and $j \in [1,W]$. $\gamma$ and $\beta$ are a scale and shift parameter respectively, $\epsilon$ is a small constant to preserve numerical stability.</p>
<script type="math/tex; mode=display">\hat{h_{ncij}} = \gamma \frac{h_{ncij} - \sum_{k \in \omega}w_k\mu_k}{ \sqrt{ \sum_{k \in \omega} w'_k \sigma_k^2 + \epsilon}} + \beta</script><p>where $\Omega = { \text{in, ln, bn} }$:</p>
<script type="math/tex; mode=display">\mu_{in} = \frac{1}{HW} \sum_{i,j}^{H,W} h_{ncij}, \quad \sigma_{in}^2 = \frac{1}{HW} \sum_{i,j}^{H,W} (h_{ncij} - \mu_{in})^2</script><script type="math/tex; mode=display">\mu_{ln} = \frac{1}{C} \sum_{c=1}^{C} \mu_{in}, \quad \sigma_{ln}^2 = \frac{1}{C} \sum_{c=1}^{C} (\sigma_{in}^2 + \mu_{in}^2)^2 -\mu_{ln}^2</script><script type="math/tex; mode=display">\mu_{bn} = \frac{1}{N} \sum_{n=1}^{N} \mu_{in}, \quad \sigma_{bn}^2 = \frac{1}{N} \sum_{n=1}^{N} (\sigma_{in} + \mu_{in})^2 -\mu_{bn}^2</script><p>Different normalizers estimate statistics along different axes.<br><img data-src="/notes/images/SwitchableNorm.png" alt="upload successful"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SwitchableNorm</span>(<span class="params">x, gamma, beta, w_mean, w_var, eps=<span class="number">1e-5</span></span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x: shape [N,C,H,W]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Instance Norm</span></span><br><span class="line">    mean_in = np.mean(x, axis=(<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var_in = np.var(x, axis=(<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Layer Norm</span></span><br><span class="line">    mean_ln = np.mean(x, axis=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var_ln = np.var(x, axis=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># Batch Norm</span></span><br><span class="line">    mean_bn = np.mean(x, axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    var_bn = np.var(x, axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># Switchable Norm</span></span><br><span class="line">    mean = w_mean[<span class="number">0</span>]*mean_in + w_mean[<span class="number">1</span>]*mean_ln + w_mean[<span class="number">2</span>]*mean_bn</span><br><span class="line">    var = w_var[<span class="number">0</span>]*var_in + w_var[<span class="number">1</span>]*var_ln + w_var[<span class="number">2</span>]*var_bn</span><br><span class="line">    </span><br><span class="line">    x_normalized = (x-mean) / np.sqrt(var + eps)</span><br><span class="line">    <span class="keyword">return</span> gamma * x_normalized + beta</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p><img data-src="/notes/images/normalization-comparison.png" alt="Normalization comparison"></p>
<p>The mini-batch data has the shape [N, C, H, W], where </p>
<ul>
<li>$N$ is the batch axis</li>
<li>$C$ is the channel axis (rgb for image data)</li>
<li>$H$ and $W$ are the spatial hight and weight axis</li>
</ul>
<div class="note info">
            <p><strong>Comparison</strong>:</p><ul><li><strong>Batch Norm</strong> is applied on batch, normalizing along (N, H, W) axis, i.e. compute the mean and variance for the whole batch of input data. (It performs badly for small batch size)</li><li><strong>Layer Norm</strong> is on channel, normalizing along (C,H,W) axis, i.e. it is independent with batch dimension, by normalizing the neurons. (It is obvious for RNNs).</li><li><strong>Instance Norm</strong> is applied on image pixel, doing normalization along (H,W) axis, i.e. compute $\mu$ and $\sigma$ for each sample and channel. (It is for style transfer)</li><li><strong>Group Norm</strong>: divide the channel into groups, normalizing along the (H,W) axis and along a group of $\frac{C}{G}$ channels. </li><li><strong>Switchable Norm</strong>: dynamically learn weights for IN/LN/BN statistics in the e2e manner (c.f. ELMo).</li></ul>
          </div>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://proceedings.mlr.press/v37/ioffe15.html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cthorey.github.io/backpropagation/">What does the gradient flowing through batch normalization looks like ?</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1607.06450v1.pdf">Layer normalization</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1607.08022.pdf">Instance Normalization: The Missing Ingredient for Fast Stylization</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1806.10779.pdf">Differentiable learning-to-normalize via Switchable Normalization</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1805.11604.pdf">How Does Batch Normalization Help Optimization?</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, Biao, and Rico Sennrich. &quot;Root mean square layer normalization.&quot; Advances in Neural Information Processing Systems 32 (2019).<a href="#fnref:8" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
        <category>NN tricks</category>
        <category>Normalization</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NN tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization Methods in Deep Learning</title>
    <url>/notes/2019/05/20/NN/Optimization-methods-introduction/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><img data-src="/notes/images/optimization-comp2.gif" alt="optimization"><br><span id="more"></span></p>
<h1 id="Vanilla-gradient-descent"><a href="#Vanilla-gradient-descent" class="headerlink" title="Vanilla gradient descent"></a>Vanilla gradient descent</h1><p>Let <script type="math/tex">d_i(t) = \frac{\partial E}{\partial w_i(t)}</script> be the gradient of the error function $E$ w.r.t a weight <script type="math/tex">w_i</script> at update time $t$.</p>
<ul>
<li>“Vanilla” gradient descent updates the weight along the nagative gradient direction:<script type="math/tex; mode=display">\Delta w_i(t) = - \eta d_i(t)</script><script type="math/tex; mode=display">w_i(t+1) = w_i(t) + \Delta w_i(t)</script>where $\eta$ denotes the learning rate.</li>
</ul>
<h2 id="How-to-set-the-learning-rate"><a href="#How-to-set-the-learning-rate" class="headerlink" title="How to set the learning rate?"></a>How to set the learning rate?</h2><p>Initialize the $\eta$, and update as the training processes</p>
<ul>
<li>Learning rate schedules: typically initial larger steps followed by smaller steps for fine tuning: results in <strong>faster convergence and better solutions</strong>.</li>
</ul>
<div class="note info">
            <p><strong>Learning Rate Schedules</strong>:</p><ol><li><strong>Time-dependent</strong> schedules:<script type="math/tex; mode=display">\Delta w_i(t) = - \eta(t) d_i(t)</script><ul><li><strong>Piecewise constant</strong>: pre-determined $\eta$ for each epoch</li><li><strong>Exponential</strong>:<br><script type="math/tex">\eta(t) = \eta(0) \exp(-t/r)</script>, where $r \sim \text{training set size}$</li><li><strong>Reciprocal</strong>:<br><script type="math/tex">\eta(t) = \eta(0)(1+ \frac{t}{r})^{-c}</script>, where $c \sim 1$.</li></ul></li><li><strong>Performance-dependent</strong> $\eta$: fixd $\eta$ until validation set stops improving, then halve $\eta$ each epoch (i.e. constant, then exponential)</li></ol>
          </div>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><script type="math/tex; mode=display">\Delta w_i(t) = - \eta d_i(t) + \alpha \Delta w_i(t-1)</script><ul>
<li>Momentum hyperparameter: $\alpha \sim 0.9$</li>
<li><p>Momentum term encourages the weight change to go in the  previous direction</p>
</li>
<li><p><strong>Problems</strong>: tuning learning rate and momentum parameters can be expensive.</p>
</li>
</ul>
<h1 id="Adaptive-learning-rates"><a href="#Adaptive-learning-rates" class="headerlink" title="Adaptive learning rates"></a>Adaptive learning rates</h1><ul>
<li>Tuning learning rate parameters is expensive (grid search)<div class="note warn">
            <ul><li><strong>AdaGrad</strong>: normalize the update for each weight</li><li><strong>RMSProp</strong>: AdaGrad forces the learning rate to always decrease, this constraint is relexed with RMSProp</li><li><strong>Adam</strong>: “RMSProp with momentum”</li></ul>
          </div>
</li>
</ul>
<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><ul>
<li>Separate, nomalized update for each weight</li>
<li>Normalized by the sum squared gradient $S$<script type="math/tex; mode=display">\begin{align}
S_i(0) &= 0 \\
S_i(t) & = S_i(t-1) + d_i(t)^2  \\
\Delta w_i(t)& = \frac{-\eta}{\sqrt{S_i(t)}+\epsilon} d_i(t) 
\end{align}</script>where $\epsilon \sim 10^{-8}$ is a small constant to prevent division by zero errors.</li>
<li>The update step for <script type="math/tex">w_i</script> is normalized by the (square root of) the sum squared gradients for that parameter<ul>
<li>Weights with larger gradient magnitudes will have smaller effective learning rates</li>
<li><script type="math/tex">S_i</script> cannot get smaller, so the effective learningr rates monotonically decrease</li>
</ul>
</li>
<li>AdaGrad can decrease the <strong>effective learning rate</strong> too aggressively in NNs.</li>
</ul>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><ul>
<li>Backgroud: RProp<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Riedmiller, M., & Braun, H. (1993, March). A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Proceedings of the IEEE international conference on neural networks (Vol. 1993, pp. 586-591).
">[3]</span></a></sup> is for batch gradient descent with an adaptive learning rate for each parameter, and uses only the sign of the gradient (equivalent to normalizing by the gradient)</li>
<li>RMSProp can be viewed as a <code>stochastic gradient descent version of RProp normalized by a moving average of the squared gradient</code><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[csc321 Neural Networks for Machine Learning Lecure6](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
">[4]</span></a></sup>. It is similar to AdaGrad, but replacing the sum by a moving average for $S$:<script type="math/tex; mode=display">\begin{align}
S_i(t) & = \beta S_i(t-1) + (1-\beta)d_i(t)^2  \\
\Delta w_i(t)& = \frac{-\eta}{\sqrt{S_i(t)}+\epsilon} d_i(t) 
\end{align}</script>where $\beta \sim 0.9$ is the decay rate.</li>
<li>Effective learning rates no longer guaranteed to decrease.</li>
</ul>
<h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><div class="note danger">
            <p><strong>Background</strong>:<br>AdaGrad has two main drawbacks:</p><ol><li>the continual decay of learning rates throughout training;</li><li>the need for a manually selected global learning rate.</li></ol>
          </div>
<p>Let $\beta$ denote the decay rate.</p>
<script type="math/tex; mode=display">\begin{align}
S_i(t) & = \beta S_i(t-1) + (1-\beta)d_i(t)^2  &\text{accumulate gradients}\\

\mathbf{\Delta w_i(t)}& = - \frac{\sqrt{ C_i(t)+\epsilon}}{\sqrt{ S_i(t)+\epsilon}} d_i(t) & \text{compute update} \\
C_i(t) &= \beta C_i(t-1) + (1-\beta)\Delta w_i(t)^2 & \text{accumulate update} \\
\end{align}</script><p>The numerator of update term is not manually $\eta$, resulting in insensitivity of pre-defined learning rate.</p>
<div class="note success">
            <p><strong>Pros</strong>:</p><ul><li>Requires <code>no manually setting of a learning rate</code>;</li><li>insensitive to hyperparameters;</li><li>minimal computation over gradient descent.</li></ul>
          </div>
<p><img data-src="/notes/images/optimization-AdaDelta.png" alt="upload successful"><br>where $\text{RMS}[x]_t = \sqrt{E[x^2]_t + \epsilon}$</p>
<h2 id="Adam-ICLR-2015"><a href="#Adam-ICLR-2015" class="headerlink" title="Adam (ICLR 2015)"></a>Adam (ICLR 2015)</h2><ul>
<li>Adam can be viewed as a variant of RMSProp with momentum:<script type="math/tex; mode=display">\begin{align}
M_i(t) & = \alpha M_i(t-1) + (1-\alpha)d_i(t) & \text{momentum-smoothed gradient}\\
S_i(t) & = \beta S_i(t-1) + (1-\beta)d_i(t)^2 &\text{RMSProp update} \\
\hat{M}_i(t) &\rightarrow M_i(t) / (1-\alpha) & \text{correct bias}\\ 
\hat{S}_i(t) &\rightarrow S_i(t) / (1-\beta) & \text{correct bias} \\
\Delta w_i(t)& = \frac{-\eta}{\sqrt{\hat{S}_i(t)}+\epsilon} \hat{M}_i(t) 
\end{align}</script>where $\alpha \sim 0.9$, $\beta \sim 0.999$, $\epsilon \sim 1e-8$</li>
</ul>
<p><img data-src="/notes/images/Optimization-Adam.png" alt="upload successful"></p>
<p><img data-src="/notes/images/optmization-comparison.gif" alt="upload successful"></p>
<h2 id="AdamW-ICLR-2018"><a href="#AdamW-ICLR-2018" class="headerlink" title="AdamW (ICLR 2018)"></a>AdamW (ICLR 2018)</h2><p><img data-src="/notes/images/optimization-AdamW.png" alt="upload successful"></p>
<h2 id="AMSGrad"><a href="#AMSGrad" class="headerlink" title="AMSGrad"></a>AMSGrad</h2><ul>
<li>AMSGrad<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Reddi, S. J., Kale, S., & Kumar, S. (2019). [On the convergence of adam and beyond](https://arxiv.org/pdf/1904.09237.pdf). arXiv preprint arXiv:1904.09237.
">[6]</span></a></sup> maintains the <code>maximum</code> of all <script type="math/tex">S_i(t)</script> unitl the present time step to normalizing the running average of the gradient instead of directly using <script type="math/tex">S_i(t)</script> in Adam.</li>
<li>AMSGrad results in a non-increasing step size and avoids pitfalls of Adam and RMSProp.<script type="math/tex; mode=display">\begin{align}
M_i(t) & = \alpha M_i(t-1) + (1-\alpha)d_i(t) & \text{momentum-smoothed gradient}\\
S_i(t) & = \beta S_i(t-1) + (1-\beta)d_i(t)^2 &\text{RMSProp update} \\
\hat{S}_i(t) &= \max(\hat{S}_i(t-1), S_i(t)) & \text{keep the maximum value} \\
\Delta w_i(t)& = \frac{-\eta}{\sqrt{\hat{S}_i(t)}+\epsilon} M_i(t) 
\end{align}</script></li>
</ul>
<h2 id="AdaBound"><a href="#AdaBound" class="headerlink" title="AdaBound"></a>AdaBound</h2><ul>
<li>AdaBound<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Luo, L., Xiong, Y., Liu, Y., & Sun, X. (2019). Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843.
">[7]</span></a></sup> applies a gradient clipping on the learning rate. It behaves like Adam at the beginning as the bounds have little impact on learning rates, and it gradually transforms to SGD.</li>
</ul>
<script type="math/tex; mode=display">\begin{align}
M_i(t) & = \alpha M_i(t-1) + (1-\alpha)d_i(t) & \text{momentum-smoothed gradient}\\
S_i(t) & = \beta S_i(t-1) + (1-\beta)d_i(t)^2 &\text{RMSProp update} \\
\eta_i(t) &= \text{Clip} \left(\frac{\eta}{\sqrt{S_i(t)}}, \eta_{l}(t), \eta_{u}(t) \right) / \sqrt{t} & \text{clip to the range }[\eta_{l}(t), \eta_{u}(t)] \text{, then scale}\\ 
\Delta w_i(t)& = \eta_i(t) M_i(t) 
\end{align}</script><h2 id="Rectified-Adam-RAdam"><a href="#Rectified-Adam-RAdam" class="headerlink" title="Rectified Adam (RAdam)"></a>Rectified Adam (RAdam)</h2><div class="note danger">
            <h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h3><ul><li>Adaptive learning rate has an undesirably large variance due to <em>the lack of samples in the early stage</em>, leading to suspicious local optima.</li></ul>
          </div>
<h3 id="Previous-solution"><a href="#Previous-solution" class="headerlink" title="Previous solution"></a>Previous solution</h3><ul>
<li>Warmup heuistics: using a <em>small learning rate in the first few epochs of training</em></li>
<li>E.g. Linear warmup, set <script type="math/tex">\alpha_t = t \alpha_0</script>, when <script type="math/tex">t<T_w</script>.<h3 id="RAdam"><a href="#RAdam" class="headerlink" title="RAdam"></a>RAdam</h3>Rectified Adam (RAdam) induced a rectification term <script type="math/tex">r_t</script> to mitigate the variance of adaptive learning rate, inspired by Exponential Moving Average (EMA)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., & Han, J. (2019). [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/pdf/1908.03265.pdf). ArXiv, abs/1908.03265.
">[10]</span></a></sup>.</li>
</ul>
<p>Let <script type="math/tex">\rho_t</script> denote the length of Simple Moving Average (SMA), <script type="math/tex">\rho_\infty \leftarrow \frac{2}{(1-\beta_2) -1}</script> be the maximum length of the approximated SMA.</p>
<script type="math/tex; mode=display">\begin{align}
M_i(t) & = \alpha M_i(t-1) + (1-\alpha)d_i(t) & \text{momentum-smoothed gradient}\\
S_i(t) & = \beta S_i(t-1) + (1-\beta)d_i(t)^2 &\text{RMSProp update} \\
\hat{M}_i(t) & \leftarrow M_i(t) / (1-\alpha) & \text{correct bias for 1st moment}\\ 
\rho_t &\leftarrow \rho_\infty - 2t \beta / (1-\beta) & \text{the length of approximated SMA} \\
\end{align}</script><p>if $\rho_t &gt; 4$ i.e., the variance is tractable:</p>
<script type="math/tex; mode=display">\begin{align}
\hat{S}_i(t) &\rightarrow S_i(t) / (1-\beta) & \text{correct bias for 2nd moment} \\
\mathbf{r_t} &\leftarrow \sqrt{\frac{(\rho_t-4)(\rho_t-2)\rho_\infty}{(\rho_t-4)(\rho_t-2)\rho_t}} & \text{compute the variance rectification term}\\
\Delta w_i(t)& = \frac{-\eta}{\sqrt{\hat{S}_i(t)}} \hat{M}_i(t) \mathbf{r_t} \\
\end{align}</script><p>else:</p>
<script type="math/tex; mode=display">\begin{align}
\Delta w_i(t)& = \eta \hat{M}_i(t) & \text{ momentum without adaptation}
\end{align}</script><p><img data-src="/notes/images/RAdam.png" alt="upload successful"></p>
<ul>
<li>The heuristic linear warmup can be viewed as setting <script type="math/tex">r_t = \frac{\min(t, T_w)}{T_w}</script></li>
</ul>
<h1 id="Lookahead"><a href="#Lookahead" class="headerlink" title="Lookahead"></a>Lookahead</h1><p>Lookahead  <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, M.R., Lucas, J., Hinton, G.E., & Ba, J. (2019). [Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/pdf/1907.08610.pdf). ArXiv, abs/1907.08610.
">[11]</span></a></sup> is orthogonal to 1) adaptive learning rate schemes, such as AdaGrad and Adam, and 2) accelerated schemes, e.g. heavy-ball and Nesterov momentum. It iteratively updates two sets of weights.</p>
<ul>
<li><p>Intuition: choose a search direction by <em>loking ahead</em> at the sequence of “fast weights”, generated by another optimizer.</p>
</li>
<li><p>Process: Lookahead first updates the “fast weights” $k$ times using any standard optimizer in the inner loop, before updating the “slow weights” once, in the direction of the <strong>final fast weights</strong>.</p>
</li>
<li><p>It is empirically robust to the suboptimal hyperparameters, changes in the inner loop optimizer, # of fast weight updates and the slow weights learning rate.</p>
</li>
</ul>
<h2 id="Lookahead-optimizer"><a href="#Lookahead-optimizer" class="headerlink" title="Lookahead optimizer"></a>Lookahead optimizer</h2><p>Lookahead maintains a set of slow weights $\phi$ and fast weights $\theta$,which synced with fast weights every $k$ updates. </p>
<ul>
<li>Init parameters <script type="math/tex">\phi_0</script>, loss function $L$</li>
<li>sync period $k$, <strong>slow</strong> weights step size $\alpha$, optimizer $A$</li>
<li>for t = 1,2,…:<ul>
<li>sync updated slow weights to the inner loop fast weights: <script type="math/tex">\theta_{t,0} \leftarrow \phi_{t-1}</script></li>
<li>for i in 1,2,…,k:<ul>
<li>sample mini-batch of data $d \sim \mathcal{D}$</li>
<li>update fast weights with standard optimizer <script type="math/tex">\theta_{t,i} \leftarrow \theta_{t, i-1} + A(L, \theta_{t,i-1},d)</script></li>
</ul>
</li>
<li>update slow weights with interpolation <script type="math/tex">\phi_t \leftarrow \phi_{t-1} + \alpha (\underbrace{\theta_{t,k} - \phi_{t-1})}_\text{linear interpolation}</script>  </li>
</ul>
</li>
</ul>
<p>After $k$ inner updates using $A$, the slow weights are updated towards the fast weights by linearly interpolating in weight space $\theta - \phi$. Then after each slow weights update, the fast weights are reset to the current slow weights.</p>
<p><img data-src="/notes/images/optimizer-Lookahead.png" alt="upload successful"></p>
<ul>
<li>Benefit: Loopahead benefits from large learning rate in the inner loop. The fast weights updates make rapid progress along the low curvature direction, whilst the <strong>slow weights</strong> help <strong>smooth out the oscillation through the parameter interpolation</strong>.</li>
</ul>
<p><strong>Slow weights trajectory</strong></p>
<ul>
<li><p>an exponential moving average (EMA) of the <strong>final fast weights</strong> within each inner loop after $k$ innerloop steps:</p>
<script type="math/tex; mode=display">\begin{align}
\phi_{t+1} & = \phi_t + \alpha (\theta_{t,k} - \phi_t) \\
      & = \alpha\left[ \theta_{t,k} + (1-\alpha) \theta_{t-1,k} + \cdots + (1-\alpha)^{t-1} \theta_{0,k} \right] + (1-\alpha)^t \phi_0
\end{align}</script></li>
<li><p>Intuition: slow weights use recent fast weights but matain the effect from previous fast weights.</p>
</li>
</ul>
<p><strong>Fast weights trajectory</strong></p>
<ul>
<li>maintaining</li>
<li>interpolating update: <script type="math/tex">\theta_{t,i+1} = \theta_{t,i} + A(L , \theta_{t,i-1},d)</script></li>
<li>resetting to current slow weights</li>
</ul>
<h2 id="Ranger"><a href="#Ranger" class="headerlink" title="Ranger"></a>Ranger</h2><p>Ranger<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d](https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d)
">[12]</span></a></sup> applies <em>RAdam</em> as the optimizer $A$ in <em>Lookahead</em> algorithms (source code: <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer)">[13]</span></a></sup>).</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cs231n.github.io/neural-networks-3/">cs231n neural-networks-3</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Riedmiller, M., &amp; Braun, H. (1993, March). A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Proceedings of the IEEE international conference on neural networks (Vol. 1993, pp. 586-591).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">csc321 Neural Networks for Machine Learning Lecure6</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kingma, D. P., &amp; Ba, J. (2014). <a href="https://arxiv.org/abs/1412.6980">Adam: A method for stochastic optimization</a>. arXiv preprint arXiv:1412.6980.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Reddi, S. J., Kale, S., &amp; Kumar, S. (2019). <a href="https://arxiv.org/pdf/1904.09237.pdf">On the convergence of adam and beyond</a>. arXiv preprint arXiv:1904.09237.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Luo, L., Xiong, Y., Liu, Y., &amp; Sun, X. (2019). Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Loshchilov, I., &amp; Hutter, F. (2017). Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., &amp; Han, J. (2019). <a href="https://arxiv.org/pdf/1908.03265.pdf">On the Variance of the Adaptive Learning Rate and Beyond</a>. ArXiv, abs/1908.03265.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, M.R., Lucas, J., Hinton, G.E., &amp; Ba, J. (2019). <a href="https://arxiv.org/pdf/1907.08610.pdf">Lookahead Optimizer: k steps forward, 1 step back</a>. ArXiv, abs/1907.08610.<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d">https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title>Relational Reasoning Networks</title>
    <url>/notes/2019/10/22/NN/Relational-Reasoning-Networks/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Reasoning the relations between objects and their properties is a hallmark of intelligence. Here are some notes about the relational reasoning neural networks.</p>
<p><img data-src='/notes/images/VLEVR dataset.png' width='60%' /><br><span id="more"></span></p>
<center> VLEVR dataset </center>

<h1 id="Relation-Network"><a href="#Relation-Network" class="headerlink" title="Relation Network"></a>Relation Network</h1><h2 id="Relation-Networks-RNs"><a href="#Relation-Networks-RNs" class="headerlink" title="Relation Networks (RNs)"></a>Relation Networks (RNs)</h2><p>Relation Networks(RNs)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., & Lillicrap, T. (2017). [A simple neural network module for relational reasoning](https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf). In Advances in neural information processing systems (pp. 4967-4976).
">[1]</span></a></sup> adopt the functional form of a neural network for relational reasoning. RNs consider the potential relations beween all object pairs. </p>
<p>“RNs learn to infer the existence and implications of object relations.” (Santoro et. al, 2017)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., & Lillicrap, T. (2017). [A simple neural network module for relational reasoning](https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf). In Advances in neural information processing systems (pp. 4967-4976).
">[1]</span></a></sup></p>
<script type="math/tex; mode=display">\begin{align}
RN(o) &= f_\phi \big( \color{red}{\pmb{a} \big(}  g_\theta (o_i, o_j) \color{red}{\big)}  \big) \\
&= f_\phi \big( \color{red}{\pmb{a} \big(} g_\theta(o_1, o_2), g_\theta(o_1, o_3), \cdots,  g_\theta(o_{m-1}, o_m), \cdots \color{red}{\big)} \big) \\
\end{align}</script><p>where $\color{red}{\pmb{a}}$ is the aggregation function. </p>
<p>When we take it as <strong>summation</strong>, the simplest form is:</p>
<script type="math/tex; mode=display">\text{RN} (o) = f_\phi \big( \sum_{i,j} g_\theta (o_i, o_j) \big)</script><p>where</p>
<ul>
<li>the input is Objects <script type="math/tex">O = \{o_1, o_2, \cdots, o_n\}</script></li>
<li><script type="math/tex">f_\phi</script> and <script type="math/tex">g_\theta</script> are two MLPs with parameters $\phi$ and $\theta$. The same MLP operates on all possible pairs. <script type="math/tex">g_\theta</script> captures the representation of pair-wise relations, and <script type="math/tex">f_\phi</script> integrates information about all pairs.</li>
</ul>
<p>The <strong>summation</strong> in RN equations indicating the <strong>order (permutation) invariance</strong> of the object set. max and average pooling can be used instead.</p>
<p><img data-src="/notes/images/Relation-Networks-2.png" width="80%"/></p>
<center> Image source: <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Raposo, D., Santoro, A., Barrett, D., Pascanu, R., Lillicrap, T., & Battaglia, P. (2017). [Discovering objects and their relations from entangled scene representations](https://arxiv.org/pdf/1702.05068). arXiv preprint arXiv:1702.05068.
">[2]</span></a></sup> </center>

<p><img data-src="/notes/images/Relation-Networks.png" alt="upload successful"></p>
<center> Visual QA achitecture<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., & Lillicrap, T. (2017). [A simple neural network module for relational reasoning](https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf). In Advances in neural information processing systems (pp. 4967-4976).
">[1]</span></a></sup> </center>

<h2 id="Wild-Relation-Network-WReN"><a href="#Wild-Relation-Network-WReN" class="headerlink" title="Wild Relation Network (WReN)"></a>Wild Relation Network (WReN)</h2><p>Wild Relation Network (WReN) do RN module multiple times to infer the inter-pannel relationships.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Barrett, D. G., Hill, F., Santoro, A., Morcos, A. S., & Lillicrap, T. (2018). [Measuring abstract reasoning in neural networks](https://arxiv.org/pdf/1807.04225). arXiv preprint arXiv:1807.04225.
">[3]</span></a></sup> Afterward, pass all scores to a softmax function.<br><img data-src="/notes/images/WReN.png" alt="upload successful"></p>
<center> Image source: <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Barrett, D. G., Hill, F., Santoro, A., Morcos, A. S., & Lillicrap, T. (2018). [Measuring abstract reasoning in neural networks](https://arxiv.org/pdf/1807.04225). arXiv preprint arXiv:1807.04225.
">[3]</span></a></sup> </center>

<h2 id="Visual-Interaction-Network-VIN"><a href="#Visual-Interaction-Network-VIN" class="headerlink" title="Visual Interaction Network(VIN)"></a>Visual Interaction Network(VIN)</h2><p><img data-src="/notes/images/VIN-pair-encoder.png" alt="upload successful"><br>Visual Interaction Network(VIN) adopts ConvNets to encoder images. Two consecutive input frames are convolved into a state code. </p>
<p><img data-src="/notes/images/VIN-Interaction-Net.png" alt="upload successful"><br>Afterward, employ RN in its <strong>Interaction Net</strong>(IN). </p>
<ul>
<li>For each slot, RN is applied to the slot’s concatenation with each other slot. </li>
<li>Then a self-dynamics net is applied to the slot itself. </li>
<li>FInally sum all the results and produce the output.</li>
</ul>
<h2 id="Relational-Memory-Core-RMC"><a href="#Relational-Memory-Core-RMC" class="headerlink" title="Relational Memory Core(RMC)"></a>Relational Memory Core(RMC)</h2><p>Relational Memory Core(RMC)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., ... & Lillicrap, T. (2018). [Relational recurrent neural networks](https://papers.nips.cc/paper/7960-relational-recurrent-neural-networks.pdf). In Advances in Neural Information Processing Systems (pp. 7299-7310).
">[4]</span></a></sup>  assembles LSTMs and non-local networks(i.e. Transformer).</p>
<p><img data-src="/notes/images/RMC-module.png" alt="upload successful"></p>
<ul>
<li>Encoding new memories<br>Let matrix $M$ denote stored memories with row-wise memories <script type="math/tex">m_i</script>. RMC apply multi-head dot product attention(MHDPA) to allow memories interacting with others. $\color{green}{[M;x]}$ include <strong>memories</strong> and <strong>new observations</strong>. The output size of $\tilde{M}$ is the same as $M$.</li>
</ul>
<p><img data-src="/notes/images/Relational-MHDPA.png" alt="upload successful"></p>
<script type="math/tex; mode=display">\tilde{M} = \text{softmax} \big( \frac{M W^q (\color{green}{[M;x]}W^k)^\top}{\sqrt{d^k}} \big) \color{green}{[M;x]}W^v</script><p><img data-src="/notes/images/Realtional-self-attention.png" alt="upload successful"></p>
<ul>
<li>Introducing recurrence into variant LSTM</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{f}_{i,t}\\ \mathbf{i}_{i,t}    \\ \mathbf{o}_{i,t}    \\ \end{array}\right]  &= \left[\begin{array}{c} W^f \\ W^i \\ W^o \end{array}\right] \mathbf{x}_t  + \left[\begin{array}{c} U^f \\ U^i \\ U^o \end{array}\right] \mathbf{h}_{t-1} + \left[\begin{array}{c} \mathbf{b}^f \\ \mathbf{b}^i \\ \mathbf{b}^o \end{array}\right] \\
m_{i,t} &= \sigma (f_{i,t}+\tilde{b}^f) \odot m_{i,t-1} + \sigma (i_{i,t}) \odot \color{green}{g_{\psi}} (\tilde{m_{i,t}}) \\
h_{i,t} &= \sigma (o_{i,t}) \odot \tanh (m_{i,t})\\
s_{i,t+1} &= (m_{i,t}, h_{i,t})
\end{align}</script><p>where <script type="math/tex">\color{green}{g_{\psi}}</script> is a row/memory-wise MLP with layer normalization.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., ... & Lillicrap, T. (2018). [Relational recurrent neural networks](https://papers.nips.cc/paper/7960-relational-recurrent-neural-networks.pdf). In Advances in Neural Information Processing Systems (pp. 7299-7310).
">[4]</span></a></sup></p>
<h1 id="recurrent-Memory-Attention-and-Composition-MAC"><a href="#recurrent-Memory-Attention-and-Composition-MAC" class="headerlink" title="recurrent Memory, Attention and Composition (MAC)"></a>recurrent Memory, Attention and Composition (MAC)</h1><p>The MAC recurrent cell consists of control unit, read unit and write unit.</p>
<ul>
<li>control unit: attends to different parts of the task question (question)</li>
<li>read unit: extacts information out of knowledge base (image in VQA task)</li>
<li>write unit: integrates the retrieved information into the memory state</li>
</ul>
<p><img data-src="/notes/images/MAC-cell.png" alt="upload successful"></p>
<p>Input: </p>
<ul>
<li>concat the last states of bi-LSTM on textual questions as $\pmb{q}$</li>
<li>convolve image as the knowledge base <script type="math/tex">\pmb{K}^{H \times W \times d}</script></li>
</ul>
<h2 id="Control-unit"><a href="#Control-unit" class="headerlink" title="Control unit"></a>Control unit</h2><p><img data-src="/notes/images/MAC-control-unit.png" alt="upload successful"></p>
<p>Given the contextual question word <script type="math/tex">\pmb{cw}_1, \cdots, \pmb{cw}_S</script>, the question representation <script type="math/tex">\pmb{q}_i</script>, the previous control state <script type="math/tex">\pmb{c}_{i-1}</script>.</p>
<ol>
<li>Concat <script type="math/tex">\pmb{q}_i</script> and <script type="math/tex">\pmb{c}_{i-1}</script> and feed into a FFNN.</li>
<li>Measure the similarity between <script type="math/tex">cq_i</script> and each question word <script type="math/tex">\pmb{cw}_s</script>; then use a softmax layer to normalize the weights, aquiring attention distribution.</li>
<li>Weighted averaging the question context words, and get current control state <script type="math/tex">\pmb{c}_i</script></li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
cq_i &= W^{d \times 2d} [\pmb{c}_{i-1}, \pmb{q}] + b^d \\
ca_{i,s} &= W^{1\times d} (cq \odot \pmb{cw_s}) + b^1 \\
cw_{i,s} &= \text{softmax} (ca_{i,s}) \\
\pmb{c}_i &= \sum_{s=1}^S cw_{i,s} \cdot \pmb{cw}_s \\
\end{align}</script><h2 id="Read-unit"><a href="#Read-unit" class="headerlink" title="Read unit"></a>Read unit</h2><p><img data-src="/notes/images/MAC-Read-unit.png" alt="upload successful"></p>
<ol>
<li>Interact between the knowledge-based element <script type="math/tex">\pmb{k}_{h,w}</script> and memory <script type="math/tex">\pmb{m}_{i-1}</script>, get <script type="math/tex">I_{i,h,w}</script></li>
<li>concat <script type="math/tex">[I_{i,h,w}; \pmb{k}_{h,w}]</script> and feed into a dense layer</li>
<li>compute attention distribution over the knowledge base and finally do weighted average.</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
I_{i,h,w} &= [W_m^{d \times d} \pmb{m}_{i-1} + b_m^d] \odot [W_k^{d \times d} \pmb{k}_{h,w} + b_k^d] \\
I'_{i,h,w} &= W^{d \times 2d} [I_{i,h,w}, \pmb{k}_{h,w}] + b^d \\
ra_{i,h,w} &= W^{d \times d} (\pmb{c}_i \odot I'_{i,h,w}) + b^d \\
rv_{i,h.w} &= \text{softmax}(ra_{i,h,w}) & \text{attention distribution}\\
\pmb{r}_i &= \sum_{h,w=1,1}^{H,W} rv_{i,h,w} \cdot \pmb{k}_{h,w}
\end{align}</script><h2 id="Write-unit"><a href="#Write-unit" class="headerlink" title="Write unit"></a>Write unit</h2><p><img data-src="/notes/images/MAC-Write-unit.png" alt="upload successful"></p>
<script type="math/tex; mode=display">
\begin{align}
m_i^\text{info} &= W^{d \times 2d} [\pmb{r}_i, \pmb{m}_{i-1}] + b^d \\
sa_{ij} &= \text{softmax} \big( W^{1 \times d} (\pmb{c_i} \odot \pmb{c_j}) + b^1 \big) & \text{self attention }\\
m_i^\text{sa} &= \sum_{j=1}^{i-1} sa_{ij} \cdot \pmb{m_j} \\
m'_i &= W_s^{d \times d} m_i^{sa} + W_p^{d \times d} m_i^\text{info} + b^d \\
c'_i &= W^{1 \times d} \pmb{c_i} + b^1 \\
\pmb{m_i} &= \sigma(c'_i) \pmb{m_{i-1}} + (1-\sigma(c'_i))m'_i & \text{memory gate}

\end{align}</script><h2 id="Output-unit"><a href="#Output-unit" class="headerlink" title="Output unit"></a>Output unit</h2><p>Concat $q$ and <script type="math/tex">\pmb{m_p}</script>, then pass 2-layer FFCC followed by a softmax function.<br><img data-src="/notes/images/MAC-Output-unit.png" alt="upload successful"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., &amp; Lillicrap, T. (2017). <a href="https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf">A simple neural network module for relational reasoning</a>. In Advances in neural information processing systems (pp. 4967-4976).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Raposo, D., Santoro, A., Barrett, D., Pascanu, R., Lillicrap, T., &amp; Battaglia, P. (2017). <a href="https://arxiv.org/pdf/1702.05068">Discovering objects and their relations from entangled scene representations</a>. arXiv preprint arXiv:1702.05068.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Barrett, D. G., Hill, F., Santoro, A., Morcos, A. S., &amp; Lillicrap, T. (2018). <a href="https://arxiv.org/pdf/1807.04225">Measuring abstract reasoning in neural networks</a>. arXiv preprint arXiv:1807.04225.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., ... &amp; Lillicrap, T. (2018). <a href="https://papers.nips.cc/paper/7960-relational-recurrent-neural-networks.pdf">Relational recurrent neural networks</a>. In Advances in Neural Information Processing Systems (pp. 7299-7310).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hudson, D. A., &amp; Manning, C. D. (2018). <a href="https://arxiv.org/pdf/1803.03067">Compositional attention networks for machine reasoning</a>. arXiv preprint arXiv:1803.03067.<a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
        <category>Relation</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>Relation</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer Variants: A Peek</title>
    <url>/notes/2019/10/17/NN/Transformer-variants-a-peek/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>This is an introduction of variant Transformers.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). In Advances in neural information processing systems (pp. 5998-6008).
">[1]</span></a></sup></p>
<span id="more"></span>
<div class="note success">
            <p><strong>Relevant notes</strong>:</p><ol><li><a href="/notes/2019/01/22/NLP/Attention-in-a-nutshell/#Transformer">Transformer detailed explanation</a></li><li><a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/">Transformer variant architectures</a></li><li><a href="/notes/2019/12/14/NLP/BERTology-an-introduction/">BERTology introduction</a></li></ol>
          </div>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>The details of transformer is explained in previous blogs. The schema of Transformer is as following fig.</p>
<ul>
<li>Architecture<br><img data-src='/notes/images/transformer.png' width='60%'/></li>
<li>Decoding<br><img data-src='/notes/images/transformer_decoding.gif'/></li>
</ul>
<h1 id="Vanilla-Transformer"><a href="#Vanilla-Transformer" class="headerlink" title="Vanilla Transformer"></a>Vanilla Transformer</h1><p>It is impossible to preprocess the entire context sequence in the whole corpus from the beginning, due to the limited resource in practice.</p>
<p><img data-src='/notes/images/vanilla-transformer-LM.gif' width='100%'/></p>
<p><strong>Vanilla Transformer</strong> (Al-Rfou et. al 2019)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Al-Rfou, R., Choe, D., Constant, N., Guo, M., & Jones, L. (2019, July). [Character-level language modeling with deeper self-attention](https://www.aaai.org/ojs/index.php/AAAI/article/download/4182/4060). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 3159-3166).
">[2]</span></a></sup> splits the entire corpus into shorter segments, and train within each segment. This leads to the <strong><span class="label danger">context fragmentation problem</span></strong> by ignoreing all contextual information from previous segments.</p>
<p>As in above fig., information never flows across segements.</p>
<ul>
<li>Evaluation<br><img data-src='/notes/images/transformer-vanilla-eval.gif' width='100%'/><br>During evaluation, for each output step, the segment shifts right by only one position, which hurts the decoding efficiency and speed.</li>
</ul>
<h1 id="Relative-Positional-Representation-RPR"><a href="#Relative-Positional-Representation-RPR" class="headerlink" title="Relative Positional Representation(RPR)"></a>Relative Positional Representation(RPR)</h1><ul>
<li><strong>Relation-aware self-attn</strong><br>Consider the pairwise relationships between input elements, which can be seen as a labeled, directed fully-connected graph. Let <script type="math/tex">a_{ij}^V,a_{ij}^K \in \mathbb{R}^{d_a}</script> represent the edge between input elements <script type="math/tex">x_i</script> and <script type="math/tex">x_j</script>.</li>
</ul>
<p>Then add the pairwise information to the sublayer output:</p>
<script type="math/tex; mode=display">\begin{align}
e_{ij} &= \frac{x_i W^Q (x_j W^K \color{red}{+a_{ij}^K})^\top}{\sqrt{d_z}} \\
&= \frac{x_i W^Q (x_jW^K)^\top + \overbrace{\color{green}{\pmb{x_iW^Q(a_{ij}^K)^\top}}}^\text{efficient implementation}}{\sqrt{d_z}} \\
\alpha_{ij} &= \frac{\exp e_{ij}}{\sum_{k=1}^n \exp e_{ik}} \\
z_i &= \sum_{j=1}^n \alpha_{ij}(x_jW^V \color{red}{+ a_{ij}^V} )\\
\end{align}</script><ul>
<li>Clip RPR<br>$k$ denotes the maximum relative position. The relative position information beyond $k$ will be clipped to the maximum value, which generalizes to the unseen sequence lengths during training.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). [Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155). arXiv preprint arXiv:1803.02155.
">[5]</span></a></sup> In other words, RPR only considers context in a fixed window size $2k+1$, indicating $k$ elements on the l.h.s, and $k$ elements on the r.h.s, as well as itself.</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\text{clip}(x,k) &= \max(-k, \min(k,x)) \\
a_{ij}^K &= w_{\text{clip}(j-i, k)}^K \\
a_{ij}^V &= w_{\text{clip}(j-i, k)}^V
\end{align}</script><p>where rpr <script type="math/tex">w^K = (w_{-k}^K, \cdots, w_k^K) \in \mathbb{R}^{d_a}</script> and <script type="math/tex">w^V = (w_{-k}^V, \cdots, w_k^V) \in \mathbb{R}^{d_a}</script> are learnable.</p>
<p>Trainable param number:</p>
<ul>
<li>MADPA: <script type="math/tex">\overbrace{4 \times \big(\text{d_model} \times \text{d_model} + \text{d_model} \big)}^\text{4 dense layers}</script></li>
<li><p>MADPA with RPR: <script type="math/tex">\underbrace{4 \times \big(\text{d_model} \times \text{d_model} + \text{d_model} \big)}_\text{4 dense layers} + \underbrace{\color{red}{2 \times (\text{seq_len}^2 \times d_k )}}_\text{2 RPR matrices}</script> </p>
</li>
<li><p>My PyTorch implementation</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention_RPR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; @ author: Yekun CHAI &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, h, max_relative_position, dropout=<span class="number">.0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        multi-head attention</span></span><br><span class="line"><span class="string">        :param h: nhead</span></span><br><span class="line"><span class="string">        :param d_model: d_model</span></span><br><span class="line"><span class="string">        :param dropout: float</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention_RPR, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#  assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = utils.clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        self.max_relative_position = max_relative_position</span><br><span class="line">        self.vocab_size = max_relative_position * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">        self.embed_K = nn.Embedding(self.vocab_size, self.d_k)</span><br><span class="line">        self.embed_V = nn.Embedding(self.vocab_size, self.d_k)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        L : target sequence length</span></span><br><span class="line"><span class="string">        S : source sequence length:</span></span><br><span class="line"><span class="string">        N : batch size</span></span><br><span class="line"><span class="string">        E : embedding dim</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        :param query: (N,L,E)</span></span><br><span class="line"><span class="string">        :param key: (N,S,E)</span></span><br><span class="line"><span class="string">        :param value: (N,S,E)</span></span><br><span class="line"><span class="string">        :param mask:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)  <span class="comment"># batch size</span></span><br><span class="line">        seq_len = query.size(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1) split embedding dim to h heads : from d_model =&gt; h * d_k</span></span><br><span class="line">        <span class="comment"># dim: (nbatch, h, seq_length, d_model//h)</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) rpr</span></span><br><span class="line">        relation_keys = self.generate_relative_positions_embeddings(seq_len, seq_len, self.embed_K)</span><br><span class="line">        relation_values = self.generate_relative_positions_embeddings(seq_len, seq_len, self.embed_V)</span><br><span class="line">        logits = self._relative_attn_inner(query, key, relation_keys, <span class="literal">True</span>)</span><br><span class="line">        weights = self.dropout(F.softmax(logits, -<span class="number">1</span>))</span><br><span class="line">        x = self._relative_attn_inner(weights, value, relation_values, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        <span class="comment"># dim: (nbatch, h, d_model)</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate_relative_positions_matrix</span>(<span class="params">self, len_q, len_k</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        genetate rpr matrix</span></span><br><span class="line"><span class="string">        ---------------------------</span></span><br><span class="line"><span class="string">        :param len_q: seq_len</span></span><br><span class="line"><span class="string">        :param len_k: seq_len</span></span><br><span class="line"><span class="string">        :return: rpr matrix, dim: (len_q, len_q)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> len_q == len_k</span><br><span class="line">        range_vec_q = range_vec_k = torch.arange(len_q)</span><br><span class="line">        distance_mat = range_vec_k.unsqueeze(<span class="number">0</span>) - range_vec_q.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        disntance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)</span><br><span class="line">        <span class="keyword">return</span> disntance_mat_clipped + self.max_relative_position</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_relative_positions_embeddings</span>(<span class="params">self, len_q, len_k, embedding_table</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        generate relative position embedding</span></span><br><span class="line"><span class="string">        ----------------------</span></span><br><span class="line"><span class="string">        :param len_q:</span></span><br><span class="line"><span class="string">        :param len_k:</span></span><br><span class="line"><span class="string">        :return: rpr embedding, dim: (len_q, len_q, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        relative_position_matrix = self._generate_relative_positions_matrix(len_q, len_k)</span><br><span class="line">        <span class="keyword">return</span> embedding_table(relative_position_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_relative_attn_inner</span>(<span class="params">self, x, y, z, transpose</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        efficient implementation</span></span><br><span class="line"><span class="string">        ------------------------</span></span><br><span class="line"><span class="string">        :param x: </span></span><br><span class="line"><span class="string">        :param y: </span></span><br><span class="line"><span class="string">        :param z: </span></span><br><span class="line"><span class="string">        :param transpose: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nbatches = x.size(<span class="number">0</span>)</span><br><span class="line">        heads = x.size(<span class="number">1</span>)</span><br><span class="line">        seq_len = x.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (N, h, s, s)</span></span><br><span class="line">        xy_matmul = torch.matmul(x, y.transpose(-<span class="number">1</span>, -<span class="number">2</span>) <span class="keyword">if</span> transpose <span class="keyword">else</span> y)</span><br><span class="line">        <span class="comment"># (s, N, h, d) =&gt; (s, N*h, d)</span></span><br><span class="line">        x_t_v = x.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(seq_len, nbatches * heads, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (s, N*h, d) @ (s, d, s) =&gt; (s, N*h, s)</span></span><br><span class="line">        x_tz_matmul = torch.matmul(x_t_v, z.transpose(-<span class="number">1</span>, -<span class="number">2</span>) <span class="keyword">if</span> transpose <span class="keyword">else</span> z)</span><br><span class="line">        <span class="comment"># (N, h, s, s)</span></span><br><span class="line">        x_tz_matmul_v_t = x_tz_matmul.view(seq_len, nbatches, heads, -<span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">return</span> xy_matmul + x_tz_matmul_v_t</span><br></pre></td></tr></table></figure>
</li>
<li><p>Tensorflow implementation: <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Tensor2Tensor tensorflow code](https://github.com/tensorflow/tensor2tensor/blob/9e0a894034d8090892c238df1bd9bd3180c2b9a3/tensor2tensor/layers/common_attention.py#L1556-L1587)
">[7]</span></a></sup></p>
</li>
</ul>
<h1 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h1><p>Transformer-XL<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). [Transformer-xl: Attentive language models beyond a fixed-length context](https://arxiv.org/pdf/1901.02860). arXiv preprint arXiv:1901.02860.
">[3]</span></a></sup> is capable of learning the long-term dependency between different context fragments in Vanilla Transformers. It mainly employs the segment-level recurrence and relative positional encoding scheme.</p>
<h2 id="Segment-level-recurrence"><a href="#Segment-level-recurrence" class="headerlink" title="Segment-level recurrence"></a>Segment-level recurrence</h2><p><img data-src='/notes/images/transformer-xl-segment-level-recurrence.gif' width='100%'/><br>During training, transformer-xl adopts both current and the previous segments, levaraging the recurrence mechanism on segement level.</p>
<p>Let the consecutive segment of length $L$ be <script type="math/tex">\pmb{s}_\tau = [x_{\tau,1}, \cdots, x_{\tau,L}]</script> and <script type="math/tex">\pmb{s}_{\tau+1}=[x_{\tau+1,1}, \cdots, x_{\tau+1,L}]</script>. Denote the $d$-dimensional hidden state of $n$-th layer for the $\tau$-th segment <script type="math/tex">\pmb{s}_\tau</script>, be <script type="math/tex">\pmb{h}_\tau^n \in \mathbb{R}^{L \times d}</script>.</p>
<script type="math/tex; mode=display">
\begin{align}
\pmb{q}_{\tau+1}^{n} &= \pmb{h}_{\color{red}{\tau+1}}^{n-1} \pmb{W}_q^\top \\
\pmb{k}_{\tau+1}^{n} &= [\overbrace{\text{SG}(\pmb{h}_{\color{red}{\tau}}^{n-1})}^\text{stop gradient}; \pmb{h}_{\color{red}{\tau+1}}^{n-1})] \pmb{W}_k^\top & \text{concat hidden states of prev segments} \\
\pmb{v}_{\tau+1}^{n} &= [\underbrace{\text{SG}(\pmb{h}_{\color{red}{\tau}}^{n-1})}_\text{stop gradient}; \pmb{h}_{\color{red}{\tau+1}}^{n-1})] \pmb{W}_v^\top & \text{concat hidden states of prev segments}\\
\pmb{h}_{\tau+1}^n &= \text{Transformer}(\pmb{q}_{\tau+1}^n, \pmb{k}_{\tau+1}^n, \pmb{v}_{\tau+1}^{n}) \\

\end{align}</script><p>Thus, the recurrent dependency between <script type="math/tex">\pmb{h}_{\tau+1}^n</script> and <script type="math/tex">\pmb{h}_{\tau}^{n-1}</script> <strong>shifts one layer vertically and one segment horizontally</strong>, unlike the recurrence of same layer in RNNs. As a result, the largest long-range dependency length is linearly w.r.t # of layers times segment length, i.e. $O(N \times L)$.</p>
<ul>
<li>Evaluation<br>During evaluation process, the representation from previous segments can be reused, which is much faster compared with vanilla Transformers (as below fig.).<br><img data-src='/notes/images/transformer-xl-eval.gif' width='100%'/></li>
</ul>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><h3 id="Absolute-Positional-Encoding"><a href="#Absolute-Positional-Encoding" class="headerlink" title="Absolute Positional Encoding"></a>Absolute Positional Encoding</h3><ul>
<li>Problems: In the segment $\tau$, using the same absolute positional encoding <script type="math/tex">\pmb{U}_{1:L}</script> for all segments cannot distinguish the positional difference between the same place in different segments, i.e. <script type="math/tex">x_{\tau,j}</script> and <script type="math/tex">x_{\tau+1,j}</script> for any $j=1, \cdots, L$.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). In Advances in neural information processing systems (pp. 5998-6008).
">[1]</span></a></sup></li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\pmb{A}_{i,j}^\text{abs} &= q_i^\top k_j \\ & = \big(W_q(E+\color{red}{U})_{i,\bullet} \big)^\top \big(W_k(E+\color{orange}{U})_{\bullet,j}\big) \\
&= (E_{i,\bullet}^\top W_q^\top + \color{red}{U_{i,\bullet}^\top W_q^\top} ) (W_k E_{\bullet,j} + \color{orange}{W_k U_{\bullet,j}}) \\
&= \underbrace{E_{i,\bullet}^\top W_q^\top W_k E_{\bullet,j} }_\text{(a)} + \underbrace{E_{i,\bullet}^\top W_q^\top \color{orange}{W_k U_{\bullet,j}} }_\text{(b)} \\&+ \underbrace{\color{red}{U_{i,\bullet}^\top W_q^\top} W_k E_{\bullet,j}}_\text{(c)} + \underbrace{ \color{red}{U_{i,\bullet}^\top W_q^\top} \color{orange}{W_k U_{\bullet,j}} }_\text{(d)}
\end{align}</script><p>Here,</p>
<ul>
<li>(a) captures <strong>content-based information</strong>, i.e., how much attention the word in row-$i$ pays  to word in col-$j$ despite the position.</li>
<li>(b) captures <strong>content-dependent positional bias</strong>, representing how much the word in row-$i$ should attend to position $j$. </li>
<li>(c) defines the <strong>global content biases</strong>, denoting how much the position-$i$ should attend to words in $j$-th position.</li>
<li>(d) denotes the <strong>global positional bias</strong>, i.e., the soft attention that words in position $i$ should pay to a row in position $j$.</li>
</ul>
<h3 id="Relative-positional-encoding"><a href="#Relative-positional-encoding" class="headerlink" title="Relative positional encoding"></a>Relative positional encoding</h3><p><strong>Solution</strong>: use relative positional encoding. Conceptionally, positional encoding (pe) gives the <strong>temporal clue or biases</strong> about how information should be gathered, i.e., where to attend.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). [Transformer-xl: Attentive language models beyond a fixed-length context](https://arxiv.org/pdf/1901.02860). arXiv preprint arXiv:1901.02860.
">[3]</span></a></sup> It is sufficient to know the relative distance beween each key vector <script type="math/tex">k_{\tau,j}</script> and itself <script type="math/tex">k_{\tau,i}</script>, i.e. $i-j$.</p>
<p>Replacement:</p>
<ol>
<li>replace all absolute pe’s <script type="math/tex">U_j</script> in (b) and (d) with relative counterpart <script type="math/tex">\color{cyan}{R_{i-j}}</script>, which is a <strong>sinusoid encoding matrix</strong> without learnable weights.</li>
<li>replace the query <script type="math/tex">\color{red}{U_{i,\bullet}^\top W_q^\top}</script> with a trainable parameter $\color{blue}{u \in \mathbb{R}^d}$ and similarly, $\color{blue}{v \in \mathbb{R}^d}$ in (d). Because the query vector is the same for all query positions, meaning that <strong>the query bias attending to words at various positions should be identical</strong>, no matter the query positions. </li>
<li>substitude the weight of key vector with two matrices <script type="math/tex">\color{Salmon}{W_{k,E}}</script> and <script type="math/tex">\color{ForestGreen}{W_{k,R}}</script> respectively, to produce the $\color{Salmon}{\text{content-based}}$ and $\color{Green}{\text{location-based}}$ key vectors.<script type="math/tex; mode=display">
\begin{align}
A_{i,j}^\text{rel} &= \underbrace{E_{i,\bullet}^\top W_q^\top \color{Salmon}{W_{k,E}}  E_{\bullet,j} }_\text{(a)} + \underbrace{E_{i,\bullet}^\top W_q^\top \color{Green}{W_{k,R} }  \color{cyan}{R_{i-j}} }_\text{(b)} \\&+ \underbrace{\color{blue}{u^\top} \color{Salmon}{W_{k,E}} E_{\bullet,j}}_\text{(c)} + \underbrace{ \color{blue}{v^\top} \color{Green}{W_{k,R}} \color{Cyan}{R_{i-j}} }_\text{(d)}
\end{align}</script></li>
</ol>
<p>Thus, </p>
<ul>
<li>(a) denotes <strong>content-based addressing</strong> </li>
<li>(b) captures <strong>content-dependent positional bias</strong></li>
<li>(c) denotes the <strong>global bias</strong></li>
<li>(d) represents the <strong>global positional bias</strong></li>
</ul>
<p>The PyTorch implementation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerXL</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, n_head, d_head, mem_len, n_layer, clamp_len, tgt_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ext_len, dropatt, d_inner, pre_lnorm, dropout</span>):</span></span><br><span class="line">        self.n_layer = n_layer</span><br><span class="line">        self.mem_len = mem_len</span><br><span class="line">        self.word_emb = _</span><br><span class="line">        self.clamp_len = clamp_len</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_head = d_head</span><br><span class="line">        self.drop = nn.Dropout(p=dropout)</span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layer):</span><br><span class="line">            self.layers = self.layers.append(RelPartialLearnableDecLayer(</span><br><span class="line">                n_head, d_model, d_head, d_inner, dropout, tgt_len=tgt_len,</span><br><span class="line">                ext_len=ext_len, mem_len=mem_len, dropatt=dropatt, pre_lnorm=pre_lnorm))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.pos_emb = PositionEmbedding(self.d_model)</span><br><span class="line">        self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))</span><br><span class="line">        self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_mems</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.mem_len &gt; <span class="number">0</span>:</span><br><span class="line">            mems = []</span><br><span class="line">            param = <span class="built_in">next</span>(self.parameters())</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.n_layer + <span class="number">1</span>):</span><br><span class="line">                empty = torch.empty(<span class="number">0</span>, dtype=param.dtype, device=param.device)</span><br><span class="line">                mems.append(empty)</span><br><span class="line">            <span class="keyword">return</span> mems</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># do not use mems</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_mems</span>(<span class="params">self, hids, mems, qlen, mlen</span>):</span></span><br><span class="line">        <span class="keyword">if</span> mems <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(hids) == <span class="built_in">len</span>(mems), <span class="string">&#x27;len(hids) != len(mems)!&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            new_mems = []</span><br><span class="line">            end_idx = mlen + qlen</span><br><span class="line">            beggin_idx = <span class="built_in">max</span>(<span class="number">0</span>, end_idx - self.mem_len)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hids)):</span><br><span class="line">                cat = torch.cat((mems[i], hids[i]), dim=<span class="number">0</span>)</span><br><span class="line">                new_mems.append(cat[beggin_idx:end_idx].detach())</span><br><span class="line">        <span class="keyword">return</span> new_mems</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward</span>(<span class="params">self, inp, mems=<span class="literal">None</span></span>):</span></span><br><span class="line">        qlen, bsz = inp.szie()</span><br><span class="line">        word_emb = self.word_emb(inp)</span><br><span class="line"></span><br><span class="line">        mlen = mems[<span class="number">0</span>].size(<span class="number">0</span>) <span class="keyword">if</span> mems <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        klen = mlen + qlen</span><br><span class="line"></span><br><span class="line">        dec_attn_mask = torch.triu(word_emb.new_ones(qlen, klen), diagnal=<span class="number">1</span> + mlen).byte()[:, :, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        hiddens = []</span><br><span class="line">        pos_seq = torch.arange(klen - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1.0</span>, device=word_emb.device, dtype=word_emb.dtype)</span><br><span class="line">        <span class="keyword">if</span> self.clamp_len &gt; <span class="number">0</span>:</span><br><span class="line">            pos_seq.clamp_(<span class="built_in">max</span>=self.clamp_len)</span><br><span class="line">        pos_emb = self.pos_emb(pos_seq)</span><br><span class="line"></span><br><span class="line">        core_out = self.drop(word_emb)</span><br><span class="line">        pos_emb = self.drop(pos_emb)</span><br><span class="line"></span><br><span class="line">        hiddens.append(core_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            mems_i = <span class="literal">None</span> <span class="keyword">if</span> mems <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> mems[i]</span><br><span class="line">            core_out = layer(core_out, pos_emb, self.r_w_bias, self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)</span><br><span class="line">            hiddens.append(core_out)</span><br><span class="line"></span><br><span class="line">        core_out = self.drop(core_out)</span><br><span class="line"></span><br><span class="line">        new_mems = self._update_mems(hiddens, mems, mlen, qlen)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> core_out, new_mems</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y, *mems</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> mems: mems = self.init_mems()</span><br><span class="line"></span><br><span class="line">        tgt_len = y.size(<span class="number">0</span>)</span><br><span class="line">        hidden, new_mems = self._forward(x, mems=mems)</span><br><span class="line">        pred_hid = hidden[-tgt_len:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RelPartialLearnableDecLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, n_head, d_head, d_inner, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RelPartialLearnableDecLayer, self).__init__()</span><br><span class="line">        self.dec_attn = RelPartialLearnableMHDPA(n_head, d_model, d_head, dropout, **kwargs)</span><br><span class="line">        self.ffn = PositionwiseFF(d_model, d_inner, dropout, pre_lnorm=kwargs.get(<span class="string">&#x27;pre_lnorm&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inp, r, r_w_bias, r_r_bias, dec_attn_mask=<span class="literal">None</span>, mems=<span class="literal">None</span></span>):</span></span><br><span class="line">        out = self.dec_attn(inp, r, r_w_bias, r_r_bias, dec_attn_mask, mems)</span><br><span class="line">        out = self.ffn(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RelPartialLearnableMHDPA</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_head, d_model, d_head, dropout, tgt_len=<span class="literal">None</span>, mem_len=<span class="literal">None</span>, pre_lnorm=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RelPartialLearnableMHDPA, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_head = d_head</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.qkv_net = nn.Linear(d_model, <span class="number">3</span> * n_head * d_head, bias=<span class="literal">False</span>)</span><br><span class="line">        self.drop = nn.Dropout(p=dropout)</span><br><span class="line">        self.dropatt = nn.Dropout(p=dropout)</span><br><span class="line">        self.o_net = nn.Linear(n_head * d_head, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">        self.scale = <span class="number">1</span> / (d_head ** <span class="number">.5</span>)</span><br><span class="line">        self.pre_ln = pre_lnorm</span><br><span class="line">        <span class="comment"># xl</span></span><br><span class="line">        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_rel_shift</span>(<span class="params">self, x, zero_triu=<span class="literal">False</span></span>):</span></span><br><span class="line">        bsz, klen, n_head, d_head = x.size()</span><br><span class="line">        zero_pad = torch.zeros((bsz, <span class="number">1</span>, n_head, d_head), device=x.device, dtype=x.dtype)</span><br><span class="line">        x_padded = torch.cat((zero_pad, x), <span class="number">1</span>)  <span class="comment"># bsz, klen+1, n_head, d_head</span></span><br><span class="line">        x_padded = x_padded.view(klen + <span class="number">1</span>, bsz, n_head, d_head)</span><br><span class="line">        x = x_padded[<span class="number">1</span>:].view_as(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> zero_triu:</span><br><span class="line">            ones = torch.ones((x.size(<span class="number">0</span>), x.size(<span class="number">1</span>)))</span><br><span class="line">            x = x * torch.tril(ones, x.size(<span class="number">1</span>) - x.size(<span class="number">0</span>))[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, w, r, r_w_bias, r_r_bias, attn_mask=<span class="literal">None</span>, mems=<span class="literal">None</span></span>):</span></span><br><span class="line">        qlen, rlen, bsz = w.size(<span class="number">0</span>), r.size(<span class="number">0</span>), w.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mems <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cat = torch.cat((mems, w), <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> self.pre_ln:</span><br><span class="line">                w_heads = self.qkv_net(self.layer_norm(cat))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                w_heads = self.qkv_net(cat)</span><br><span class="line"></span><br><span class="line">            r_head_k = self.r_net(r)</span><br><span class="line">            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, <span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">            w_head_q = w_head_q[-qlen:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.pre_ln:</span><br><span class="line">                w_heads = self.qkv_net(self.layer_norm(w))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                w_heads = self.qkv_net(w)</span><br><span class="line">            r_head_k = self.r_net(r)</span><br><span class="line">            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, <span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        klen = w_head_k.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)  <span class="comment"># qlen, bsz, n_head, d_head</span></span><br><span class="line">        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)  <span class="comment"># memlen + qlen, bsz, n_head, d_head</span></span><br><span class="line">        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)  <span class="comment"># memlen + qlen, bsz, n_head, d_head</span></span><br><span class="line"></span><br><span class="line">        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)  <span class="comment"># qlen x n_head x d_head</span></span><br><span class="line"></span><br><span class="line">        rw_head_q = w_head_q + r_w_bias</span><br><span class="line">        AC = torch.einsum(<span class="string">&#x27;ibnd, jbnd-&gt;ijbn&#x27;</span>, (rw_head_q, w_head_q))</span><br><span class="line"></span><br><span class="line">        rr_head_q = w_head_q + r_r_bias</span><br><span class="line">        BD = torch.einsum(<span class="string">&#x27;ibnd,jnd-&gt;ijbn&#x27;</span>, (rr_head_q, r_head_k))</span><br><span class="line">        BD = self._rel_shift(BD)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># qlen, klen, bsz, n_head</span></span><br><span class="line">        attn_score = AC + BD</span><br><span class="line">        attn_score.mul_(self.scale)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> attn_mask.<span class="built_in">any</span>().item():</span><br><span class="line">            <span class="keyword">if</span> attn_mask.dim() == <span class="number">2</span>:</span><br><span class="line">                attn_score = attn_mask.<span class="built_in">float</span>().masked_fill(attn_mask[<span class="literal">None</span>, :, :, <span class="literal">None</span>], -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)).type_as(</span><br><span class="line">                    attn_score)</span><br><span class="line">            <span class="keyword">elif</span> attn_mask.dim() == <span class="number">3</span>:</span><br><span class="line">                attn_score = attn_mask.<span class="built_in">float</span>().masked_fill(attn_mask[:, :, :, <span class="literal">None</span>], -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)).type_as(attn_score)</span><br><span class="line"></span><br><span class="line">        attn_p = F.softmax(attn_score, -<span class="number">1</span>)</span><br><span class="line">        attn_p = self.dropatt(attn_p)</span><br><span class="line"></span><br><span class="line">        attn_vec = torch.einsum(<span class="string">&#x27;ijbn,jbnd-&gt;ibnd&#x27;</span>, (attn_p, w_head_v))</span><br><span class="line"></span><br><span class="line">        attn_vec = attn_vec.contiguous().view(attn_vec.size(<span class="number">0</span>), attn_vec.size(<span class="number">1</span>), self.n_head * self.d_head)</span><br><span class="line"></span><br><span class="line">        attn_out = self.o_net(attn_vec)</span><br><span class="line">        attn_out = self.drop(attn_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_ln:</span><br><span class="line">            out = w + attn_out</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out = self.layer_norm(w + attn_out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFF</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner, dropout, pre_ln=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_inner = d_inner</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.coreNet = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_inner), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(d_inner, d_model),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">        )</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line">        self.pre_ln = pre_ln</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inp</span>):</span></span><br><span class="line">        core_out = self.coreNet(inp)</span><br><span class="line">        <span class="keyword">if</span> self.pre_ln:</span><br><span class="line">            out = core_out + inp</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out = self.layer_norm(inp + core_out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionEmbedding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; R_&#123;i-j&#125; in Att_rel in xl &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_emb</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionEmbedding, self).__init__()</span><br><span class="line">        self.d_emb = d_emb</span><br><span class="line">        inv_freq = <span class="number">1</span> / (<span class="number">10000</span> ** (torch.arange(<span class="number">.0</span>, d_emb, <span class="number">2.0</span>) / d_emb))</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;inv_freq&#x27;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pos_seq, bsz=<span class="literal">None</span></span>):</span></span><br><span class="line">        sinuisoid_inp = torch.ger(pos_seq, self.inv_freq)  <span class="comment"># outer product</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bsz <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> pos_seq[:, <span class="literal">None</span>, :].expand(-<span class="number">1</span>, bsz, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> pos_seq[:, <span class="literal">None</span>, :]</span><br></pre></td></tr></table></figure>
<h4 id="Comparison-with-Shaw-et-al-2018"><a href="#Comparison-with-Shaw-et-al-2018" class="headerlink" title="Comparison with Shaw et. al(2018)"></a>Comparison with Shaw et. al(2018)</h4><p>Relative positional representation (RPR) (Shaw et. al, 2018) merely leveraged relative postional embedding, throwing away the sinusoid hard encodings. The RPR term <script type="math/tex">\color{red}{a_{ij}^K}</script> introduces the trainable parameters. See my attention blog <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Attention in a nutshell!](/notes/2019/01/22/NLP/Attention-in-a-nutshell)
">[6]</span></a></sup> for more details.</p>
<ul>
<li>The terms in the numerator correspond to terms (a) and (b) in relative PE in Transformer-XL. It is obvious that RPR shows a lack of the (c) and (d) terms.</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
e_{ij} &= \frac{x_i W^Q (x_j W^K \color{red}{+a_{ij}^K})^T}{\sqrt{d_z}} \\
&= \frac{ \overbrace{x_i W^Q (x_jW^K)^T}^\text{(a)} + \overbrace{\color{green}{\pmb{x_iW^Q(a_{ij}^Q)^T}}}^\text{(b)}}{\sqrt{d_z}} \\
\alpha_{ij} &= \frac{\exp e_{ij}}{\sum_{k=1}^n \exp e_{ik}} \\
z_i &= \sum_{j=1}^n \alpha_{ij}(x_jW^V \color{red}{+ a_{ij}^V} )\\
\end{align}</script><h1 id="R-Transformer"><a href="#R-Transformer" class="headerlink" title="R-Transformer"></a>R-Transformer</h1><ul>
<li>Argument: multi-head attention only learn the global dependencies, but it <span class="label danger">ignores the inherent local structures</span>.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Z., Ma, Y., Liu, Z., & Tang, J. (2019). [R-Transformer: Recurrent Neural Network Enhanced Transformer](https://arxiv.org/pdf/1907.05572). arXiv preprint arXiv:1907.05572.
">[4]</span></a></sup></li>
</ul>
<h2 id="LocalRNN"><a href="#LocalRNN" class="headerlink" title="LocalRNN"></a>LocalRNN</h2><p><strong>R-Transformer</strong><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Z., Ma, Y., Liu, Z., & Tang, J. (2019). [R-Transformer: Recurrent Neural Network Enhanced Transformer](https://arxiv.org/pdf/1907.05572). arXiv preprint arXiv:1907.05572.
">[4]</span></a></sup> employs <em>LocalRNN</em> to model local structures, only focusing on local short-term dependencies with a local short sequence of length $M$: <script type="math/tex">x_{t-M-1}, x_{t-M-2}, \cdots, x_t</script>. The last hidden state <script type="math/tex">h_t</script> is the representation of the local short sequences of a fixed length $M$.</p>
<ul>
<li>LocalRNNs pad $(M-1)$ positions before the start of a sequence.</li>
<li>R-Transformers <strong>do not use any position embeddings</strong>.</li>
<li>Here, the LocalRNN resembles the 1D ConvNets but the op for each window is not convolution. However, the conv op completely ignores the sequential information of positions within the local window.</li>
</ul>
<p><img data-src="/notes/images/R-Transformer.png" alt="upload successful"></p>
<center>Image source:<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Z., Ma, Y., Liu, Z., & Tang, J. (2019). [R-Transformer: Recurrent Neural Network Enhanced Transformer](https://arxiv.org/pdf/1907.05572). arXiv preprint arXiv:1907.05572.
">[4]</span></a></sup></center>

<p>Given sequence of length $m$: <script type="math/tex">x_1, x_2, x_3, \cdots, x_m</script> and window size $k=4$, localRNN encodes segmented short sub-sequence as:</p>
<script type="math/tex; mode=display">
\begin{align}
&\text{the 1-st LocalRNN:} & \quad \text{<pad>}, \text{<pad>}, \text{<pad>}, x_1 \\
&\text{the 2-nd LocalRNN:} & \quad \text{<pad>}, \text{<pad>}, x_1, x_2  \\
&\text{the 3-rd LocalRNN:} & \quad  \text{<pad>}, x_1, x_2, x_3 \\
&\text{the 4-th LocalRNN:} & \quad x_1, x_2, x_3, x_4  \\
&\text{the 5-th LocalRNN:} & \quad x_2, x_3, x_4, x_5 \\
&\text{the 6-th LocalRNN:} & \quad x_3, x_4, x_5, x_6  \\
& \cdots & \cdots\\
&\text{the m-th LocalRNN:} & \quad \underbrace{x_{m-3}, x_{m-2}, x_{m-1}, x_m}_\text{window size k=4}
\end{align}</script><p><img data-src="/notes/images/R-transformer-my-draw.png" alt="upload successful"></p>
<p>When doing implementation,</p>
<ol>
<li>first pad the sequence with embeddings of all 0s on the left hand side (kernel size-1) positions;</li>
<li>then segment the subsequence of window size $k$, with one position shift right per time step. (See above digram.)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocalRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; R transformer&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, output_size, window_size, rnn_type=<span class="string">&#x27;GRU&#x27;</span>, MAX_LENGTH=<span class="number">10000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LocalRNN, self).__init__()</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">&#x27;GRU&#x27;</span>:</span><br><span class="line">            <span class="comment"># set `batch_first`=True so that the input and output dim are both (nBatch, seq_len, d_model)</span></span><br><span class="line">            self.rnn = nn.GRU(output_size, output_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> rnn_type == <span class="string">&#x27;LSTM&#x27;</span>:</span><br><span class="line">            self.rnn = nn.LSTM(output_size, output_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(output_size, output_size, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># generate segments according to window_size.</span></span><br><span class="line">        <span class="comment"># -&gt; e.g. window size = 4, generate [1,2,3,4,</span></span><br><span class="line">        <span class="comment">#                                      2,3,4,5,</span></span><br><span class="line">        <span class="comment">#                                        3,4,5,6,</span></span><br><span class="line">        <span class="comment">#                                          4,5,6,7,</span></span><br><span class="line">        <span class="comment">#                                              ...</span></span><br><span class="line">        <span class="comment">#                                                  MAX_LEN - 1 -k ,... , MAX_LEN-2, MAX_LEN-1]</span></span><br><span class="line">        idx = [i <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(window_size - <span class="number">1</span>, MAX_LENGTH) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(j - (window_size - <span class="number">1</span>), j + <span class="number">1</span>)]</span><br><span class="line">        self.idx = torch.LongTensor(idx)</span><br><span class="line">        <span class="comment"># padding (k-1) before the beginning of the sequence</span></span><br><span class="line">        self.zeros_pad = torch.zeros((window_size - <span class="number">1</span>, input_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; regard window size dim as batch dim&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span>, <span class="string">&#x27;3 dimensions of input expected!&#x27;</span></span><br><span class="line">        nbatches, seq_len, d_model = x.size()</span><br><span class="line"></span><br><span class="line">        x = self._gather_seg_sequence(x)</span><br><span class="line">        output, _ = self.rnn(x)</span><br><span class="line">        h_last_per_batch = output[:, -<span class="number">1</span>, :]</span><br><span class="line">        <span class="keyword">return</span> h_last_per_batch.view(nbatches, seq_len, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gather_seg_sequence</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        nbatch, seq_len, d_model = x.size()</span><br><span class="line">        <span class="comment"># use `repeat` to pad one batch -&gt; (nbatch, k01, input_size)</span></span><br><span class="line">        zeros = self.zeros_pad.repeat(bsz, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># concat padded zeros and the sequence along the sequence dim</span></span><br><span class="line">        x = torch.cat((zeros, x), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># gather the corresponding embeddings along the sequence dim (1)</span></span><br><span class="line">        idx = self.idx[:self.window_size * seq_len]  <span class="comment">#</span></span><br><span class="line">        x_ = torch.index_select(<span class="built_in">input</span>=x, dim=<span class="number">1</span>, index=idx)</span><br><span class="line">        <span class="comment"># reshape -&gt; (bsz * seq_len, window_size, d_model)</span></span><br><span class="line">        x_ = x_.reshape(nbatch * seq_len, self.window_size, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x_</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    a residual connection followed by a layer norm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Apply residual connection to any sublayer with the same size&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocalRNNLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout=<span class="number">.0</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LocalRNNLayer, self).__init__()</span><br><span class="line">        self.local_rnn = LocalRNN(size, size, window_size=<span class="number">5</span>)</span><br><span class="line">        self.sublayer = SublayerConnection(size, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer(x, self.local_rnn)</span><br></pre></td></tr></table></figure>
<p>For $i$-th layer, (<script type="math/tex">i \in {1,2,\cdots,N}</script>)</p>
<script type="math/tex; mode=display">\begin{align}
h_1^i, h_2^i, \cdots, h_T^i &= \text{LocalRNN}(x_1^i, x_2^i, \cdots, x_T^i) \\
\hat{h}_1^i, \hat{h}_2^i, \cdots, \hat{h}_T^i &= \text{LayerNorm}(h_1^i + x_1^i, h_2^i+x_2^i, \cdots, h_T^i+x_T^i) \\
o_1^i,o_2^i,\cdots,o_T^{i} &= \text{Transformer-Layer}(\hat{h}_1^i, \hat{h}_2^i, \cdots, \hat{h}_T^i)
\end{align}</script><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Al-Rfou, R., Choe, D., Constant, N., Guo, M., &amp; Jones, L. (2019, July). <a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/4182/4060">Character-level language modeling with deeper self-attention</a>. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 3159-3166).<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., &amp; Salakhutdinov, R. (2019). <a href="https://arxiv.org/pdf/1901.02860">Transformer-xl: Attentive language models beyond a fixed-length context</a>. arXiv preprint arXiv:1901.02860.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, Z., Ma, Y., Liu, Z., &amp; Tang, J. (2019). <a href="https://arxiv.org/pdf/1907.05572">R-Transformer: Recurrent Neural Network Enhanced Transformer</a>. arXiv preprint arXiv:1907.05572.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Shaw, P., Uszkoreit, J., &amp; Vaswani, A. (2018). <a href="https://arxiv.org/pdf/1803.02155">Self-attention with relative position representations</a>. arXiv preprint arXiv:1803.02155.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="/notes/2019/01/22/NLP/Attention-in-a-nutshell">Attention in a nutshell!</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/tensorflow/tensor2tensor/blob/9e0a894034d8090892c238df1bd9bd3180c2b9a3/tensor2tensor/layers/common_attention.py#L1556-L1587">Tensor2Tensor tensorflow code</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.peterbloem.nl/blog/transformers">TRANSFORMERS FROM SCRATCH</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Go Deeper in Convolutions: a Peek </title>
    <url>/notes/2019/08/28/NN/go-deeper-in-Convolutions-a-Peek/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><img data-src="/notes/images/convolutions.gif" alt="upload successful"></p>
<p>The main aim of conv op is to extract useful features for downstream tasks. And different filters could intuitionally extract different aspect of features via backprop during training. Afterward, all the extracted features are combined to make decisions.</p>
<span id="more"></span>
<h1 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a>Convolutional Networks</h1><p>ConvNets including local receptive fields, weight sharing, and pooling, leading to:</p>
<ol>
<li>Modeling the spatial structure</li>
<li>Translation invariance</li>
<li>Local feature detection</li>
</ol>
<p>Standard discrete convolution:</p>
<script type="math/tex; mode=display">(f*k)(\mathbf{p}) = \sum_{s+t=p}F(s)k(t)</script><div class="note warning">
            <ul><li>channels == feature maps.</li><li>channels are used to describe the structure of a layer, while the kernel is used to describe the structure of a filter.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[A Comprehensive Introduction to Different Types of Convolutions in Deep Learning](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)">[5]</span></a></sup></li><li>A “kernel” refers to a 2D array of weights, while a “filter” refers to a 3D structure of multiple kernels stacked together. In 2D filters, filter==kernel; in 3D filters, a filter -&gt; a collection of stacked kernels.</li><li>Each filter provides one output channel.</li></ul>
          </div>
<h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><ul>
<li>Weights (i.e. the conv kernel) are shared across all hidden states</li>
<li>Spatial correspondence between pixels and hidden units (“2D matrix of hidden units”=”feature map”)</li>
<li><code>Translation invariance</code>: extract the same features irrespective of where an image patch is located in the input.</li>
</ul>
<h3 id="convolution-v-s-cross-correlation"><a href="#convolution-v-s-cross-correlation" class="headerlink" title="convolution v.s. cross-correlation"></a>convolution v.s. cross-correlation</h3><ul>
<li>Convolution in signal processing:<script type="math/tex; mode=display">(f * g)(t) = \int_{-\infty}^\infty f(\tau) g(t-\tau)d\tau</script>the filter $g$ is reversed and then slides along the axis.</li>
<li>Cross-correlation is sliding non-reversed filter $g$ through the function $f$.</li>
</ul>
<p><img data-src='/notes/images/convolution-vs-cross-correlation.png' width='60%'/></p>
<h2 id="2D-convolution"><a href="#2D-convolution" class="headerlink" title="2D convolution"></a>2D convolution</h2><ul>
<li>Input: 4-dim tensor (N,$C_\text{in}$,H,W) -&gt; (minibatch-size, num-featuremaps, x, y)</li>
<li>Filters: 4-dim tensor (<script type="math/tex">C_\text{out}</script>, <script type="math/tex">C_\text{in}</script>, kernel_dim1, kernel_dim2)</li>
<li>Output: 4-dim tensor (N, $C_\text{out}$, output_dim1, output_dim2)</li>
</ul>
<p>2D-convolution op sums all the element-wise convolution results along <script type="math/tex">c_\text{in}</script> axis -&gt; squeeze the <script type="math/tex">c_\text{in}</script> axis to 1 by sum op.</p>
<p><img data-src="/notes/images/Conv-2D-multi.png" alt="upload successful"></p>
<h3 id="Conv-layer"><a href="#Conv-layer" class="headerlink" title="Conv layer"></a>Conv layer</h3><p><strong>The filter depth is the same as the input layer depth</strong>.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[A Comprehensive Introduction to Different Types of Convolutions in Deep Learning](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)
">[5]</span></a></sup> We can intuitionally think multi-channel conv op as sliding a 3D filter matrix through the input layer, performing element-wise multiplication and addition, where the 3D filter moves only in 2-direction,i.e. height and width of the image.<br><img data-src="/notes/images/Conv-2D.png" width="50%"/></p>
<ul>
<li>Input size: <script type="math/tex">W_1 \times H_1 \times D_1</script></li>
<li>Requires 4 hyperparams:<ol>
<li># of filters $K$</li>
<li>filter size $F$</li>
<li>stride $S$</li>
<li>padding size $P$</li>
</ol>
</li>
<li>Output size  <script type="math/tex">W_2 \times H_2 \times D_2</script><ol>
<li>$W_2 = \text{lower_bound}\frac{W_1 + 2P -F}{S} + 1$</li>
<li>$H_2 = \text{lower_bound}\frac{H_1 + 2P-F}{S} + 1$</li>
<li>$D_2 = K$</li>
</ol>
</li>
<li># of parameters <script type="math/tex; mode=display">
\underbrace{F \cdot F \cdot D_1 \cdot K}_\text{ # of weight params} + \underbrace{K}_\text{ # of biases}</script></li>
</ul>
<h3 id="Time-complexity-of-conv-op"><a href="#Time-complexity-of-conv-op" class="headerlink" title="Time complexity of conv op"></a>Time complexity of conv op</h3><p>The time complexity of <strong>single Conv layer</strong>:</p>
<script type="math/tex; mode=display">O( \underbrace{ \text{(out-feature-map-size)}^2 }_\text{the area of output feature map} \cdot \underbrace{\text{(Kernel-size)}^2}_\text{area of single kernel} \cdot C_\text{in} \cdot C_\text{out} )</script><ul>
<li>where <script type="math/tex">C_\text{in}</script> and <script type="math/tex">C_\text{out}</script> denote the # of output channel.</li>
</ul>
<p>Total time complexity of <strong>all conv layers</strong> (sum over all the conv layers):</p>
<script type="math/tex; mode=display">O \big(\sum_{l=1}^D \underbrace{ \text{(out-feature-map-size)}_l^2 }_\text{the area of output feature map} \cdot \underbrace{\text{(Kernel-size)}_l^2}_\text{area of single kernel}  \cdot C_{l-1} \cdot C_l \big)</script><p>where </p>
<ul>
<li>$D$ is the depth of conv layers</li>
<li><script type="math/tex">C_{l-1}</script> means the input channel # of current layer, i.e. the output channel # of previous layer. </li>
</ul>
<h3 id="Space-complexity-of-conv-op"><a href="#Space-complexity-of-conv-op" class="headerlink" title="Space complexity of conv op"></a>Space complexity of conv op</h3><p>The space complexity includes the # of weights and the size of output feature map.</p>
<script type="math/tex; mode=display">O(\sum_{l=1}^D \text{(kernel-size)}^2_l \cdot C_{l-1} \cdot C_l + \sum_{l-1}^D \text{(feature-map-size)}^2 \cdot C_l )</script><h3 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h3><ul>
<li>Input size <script type="math/tex">W_1 \times H_1 \times D_1</script></li>
<li>Requires 2 hyperparams:<ol>
<li>pooling size $F$</li>
<li>stride size $S$</li>
</ol>
</li>
<li>Output size <script type="math/tex">W_2 \times H_2 \times D_2</script>:<ol>
<li>$W_2 = (W_1-F)/S + 1$</li>
<li>$H_2 = (H_1-F)/S +1$</li>
<li>$D_2 = D_1$</li>
</ol>
</li>
<li>Zero parameters since it computes a fixed function</li>
<li>Discarding pooling layers has shown to be imortant in training good generative models, e.g, VAEs and GANs <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)
">[1]</span></a></sup></li>
</ul>
<h3 id="Conv-implementation"><a href="#Conv-implementation" class="headerlink" title="Conv implementation"></a>Conv implementation</h3><p><code>im2col</code></p>
<p><img data-src='/notes/images/conv-implementation.png' width='80%'/></p>
<h2 id="1D-convolution"><a href="#1D-convolution" class="headerlink" title="1D convolution"></a>1D convolution</h2><p>Convolve along one axis (i.e. the sequential direction in NLP), with the kernel width the same as the input length, which is commonly used in TextCNNs. The kernel width <script type="math/tex">L_\text{in}</script> is the same as the input width, i.e. the size of embedding.</p>
<p>The shape of dilated conv2d:</p>
<ul>
<li>Input <script type="math/tex">(N, C_\text{in}, L_\text{in})</script></li>
<li>Output: <script type="math/tex">(N, C_\text{out}, L_\text{out})</script></li>
</ul>
<script type="math/tex; mode=display">L_\text{out}=\lfloor \frac{L_\text{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel_size} -1) -1 }{\text{stride}} + 1 \rfloor</script><p><img data-src="/notes/images/conv-1d.png" width="30%"/></p>
<h2 id="1x1-convolution"><a href="#1x1-convolution" class="headerlink" title="1x1 convolution"></a>1x1 convolution</h2><p>$1 \times 1$ convolution is proposed in Network in Network (NIN)<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lin, M., Chen, Q., & Yan, S. (2013). [Network in network](https://arxiv.org/pdf/1312.4400.pdf). arXiv preprint arXiv:1312.4400.
">[6]</span></a></sup>. It can be used for:</p>
<ul>
<li>dimensionality reduction for efficient computations</li>
<li>efficient low dimensional embedding, or feature pooling</li>
<li>applying non-linearity again after convolution.</li>
</ul>
<p>$1 \times 1$ convolution can be used to reduce the dimension depth-wise. </p>
<p><img data-src='/notes/images/1x1-conv.png' width='60%'/></p>
<h2 id="Transposed-convolution-Deconvolution"><a href="#Transposed-convolution-Deconvolution" class="headerlink" title="Transposed convolution(Deconvolution)"></a>Transposed convolution(Deconvolution)</h2><p>Transposed convolution (a.k.a deconvolution, fractionally strided convolution) is used to perform <strong>up-sampling</strong>, i.e. doing transformations in the opposite direction of a normal convolution.<br>It can be used in:</p>
<ul>
<li>generating high-resolution images</li>
<li>mapping a low dimensional feature map to high dimensional space</li>
</ul>
<p><img data-src='/notes/images/transposed-conv.gif' width='40%'/></p>
<ul>
<li>Input dim <script type="math/tex">(N, C_\text{in}, H_\text{in}, W_\text{in})</script></li>
<li>output dim <script type="math/tex">(N, C_\text{out}, H_\text{out}, W_\text{out})</script><br>where the dim:<script type="math/tex; mode=display">H_\text{out} = \text{stride}[0] \times (H_\text{in}-1) - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel[0] - 1}) + \text{out_pad[0]} + 1</script><script type="math/tex; mode=display">W_\text{out} = \text{stride}[1] \times (W_\text{in}-1) - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel[1] - 1}) + \text{out_pad[1]} + 1</script></li>
</ul>
<p><img data-src='/notes/images/transposed conv-implementation.png' width='80%'/></p>
<center>Transposed conv implementation</center>



<h3 id="Checkerboard-artifacts"><a href="#Checkerboard-artifacts" class="headerlink" title="Checkerboard artifacts"></a>Checkerboard artifacts</h3><p>Checkerboard artifacts result from “uneven overlap” pf transposed convolution<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)
">[8]</span></a></sup>. As shown in the following image, images on the bottom are the result of transposed convolution on the top row images. With transposed conv, a layer with a small size can be mapped to a layer with the larger size.</p>
<p>“The transposed convolution has uneven overlap when the filter size is not divisible by the stride. This “uneven overlap” puts more of the paint in some places than others, thus creates the checkerboard effects.”<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)
">[7]</span></a></sup></p>
<p><img data-src="/notes/images/deconv-checkerboard-artifacts.png" alt="upload successful"></p>
<center> Image source: <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)
">[8]</span></a></sup></center>

<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><ol>
<li>Assure that the filter size can be divided by the stride, to avoid overlap issue</li>
<li>Use transposed Conv with stride = 1</li>
</ol>
<p>A better up-sampling approach:</p>
<ul>
<li>resize the image first (using nearest-neighbor interpolation or bilinear interpolation) and then do a conv layer. -&gt; avoid checkerboard effects</li>
</ul>
<h2 id="Dilated-convolution-Atrous-conv"><a href="#Dilated-convolution-Atrous-conv" class="headerlink" title="Dilated convolution (Atrous conv)"></a>Dilated convolution (Atrous conv)</h2><p>Dilated convolutions inflate the kernel by inserting spaces between the kernel elements, with usually l-1 spaces inserted between kernel elements.<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2014). [Semantic image segmentation with deep convolutional nets and fully connected crfs](https://arxiv.org/abs/1412.7062). arXiv preprint arXiv:1412.7062.
">[9]</span></a></sup></p>
<p>With dilated conv, we can observe a large receptive field without increasing the kernel size, which is effective when stacking multiple dilated convolutions one after another.</p>
<script type="math/tex; mode=display">(f*_l k)(\mathbf{p}) = \sum_{\mathbf{s}+l\mathbf{t}=\mathbf{p}}F(\mathbf{s})k(\mathbf{t})</script><p><img data-src='/notes/images/dilated-conv.gif' width='50%'/></p>
<p>In PyTorch, <code>dilation</code> param means the space between kernel elements, with default 1.<br>Dilation =1 means no dilation.</p>
<p>The shape of dilated conv2d:</p>
<ul>
<li>Input <script type="math/tex">(N, C_\text{in}, H_\text{in}, W_\text{in})</script></li>
<li>Output: <script type="math/tex">(N, C_\text{out}, H_\text{out}, W_\text{out})</script></li>
</ul>
<script type="math/tex; mode=display">H_\text{out}=\lfloor \frac{H_\text{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel_size} -1) -1 }{\text{stride}} + 1 \rfloor</script><script type="math/tex; mode=display">W_\text{out}=\lfloor \frac{W_\text{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel_size} -1) -1 }{\text{stride}} + 1 \rfloor</script><h2 id="Separable-convolutions"><a href="#Separable-convolutions" class="headerlink" title="Separable convolutions"></a>Separable convolutions</h2><h3 id="Spatially-separable-convolutions"><a href="#Spatially-separable-convolutions" class="headerlink" title="Spatially separable convolutions"></a>Spatially separable convolutions</h3><p>Spatially separable conv divides kernels into two vectors, and do convolution one by one.</p>
<ul>
<li>drawbacks: not all filters can be divided into two smaller kernels. The training result may be sub-optimal.</li>
</ul>
<p><img data-src='/notes/images/spatially-sparable-conv-1-channel.png' width='50%'/></p>
<h3 id="Depthwise-separable-convolutions"><a href="#Depthwise-separable-convolutions" class="headerlink" title="Depthwise separable convolutions"></a>Depthwise separable convolutions</h3><p>The depth-wise separable convolutions consist of two steps: depthwise convolutions and 1x1 convolutions. It is commonly used in MobileNet and Xception.</p>
<ul>
<li>Pros: efficient!</li>
</ul>
<p><img data-src='/notes/images/depth-wise-separable-conv.png' width='80%'/></p>
<h2 id="Grouped-convolutions"><a href="#Grouped-convolutions" class="headerlink" title="Grouped convolutions"></a>Grouped convolutions</h2><p>Grouped convolutions are proposed by AlexNet paper. Filters are separated into different groups, each group is responsible for a conventional 2D convolutions with a certain depth.<br><img data-src='/notes/images/Grouped-conv.png' width='80%'/></p>
<h2 id="Variant-convolutions"><a href="#Variant-convolutions" class="headerlink" title="Variant convolutions"></a>Variant convolutions</h2><ul>
<li>Flattened Conv</li>
<li>Shuffled Grouped Conv (ShuffleNet)</li>
<li>Pointwise grouped Conv (ShuffleNet)</li>
</ul>
<h1 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h1><p>Inception module is designed to increase the depth and width of the network while keeping the computational cost constant.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). [Going deeper with convolutions](https://arxiv.org/abs/1409.4842). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).
">[4]</span></a></sup></p>
<p><img data-src='/notes/images/Inception-1.png' width='70%'/></p>
<p>Inception modules apply 1x1 convolution to reduce dimensions before the expensive 3x3 and 5x5 convolutions. 1x1 can also include the use of rectified linear activation.</p>
<p>Pros:<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). [Going deeper with convolutions](https://arxiv.org/abs/1409.4842). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).
">[4]</span></a></sup></p>
<ul>
<li>increase the # of units at each stage significantly without an uncontrolled blow-up in computational complexity</li>
<li>Align with the intuition that, aggregate processed visual information at different scales could be useful, so that following stage can abstract features from different scales simultaneously.</li>
</ul>
<p><img data-src='/notes/images/Inception-2.png' width='70%'/></p>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>ResNet<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="He, K., Zhang, X., Ren, S., & Sun, J. (2016). [Deep residual learning for image recognition](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
">[11]</span></a></sup> was proposed to explicitly fit a residual mapping. Instead of directly fitting the  desired mapping as $\mathcal{H}(\pmb{x})$, use stacked non-linear layers to fit the residual mapping <script type="math/tex">\mathcal{F}(\pmb{x}):= \mathcal{H}(\pmb{x}) - \pmb{x}</script>. The original mapping is thus recast into $\mathcal{F}(\pmb{x})+\pmb{x}$.</p>
<ul>
<li>Intuitionally, to the extreme, if an identity mapping were optimal, pushing the residual to zero is much easier than to fit an identity mapping by a stack of non-linear layers.<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="He, K., Zhang, X., Ren, S., & Sun, J. (2016). [Deep residual learning for image recognition](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
">[11]</span></a></sup></li>
<li>“+” shortcut connections introduce nor extra parameter neither computational complexity.</li>
</ul>
<p><img data-src='/notes/images/ResNet.png' width='50%'/></p>
<center> Image source:<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="He, K., Zhang, X., Ren, S., & Sun, J. (2016). [Deep residual learning for image recognition](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
">[11]</span></a></sup></center>

<h2 id="ResNet-vs-Highway-Nets"><a href="#ResNet-vs-Highway-Nets" class="headerlink" title="ResNet vs Highway Nets"></a>ResNet vs Highway Nets</h2><ul>
<li>“Highway nets” can be seen as a shortcut connection with gated functions, whose gates are data-dependent and have parameters to be trained.</li>
<li>To the extreme, if the gate in Highway nets is closed (~0), the highway net is non-residual at all.</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/31575074">blog (in Chinese): complexity analysis of CNNs (in Chinese)</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">He, K., &amp; Sun, J. (2015). <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf">Convolutional neural networks at constrained time cost</a>. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 5353-5360).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... &amp; Rabinovich, A. (2015). <a href="https://arxiv.org/abs/1409.4842">Going deeper with convolutions</a>. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">A Comprehensive Introduction to Different Types of Convolutions in Deep Learning</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lin, M., Chen, Q., &amp; Yan, S. (2013). <a href="https://arxiv.org/pdf/1312.4400.pdf">Network in network</a>. arXiv preprint arXiv:1312.4400.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1603.07285">A guide to convolution arithmetic for deep learning</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://distill.pub/2016/deconv-checkerboard/">Deconvolution and Checkerboard Artifacts</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., &amp; Yuille, A. L. (2014). <a href="https://arxiv.org/abs/1412.7062">Semantic image segmentation with deep convolutional nets and fully connected crfs</a>. arXiv preprint arXiv:1412.7062.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0">Up-sampling with Transposed Convolution</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep residual learning for image recognition</a>. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Dumoulin, V., &amp; Visin, F. (2016). <a href="https://arxiv.org/pdf/1603.07285">A guide to convolution arithmetic for deep learning</a>. arXiv preprint arXiv:1603.07285.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/48501100">Transposed CNN (in Chinese)</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NN</category>
        <category>CNNs</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>CNNs</tag>
      </tags>
  </entry>
  <entry>
    <title>Scaling Up Large Language Models: A Summary</title>
    <url>/notes/2021/10/09/PTMs/Scaling-Up-LLMs/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A summary of large language models (LLMs) on a large scale (beyond 10B).<br><span id="more"></span></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>As shown in following table, I summarize the most popular LLMs at a large scale. It is clear that the size of LLMs has become larger and larger in recent years, ranging from 2.6 billion to even 175 billion parameters. Although the training methods are different among these models, they all use Transformers as the standard backbone in LLMs due to the nature of efficient parallel computing and training. Since training large-scale models needs massive unsupervised data corpus, research on scaling up PTMs focuses on high-resource languages such as English and Chinese.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">#Params</th>
<th style="text-align:center">#Training Tokens</th>
<th style="text-align:center">Masked LM</th>
<th style="text-align:center">Causal LM</th>
<th style="text-align:center">Prefix LM</th>
<th style="text-align:center">Seq2Seq LM</th>
<th style="text-align:center">Pre-Training Data</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">T5</td>
<td style="text-align:center">11B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">C4 Corpus (~750GB)</td>
</tr>
<tr>
<td style="text-align:center">mT5</td>
<td style="text-align:center">13B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">mC4 Corpus (6.3T tokens)</td>
</tr>
<tr>
<td style="text-align:center">Switch Transformers</td>
<td style="text-align:center"><strong>1751B</strong></td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">C4 Corpus (~750GB)</td>
</tr>
<tr>
<td style="text-align:center">CPM-2</td>
<td style="text-align:center">11B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">WuDao Corpus (2.3TB Chinese + 300GB English)</td>
</tr>
<tr>
<td style="text-align:center">CPM-2-MoE</td>
<td style="text-align:center">198B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">WuDao Corpus (2.3TB Chinese + 300GB English)</td>
</tr>
<tr>
<td style="text-align:center">Turing-NLG</td>
<td style="text-align:center">17B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">English data</td>
</tr>
<tr>
<td style="text-align:center">GPT-3</td>
<td style="text-align:center">175B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">cleaned CommonCrawl, WebText</td>
</tr>
<tr>
<td style="text-align:center">CPM</td>
<td style="text-align:center">2.6B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Chinese corpus (100GB)</td>
</tr>
<tr>
<td style="text-align:center">HyperCLOVA</td>
<td style="text-align:center">204B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Korean data</td>
</tr>
<tr>
<td style="text-align:center">PanGu-$\alpha$</td>
<td style="text-align:center">200B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Chinese data (1.1TB, 250B tokens)</td>
</tr>
<tr>
<td style="text-align:center">DeBERTa1.5B</td>
<td style="text-align:center">1.5B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">English corpus</td>
</tr>
<tr>
<td style="text-align:center">ERNIE 3.0</td>
<td style="text-align:center">10B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Chinese data (4TB); English</td>
</tr>
<tr>
<td style="text-align:center">Yuan 1.0</td>
<td style="text-align:center">245B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Chinese data (5TB)</td>
</tr>
<tr>
<td style="text-align:center">Megatron-Turing NLG</td>
<td style="text-align:center">530B</td>
<td style="text-align:center">270B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">The Pile, CommonCrawl, RealNews, CC-Stories</td>
</tr>
<tr>
<td style="text-align:center">OPT</td>
<td style="text-align:center">175B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">BookCorpus, Stories, CCNews (RoBERTa) The Pile PuhsShift.io Reddit.</td>
</tr>
<tr>
<td style="text-align:center">Gopher</td>
<td style="text-align:center">280B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">MassiveText（10.5TB data) including webpages, books, news article, code.</td>
</tr>
<tr>
<td style="text-align:center">Jurassic-1</td>
<td style="text-align:center">178B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">GPT-3 data</td>
</tr>
<tr>
<td style="text-align:center">Chinchilla</td>
<td style="text-align:center">70B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Same as Gopher.</td>
</tr>
<tr>
<td style="text-align:center">Sparrow</td>
<td style="text-align:center">70B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">LaMDA</td>
<td style="text-align:center">137B</td>
<td style="text-align:center">168B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">2.97B documents, 1.12B dialogs, and 13.39B dialog utterances, for a total of 1.56T words</td>
</tr>
<tr>
<td style="text-align:center">PaLM</td>
<td style="text-align:center">540B</td>
<td style="text-align:center">780B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">780B token, dataset from LamDA, GLaM, and code</td>
</tr>
<tr>
<td style="text-align:center">BLOOM</td>
<td style="text-align:center">176B</td>
<td style="text-align:center">366B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">ROOTS dataset of 498 huggingface datasets. 46 natural languages，13 programming languages.</td>
</tr>
<tr>
<td style="text-align:center">GLM-130B</td>
<td style="text-align:center">130B</td>
<td style="text-align:center">400B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">English: 1.2T the Pile Chinese: 1T Chinese WuDao corpora; 250GB crawled from online forum, encyclopedia, QA.</td>
</tr>
<tr>
<td style="text-align:center">ChatGLM-6B</td>
<td style="text-align:center">6B</td>
<td style="text-align:center">1T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Chinese-English bilingual data.</td>
</tr>
<tr>
<td style="text-align:center">LLaMA</td>
<td style="text-align:center">65B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">1.4T token. CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, StackExchange.</td>
</tr>
<tr>
<td style="text-align:center">Alpaca</td>
<td style="text-align:center">7B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">52K instruction-following data</td>
</tr>
<tr>
<td style="text-align:center">Vicuna</td>
<td style="text-align:center">1.3B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">finetune 70K ChatGPT data</td>
</tr>
<tr>
<td style="text-align:center">ChatRWKV (100% RNN)</td>
<td style="text-align:center">14B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">Galactica</td>
<td style="text-align:center">120B</td>
<td style="text-align:center">450B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">106 billion tokens from papers, reference material, encyclopedias and other scientific sources</td>
</tr>
<tr>
<td style="text-align:center">Codex</td>
<td style="text-align:center">12B</td>
<td style="text-align:center">100B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">159GB Python code.</td>
</tr>
<tr>
<td style="text-align:center">AlphaCode</td>
<td style="text-align:center">41B/9B</td>
<td style="text-align:center">967B/ 1250B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">715GB code from GitHub.</td>
</tr>
<tr>
<td style="text-align:center">Flamingo</td>
<td style="text-align:center">80B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">M3W, 43M webpaes Image/video-text pairs: ALIGN, LTIP/VTP</td>
</tr>
<tr>
<td style="text-align:center">BEiT-3</td>
<td style="text-align:center">1.9B</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">21M image-text pairs, 14M images, 160GB documents</td>
</tr>
<tr>
<td style="text-align:center">Kosmos-1</td>
<td style="text-align:center">1.6B</td>
<td style="text-align:center">360B</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">1. text: The Pile, Common Crawl exclude GitHub/arXiv/Stack Exchange/PubMed Central, + CC-Stories, RealNews  2. Image-caption pairs LAION-2B/400M/COYO-700M/Comceptual Captions  3. Interleaved image-text data from Common Crawl</td>
</tr>
<tr>
<td style="text-align:center">GPT-4</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Open-sourced data and third-party data</td>
</tr>
<tr>
<td style="text-align:center">Llama 1</td>
<td style="text-align:center">65B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">English CC (67\%), C4 (15\%), GitHub (4.5\%), Wiki (4.5\%), Gutenberg and Books3 (4.5\%), ArXiv (2.5\%), Stack Exchange (2\%)</td>
</tr>
<tr>
<td style="text-align:center">Llama 2</td>
<td style="text-align:center">70B</td>
<td style="text-align:center">1.8T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">A new mix of data from publicly available sources</td>
</tr>
<tr>
<td style="text-align:center">Llama 3</td>
<td style="text-align:center">405B</td>
<td style="text-align:center">15.6T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Gemma</td>
<td style="text-align:center">7B</td>
<td style="text-align:center">6T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Primarily-English data from web documents, mathematics, and code.</td>
</tr>
<tr>
<td style="text-align:center">Gemma 2</td>
<td style="text-align:center">27B</td>
<td style="text-align:center">13T</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">Web documents, code, and science articles.</td>
</tr>
<tr>
<td style="text-align:center">Gemini</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">✘</td>
<td style="text-align:center">-</td>
</tr>
</tbody>
</table>
</div>
<p>According to different design of pre-training architectures, large-scale LLMs can be generally classified into three classes: <em>encoder only</em>, <em>decoder only</em>, and <em>encoder-decoder</em>. The majority of large LLMs leverage the <em>decoder only</em> and <em>encoder-decoder</em> architecture whereas seldom large models adopt the <em>encoder only</em> design. This is due to that <em>encoder only</em> architectures, such as BERT and DeBERTa, employ stacked Transformer encoders only to attend to bidirectional contexts in language, in which their bidirectional nature prevent them from applying to NLG tasks. In contrast, <em>decoder only</em> models are good at NLG tasks by nature and can perform NLU tasks via prompt-based methods. Examples inlucde GPT series and Turing-NLG. </p>
<ul>
<li><strong>Encoder only</strong>, i.e., pre-training on stacked Transformer encoders. Examples: DeBERTa<sub>1.5B</sub><sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="He, Pengcheng, et al. ["Deberta: Decoding-enhanced bert with disentangled attention."](https://arxiv.org/pdf/2006.03654) arXiv preprint arXiv:2006.03654 (2020).
">[10]</span></a></sup>.</li>
<li><strong>Decoder only</strong>. This line of large PTMs pre-trained Transformer decoders by applying auto-regressive masks to prevent the current token from attending to future ones. Examples: Turing-NLG <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Turing-NLG: A 17-billion-parameter language model by Microsoft](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/). February 13, 2020.
">[5]</span></a></sup>, GPT-3 <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Brown, Tom B., et al. ["Language models are few-shot learners."](https://arxiv.org/pdf/2005.14165) arXiv preprint arXiv:2005.14165 (2020).
">[6]</span></a></sup>, CPM <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Zhengyan, et al. ["CPM: A large-scale generative Chinese pre-trained language model."](https://arxiv.org/pdf/2012.00413.pdf) AI Open 2 (2021): 93-99.
">[7]</span></a></sup>, HyperCLOVA  <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Kim, Boseop, et al. ["What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers."](https://arxiv.org/pdf/2109.04650.pdf) arXiv preprint arXiv:2109.04650 (2021).
">[8]</span></a></sup>, PanGu-$\alpha$ <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zeng, Wei, et al. ["PanGu-$\alpha $: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation."](https://arxiv.org/pdf/2104.12369.pdf) arXiv preprint arXiv:2104.12369 (2021).
">[9]</span></a></sup>, Yuan 1.0 <sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wu, Shaohua, et al. ["Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning."](https://arxiv.org/pdf/2110.04725) arXiv preprint arXiv:2110.04725 (2021).
">[12]</span></a></sup>, Megatron-Turing NLG <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Paresh Kharya and Ali Alvi, [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/). Oct 11, 2021.">[13]</span></a></sup>.</li>
<li><strong>Encoder-decoder</strong>, including (1) conventional sequence-to-sequence encoder-decoder, such as T5 <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="(T5) Raffel, Colin, et al. ["Exploring the limits of transfer learning with a unified text-to-text transformer."](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) JMLR (2020).
">[1]</span></a></sup>, mT5 <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="(mT5) Xue, Linting, et al. ["mt5: A massively multilingual pre-trained text-to-text transformer."](https://arxiv.org/pdf/2010.11934) arXiv preprint arXiv:2010.11934 (2020).
">[2]</span></a></sup>, CPM-2 <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Zhengyan, et al. "CPM-2: Large-scale Cost-effective Pre-trained Language Models." arXiv preprint arXiv:2106.10715 (2021).
">[3]</span></a></sup>; and (2) Unified encoder-decoder, such as ERNIE 3.0 <sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sun, Yu, et al. ["Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation."](https://arxiv.org/pdf/2107.02137) arXiv preprint arXiv:2107.02137 (2021).
">[11]</span></a></sup>.</li>
</ul>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2021scaling-ptms-summary,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Scaling Up Large Language Models: A Summary&#125;&#125;,</span><br><span class="line">  year = &#123;2021&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2021/10/09/PTMs/Scaling-Up-LLMs/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">(T5) Raffel, Colin, et al. <a href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf">&quot;Exploring the limits of transfer learning with a unified text-to-text transformer.&quot;</a> JMLR (2020).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">(mT5) Xue, Linting, et al. <a href="https://arxiv.org/pdf/2010.11934">&quot;mt5: A massively multilingual pre-trained text-to-text transformer.&quot;</a> arXiv preprint arXiv:2010.11934 (2020).<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, Zhengyan, et al. &quot;CPM-2: Large-scale Cost-effective Pre-trained Language Models.&quot; arXiv preprint arXiv:2106.10715 (2021).<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Fedus, William, Barret Zoph, and Noam Shazeer. <a href="https://arxiv.org/pdf/2101.03961">&quot;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.&quot;</a> arXiv preprint arXiv:2101.03961 (2021).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing-NLG: A 17-billion-parameter language model by Microsoft</a>. February 13, 2020.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Brown, Tom B., et al. <a href="https://arxiv.org/pdf/2005.14165">&quot;Language models are few-shot learners.&quot;</a> arXiv preprint arXiv:2005.14165 (2020).<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, Zhengyan, et al. <a href="https://arxiv.org/pdf/2012.00413.pdf">&quot;CPM: A large-scale generative Chinese pre-trained language model.&quot;</a> AI Open 2 (2021): 93-99.<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Kim, Boseop, et al. <a href="https://arxiv.org/pdf/2109.04650.pdf">&quot;What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers.&quot;</a> arXiv preprint arXiv:2109.04650 (2021).<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zeng, Wei, et al. <a href="https://arxiv.org/pdf/2104.12369.pdf">&quot;PanGu-$\alpha $: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation.&quot;</a> arXiv preprint arXiv:2104.12369 (2021).<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">He, Pengcheng, et al. <a href="https://arxiv.org/pdf/2006.03654">&quot;Deberta: Decoding-enhanced bert with disentangled attention.&quot;</a> arXiv preprint arXiv:2006.03654 (2020).<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sun, Yu, et al. <a href="https://arxiv.org/pdf/2107.02137">&quot;Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation.&quot;</a> arXiv preprint arXiv:2107.02137 (2021).<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wu, Shaohua, et al. <a href="https://arxiv.org/pdf/2110.04725">&quot;Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning.&quot;</a> arXiv preprint arXiv:2110.04725 (2021).<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Paresh Kharya and Ali Alvi, <a href="https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model</a>. Oct 11, 2021.<a href="#fnref:13" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>LLM</category>
        <category>Scaling</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning Toolkit Installation</title>
    <url>/notes/2019/03/10/StackOverflow/DL-GPU-installation/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>This is a brief records of pitfalls and problems we met on GPU environment configuration.</p>
<span id="more"></span>
<h1 id="Useful-toolkit"><a href="#Useful-toolkit" class="headerlink" title="Useful toolkit"></a>Useful toolkit</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check the cuda version</span></span><br><span class="line">$ cat /usr/<span class="built_in">local</span>/cuda/version.txt </span><br><span class="line"></span><br><span class="line"><span class="comment"># check the cudnn version</span></span><br><span class="line">$ cat /usr/<span class="built_in">local</span>/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the GPU driver version</span></span><br><span class="line">cat /proc/driver/nvidia/version</span><br></pre></td></tr></table></figure>
<h1 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h1><h2 id="Reinstall-GPU-driver"><a href="#Reinstall-GPU-driver" class="headerlink" title="Reinstall GPU driver"></a>Reinstall GPU driver</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Remove old drivers</span></span><br><span class="line">$ sudo apt-get purge nvidia-*</span><br><span class="line"></span><br><span class="line">$ sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">$ sudo apt-get update</span><br><span class="line"><span class="comment"># If unsure about the driver version, run `ubuntu-drivers devices` to figure out</span></span><br><span class="line">$ sudo apt-get install nvidia-&lt;driver-version&gt;</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line">$ nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># open nvidia settings -&gt; PRIME Profile </span></span><br><span class="line">nvidia-settings</span><br></pre></td></tr></table></figure>
<h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><h3 id="Ubuntu-1"><a href="#Ubuntu-1" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Download &amp; install cuda 10.1</span></span><br><span class="line">$ wget https://developer.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1604-10-1-local-10.1.105-418.39_1.0-1_amd64.deb</span><br><span class="line"></span><br><span class="line">$ sudo dpkg -i cuda-repo-&lt;name&gt;.deb</span><br><span class="line">$ sudo apt-key add /var/cuda-repo-10-1-local-10.1.105-418.39/7fa2af80.pub </span><br><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install cuda-10-1</span><br><span class="line"></span><br><span class="line"><span class="comment"># config ~/.bashrc </span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-10.1/bin:<span class="variable">$PATH</span> &gt;&gt; ~/.bashrc </span><br><span class="line">$ <span class="built_in">echo</span> <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-10.1/lib64:<span class="variable">$LD_LIBRARY_PATH</span> &gt;&gt; ~/.bashrc </span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<h3 id="CentOS"><a href="#CentOS" class="headerlink" title="CentOS"></a>CentOS</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget https://developer.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-rhel7-10-1-local-10.1.105-418.39-1.0-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">$ sudo rpm -i cuda-repo-rhel7-10-1-local-10.1.105-418.39-1.0-1.x86_64.rpm</span><br><span class="line">$ sudo yum clean all</span><br><span class="line"><span class="comment"># installed at /usr/local/cuda-10.1</span></span><br><span class="line">$ sudo yum install cuda</span><br><span class="line"></span><br><span class="line"><span class="comment"># config ~/.bashrc </span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-10.1/bin:<span class="variable">$PATH</span> &gt;&gt; ~/.bashrc </span><br><span class="line">$ <span class="built_in">echo</span> <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-10.1/lib64:<span class="variable">$LD_LIBRARY_PATH</span> &gt;&gt; ~/.bashrc </span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><h2 id="Flax"><a href="#Flax" class="headerlink" title="Flax"></a>Flax</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install --upgrade pip</span><br><span class="line"><span class="comment"># Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.</span></span><br><span class="line">pip install --upgrade <span class="string">&quot;jax[cuda]&quot;</span> -f https://storage.googleapis.com/jax-releases/jax_releases.html  <span class="comment"># Note: wheels only available on linux.</span></span><br></pre></td></tr></table></figure>
<p>Check if gpu works:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import jax</span><br><span class="line">&gt;&gt;&gt; jax.devices()</span><br><span class="line"></span><br><span class="line"><span class="comment"># or </span></span><br><span class="line"></span><br><span class="line">from jax.lib import xla_bridge</span><br><span class="line"><span class="built_in">print</span>(xla_bridge.get_backend().platform)</span><br></pre></td></tr></table></figure></p>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Install</span></span><br><span class="line"><span class="comment"># This command will include all required packages including compatible cuda and cudnn.</span></span><br><span class="line">$ conda create --name tf_gpu tensorflow-gpu[=1.15]</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check if tensorflow-gpu works</span></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">&gt;&gt; a = tf.constant(5.0)</span><br><span class="line">&gt;&gt; b = tf.constant(6.0)</span><br><span class="line">&gt;&gt; c = a * b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the graph in a session.</span></span><br><span class="line">&gt;&gt; sess = tf.compat.v1.Session() <span class="comment"># v2</span></span><br><span class="line">&gt;&gt; sess = tf.Session() <span class="comment"># v1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the tensor `c`.</span></span><br><span class="line">&gt;&gt; <span class="built_in">print</span>(sess.run(c))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or, only for v1:</span></span><br><span class="line">&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt; sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></table></figure>
<h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>Install PyTorch (gpu)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Install</span></span><br><span class="line">conda install pytorch torchvision cudatoolkit=10.1 -c pytorch</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check if pytorch works on gpu</span></span><br><span class="line">&gt;&gt; import torch; torch.cuda.is_available()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(torch.__version__)&quot;</span></span><br><span class="line">python -c <span class="string">&quot;import torch; print(torch.cuda.is_available())&quot;</span></span><br><span class="line">python -c <span class="string">&quot;import torch; print(torch.backends.cudnn.version())&quot;</span></span><br><span class="line">python -c <span class="string">&quot;import torch; print(torch.__version__)&quot;</span></span><br><span class="line">python -c <span class="string">&quot;import torch; print(torch.cuda.is_available())&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="NCCL"><a href="#NCCL" class="headerlink" title="NCCL"></a>NCCL</h2><p>Install NCCL<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[NVDIA-NCCL install](https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html)">[3]</span></a></sup></p>
<ol>
<li>Install from official deb<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. Install from local deb</span></span><br><span class="line">sudo dpkg -i nccl-repo-&lt;version&gt;.deb</span><br><span class="line"><span class="comment"># Or, from network deb</span></span><br><span class="line"><span class="comment"># sudo dpkg -i nvidia-machine-learning-repo-&lt;version&gt;.deb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Update APT database</span></span><br><span class="line">sudo apt update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Install the libnccl2 package with APT</span></span><br><span class="line">sudo apt install libnccl2 libnccl-dev</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h2>Install Apex<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/NVIDIA/apex</span><br><span class="line">$ <span class="built_in">cd</span> apex</span><br><span class="line">$ pip install -v --no-cache-dir --global-option=<span class="string">&quot;--cpp_ext&quot;</span> --global-option=<span class="string">&quot;--cuda_ext&quot;</span> ./</span><br><span class="line"></span><br><span class="line"><span class="comment"># If report error: Given no hashes to check 137 links for project</span></span><br><span class="line"><span class="comment"># Solution:</span></span><br><span class="line">$ python3 setup.py install</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># conda export yml</span></span><br><span class="line">conda env <span class="built_in">export</span> &gt; environment.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># conda install from yml</span></span><br><span class="line">conda env create -f envrionment.yml</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc">Tensorflow GPU Installation</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.tensorflow.org/install/gpu">tensorflow.org</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html">NVDIA-NCCL install</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Toolkit</category>
      </categories>
      <tags>
        <tag>Toolkits</tag>
      </tags>
  </entry>
  <entry>
    <title>Implementation Practical Misc</title>
    <url>/notes/2019/09/10/Programming/Implementation-Magic-Misc-in-ML/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Some implementation magic.</p>
<span id="more"></span>
<h1 id="RL"><a href="#RL" class="headerlink" title="RL"></a>RL</h1><h2 id="Cumulative-sum"><a href="#Cumulative-sum" class="headerlink" title="Cumulative sum"></a>Cumulative sum</h2><p>Cumulative sum of state-value function with discounting.</p>
<h3 id="scipy-signal-lfilter"><a href="#scipy-signal-lfilter" class="headerlink" title="scipy.signal.lfilter"></a>scipy.signal.lfilter</h3><script type="math/tex; mode=display">G \leftarrow \sum_{t=1}^T \gamma^{i-1}R_i</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discount_cumsum</span>(<span class="params">x, discount</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;   </span></span><br><span class="line"><span class="string">  https://github.com/openai/spinningup/blob/2e0eff9bd019c317af908b72c056a33f14626602/spinup/algos/trpo/core.py#L88</span></span><br><span class="line"><span class="string">  magic from rllab for computing discounted cumulatice sums of vectors</span></span><br><span class="line"><span class="string">  ==========</span></span><br><span class="line"><span class="string">  input:</span></span><br><span class="line"><span class="string">      vector x,</span></span><br><span class="line"><span class="string">       [x0,</span></span><br><span class="line"><span class="string">        x1,</span></span><br><span class="line"><span class="string">        x2]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">  output:</span></span><br><span class="line"><span class="string">     [x0 + discount * x1 + discount^2 * x2,</span></span><br><span class="line"><span class="string">      x1 + discount *x2,</span></span><br><span class="line"><span class="string">      x2]</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> scipy.signal.lfilter([<span class="number">1</span>], [<span class="number">1</span>, <span class="built_in">float</span>(-discount)], x[::-<span class="number">1</span>], axis=<span class="number">0</span>)[::-<span class="number">1</span>]    </span><br></pre></td></tr></table></figure>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><p>TD(n)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discount_with_dones</span>(<span class="params">rewards, dones, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    https://github.com/openai/baselines/blob/229a772b81155695a2692066b1ef3e7b77f5993a/baselines/a2c/utils.py#L147</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    discounted = []</span><br><span class="line">    r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> reward, done <span class="keyword">in</span> <span class="built_in">zip</span>(rewards[::-<span class="number">1</span>], dones[::-<span class="number">1</span>]):</span><br><span class="line">        r = reward + gamma*r*(<span class="number">1.</span>-done) <span class="comment"># fixed off by one bug</span></span><br><span class="line">        discounted.append(r)</span><br><span class="line">    <span class="keyword">return</span> discounted[::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">mb_rewards, mb_dones, _, _ = replay_buffer.sample(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n, (rewards, dones, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(mb_rewards, mb_dones, last_values)):</span><br><span class="line">    <span class="keyword">if</span> dones[-<span class="number">1</span>] == <span class="number">0</span>: </span><br><span class="line">        rewards = discount_with_dones(rewards+[value], dones+[<span class="number">0</span>], self.gamma)[:-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># terminal</span></span><br><span class="line">        rewards = discount_with_dones(rewards, dones, self.gamma)</span><br><span class="line"></span><br><span class="line">    mb_rewards[n] = rewards</span><br></pre></td></tr></table></figure></p>
<h2 id="Training-tricks"><a href="#Training-tricks" class="headerlink" title="Training tricks"></a>Training tricks</h2><h3 id="Entropy-normalization"><a href="#Entropy-normalization" class="headerlink" title="Entropy normalization"></a>Entropy normalization</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reference: baseline A2C</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">logits</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; PyTorch &quot;&quot;&quot;</span></span><br><span class="line">    row_max, _ = torch.<span class="built_in">max</span>(logits, -<span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">    a0 = logits - row_max</span><br><span class="line">    ea0 = torch.exp(a0)</span><br><span class="line">    z0 = torch.<span class="built_in">sum</span>(ea0, -<span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">    p0 = ea0 / z0</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(p0 * torch.log(z0) - a0, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">ent = entropy(logits).mean()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">logits</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; TensorFlow &quot;&quot;&quot;</span></span><br><span class="line">    a0 = logits - tf.reduce_max(logits, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    ea0 = tf.exp(a0)</span><br><span class="line">    z0 = tf.reduce_sum(ea0, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    p0 = ea0 / z0</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="all-reduce"><a href="#all-reduce" class="headerlink" title="all-reduce"></a>all-reduce</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># average grad of target layers in multiprocessing training</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">avg_grad</span>(<span class="params">layers: <span class="built_in">tuple</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; (PyTorch) gradients averaging &quot;&quot;&quot;</span></span><br><span class="line">    size = <span class="built_in">float</span>(dist.get_world_size())</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> layer.parameters():</span><br><span class="line">            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)</span><br><span class="line">            param.grad.data /= size</span><br></pre></td></tr></table></figure>
<h1 id="DL"><a href="#DL" class="headerlink" title="DL"></a>DL</h1><h2 id="One-hot-encoding"><a href="#One-hot-encoding" class="headerlink" title="One-hot encoding"></a>One-hot encoding</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span>(<span class="params">col: <span class="built_in">int</span>, row: <span class="built_in">int</span>, one_hot_index: torch.LongTensor</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; PyTorch &quot;&quot;&quot;</span></span><br><span class="line">    y_one_hot = torch.FloatTensor(col, row)</span><br><span class="line">    y_one_hot.zero_()</span><br><span class="line">    y_one_hot.scatter_(<span class="number">1</span>, one_hot_index, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y_one_hot</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Programming practical</category>
      </categories>
      <tags>
        <tag>Programming practical</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell Command Zoo in Data Processing</title>
    <url>/notes/2019/01/20/Programming/Shell-command-zoo-in-data-processing/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Bash scripts are powerful in data processing. This post records some tips that I encountered during the data processing, and will keep updated.<br><span id="more"></span></p>
<h1 id="Data-statistics"><a href="#Data-statistics" class="headerlink" title="Data statistics"></a>Data statistics</h1><h2 id="Handle-duplicate"><a href="#Handle-duplicate" class="headerlink" title="Handle duplicate"></a>Handle duplicate</h2><ul>
<li><p>Obtain the unique lines of a file:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sort -u <span class="string">&quot;&lt;filename&gt;&quot;</span> -o <span class="string">&quot;&lt;output_filename&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># slightly lower</span></span><br><span class="line">sort <span class="string">&quot;&lt;filename&gt;&quot;</span> | uniq &gt; <span class="string">&quot;&lt;output_filename&gt;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Random sort</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -R: sort by random order</span></span><br><span class="line">sort -R <span class="string">&quot;&lt;filename&gt;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Count the number of each lines</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -i: ignores the case </span></span><br><span class="line"><span class="comment"># -c: count the # of lines</span></span><br><span class="line">sort <span class="string">&quot;&lt;filename&gt;&quot;</span> | uniq -ic</span><br></pre></td></tr></table></figure>
</li>
<li><p>Print duplicate lines</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># print all duplicate lines</span></span><br><span class="line">sort <span class="string">&quot;&lt;filename&gt;&quot;</span> | uniq -iD</span><br><span class="line"></span><br><span class="line"><span class="comment"># print only uniq duplicate lines</span></span><br><span class="line">sort <span class="string">&quot;&lt;filename&gt;&quot;</span> | uniq -iD |uniq -i</span><br><span class="line"><span class="comment"># or case-sensitive:</span></span><br><span class="line">sort <span class="string">&quot;&lt;filename&gt;&quot;</span> | uniq -D |uniq -i</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Count-the-occurrence-of-a-string-“xxx”"><a href="#Count-the-occurrence-of-a-string-“xxx”" class="headerlink" title="Count the occurrence of a string “xxx”."></a>Count the occurrence of a string “xxx”.</h2><ol>
<li>vim mode<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">:%s/<span class="string">&quot;xxx&quot;</span>//gn</span><br></pre></td></tr></table></figure></li>
<li>grep<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grep -o <span class="string">&quot;xxx&quot;</span> filename | wc -l</span><br></pre></td></tr></table></figure>
 If count strings [“xxx”, “yyy”]:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grep -o <span class="string">&quot;xxx\|yyy&quot;</span> filename | wc -l</span><br></pre></td></tr></table></figure>
<h2 id="Training-and-dev-set-split"><a href="#Training-and-dev-set-split" class="headerlink" title="Training and dev set split"></a>Training and dev set split</h2><ol>
<li><strong>Split train and test set</strong>:<br>Given unsplit data file “data.tsv”</li>
</ol>
<ul>
<li>Shuffle among lines<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ shuf <span class="string">&quot;data.tsv&quot;</span> -o  <span class="string">&quot;shuffle.tsv&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>Split into train/dev sets</li>
</ol>
<ul>
<li>Count # of lines:<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wc -l <span class="string">&quot;shuffle.tsv&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>Split 90% into train set and 10% into devset:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ head -n &lt;#0.9count&gt; &quot;shuffle.tsv&quot; &gt; &quot;train.tsv&quot;</span><br><span class="line">$ tail -n &lt;#0.1count&gt; &quot;shuffle.tsv&quot; &gt; &quot;dev.tsv&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li>Split into train / dev / test sets</li>
</ol>
<ul>
<li>Count # of lines:<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wc -l <span class="string">&quot;shuffle.tsv&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>Split 80%/10%/10% into train/dev/test sets:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ head -n &lt;#0.8count&gt; &quot;shuffle.tsv&quot; &gt; &quot;train.tsv&quot;</span><br><span class="line">$ sed -n &quot;&lt;#0.8count+1&gt;,&lt;#0.9count&gt;p&quot; shuffle.tsv &gt; &quot;dev.tsv&quot;</span><br><span class="line">$ tail -n &lt;#0.1count&gt; &quot;shuffle.tsv&quot; &gt; &quot;dev.tsv&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Check-data"><a href="#Check-data" class="headerlink" title="Check data"></a>Check data</h2><ol>
<li>Check the specific line given files<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -n &lt;line_num&gt;p &lt;filename.txt&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Job-running"><a href="#Job-running" class="headerlink" title="Job running"></a>Job running</h1><h2 id="Redirection"><a href="#Redirection" class="headerlink" title="Redirection"></a>Redirection</h2><h3 id="Redirect-output-to-file"><a href="#Redirect-output-to-file" class="headerlink" title="Redirect output to file"></a>Redirect output to file</h3><p>Redirect the standard output (<code>stdout</code>) and standard error (<code>stderr</code>) to an output file.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bash_command] &gt; <span class="string">&quot;&lt;out_file_name&gt;&quot;</span> 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0 is stdin</span></span><br><span class="line"><span class="comment"># 1 is stdout</span></span><br><span class="line"><span class="comment"># 2 is stderr</span></span><br></pre></td></tr></table></figure><br><div class="note info">
            <ul><li>File descriptor <code>1</code> is the standard output (<code>stdout</code>), and file descriptor <code>2</code> is the standard error (<code>stderr</code>).</li><li><code>&amp;</code> indicates that what follows is a <strong>file descriptor</strong> and not a filename.</li></ul>
          </div></p>
<h3 id="Output-the-stdout-to-both-screen-and-file"><a href="#Output-the-stdout-to-both-screen-and-file" class="headerlink" title="Output the stdout to both screen and file"></a>Output the stdout to both screen and file</h3><p>Copy standard input to each FILE, and also to standard output.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bash_command] | tee <span class="string">&quot;&lt;out_filename&gt;&quot;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Run-jobs-in-background"><a href="#Run-jobs-in-background" class="headerlink" title="Run jobs in background"></a>Run jobs in background</h2><h3 id="ampersand-amp"><a href="#ampersand-amp" class="headerlink" title="ampersand(&amp;)"></a>ampersand(<code>&amp;</code>)</h3><p>Ampersand(<code>&amp;</code>) starts a subprocess (i.e. children process to the current bash session), and will terminate when exiting current session.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[bash_command] &amp;</span><br></pre></td></tr></table></figure></p>
<h3 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a><code>nohup</code></h3><p><code>nohup</code> caches the hangup signal, i.e. the subprocess will still run when closing the current process.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nohup [bash_command]</span><br></pre></td></tr></table></figure><br><div class="note primary">
            <p>It can be stopped by press <code>Ctrl</code> + <code>z</code>. <code>Ctrl</code> + <code>z</code> does not woking when <code>&amp;</code> exists.</p>
          </div></p>
<h3 id="nohup-ampersand-amp-redirection"><a href="#nohup-ampersand-amp-redirection" class="headerlink" title="nohup + ampersand(&amp;) + redirection"></a><code>nohup</code> + ampersand(<code>&amp;</code>) + redirection</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nohup [bash_command] &gt; <span class="string">&quot;&lt;out.filename&gt;&quot;</span> 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<h1 id="File-management"><a href="#File-management" class="headerlink" title="File management"></a>File management</h1><ul>
<li>find <strong>files</strong> larger than 100M in the current directory:<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find . -<span class="built_in">type</span> &lt;type-name&gt; -size +/-&lt;file-size&gt;</span><br><span class="line"><span class="comment"># e.g. find . -type f -size +100M </span></span><br><span class="line"><span class="comment"># &lt;type-name&gt;: d: directory, f: file</span></span><br><span class="line"><span class="comment"># +: &gt;, -: &lt;</span></span><br><span class="line"><span class="comment"># &lt;file-size&gt;: k/M/G</span></span><br></pre></td></tr></table></figure></li>
<li><p>find large <strong>directories</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ du -h --max-depth=1</span><br><span class="line"></span><br><span class="line">$ du -hm --max-depth=2 | sort -nr | head -12</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reports the amount of disk space</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># report all dirs</span></span><br><span class="line"><span class="built_in">cd</span> /</span><br><span class="line">du -sh *</span><br><span class="line"></span><br><span class="line"><span class="comment"># subdir</span></span><br><span class="line">du -lh --max-depth=1</span><br></pre></td></tr></table></figure>
</li>
<li></li>
</ul>
<h1 id="Count"><a href="#Count" class="headerlink" title="Count"></a>Count</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># count the # of lines in a file</span></span><br><span class="line">$ wc -l &lt;filepath&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># count the # of files in a directory (non-recursive)</span></span><br><span class="line">$ ls | wc -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># count the # of files recursively in a directory</span></span><br><span class="line">$ find &lt;directory&gt; -<span class="built_in">type</span> f | wc -l</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">tree &lt;directory&gt;</span><br></pre></td></tr></table></figure>
<h1 id="Unzip-Chinese-character"><a href="#Unzip-Chinese-character" class="headerlink" title="Unzip Chinese character"></a>Unzip Chinese character</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">unzip -O GBK &lt;filename&gt;.zip</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch Notes</title>
    <url>/notes/2019/09/06/Programming/PyTorch-notes/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><img data-src='/notes/images/pytorch-logo-only.png' width='30%'/></p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h1 id="Basic-op"><a href="#Basic-op" class="headerlink" title="Basic op"></a>Basic op</h1><h2 id="numpy-to-from-tensor"><a href="#numpy-to-from-tensor" class="headerlink" title="numpy to/from tensor"></a>numpy to/from tensor</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># numpy -&gt; tensor</span></span><br><span class="line">np_array = np.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">torch_tensor = torch.from_numpy(np_array)</span><br><span class="line"><span class="comment"># or:</span></span><br><span class="line">torch_tensor = torch.FloatTensor(np_array)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor -&gt; numpy</span></span><br><span class="line">np_array = torch_tensor.numpy()</span><br></pre></td></tr></table></figure>
<h2 id="contiguous"><a href="#contiguous" class="headerlink" title="contiguous"></a>contiguous</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(...)</span><br></pre></td></tr></table></figure>
<h2 id="Parameter-v-s-register-buffer"><a href="#Parameter-v-s-register-buffer" class="headerlink" title="Parameter v.s. register_buffer"></a>Parameter v.s. register_buffer</h2><ul>
<li><p><code>nn.Parameter</code> is considered a module parameter and will appear in <code>parameters()</code> iterator. This would do backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn.Parameter(data: Tensor, required_grad:<span class="built_in">bool</span> = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>register_buffer</code> add a persistant buffer to the module. It is used to register a buffer, not a parameter. It cannot do backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.register_buffer(name:<span class="built_in">str</span>, tensor: Tensor)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Multiplication"><a href="#Multiplication" class="headerlink" title="Multiplication"></a>Multiplication</h2><h3 id="torch-einsum"><a href="#torch-einsum" class="headerlink" title="torch.einsum"></a>torch.einsum</h3><p>multi-linear expressions, i.e. sums of products. Use Einstein summation convention</p>
<p><code>torch.einsum(equation, *operands)</code> → Tensor<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">As = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">Bs = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;bij,bjk-&gt;bik&#x27;</span>, As, Bs) <span class="comment"># batch matrix multiplication</span></span><br></pre></td></tr></table></figure></p>
<h3 id="torch-ger"><a href="#torch-ger" class="headerlink" title="torch.ger"></a>torch.ger</h3><p><code>torch.ger(input, vec2, out=None)</code> → Tensor<br>outer product<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v1 = torch.arange(<span class="number">1.</span>, <span class="number">5.</span>)</span><br><span class="line">v2 = torch.arange(<span class="number">1.</span>, <span class="number">4.</span>)</span><br><span class="line">torch.ger(v1, v2)</span><br><span class="line"><span class="comment"># tensor([[  1.,   2.,   3.],</span></span><br><span class="line">        [  <span class="number">2.</span>,   <span class="number">4.</span>,   <span class="number">6.</span>],</span><br><span class="line">        [  <span class="number">3.</span>,   <span class="number">6.</span>,   <span class="number">9.</span>],</span><br><span class="line">        [  <span class="number">4.</span>,   <span class="number">8.</span>,  <span class="number">12.</span>]])</span><br></pre></td></tr></table></figure></p>
<h2 id="dimension"><a href="#dimension" class="headerlink" title="dimension"></a>dimension</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.randn(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">t.dim() <span class="comment"># 3</span></span><br><span class="line">t.size() <span class="comment"># torch.Size([4, 5, 6])</span></span><br><span class="line">t.shape <span class="comment"># torch.Size([4, 5, 6])</span></span><br><span class="line">t.size(<span class="number">0</span>) <span class="comment"># 4</span></span><br><span class="line">t.size(-<span class="number">1</span>) <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<h2 id="nn-Parameter"><a href="#nn-Parameter" class="headerlink" title="nn.Parameter"></a>nn.Parameter</h2><p><code>torch.nn.Parameter</code>, a subclass of <code>torch.Tensor</code>, could automatically add the data into the list of parameters and could appear in <code>Module.parameters</code> iterator. It can be automatically optimized by the optimizer if in optimized parameter list. Its arguments:</p>
<ul>
<li>data (Tensor): parameter tensor.</li>
<li>requires_grad (bool, optional): if the parameter requires gradient. Default: True.</li>
</ul>
<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><h2 id="byte"><a href="#byte" class="headerlink" title="byte()"></a>byte()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">t.byte()</span><br><span class="line"><span class="comment"># equals to</span></span><br><span class="line">t.to(torch.uint8)</span><br></pre></td></tr></table></figure>
<h2 id="topk"><a href="#topk" class="headerlink" title="topk()"></a>topk()</h2><p><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)</code> -&gt; (Tensor, LongTensor)</p>
<ul>
<li>a namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1.</span>, <span class="number">6.</span>)</span><br><span class="line"><span class="comment"># tensor([ 1.,  2.,  3.,  4.,  5.])</span></span><br><span class="line">torch.topk(x, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))</span></span><br></pre></td></tr></table></figure>
<h1 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h1><h2 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h2><p><code>torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></p>
<ul>
<li>negative log likelihood loss. It is useful to train a classification problem with C classes</li>
<li><code>size_average</code>, <code>reduce</code> - deprecated</li>
<li><code>reduction</code>: (‘none’, ‘mean’ (default), ‘sum’)</li>
<li>It requires adding a <code>LogSoftmax</code> layer as the last layer. Combining <code>LogSoftmax</code> with <code>NLLLoss</code> is the same as using <code>CrossEntropyLoss</code>.</li>
</ul>
<h1 id="Optim"><a href="#Optim" class="headerlink" title="Optim"></a>Optim</h1><h2 id="Per-parameter-optim"><a href="#Per-parameter-optim" class="headerlink" title="Per-parameter optim"></a>Per-parameter optim</h2><p>Pass in an iterable of <code>dict</code>s.<br>E.g. specify per-layer learning rates:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">            &#123;&#x27;params&#x27;: model.base.parameters()&#125;, # default lr</span><br><span class="line">            &#123;&#x27;params&#x27;: model.classifier.parameters(), &#x27;lr&#x27;: 1e-3&#125;</span><br><span class="line">          ], </span><br><span class="line">          lr=1e-2, # default</span><br><span class="line">          momentum=.9 # for all params</span><br><span class="line">      )</span><br></pre></td></tr></table></figure></p>
<h2 id="Optim-step"><a href="#Optim-step" class="headerlink" title="Optim step"></a>Optim step</h2><h3 id="Optimizer-step"><a href="#Optimizer-step" class="headerlink" title="Optimizer.step()"></a>Optimizer.step()</h3><p>Step once the gradients are computed <code>loss.backward()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># zero gradients</span></span><br><span class="line">    output = model(<span class="built_in">input</span>) <span class="comment"># foward pass</span></span><br><span class="line">    loss = loss_fn(output, target) <span class="comment"># calculate loss</span></span><br><span class="line">    loss.backward() <span class="comment"># do backprop, compute gradients</span></span><br><span class="line">    optimizer.step() <span class="comment"># update parameters</span></span><br></pre></td></tr></table></figure>
<h2 id="Optim-algorithms"><a href="#Optim-algorithms" class="headerlink" title="Optim algorithms"></a>Optim algorithms</h2><p><a href="/notes/2019/05/20/NN/Optimization-methods-introduction">Optimization methods in deep learning</a></p>
<h3 id="Optimizer-step-closure"><a href="#Optimizer-step-closure" class="headerlink" title="Optimizer.step(closure)"></a>Optimizer.step(closure)</h3><p>Some algorithms like Conjugate Gradient and LBFGS requries re-evaluate multiple times, so pass in a closure to clear the gradients, compute the loss, finally return.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closure</span>():</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    optimizer.step(closure) <span class="comment"># pass in a closure</span></span><br></pre></td></tr></table></figure>
<h3 id="Adjust-learning-rate"><a href="#Adjust-learning-rate" class="headerlink" title="Adjust learning rate"></a>Adjust learning rate</h3><p>use <code>torch.optim.le_scheduler</code><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = ...</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    evaluate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure></p>
<h2 id="gradient-clipping"><a href="#gradient-clipping" class="headerlink" title="gradient clipping"></a>gradient clipping</h2><ol>
<li>clip by value, set threshold</li>
<li>clip_norm.<script type="math/tex; mode=display">\text{grad_clip_norm(t)}=\left\{
             \begin{array}{ll}
               t * \frac{\text{max norm}}{||t||_2} \quad  ||t||_2 \leq \text{max norm} \\
               t \quad \text{otherwise}\\
             \end{array}
 \right.</script></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_grad_norm_</span>(<span class="params">parameters, max_norm, norm_type=<span class="number">2</span></span>):</span></span><br><span class="line">    parameters = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>, parameters))</span><br><span class="line">    max_norm = <span class="built_in">float</span>(max_norm))</span><br><span class="line">    norm_type = <span class="built_in">float</span>(norm_type)</span><br><span class="line">    <span class="keyword">if</span> norm_type = torch._six.inf</span><br><span class="line">    	total_norm = <span class="built_in">max</span>(p.grad.data.<span class="built_in">abs</span>().<span class="built_in">max</span>() <span class="keyword">for</span> p <span class="keyword">in</span> parameters)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	total_norm = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> parameters:</span><br><span class="line">            param_norm = p.grad.data.norm(norm_type)</span><br><span class="line">            total_norm += param_norm.item() ** norm_type</span><br><span class="line">        total_norm = total_norm ** (<span class="number">1.</span>/ norm_type)</span><br><span class="line">    clip_coef = max_norm / (total_norm + <span class="number">1e-6</span>)</span><br><span class="line">    <span class="keyword">if</span> clip_coef &lt; <span class="number">1</span>:</span><br><span class="line">    	<span class="keyword">for</span> p <span class="keyword">in</span> parameters:</span><br><span class="line">            p.grad.data.mul_(clip_coef)</span><br><span class="line">    <span class="keyword">return</span> total_norm</span><br></pre></td></tr></table></figure>
<h1 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h1><h2 id="Define-layers"><a href="#Define-layers" class="headerlink" title="Define layers"></a>Define layers</h2><p>Layers should be directly set as the attribute of a children class of <code>torch.nn.Module</code>, so that <code>model.parameters()</code> can be directly pass to <code>torch.nn.optim</code>. Otherwise, additionally parameters should be passed following model.parameters().</p>
<ul>
<li>In other words, if layers are wrapped by a parent data structure like dict(), the model.parameters() cannot get all the layer parameters to be optimized, so that the first augument of optimizer should be manually set. (as below python code)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SharedLayers, self).__init__()</span><br><span class="line">        d = &#123;&#125;</span><br><span class="line">        d[<span class="string">&#x27;f1&#x27;</span>] = nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">        d[<span class="string">&#x27;f2&#x27;</span>] = nn.Linear(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        self.d = d</span><br><span class="line">        self.f3 = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">        self.loss = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.d[<span class="string">&#x27;f1&#x27;</span>](x)</span><br><span class="line">        x = self.d[<span class="string">&#x27;f2&#x27;</span>](x)</span><br><span class="line">        x = self.f3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    net = Net()</span><br><span class="line">    x = torch.rand(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line">    y = torch.rand(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer = optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        y_ = net(x)</span><br><span class="line">        loss = F.mse_loss(y_, y)</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        loss.backward() <span class="comment"># do backprop</span></span><br><span class="line">        optimizer.step() <span class="comment"># do not optimize layers wrapped in net.d !</span></span><br><span class="line">        <span class="comment"># [p for p in l.d[&#x27;f1&#x27;].parameters()] never change!</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Also, if we need to use gpu to run, often we do: Net().to(device). But if the there are layers encompassed by a dict attribute in the modulde class, we have to do layer.to(device) individually.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SharedLayers, self).__init__()</span><br><span class="line">        d = &#123;&#125;</span><br><span class="line">        d[<span class="string">&#x27;f1&#x27;</span>] = nn.Linear(<span class="number">20</span>, <span class="number">10</span>).to(device)</span><br><span class="line">        d[<span class="string">&#x27;f2&#x27;</span>] = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(device)</span><br><span class="line">        self.d = d</span><br><span class="line">        self.f3 = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">        self.loss = nn.MSELoss()</span><br><span class="line">	</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    net = Net().to(device)</span><br><span class="line">    x = torch.rand(<span class="number">1</span>, <span class="number">20</span>).to(device)</span><br><span class="line">    y = torch.rand(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="NaN"><a href="#NaN" class="headerlink" title="NaN"></a>NaN</h2><p>If there exists <code>NaN</code>:</p>
<ol>
<li>If within iteration 100, it may be due to the big learning rate. Try to reduce the learning rate 1/2~1/10.</li>
<li>If use RNNs, may be because of the gradient exploration. Solution: add “gradient clipping”</li>
<li>Division by 0.</li>
<li>Take logarithm of 0 or negative number, e.g. calculating entropy or cross entropy.</li>
<li>In exponential computation, the result is INF/INF, e.g. softmax. Solution: minus the maximum if possible.</li>
</ol>
<h2 id="Count-the-parameter-numbers"><a href="#Count-the-parameter-numbers" class="headerlink" title="Count the parameter numbers"></a>Count the parameter numbers</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># approach 1</span></span><br><span class="line">model_parameters = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line">tot_params = <span class="built_in">sum</span>([np.prod(p.size()) <span class="keyword">for</span> p <span class="keyword">in</span> model_parameters])</span><br><span class="line"></span><br><span class="line"><span class="comment"># approach 2 (count the trainable params)</span></span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br></pre></td></tr></table></figure>
<h2 id="Configuration-error"><a href="#Configuration-error" class="headerlink" title="Configuration error"></a>Configuration error</h2><h2 id="MacOSX"><a href="#MacOSX" class="headerlink" title="MacOSX"></a>MacOSX</h2><p><code>import torch</code> error</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/.../lib/python3.6/site-packages/torch/__init__.py&quot;</span>, line 79, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from torch._C import *</span><br><span class="line">  ...</span><br><span class="line">ImportError: dlopen(/.../lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/<span class="built_in">local</span>/opt/libomp/lib/libomp.dylib</span><br><span class="line">  Referenced from: /.../lib/python3.6/site-packages/torch/lib/libshm.dylib</span><br><span class="line">  Reason: image not found</span><br><span class="line">  </span><br><span class="line"><span class="comment"># solution </span></span><br><span class="line">$ brew install libomp</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Sparse Matrix in Data Processing</title>
    <url>/notes/2020/04/03/Programming/Sparse-matrix-in-data-processing/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>It is wasteful to store zeros elements in a <code>sparse matrix</code>, especially for incrementally data. When constructing <code>tf-idf</code> and <code>bag-of-words</code> features or saving <code>graph ajacent matrix</code>, non-efficient sparse matrix storage might lead to the <code>memory error</code>. To circumvent this problems, efficient sparse matrix storage is a choice.</p>
<span id="more"></span>
<h1 id="Sparse-matrix"><a href="#Sparse-matrix" class="headerlink" title="Sparse matrix"></a>Sparse matrix</h1><p>A martix is sparse when its sparsity is greater than 0.5, where the <strong>sparsity</strong> of a matrix is the # of zero-valued elements divided by the total # of elements (which is equal to 1 minus the <strong>density</strong> of the matrix).<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[wiki: sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix)
">[2]</span></a></sup></p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>Each entry in the array represents the element <script type="math/tex">a_{i,j}</script> of the matrix and is accessed by the two indices $i$ (row index) and $j$ (colomn index).<br>Sparse matrix can reduce sustantial memory requirements by storing only the non-zero entries.</p>
<p>Formats can be divided into:</p>
<blockquote>
<p>those support <strong>efficient modification</strong>, such as DOK, LIL or COO, which are typically used to construct the matrices.<br>those that support efficient access and matrix operations, such as CSR or CSC.</p>
</blockquote>
<h2 id="Dictionary-of-keys-DOK"><a href="#Dictionary-of-keys-DOK" class="headerlink" title="Dictionary of keys (DOK)"></a>Dictionary of keys (DOK)</h2><ul>
<li>DOK is storing a dictionary that mays <code>(row, column)</code>-pairs to the valu of the elements. Elements that are missing from the dictionary are zeros. </li>
<li>DOK based sparse matrix is <em>efficient for constructing sparse matrices incrementally</em>.</li>
<li>Allows for efficient O(1) access of individual elements. Duplicates are not allowed. Can be efficiently converted to a coo_matrix once constructed.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[scipy dok_matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.dok_matrix.html)
">[3]</span></a></sup></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scipy</span>.<span class="title">sparse</span>.<span class="title">dok_matrix</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">	arg1, shape=<span class="literal">None</span>, dtype=<span class="literal">None</span>, copy=<span class="literal">False</span></span></span></span><br><span class="line"><span class="params"><span class="class"></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"># <span class="title">with</span> <span class="title">a</span> <span class="title">dense</span> <span class="title">matrix</span>, <span class="title">D</span></span></span><br><span class="line"><span class="class"><span class="title">dok_matrix</span>(<span class="params">D</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"># <span class="title">with</span> <span class="title">a</span> <span class="title">sparse</span> <span class="title">matrix</span>, <span class="title">S</span></span></span><br><span class="line"><span class="class"><span class="title">dok_matrix</span>(<span class="params">S</span>) </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"># <span class="title">create</span> <span class="title">the</span> <span class="title">matrix</span> <span class="title">with</span> <span class="title">initial</span> <span class="title">shape</span> (<span class="params">M,N</span>) <span class="title">dtype</span> <span class="title">is</span> <span class="title">optional</span>, <span class="title">defaulting</span> <span class="title">to</span> <span class="title">dtype</span>=’<span class="title">d</span>’</span></span><br><span class="line"><span class="class"><span class="title">dok_matrix</span>(<span class="params">(<span class="params">M,N</span>), [dtype]</span>) </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="title">attributes</span>:</span></span><br><span class="line">---------------------</span><br><span class="line">dtype	(dtype) Data <span class="built_in">type</span> of the matrix</span><br><span class="line">shape	(<span class="number">2</span>-<span class="built_in">tuple</span>) Shape of the matrix</span><br><span class="line">ndim	(<span class="built_in">int</span>) Number of dimensions (this <span class="keyword">is</span> always <span class="number">2</span>)</span><br><span class="line">nnz	Number of nonzero elements</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="List-of-lists-LIL"><a href="#List-of-lists-LIL" class="headerlink" title="List of lists (LIL)"></a>List of lists (LIL)</h2><ul>
<li>LIL stores one list per row, with each entry containing the column index and the value, where entries are sorted by column index for faster indexing.<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[scipy lil_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html)
">[4]</span></a></sup></li>
<li><em>Also efficient for constructing sparse matrices incrementally</em></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scipy</span>.<span class="title">sparse</span>.<span class="title">lil_matrix</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    arg1, shape=<span class="literal">None</span>, dtype=<span class="literal">None</span>, copy=<span class="literal">False</span></span>)</span></span><br><span class="line"><span class="class">&quot;&quot;&quot; <span class="title">Row</span>-<span class="title">based</span> <span class="title">list</span> <span class="title">of</span> <span class="title">lists</span> <span class="title">sparse</span> <span class="title">matrix</span> &quot;&quot;&quot;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">lil_matrix</span>(<span class="params">D</span>) # <span class="title">with</span> <span class="title">a</span> <span class="title">dense</span> <span class="title">matrix</span> <span class="title">or</span> <span class="title">rank</span>-2 <span class="title">ndarray</span> <span class="title">D</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">lil_matrix</span>(<span class="params">S</span>) # <span class="title">with</span> <span class="title">another</span> <span class="title">sparse</span> <span class="title">matrix</span> <span class="title">S</span> (<span class="params">equivalent to S.tolil(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">lil_matrix</span>(<span class="params">(<span class="params">M, N</span>), [dtype]</span>) # <span class="title">to</span> <span class="title">construct</span> <span class="title">an</span> <span class="title">empty</span> <span class="title">matrix</span> <span class="title">with</span> <span class="title">shape</span> (<span class="params">M, N</span>) <span class="title">dtype</span> <span class="title">is</span> <span class="title">optional</span>, <span class="title">defaulting</span> <span class="title">to</span> <span class="title">dtype</span>=’<span class="title">d</span>’.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="title">attributes</span>:</span></span><br><span class="line">---------------------</span><br><span class="line">dtype: dtype. Data <span class="built_in">type</span> of the matrix</span><br><span class="line"></span><br><span class="line">shape2:<span class="built_in">tuple</span>. Get shape of a matrix.</span><br><span class="line"></span><br><span class="line">ndim: <span class="built_in">int</span>. Number of dimensions (this <span class="keyword">is</span> always <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">nnz: Number of stored values, including explicit zeros.</span><br><span class="line"></span><br><span class="line">data: LIL <span class="built_in">format</span> data array of the matrix</span><br><span class="line"></span><br><span class="line">rows: LIL <span class="built_in">format</span> row index array of the matrix</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<div class="note info">
            <p><strong>Advantages</strong> of the LIL format</p><ul><li>supports <code>flexible slicing</code></li><li>changes to the matrix sparsity structure are efficient</li></ul><p><strong>Disadvantages</strong> of the LIL format</p><ul><li><strong>arithmetic operations</strong> LIL + LIL are slow (consider <strong>CSR</strong> or <strong>CSC</strong>)</li><li>slow column slicing (consider CSC)</li><li>slow matrix vector products (consider CSR or CSC)</li></ul><p>Intended <strong>Usage</strong></p><blockquote><p>LIL is a convenient format for constructing sparse matrices<br>once a matrix has been constructed, convert to <em>CSR</em> or <em>CSC</em> format for <em>fast arithmetic and matrix vector operations</em><br>consider using the <code>COO format</code> when <strong>constructing large matrices</strong></p></blockquote>
          </div>
<h2 id="Coordinate-list-COO"><a href="#Coordinate-list-COO" class="headerlink" title="Coordinate list (COO)"></a>Coordinate list (COO)</h2><ul>
<li>COO stores a list of <code>(row, column, value)</code> tuples, where entries are sorted first by row index and then by column index. ALso known as the <code>ijv</code> or <code>triplet</code> format.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[scipy coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html)
">[5]</span></a></sup></li>
<li><em>Also efficient for constructing sparse matrices incrementally</em></li>
<li><code>A[i[k], j[k]] = data[k]</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scipy</span>.<span class="title">sparse</span>.<span class="title">coo_matrix</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">	arg1, shape=<span class="literal">None</span>, dtype=<span class="literal">None</span>, copy=<span class="literal">False</span></span>)</span></span><br><span class="line"><span class="class">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="title">coo_matrix</span>(<span class="params">D</span>) # <span class="title">with</span> <span class="title">a</span> <span class="title">dense</span> <span class="title">matrix</span> <span class="title">D</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">coo_matrix</span>(<span class="params">S</span>) # <span class="title">with</span> <span class="title">another</span> <span class="title">sparse</span> <span class="title">matrix</span> <span class="title">S</span> (<span class="params">equivalent to S.tocoo(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">coo_matrix</span>(<span class="params">(<span class="params">M, N</span>), [dtype]</span>) # <span class="title">to</span> <span class="title">construct</span> <span class="title">an</span> <span class="title">empty</span> <span class="title">matrix</span> <span class="title">with</span> <span class="title">shape</span> (<span class="params">M, N</span>) <span class="title">dtype</span> <span class="title">is</span> <span class="title">optional</span>, <span class="title">defaulting</span> <span class="title">to</span> <span class="title">dtype</span>=’<span class="title">d</span>’.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">coo_matrix</span>(<span class="params">(<span class="params">data, (<span class="params">i, j</span>)</span>), [shape=(<span class="params">M, N</span>)]</span>) # <span class="title">to</span> <span class="title">construct</span> <span class="title">from</span> <span class="title">three</span> <span class="title">arrays</span>:</span></span><br><span class="line">    <span class="number">1.</span> data[:]</span><br><span class="line">        the entries of the matrix, <span class="keyword">in</span> <span class="built_in">any</span> order</span><br><span class="line">    <span class="number">2.</span> i[:]</span><br><span class="line">        the row indices of the matrix entries</span><br><span class="line">    <span class="number">3.</span> j[:] </span><br><span class="line">        the column indices of the matrix entries</span><br><span class="line">    Where A[i[k], j[k]] = data[k]. When shape <span class="keyword">is</span> <span class="keyword">not</span> specified, it <span class="keyword">is</span> inferred <span class="keyword">from</span> the index arrays</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">A[i[k], j[k]] = data[k]</span></span><br></pre></td></tr></table></figure>
<div class="note info">
            <p><strong>Advantages</strong> of the COO format</p><ol><li>facilitates fast conversion among sparse formats</li><li>permits duplicate entries (see example)</li><li>very fast conversion to and from CSR/CSC formats</li></ol><p><strong>Disadvantages</strong> of the COO format</p><ul><li>does not directly support <code>arithmetic operations</code> and <code>slicing</code></li></ul><p>Intended <strong>Usage</strong></p><ol><li>COO is a fast format for <em>constructing sparse matrices</em></li><li>Once a matrix has been constructed, convert to <em>CSR</em> or <em>CSC</em> format for <em>fast arithmetic and matrix vector operations</em></li><li>By default when converting to CSR or CSC format, duplicate (i,j) entries will be summed together. This facilitates efficient construction of finite element matrices and the like. (see example)</li></ol>
          </div>
<h2 id="Compressed-sparse-row-CSR-CRS-or-Yale-format"><a href="#Compressed-sparse-row-CSR-CRS-or-Yale-format" class="headerlink" title="Compressed sparse row (CSR, CRS or Yale format)"></a>Compressed sparse row (CSR, CRS or Yale format)</h2><p>The <em>compressed sparse row</em> (CSR) or <em>compressed row storage</em> (CRS) or Yale format stores the matrix in three 1-dim arrays <code>(data, col_indices, row_indptr)</code>, respectively containing non-zero values.<br>Let <code>NNZ</code> denote the # of nonzero entries.<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[scipy csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)
">[6]</span></a></sup></p>
<ul>
<li>The arrays <code>data</code> (non-zero values) and <code>col_indices</code> (column indices) are of length <code>NNZ</code>.</li>
<li>The array <code>row_indptr</code> has one element per row in the matrix and encodes the index in <code>data</code> where the given row starts. </li>
<li>The first value of <code>row_indptr</code> is always 0 and the last is always set to <code>NNZ</code>.</li>
</ul>
<p>Insantiate with:</p>
<ol>
<li><code>csr_matrix(D)</code> , with a dense matrix or rank-2 ndarray D</li>
<li><code>csr_matrix(S)</code> , with another sparse matrix S (equivalent to S.tocsr())</li>
<li><code>csr_matrix((M, N), [dtype])</code> , to construct an empty matrix with shape (M, N) dtype is optional, defaulting to dtype=’d’.</li>
<li><code>csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])</code>, where data, row_ind and col_ind satisfy the relationship a[row_ind[k], col_ind[k]] = data[k].</li>
<li><code>csr_matrix((data, indices, indptr), [shape=(M, N)])</code>.<ul>
<li><code>row = row_indptr[i: i+1]</code></li>
<li><code>col = indices[row]</code></li>
<li><code>A[row,col] = data[row]</code></li>
</ul>
</li>
</ol>
<div class="note info">
            <p><strong>Advantages</strong> of the CSC format</p><ul><li>efficient arithmetic operations CSR + CSR, CSR * CSR, etc.</li><li>efficient column slicing</li><li>fast matrix vector products</li></ul><p><strong>Disadvantages</strong> of the CSC format</p><ul><li>slow column slicing operations (consider CSC)</li><li>changes to the sparsity structure are expensive (consider LIL or DOK)</li></ul>
          </div>
<h2 id="Compressed-sparse-column-CSC-or-CCS"><a href="#Compressed-sparse-column-CSC-or-CCS" class="headerlink" title="Compressed sparse column (CSC or CCS)"></a>Compressed sparse column (CSC or CCS)</h2><p>CSC is similar to CSR except that values are read first by column, storing row indices for each value.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>indptr = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>]) <span class="comment"># col_indptr</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># row_indices</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csc_matrix((data, indices, indptr), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<div class="note info">
            <p><strong>Advantages</strong> of the CSC format</p><ul><li>efficient arithmetic operations CSC + CSC, CSC * CSC, etc.</li><li>efficient column slicing</li><li>fast matrix vector products (CSR, BSR may be faster)</li></ul><p><strong>Disadvantages</strong> of the CSC format</p><ul><li>slow row slicing operations (consider CSR)</li><li>changes to the sparsity structure are expensive (consider LIL or DOK)</li></ul>
          </div>
<h1 id="Shuffle-sparse-matrix"><a href="#Shuffle-sparse-matrix" class="headerlink" title="Shuffle sparse matrix"></a>Shuffle sparse matrix</h1><p>When generating batches of training/test data, <code>sklearn.utils.shuffle</code> can help.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating coo matrix</span></span><br><span class="line">x_coo = sp.coo_matrix((data, (row, col)), shape=shape)</span><br><span class="line"><span class="comment"># shuffle</span></span><br><span class="line">x, y, x_stat = shuffle(x, y, x_csr, random_state=seed)</span><br><span class="line"><span class="comment"># in a data generator</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">for</span> offset <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_samples, bsz):</span><br><span class="line">        x_batch = np.array(x[offset: offset + bsz])</span><br><span class="line">        y_batch = np.array(y[offset: offset + bsz])</span><br><span class="line">        x_csr_batch = x_stat[offset: offset + bsz, :] <span class="comment"># batch slice</span></span><br><span class="line">        <span class="comment"># yield x_batch, y_batch, adj_batch, x_csr_batch</span></span><br></pre></td></tr></table></figure>
<h1 id="Feed-to-TensorFlow"><a href="#Feed-to-TensorFlow" class="headerlink" title="Feed to TensorFlow"></a>Feed to TensorFlow</h1><h2 id="Convert-to-dense-matrix"><a href="#Convert-to-dense-matrix" class="headerlink" title="Convert to dense matrix"></a>Convert to dense matrix</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_dense_batch = x_csr_batch.todense()</span><br><span class="line"><span class="comment"># then feed to `tf.placeholder` </span></span><br></pre></td></tr></table></figure>
<h2 id="Feed-to-tf-sparse-placeholder"><a href="#Feed-to-tf-sparse-placeholder" class="headerlink" title="Feed to tf.sparse.placeholder"></a>Feed to <code>tf.sparse.placeholder</code></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.compat.v1.Session()</span><br><span class="line"></span><br><span class="line">sparse_input = tf.compat.v1.sparse.placeholder(tf.float32, (<span class="literal">None</span>, num_words), name=<span class="string">&#x27;sp_placeholder&#x27;</span>)</span><br><span class="line">x_coo = x_csr.tocoo()</span><br><span class="line">sp_vals = tf.compat.v1.SparseTensorValue(np.array([x_coo.row, x_coo.col]).T, x_coo.data, x_coo.shape)&#125;)</span><br><span class="line"><span class="comment"># feed dict</span></span><br><span class="line">sess.run(train_op, &#123;sparse_input: sp_vals&#125;)</span><br></pre></td></tr></table></figure>
<p>After feeding into the tensorflow computational graph, use <code>tf.sparse</code> since <em>SparseTensor</em> cannot be directly feed into a <em>Tensor</em> op! </p>
<ol>
<li><p>Use <code>tf.sparse</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([dim1, dim2], mean=<span class="number">.0</span>, stddev=<span class="number">.01</span>))</span><br><span class="line">b = tf.Variable(tf.random_normal([dim2], mean=<span class="number">.0</span>, stddev=<span class="number">.01</span>))</span><br><span class="line">sp_x = tf.sparse.sparse_dense_matmul(sparse_input, w) + b</span><br></pre></td></tr></table></figure>
</li>
<li><p>Convert the <code>SparseTensor</code> to dense <code>Tensor</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.sparse.to_dense(</span><br><span class="line">    sp_input, default_value=<span class="literal">None</span>, validate_indices=<span class="literal">True</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">-------------------</span></span><br><span class="line"><span class="string">    sp_input: SparseTensor</span></span><br><span class="line"><span class="string">    default_value: Scalar value to set for indices not specified in sp_input. Defaults to zero.</span></span><br><span class="line"><span class="string">    validate_indices: boolean (default True). If True, indices are checked to assure sorted lexicographic order and no repeats.</span></span><br><span class="line"><span class="string">    name (optional): a name prefix for returned tensors.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">dense_x = tf.sparse.to_dense(sparse_input)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.geeksforgeeks.org/sparse-matrix-representations-set-3-csr/">geeksforgeeks: sparse matrix</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Sparse_matrix">wiki: sparse matrix</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.dok_matrix.html">scipy dok_matrix</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html">scipy lil_matrix</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html">scipy coo_matrix</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy csr_matrix</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html">scipy csc_matrix</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Programming practical</category>
        <category>Numerical computation</category>
      </categories>
      <tags>
        <tag>Programming practical</tag>
        <tag>Numerical computation</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes of TensorFlow v1.x</title>
    <url>/notes/2019/02/28/Programming/TensorFlow-v1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Here is some useful notes/tricks of <code>Tensorflow 1.x</code>, a powerful deep learning framework developed by Google, Inc.<br><span id="more"></span></p>
<h1 id="Op"><a href="#Op" class="headerlink" title="Op"></a>Op</h1><h2 id="placeholder"><a href="#placeholder" class="headerlink" title="placeholder"></a>placeholder</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tf.placeholder_with_default(</span><br><span class="line">	input, shape, name=None</span><br><span class="line">)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">args:</span><br><span class="line">    input: a tensor. the default value to produce when output is not fed</span><br><span class="line">    shape: a `tf.TensorShape` / list(int), possibly partial shape</span><br><span class="line">    name(optional): tensor name</span><br><span class="line">returns:</span><br><span class="line">	a Tensor.</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><ol>
<li><p>eval() <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Stackoverflow: difference between `Session.run()` and `Tensor.eval()`](https://stackoverflow.com/questions/33610685/in-tensorflow-what-is-the-difference-between-session-run-and-tensor-eval)
">[2]</span></a></sup></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Tensor.<span class="built_in">eval</span>(feed_dict=<span class="literal">None</span>, session=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">args:</span></span><br><span class="line"><span class="string">    feed_dict: feed_dict like `session.run()`</span></span><br><span class="line"><span class="string">    session: specify the session to evaluate the tensor. If none, the default session will be used.</span></span><br><span class="line"><span class="string">   </span></span><br><span class="line"><span class="string">returns:</span></span><br><span class="line"><span class="string">	A numpy array of values</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>tf.group()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.group(</span><br><span class="line">    *inputs, name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">Args:</span><br><span class="line">	`*<span class="built_in">input</span>`: zero <span class="keyword">or</span> more tensors to group</span><br><span class="line">    `name`(optional): op name</span><br></pre></td></tr></table></figure>
</li>
<li><p>get the batch size from placeholder</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bsz = tf.shape(placeholder)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Tensor-manupulation"><a href="#Tensor-manupulation" class="headerlink" title="Tensor manupulation"></a>Tensor manupulation</h1><h2 id="tf-where"><a href="#tf-where" class="headerlink" title="tf.where"></a>tf.where</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.where(</span><br><span class="line">    condition, x=<span class="literal">None</span>, y=<span class="literal">None</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Args:</span></span><br><span class="line"><span class="string">    condition: bool tensor</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    If x== y == None, return the coordinates of true elements of condition. </span></span><br><span class="line"><span class="string">    Dim -&gt; (# of true elements, condition.shape[-1])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather"></a>tf.gather</h2><ul>
<li><code>tf.gather</code> slices the <code>params</code> with indix <code>indices</code> along <code>axis</code>. </li>
<li>output shape = <strong>params.shape[:axis] + </strong><code>indices.shape[batch_dims:]</code> <strong>+ params.shape[axis+1:]</strong>. The middle term indicates the axis to slice on.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.gather(</span><br><span class="line">    params, indices, axis=<span class="literal">None</span>, batch_dims=<span class="number">0</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">Args:</span><br><span class="line">    params: tensor to gather <span class="keyword">from</span></span><br><span class="line">    indices: tensor to index</span><br><span class="line">    axis: axis <span class="keyword">in</span> params to gather indices <span class="keyword">from</span>. Default the 1st non-batch dimension.</span><br><span class="line">    batch_dims: <span class="built_in">int</span>. &lt; rank(indices)</span><br></pre></td></tr></table></figure>
<h2 id="tf-gather-nd"><a href="#tf-gather-nd" class="headerlink" title="tf.gather_nd"></a>tf.gather_nd</h2><ul>
<li>Slice the <code>params</code> with the specified shape of <code>indices</code>.</li>
<li>Slice on the first <code>N</code> dims, where <code>N=indices.shape[-1]</code>. i.e., # of the slice op.</li>
<li>indices.shape[-1] &lt;= params.rank<ul>
<li>if equal, slice the element</li>
<li>if not equal, slice along the indices.shape[-1] axis.</li>
</ul>
</li>
<li>out.shape = <code>indices.shape[:-1</code>] + <strong>params.shape[indices.shape[-1]:]</strong>. <code>indices.shape[-1]</code> indicate the dim after slicing.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.gather_nd(</span><br><span class="line">    params, indices, batch_dims=<span class="number">0</span>, name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="matrix-mask"><a href="#matrix-mask" class="headerlink" title="matrix mask"></a>matrix mask</h2><ol>
<li>tf.max_band_part<br>Copy a tensor setting everything outside a central band in each innermost matrix to zero. That is, elements below the “num_lower” (if not -1) and above the “num_upper” are set to zeros.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">in_band(m, n) = (num_lower &lt; 0 || (m-n) &lt;= num_lower)) &amp;&amp; (num_upper &lt; 0 || (n-m) &lt;= num_upper</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># deprecated: tf.matrix_band_part</span></span><br><span class="line">tf.linalg.band_part(<span class="built_in">input</span>, num_lower, num_upper, name=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> input: tensor</span></span><br><span class="line"><span class="string"> num_lower: int</span></span><br><span class="line"><span class="string"> num_upper: int</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># example</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if &#x27;input&#x27; is [[ 0,  1,  2, 3]</span></span><br><span class="line"><span class="comment">#                [-1,  0,  1, 2]</span></span><br><span class="line"><span class="comment">#                [-2, -1,  0, 1]</span></span><br><span class="line"><span class="comment">#                [-3, -2, -1, 0]],</span></span><br><span class="line">                 </span><br><span class="line"><span class="comment"># tf.matrix_band_part(input, 1, -1) ==&gt; [[ 0,  1,  0, 0]</span></span><br><span class="line"><span class="comment">#                                        [-1,  0,  1, 0]</span></span><br><span class="line"><span class="comment">#                                        [ 0, -1,  0, 1]</span></span><br><span class="line"><span class="comment">#                                        [ 0,  0, -1, 0]],</span></span><br></pre></td></tr></table></figure>
<h1 id="Gradient-Clipping"><a href="#Gradient-Clipping" class="headerlink" title="Gradient Clipping"></a>Gradient Clipping</h1><h2 id="tf-clip-by-norm"><a href="#tf-clip-by-norm" class="headerlink" title="tf.clip_by_norm"></a>tf.clip_by_norm</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.clip_by_norm(t, clip_norm, axes=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: grad tensor to be clipped</span></span><br><span class="line"><span class="string">clip_norm: A maximum clipping norm</span></span><br><span class="line"><span class="string">axes: dimension for gradient clipping. Default None indicates all dimensions.</span></span><br><span class="line"><span class="string">name: op name (optional).</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t * clip_norm / l2norm(t)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">\text{clip_by_norm(t)}=\left\{
                \begin{array}{ll}
                  t * \frac{\text{clip norm}}{||t||_2} \quad & ||t||_2 \geq \text{clip norm} \\
                  t \quad &\text{otherwise}\\
                \end{array}
    \right.</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example</span></span><br><span class="line">opt = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-08</span>)</span><br><span class="line">grads_vals = self.opt.compute_gradients(self.loss)</span><br><span class="line">grads_vals = [(tf.clip_by_norm(g, clip_norm), v) <span class="keyword">for</span> g, v <span class="keyword">in</span> grads_vals <span class="keyword">if</span> g <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line">train_op = opt.apply_gradients(grads_vals)</span><br></pre></td></tr></table></figure>
<h2 id="tf-clip-by-global-norm"><a href="#tf-clip-by-global-norm" class="headerlink" title="tf.clip_by_global_norm"></a>tf.clip_by_global_norm</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.clip_by_global_norm(t_list, clip_norm, use_norm=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t_list:  a tuple or list of tensors</span></span><br><span class="line"><span class="string">clip_norm: A maximum clipping norm</span></span><br><span class="line"><span class="string">use_norm (optional): specify the global norm if already computed.</span></span><br><span class="line"><span class="string">name (optional): op name.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t_list[i] * clip_norm / <span class="built_in">max</span>(global_norm, clip_norm)</span><br><span class="line">global_norm = sqrt(<span class="built_in">sum</span>([l2norm(t)**<span class="number">2</span> <span class="keyword">for</span> t <span class="keyword">in</span> t_list]))</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">\text{clip_by_global_norm}(t_i)=\left\{
                \begin{array}{ll}
                  t_i * \frac{\text{clip norm}}{\text{avg norm}} \quad  &\text{avg norm} \geq \text{clip norm} \\
                  t_i \quad &\text{otherwise}\\
                \end{array}
    \right.
    \\
    \text{where} \quad \textit{ avg norm} = \sqrt{\sum_i \Vert t_i \Vert_2^2}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example</span></span><br><span class="line">opt = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-08</span>)</span><br><span class="line">params = tf.trainable_variables()</span><br><span class="line">grads = tf.gradients(self.loss, params)</span><br><span class="line">grads, grad_norm = tf.clip_by_global_norm(grads, clip_norm=<span class="number">5</span>)</span><br><span class="line">train_op = opt.apply_gradients(<span class="built_in">zip</span>(grads, params))</span><br></pre></td></tr></table></figure>
<h2 id="tf-clip-by-averge-norm"><a href="#tf-clip-by-averge-norm" class="headerlink" title="tf.clip_by_averge_norm"></a>tf.clip_by_averge_norm</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.clip_by_average_norm(t, clip_norm, name=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: grad tensor to be clipped</span></span><br><span class="line"><span class="string">clip_norm: A maximum clipping norm</span></span><br><span class="line"><span class="string">name: op name</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t * clip_norm / l2norm_avg(t)</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">\text{clip_by_global_norm}(t_i)=\left\{
                \begin{array}{ll}
                  t_i * \frac{\text{clip norm}}{\text{avg norm}} \quad  &\text{avg norm} \geq \text{clip norm} \\
                  t_i \quad &\text{otherwise}\\
                \end{array}
    \right.
    \\
    \text{where} \quad \textit{ avg norm} = \frac{\Vert t \Vert_2}{m} = \frac{\sqrt{\sum_{j=1}^m  t_j^2 }}{m}</script><p>where $m$ is the number of tensor $t$.</p>
<h2 id="tf-clip-by-value"><a href="#tf-clip-by-value" class="headerlink" title="tf.clip_by_value"></a>tf.clip_by_value</h2><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.clip_by_value(t, clip_value_min, clip_value_max, name=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: grad tensor</span></span><br><span class="line"><span class="string">clip_value_min: clip min value</span></span><br><span class="line"><span class="string">clip_value_max: clip max value</span></span><br><span class="line"><span class="string">name: op name</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t[t &gt; clip_value_max] = clip_value_max</span><br><span class="line">t[t &lt; clip_value_min] = clip_value_min</span><br></pre></td></tr></table></figure></h2><h1 id="tf-distributions"><a href="#tf-distributions" class="headerlink" title="tf.distributions"></a>tf.distributions</h1><h2 id="Multinormial"><a href="#Multinormial" class="headerlink" title="Multinormial"></a>Multinormial</h2><p>Sample from probs and get the probabilities</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line">probs = tf.constant([[<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.6</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 1: (recommended)</span></span><br><span class="line">multinom = tf.distributions.Multinomial(</span><br><span class="line">                        total_count=tf.constant(<span class="number">1</span>,dtype=tf.float32),  <span class="comment"># sample one for each record in the batch, that is [1, batch_size] </span></span><br><span class="line">                         probs=probs)</span><br><span class="line">sampled_actions = multinom.sample()  <span class="comment"># sample one action for data in the batch</span></span><br><span class="line">predicted_actions = tf.argmax(sampled_actions, axis=-<span class="number">1</span>) </span><br><span class="line">action_probs = sampled_actions * predicted_probs </span><br><span class="line">action_probs = tf.reduce_sum(action_probs, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2: use tf.gather_nd</span></span><br><span class="line">idx = tf.multinomial(probs, <span class="number">1</span>)</span><br><span class="line">row_indices = tf.<span class="built_in">range</span>(probs.get_shape()[<span class="number">0</span>], dtype=tf.int64)</span><br><span class="line">full_indices = tf.stack([row_indices, tf.squeeze(idx)], axis=<span class="number">1</span>)</span><br><span class="line">rs = tf.gather_nd(probs, full_indices)</span><br></pre></td></tr></table></figure>
<h1 id="Categorical"><a href="#Categorical" class="headerlink" title="Categorical"></a>Categorical</h1><p>Intuited as generating sample of <code>argmax&#123; OneHotCategorical(probs)&#125;</code> itself being identical to <code>argmax&#123; Multinomial(probs, total_count=1)&#125;</code>.</p>
<h1 id="Control-flow"><a href="#Control-flow" class="headerlink" title="Control flow"></a>Control flow</h1><p><img data-src="/notes/images/tf_control_flow.png" width="85%" /></p>
<center> image source: <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[TensorFlow: control flow](https://www.youtube.com/watch?v=IzKXEbpT9Lg)
">[5]</span></a></sup> </center>

<h2 id="tf-cond"><a href="#tf-cond" class="headerlink" title="tf.cond"></a>tf.cond</h2><ul>
<li>Forward pass <code>tf.cond(pred, fn1, fn2)</code></li>
<li>Gradient <code>tf.cond(pred, grad(fn1), grad(fn2))</code></li>
</ul>
<h2 id="tf-while-loop"><a href="#tf-while-loop" class="headerlink" title="tf.while_loop"></a>tf.while_loop</h2><ul>
<li>Forward pass <code>tf.while_loop(cond_fn, body_fn, loop_vars)</code> -&gt; executes N times </li>
<li>Gradient <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.while_loop(</span><br><span class="line">    <span class="keyword">lambda</span> i, g_vars: i&lt;N,</span><br><span class="line">    <span class="keyword">lambda</span> i, g_vars: (i+<span class="number">1</span>, grad(body_fn)(g_vars)), grad_ys</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img data-src="/notes/images/tf-control-flow.png" width="65%" /></p>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># used in the SeqGAN training.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">condition</span>(<span class="params">t, output_ta</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.less(t, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">body</span>(<span class="params">t, output_ta</span>):</span></span><br><span class="line">    <span class="comment"># write in the value at the $t$-th index of TensorArray </span></span><br><span class="line">    output_ta = output_ta.write(t, [<span class="number">2</span>,<span class="number">3</span>]) </span><br><span class="line">    <span class="keyword">return</span> t+<span class="number">1</span>, output_ta</span><br><span class="line"></span><br><span class="line">t = tf.constant(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># define TensorArray</span></span><br><span class="line">output_ta = tf.TensorArray(dtype=tf.float32, size=<span class="number">1</span>, dynamic_size=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># while_loop</span></span><br><span class="line">result = tf.while_loop(condition, body, loop_vars=[t, output_ta])</span><br><span class="line">last_t, last_out = result</span><br><span class="line"></span><br><span class="line">final_out = last_out.stack() <span class="comment"># return the value in the TensorArray</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h1 id="Eager-execution-mode"><a href="#Eager-execution-mode" class="headerlink" title="Eager execution mode"></a>Eager execution mode</h1><p>Eager execution mode support the dynamic graph and print out the values of tensors when creating graphs without using <code>tf.Session()</code> for version 1.x. <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Medium: 8-things-to-do-differently-in-tensorflows-eager-execution-mode](https://medium.com/coinmonks/8-things-to-do-differently-in-tensorflows-eager-execution-mode-47cf429aa3ad)
">[4]</span></a></sup></p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># start the file with</span></span><br><span class="line">tf.enable_eager_execution()</span><br></pre></td></tr></table></figure></h2><h1 id="Model-storage-and-load"><a href="#Model-storage-and-load" class="headerlink" title="Model storage and load"></a>Model storage and load</h1><h2 id="Load-trained-model"><a href="#Load-trained-model" class="headerlink" title="Load trained model"></a>Load trained model</h2><ul>
<li>Get undefined <code>Placeholder</code> op names<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">checkpoint_path = <span class="string">&quot;&lt;ckpt-file&gt;.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">&#x27;&lt;meta-file&gt;.meta&#x27;</span>)</span><br><span class="line">imported_graph = tf.get_default_graph()</span><br><span class="line"></span><br><span class="line">placeholders = [op <span class="keyword">for</span> op <span class="keyword">in</span> imported_graph.get_operations() <span class="keyword">if</span> op.<span class="built_in">type</span> == <span class="string">&quot;Placeholder&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(placeholders))</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> placeholders:</span><br><span class="line">    <span class="built_in">print</span>(p.name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h1><h2 id="Remote-connection"><a href="#Remote-connection" class="headerlink" title="Remote connection"></a>Remote connection</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># &lt;local_port&gt;: 16006</span></span><br><span class="line">$ ssh -L 16006:127.0.0.1:6006 &lt;account&gt;@&lt;server.address&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># at remote server</span></span><br><span class="line">(remote) $ tensorboard --logdir=<span class="string">&quot;&lt;./modeldir&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># local browser visit https://127.0.0.1:16006</span></span><br></pre></td></tr></table></figure>
<h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><h2 id="GPU-designation"><a href="#GPU-designation" class="headerlink" title="GPU designation"></a>GPU designation</h2><ul>
<li><p>Designate GPU in tf<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Stackoverflow: GPU memory allocation](https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory)
">[7]</span></a></sup><sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Stackoverflow: soft-placement and log devices](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac)">[8]</span></a></sup></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&quot;0,1,2,3&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">allow_soft_placement</span></span><br><span class="line"><span class="string">---------------------</span></span><br><span class="line"><span class="string">Whether soft placement is allowed. If allow_soft_placement is true,</span></span><br><span class="line"><span class="string">an op will be placed on CPU if</span></span><br><span class="line"><span class="string">1. there&#x27;s no GPU implementation for the OP</span></span><br><span class="line"><span class="string">or</span></span><br><span class="line"><span class="string">2. no GPU devices are known or registered</span></span><br><span class="line"><span class="string">or</span></span><br><span class="line"><span class="string">3. need to co-locate with reftype input(s) which are from CPU.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">config = tf.ConfigProto(</span><br><span class="line">    allow_soft_placement=<span class="literal">True</span>, <span class="comment"># allow soft device placement</span></span><br><span class="line">    log_device_placement=<span class="literal">True</span> <span class="comment"># Whether device placements should be logged.</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># allow growth</span></span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line"><span class="comment"># or assign fixed gpu memory</span></span><br><span class="line"><span class="comment"># config.gpu_options.per_process_gpu_memory_fraction = 0.4</span></span><br><span class="line">sess = tf.session(config=config, ...)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Check GPU usage</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure>
</li>
<li><p>Peridically watch the gpu usage:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># watch the gpu usage every 10 secs</span></span><br><span class="line">$ watch -n 10 nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># --loop</span></span><br><span class="line">$ nvidia-smi -l</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Log"><a href="#Log" class="headerlink" title="Log"></a>Log</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Level</th>
<th>Level for Humans</th>
<th>Level Description                  </th>
</tr>
</thead>
<tbody>
<tr>
<td>  0</td>
<td>DEBUG</td>
<td>[Default] Print all messages       </td>
</tr>
<tr>
<td>  1</td>
<td>INFO</td>
<td>Filter out INFO messages           </td>
</tr>
<tr>
<td>  2</td>
<td>WARNING</td>
<td>Filter out INFO &amp; WARNING messages </td>
</tr>
<tr>
<td>  3</td>
<td>ERROR</td>
<td>Filter out all messages      </td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># only output ERROR, omiting INFO and WARNING</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;3&#x27;</span>  <span class="comment"># default 0</span></span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information">Disable Tensorflow debugging information</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/33610685/in-tensorflow-what-is-the-difference-between-session-run-and-tensor-eval">Stackoverflow: difference between <code>Session.run()</code> and <code>Tensor.eval()</code></a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/59799955/tensorflow-tf-multinomial-get-the-associated-probabilities-failed">Stackoverflow: tf.multinormial</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://medium.com/coinmonks/8-things-to-do-differently-in-tensorflows-eager-execution-mode-47cf429aa3ad">Medium: 8-things-to-do-differently-in-tensorflows-eager-execution-mode</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.youtube.com/watch?v=IzKXEbpT9Lg">TensorFlow: control flow</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://towardsdatascience.com/tensorflow-control-flow-tf-cond-903e020e722a">Towardsdatascience: tensorflow control flow</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory">Stackoverflow: GPU memory allocation</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac">Stackoverflow: soft-placement and log devices</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tricks of Git</title>
    <url>/notes/2019/02/14/Programming/Tricks-of-Git/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Some useful tricks of <code>git</code>.</p>
<p><img data-src="/notes/images/git-img.png" alt="upload successful"></p>
<span id="more"></span>
<h2 id="Git-tricks"><a href="#Git-tricks" class="headerlink" title="Git tricks"></a>Git tricks</h2><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><ul>
<li>Too large git file pull error!<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git fatal: pack has bad object at offset XXX: inflate returned -5</span><br></pre></td></tr></table></figure>
Solution:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone --depth=1 &quot;&lt;url&gt;&quot;</span><br><span class="line">$ git remote set-branches origin &quot;&lt;remote_branch_name&gt;&quot;</span><br><span class="line">$ git fetch --depth 1 origin &quot;&lt;remote_branch_name&gt;&quot;</span><br><span class="line">$ git checkout &quot;&lt;remote_branch_name&gt;&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="ignore-file-after-pushing-to-remote-respository"><a href="#ignore-file-after-pushing-to-remote-respository" class="headerlink" title="ignore file after pushing to remote respository"></a>ignore file after pushing to remote respository</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git rm -r --cached .</span><br></pre></td></tr></table></figure>
<h3 id="Git-large-file-removal"><a href="#Git-large-file-removal" class="headerlink" title="Git large file removal"></a>Git large file removal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># find large files</span></span><br><span class="line">git rev-list --objects --all | grep <span class="string">&quot;<span class="subst">$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk &#x27;&#123;print$1&#125;&#x27;)</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete the log in the cache</span></span><br><span class="line">git filter-branch -f --prune-empty --index-filter <span class="string">&#x27;git rm -rf --cached --ignore-unmatch &lt;file-name&gt;&#x27;</span> --tag-name-filter cat -- --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># reflog</span></span><br><span class="line">rm -rf .git/refs/original/</span><br><span class="line">git reflog expire --expire=now --all</span><br><span class="line">git fsck --full --unreachable</span><br><span class="line">git repack -A -d</span><br><span class="line">git gc --aggressive --prune=now</span><br><span class="line"></span><br><span class="line"><span class="comment"># force push</span></span><br><span class="line">git push --force</span><br></pre></td></tr></table></figure>
<h2 id="Overwrite"><a href="#Overwrite" class="headerlink" title="Overwrite"></a>Overwrite</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">error: Untracked working tree file &#x27;xxx/xxx/xxx&#x27; would be overwritten by merge.</span><br></pre></td></tr></table></figure>
<ul>
<li>Force <code>git pull</code>  to overwrite with remote version<ol>
<li>regardless of the local changes.<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git fetch --all</span><br><span class="line"><span class="comment"># or (recommend. No need to fetch all branches)</span></span><br><span class="line">git fetch origin master</span><br><span class="line"></span><br><span class="line">git reset --hard origin/master</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">git reset --hard origin/<span class="string">&quot;&lt;branch_name&gt;&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>maintain current local commits<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git branch <span class="string">&quot;&lt;new-branch—to-save-current-commits&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">git fetch --all</span><br><span class="line">git reset --hard origin/master</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">git reset --hard origin/<span class="string">&quot;&lt;branch_name&gt;&quot;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>Uncommitted changes  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git stash <span class="comment"># stash uncommitted changes</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># reapply uncommitted changes</span></span><br><span class="line">git stash pop</span><br></pre></td></tr></table></figure></li>
<li>Git reset  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure></li>
<li>Git clean  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git clean</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Branch"><a href="#Branch" class="headerlink" title="Branch"></a>Branch</h2><h3 id="Create-new-branch-and-pull-to-remote"><a href="#Create-new-branch-and-pull-to-remote" class="headerlink" title="Create new branch and pull to remote"></a>Create new branch and pull to remote</h3>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git checkout -b <span class="string">&quot;&lt;new_branch&gt;&quot;</span></span><br><span class="line">git push origin <span class="string">&quot;&lt;new_branch&gt;&quot;</span>：<span class="string">&quot;&lt;remote_new_branch&gt;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="Delete-remote-branch"><a href="#Delete-remote-branch" class="headerlink" title="Delete remote branch"></a>Delete remote branch</h3><ol>
<li><p>Push an empty branch to remote</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push origin :<span class="string">&quot;&lt;remote_branch&gt;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>delete remote</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push origin --delete <span class="string">&quot;&lt;remote_branch&gt;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Git-pull-remote-branch"><a href="#Git-pull-remote-branch" class="headerlink" title="Git pull remote branch"></a>Git pull remote branch</h3>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git pull origin <span class="string">&quot;&lt;remote_branch&gt;&quot;</span>:<span class="string">&quot;&lt;local_branch&gt;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="Check"><a href="#Check" class="headerlink" title="Check"></a>Check</h2><ul>
<li><code>git diff</code>: check the local changes</li>
<li><code>git log</code>: check all committed history</li>
<li><code>git blame &lt;my_file&gt;</code>: who changes what in <my_file></li>
<li><code>git reflog</code>: shows the changes of log in local repository, used for looking for losing jobs.</li>
</ul>
<h2 id="Reset"><a href="#Reset" class="headerlink" title="Reset"></a>Reset</h2><p><code>git reset</code> vs <code>git checkout</code> vs <code>git revert</code></p>
<ul>
<li><code>git reset</code> and <code>git checkout</code> can apply on the commit stage or the single file, whilst <code>git revert</code> is only suitable for commit stage.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Discard commits in a private branch or throw away uncommitted changes</span></span><br><span class="line">git reset --hard HEAD</span><br><span class="line"></span><br><span class="line"><span class="comment"># discard changes in the working directory</span></span><br><span class="line">git checkout &lt;my commit&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Undo commits in a public branch</span></span><br><span class="line">git revert &lt;my commit&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>git clean</code> -n : delete local untracked files.</li>
</ul>
<h2 id="Clean-all-logs"><a href="#Clean-all-logs" class="headerlink" title="Clean all logs"></a>Clean all logs</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a tmp branch</span></span><br><span class="line">git checkout --orphan tmp</span><br><span class="line"><span class="comment"># add &amp; commit</span></span><br><span class="line">git add -A</span><br><span class="line">git commit -m <span class="string">&#x27;Clean log&#x27;</span></span><br><span class="line"><span class="comment"># delete master branch</span></span><br><span class="line">git branch -D master</span><br><span class="line"><span class="comment"># rename branch tmp to master</span></span><br><span class="line">git branch -m master</span><br><span class="line"><span class="comment"># push</span></span><br><span class="line">git push -f origin master</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Git-cheat-list"><a href="#Git-cheat-list" class="headerlink" title="Git cheat list"></a>Git cheat list</h2><h3 id="Create-repository"><a href="#Create-repository" class="headerlink" title="Create repository"></a>Create repository</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> &lt;url&gt; <span class="comment"># clone remote repository</span></span><br><span class="line">$ git init <span class="comment"># init local repository</span></span><br></pre></td></tr></table></figure>
<h3 id="Change-and-commit"><a href="#Change-and-commit" class="headerlink" title="Change and commit"></a>Change and commit</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git status <span class="comment"># check the status</span></span><br><span class="line">$ git diff <span class="comment"># check changes</span></span><br><span class="line">$ git add . <span class="comment"># track all changed files</span></span><br><span class="line">$ git add &lt;file&gt; <span class="comment"># track the specified file</span></span><br><span class="line">$ git mv &lt;old&gt; &lt;new&gt; <span class="comment"># rename file</span></span><br><span class="line">$ git rm &lt;file&gt; <span class="comment"># delete file</span></span><br><span class="line">$ git commit -m <span class="string">&quot;commit msg&quot;</span> <span class="comment"># commit all changed files</span></span><br><span class="line">$ git commit --amend <span class="comment"># amend the last commit</span></span><br></pre></td></tr></table></figure>
<h3 id="Check-commit-history"><a href="#Check-commit-history" class="headerlink" title="Check commit history"></a>Check commit history</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">log</span> <span class="comment"># show commit logs</span></span><br><span class="line">$ git <span class="built_in">log</span> -p &lt;file&gt; <span class="comment">#  show commit logs of &lt;file&gt;</span></span><br><span class="line">$ git blame &lt;file&gt; <span class="comment">#  show what revision and author last modified each line of a file</span></span><br></pre></td></tr></table></figure>
<h3 id="Undo"><a href="#Undo" class="headerlink" title="Undo"></a>Undo</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git reset --hard HEAD <span class="comment"># undo all uncommitted contents in the work directory</span></span><br><span class="line">$ git checkout HEAD &lt;file&gt; <span class="comment"># undo the changed contents of an uncommitted file</span></span><br><span class="line">$ git revert &lt;commit&gt; <span class="comment"># revert specified commit</span></span><br></pre></td></tr></table></figure>
<h3 id="Branch-and-Tag"><a href="#Branch-and-Tag" class="headerlink" title="Branch and Tag"></a>Branch and Tag</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git branch 	<span class="comment"># show all local branches</span></span><br><span class="line">$ git checkout &lt;branch/tag&gt; 	<span class="comment"># git checkout to the branch/tag</span></span><br><span class="line">$ git branch &lt;new-branch&gt;	<span class="comment"># create a new branch</span></span><br><span class="line">$ git branch -d &lt;branch&gt;	<span class="comment"># delete the local branch</span></span><br><span class="line">$ git tag	<span class="comment"># show all local tags</span></span><br><span class="line">$ git tag &lt;tag-name&gt;	<span class="comment"># create tag based on the latest commit</span></span><br><span class="line">$ git tag -d &lt;tag-name&gt; 	<span class="comment"># delete tag</span></span><br></pre></td></tr></table></figure>
<h3 id="Merge-and-rebase"><a href="#Merge-and-rebase" class="headerlink" title="Merge and rebase"></a>Merge and rebase</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git merge &lt;branch&gt; 	<span class="comment"># merge &lt;branch&gt; to the current branch</span></span><br><span class="line">$ git rebase &lt;branch&gt; 	<span class="comment"># rebase &lt;branch&gt; to the current branch</span></span><br></pre></td></tr></table></figure>
<h3 id="Remote"><a href="#Remote" class="headerlink" title="Remote"></a>Remote</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git remote -v		<span class="comment"># check remote repository information</span></span><br><span class="line">$ git remote show &lt;remote&gt; 	<span class="comment"># show specified remote repository information</span></span><br><span class="line">$ git remote add &lt;remote&gt; &lt;url&gt; 	<span class="comment"># add remote repository</span></span><br><span class="line"></span><br><span class="line">$ get fetch &lt;remote&gt;	<span class="comment"># fetch code from the remote repository</span></span><br><span class="line">$ git pull &lt;remote&gt; &lt;branch&gt; 	<span class="comment"># pull and quick merge</span></span><br><span class="line">$ git push &lt;remote&gt; &lt;branch&gt; 	<span class="comment"># push and quick merge</span></span><br><span class="line">$ git push &lt;remote&gt; :&lt;branch/tag-name&gt;	<span class="comment"># delete remote branch or tag</span></span><br><span class="line">$ git push --tags	<span class="comment"># push all tags</span></span><br></pre></td></tr></table></figure>
<h1 id="Change-remote-repository"><a href="#Change-remote-repository" class="headerlink" title="Change remote repository"></a>Change remote repository</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git remote set-url origin &lt;new-url&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">git remote rm origin</span><br><span class="line">git remote add origin &lt;new-url&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">vi .git/config</span><br><span class="line"><span class="comment"># modify the url of [remote &quot;origin&quot;]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Tricks of Python</title>
    <url>/notes/2019/03/03/Programming/Tricks-of-Python/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Useful usage of python.<br><span id="more"></span></p>
<h1 id="Advanced-Trick"><a href="#Advanced-Trick" class="headerlink" title="Advanced Trick"></a>Advanced Trick</h1><h2 id="Implement-Trie-Prefix-Tree"><a href="#Implement-Trie-Prefix-Tree" class="headerlink" title="Implement Trie (Prefix Tree)"></a>Implement Trie (Prefix Tree)</h2><ul>
<li>defaultdict</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">TrieNode = <span class="keyword">lambda</span>: defaultdict(TrieNode)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.trie = TrieNode()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        reduce(<span class="built_in">dict</span>.__getitem__, word, self.trie)[<span class="string">&#x27;end&#x27;</span>] = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="keyword">return</span> reduce(<span class="keyword">lambda</span> d,k: d[k] <span class="keyword">if</span> k <span class="keyword">in</span> d <span class="keyword">else</span> TrieNode(), word, self.trie).get(<span class="string">&#x27;end&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startsWith</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">bool</span>(reduce(<span class="keyword">lambda</span> d,k: d[k] <span class="keyword">if</span> k <span class="keyword">in</span> d <span class="keyword">else</span> TrieNode(), word, self.trie).keys())</span><br></pre></td></tr></table></figure>
<ul>
<li><code>dict</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrieNode</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize your data structure here.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.data = &#123;&#125;</span><br><span class="line">        self.is_word = <span class="literal">False</span></span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.root = TrieNode()</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Inserts a word into the trie.</span></span><br><span class="line"><span class="string">        :type word: str</span></span><br><span class="line"><span class="string">        :rtype: void</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">            child = node.data.get(letter)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> child:</span><br><span class="line">                node.data[letter] = TrieNode()</span><br><span class="line">            node = node.data[letter]</span><br><span class="line">        node.is_word = <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns if the word is in the trie.</span></span><br><span class="line"><span class="string">        :type word: str</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> node.is_word </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">starts_with</span>(<span class="params">self, prefix</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns if there is any word in the trie</span></span><br><span class="line"><span class="string">        that starts with the given prefix.</span></span><br><span class="line"><span class="string">        :type prefix: str</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> prefix:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_start</span>(<span class="params">self, prefix</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns words started with prefix</span></span><br><span class="line"><span class="string">        :param prefix:</span></span><br><span class="line"><span class="string">        :return: words (list)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_get_key</span>(<span class="params">pre, pre_node</span>):</span></span><br><span class="line">            words_list = []</span><br><span class="line">            <span class="keyword">if</span> pre_node.is_word:</span><br><span class="line">                words_list.append(pre)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> pre_node.data.keys():</span><br><span class="line">                words_list.extend(_get_key(pre + <span class="built_in">str</span>(x), pre_node.data.get(x)))</span><br><span class="line">            <span class="keyword">return</span> words_list</span><br><span class="line">  </span><br><span class="line">        words = []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.starts_with(prefix):</span><br><span class="line">            <span class="keyword">return</span> words</span><br><span class="line">        <span class="keyword">if</span> self.search(prefix):</span><br><span class="line">            words.append(prefix)</span><br><span class="line">            <span class="keyword">return</span> words</span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> prefix:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">        <span class="keyword">return</span> _get_key(prefix, node)</span><br></pre></td></tr></table></figure>
<h2 id="2D-list-to-1D"><a href="#2D-list-to-1D" class="headerlink" title="2D list to 1D"></a>2D list to 1D</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">matrix = [[<span class="number">1</span>,<span class="number">5</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">13</span>],[<span class="number">12</span>,<span class="number">13</span>,<span class="number">15</span>]]</span><br><span class="line"><span class="built_in">sum</span>(matrix, [])</span><br><span class="line"><span class="comment"># [1, 5, 9, 10, 11, 13, 12, 13, 15]</span></span><br></pre></td></tr></table></figure>
<h1 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h1><h2 id="Swap-value"><a href="#Swap-value" class="headerlink" title="Swap value"></a>Swap value</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python way to swap values</span></span><br><span class="line">a, b  = b, a</span><br></pre></td></tr></table></figure>
<h2 id="chain-comparison-with-ops"><a href="#chain-comparison-with-ops" class="headerlink" title="chain comparison with ops"></a>chain comparison with ops</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b=<span class="number">6</span></span><br><span class="line"><span class="number">1</span>==b&lt;<span class="number">38</span> <span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<h1 id="Statement"><a href="#Statement" class="headerlink" title="Statement"></a>Statement</h1><h2 id="For-else"><a href="#For-else" class="headerlink" title="For else"></a>For else</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> l:</span><br><span class="line">	<span class="keyword">if</span> e == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">else</span>: <span class="comment"># called if for loop does not reach break statement</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;did not break out of for loop&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="List"><a href="#List" class="headerlink" title="List"></a>List</h1><h2 id="list-to-str"><a href="#list-to-str" class="headerlink" title="list to str"></a>list to str</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = [<span class="string">&quot;Python&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;awesome&quot;</span>]</span><br><span class="line"><span class="string">&quot; &quot;</span>.join(l)</span><br><span class="line"></span><br><span class="line">l = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">5</span>))</span><br><span class="line"><span class="string">&quot;,&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, l))</span><br></pre></td></tr></table></figure>
<h2 id="2d-array-transpose"><a href="#2d-array-transpose" class="headerlink" title="2d array transpose"></a>2d array transpose</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = [[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>], [<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>],[<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;f&#x27;</span>]]</span><br><span class="line">transposed = <span class="built_in">list</span>(<span class="built_in">zip</span>(*arr))</span><br></pre></td></tr></table></figure>
<h2 id="most-frequent-element-in-a-list"><a href="#most-frequent-element-in-a-list" class="headerlink" title="most frequent element in a list"></a>most frequent element in a list</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>]</span><br><span class="line"><span class="built_in">max</span>(<span class="built_in">set</span>(a), key=a.count)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; use Counter from collections&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">cnt = Counter(a)</span><br><span class="line">cnt.most_common(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="copy-array"><a href="#copy-array" class="headerlink" title="copy array"></a>copy array</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># a fast way of shallow copy of a list</span></span><br><span class="line">b = a[:]</span><br><span class="line"></span><br><span class="line">b = a.copy() <span class="comment"># python3 only</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># copy nested lists using copy.deepcopy</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">l2 = copy.deepcopy(l)</span><br></pre></td></tr></table></figure>
<h2 id="Remove-duplicates-in-list"><a href="#Remove-duplicates-in-list" class="headerlink" title="Remove duplicates in list"></a>Remove duplicates in list</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">items = [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">set</span>(items))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; remove dups and keep order &quot;&quot;&quot;</span></span><br><span class="line">items = [<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>]</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="built_in">list</span>(OrderedDict.fromkeys(items).keys())</span><br></pre></td></tr></table></figure>
<h2 id="Find-index-of-Min-Max"><a href="#Find-index-of-Min-Max" class="headerlink" title="Find index of Min/Max"></a>Find index of Min/Max</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minIndex</span>(<span class="params">lst</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">min</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(lst)), key=lst.__getitem__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxIndex</span>(<span class="params">lst</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">max</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(lst)), key=lst.__getitem__)</span><br><span class="line"></span><br><span class="line">minIndex(l)</span><br><span class="line">maxIndex(l)</span><br></pre></td></tr></table></figure>
<h1 id="Str"><a href="#Str" class="headerlink" title="Str"></a>Str</h1><h2 id="check-if-strs-are-permutation"><a href="#check-if-strs-are-permutation" class="headerlink" title="check if strs are permutation"></a>check if strs are permutation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">Counter(str1) == Counter(str2)</span><br></pre></td></tr></table></figure>
<h2 id="Reverse-string"><a href="#Reverse-string" class="headerlink" title="Reverse string"></a>Reverse string</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a =<span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">a[::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;</span>.join([ch <span class="keyword">for</span> ch <span class="keyword">in</span> <span class="built_in">reversed</span>(a)])</span><br></pre></td></tr></table></figure>
<h2 id="find-index"><a href="#find-index" class="headerlink" title="find index"></a>find index</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check the index of a char, else -1</span></span><br><span class="line">a =<span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">a.find(a) <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<h1 id="Dict"><a href="#Dict" class="headerlink" title="Dict"></a>Dict</h1><h2 id="Dict-get"><a href="#Dict-get" class="headerlink" title="Dict get"></a>Dict get</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line">d.get(<span class="string">&#x27;c&#x27;</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="sort-dict"><a href="#sort-dict" class="headerlink" title="sort dict"></a>sort dict</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sorted</span>(d.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="built_in">sorted</span>(d.items(), key=itemgetter(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(d, key=d.get)</span><br></pre></td></tr></table></figure>
<h2 id="merge-dict"><a href="#merge-dict" class="headerlink" title="merge dict"></a>merge dict</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d1 = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">d2 = &#123;<span class="string">&#x27;b&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;**d1, **d2&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">dict</span>(d1.items() | d2.items())</span><br><span class="line"></span><br><span class="line">d1.update(d2)</span><br></pre></td></tr></table></figure>
<h2 id="kwargs"><a href="#kwargs" class="headerlink" title="**kwargs"></a>**kwargs</h2><p><code>dictionary.pop(keyname, defaultvalue)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fn</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    v1 = kwargs.pop(<span class="string">&quot;k1&quot;</span>, <span class="string">&#x27;default1&#x27;</span>)</span><br><span class="line">    v2 = kwargs.pop(<span class="string">&quot;k2&quot;</span>, <span class="string">&#x27;default2&#x27;</span>)</span><br><span class="line">    v3 = kwargs.pop(<span class="string">&quot;k3&quot;</span>, <span class="string">&#x27;default3&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Builtin-functions"><a href="#Builtin-functions" class="headerlink" title="Builtin functions"></a>Builtin functions</h1><h2 id="Binary-Octal-Hexadecimal-integers"><a href="#Binary-Octal-Hexadecimal-integers" class="headerlink" title="Binary, Octal, Hexadecimal integers"></a>Binary, Octal, Hexadecimal integers</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># base 10 (decimal)</span></span><br><span class="line">dec = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">base 2 (binary)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># decimal to binary (base 2)</span></span><br><span class="line">bin_dec = <span class="built_in">bin</span>(dec) <span class="comment"># str: &#x27;0b101&#x27;</span></span><br><span class="line"><span class="comment"># binary to decimal</span></span><br><span class="line">dec = <span class="built_in">int</span>(bin_dec, <span class="number">2</span>) <span class="comment"># int: 5</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">base 8 (octal)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># decimal to octal (base 8)</span></span><br><span class="line">oct_dec = <span class="built_in">oct</span>(dec) <span class="comment"># str: &#x27;05&#x27;</span></span><br><span class="line"><span class="comment"># octal to decimal</span></span><br><span class="line">dec = <span class="built_in">int</span>(oct_dec, <span class="number">8</span>) <span class="comment"># int: 5</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">base 16 (hexadecimal)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># decimal to hexadecimal (base 16)</span></span><br><span class="line">hex_dec = <span class="built_in">hex</span>(dec) <span class="comment"># str: &#x27;0x5&#x27;</span></span><br><span class="line"><span class="comment"># hexadecimal to decimal</span></span><br><span class="line">dec = <span class="built_in">int</span>(hex_dec, <span class="number">16</span>) <span class="comment"># int: 5</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Bit-wise-Op"><a href="#Bit-wise-Op" class="headerlink" title="Bit-wise Op"></a>Bit-wise Op</h2><div class="note info">
            <ul><li><code>&amp;</code>: binary <code>AND</code>. Op copies a bit to the result if it exists in both operands.</li><li><code>|</code>: binary <code>OR</code>. It copies a bit to the result if it exists in either operand.</li><li><code>^</code>: binary <code>XOR</code>. It copies a bit if it is set in one operand but not both.</li><li><p><code>~</code>: binary Ones Complement. It is unary and has the effect of ‘flipping’ bits.</p></li><li><p><code>&lt;&lt;</code>: binary left shift.</p></li><li><code>&gt;&gt;</code>: binary right shift.</li></ul>
          </div>
<h2 id="Array-index"><a href="#Array-index" class="headerlink" title="Array index"></a>Array index</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">~i <span class="comment"># -i-1</span></span><br><span class="line">arr[~i] <span class="comment"># is equal to arr[-i-1]</span></span><br></pre></td></tr></table></figure>
<h2 id="r-application"><a href="#r-application" class="headerlink" title="\r application"></a><code>\r</code> application</h2><ul>
<li><code>\r</code> puts the location of the cursor at the beginning of the current row.</li>
<li><code>\b</code> backtracks one bit of the cursor.</li>
</ul>
<h3 id="Count-down"><a href="#Count-down" class="headerlink" title="Count down"></a>Count down</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">count_down = <span class="number">10</span>  <span class="comment"># count down total time ( seconds )</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count_down, <span class="number">0</span>, -<span class="number">1</span>):</span><br><span class="line">    msg = <span class="string">u&quot;\r System will exit in &quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;secs&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(msg, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">end_msg = <span class="string">&quot;end&quot;</span> + <span class="string">&quot;  &quot;</span>*(<span class="built_in">len</span>(msg)-<span class="built_in">len</span>(<span class="string">&quot;end&quot;</span>))  <span class="comment"># cover all the console output with whitespace </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;\r&quot;</span>+end_msg)</span><br></pre></td></tr></table></figure>
<h3 id="Progress-bar"><a href="#Progress-bar" class="headerlink" title="Progress bar"></a>Progress bar</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">count_down = <span class="number">10</span>  </span><br><span class="line">interval = <span class="number">1</span>  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">int</span>(count_down/interval)+<span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\r&quot;</span>+<span class="string">&quot;▇&quot;</span>*i+<span class="string">&quot; &quot;</span>+<span class="built_in">str</span>(i*<span class="number">10</span>)+<span class="string">&quot;%&quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    time.sleep(interval)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n Finished&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Loading"><a href="#Loading" class="headerlink" title="Loading"></a>Loading</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">count_down = <span class="number">10</span> </span><br><span class="line">interval = <span class="number">0.25</span>  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">int</span>(count_down/interval)):</span><br><span class="line">    ch_list = [<span class="string">&quot;\\&quot;</span>, <span class="string">&quot;|&quot;</span>, <span class="string">&quot;/&quot;</span>, <span class="string">&quot;-&quot;</span>]</span><br><span class="line">    index = i % <span class="number">4</span></span><br><span class="line">    msg = <span class="string">&quot;\r Program is running &quot;</span> + ch_list[index]</span><br><span class="line">    <span class="built_in">print</span>(msg, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    time.sleep(interval)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;\r Finished&quot;</span> + <span class="string">&quot;  &quot;</span>*<span class="built_in">len</span>(msg))</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="Functional-programming"><a href="#Functional-programming" class="headerlink" title="Functional programming"></a>Functional programming</h1><h2 id="Advanced-function"><a href="#Advanced-function" class="headerlink" title="Advanced function"></a>Advanced function</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a><code>map</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x*x</span><br><span class="line">    </span><br><span class="line">r = <span class="built_in">map</span>(f, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># =&gt; Iterator</span></span><br><span class="line"><span class="built_in">list</span>(r) <span class="comment"># [1,4,9,16]</span></span><br></pre></td></tr></table></figure>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a><code>reduce</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reduce(f, [x1,x2,x3]) &lt;=&gt; f(f(x1, x2), x3) </span><br></pre></td></tr></table></figure>
<ul>
<li>Application: <strong>str2int</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"></span><br><span class="line">DIGITS = &#123;<span class="string">&#x27;0&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;1&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;2&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;3&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;4&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;5&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;6&#x27;</span>:<span class="number">6</span>, <span class="string">&#x27;7&#x27;</span>:<span class="number">7</span>, <span class="string">&#x27;8&#x27;</span>:<span class="number">8</span>, <span class="string">&#x27;9&#x27;</span>:<span class="number">9</span>, <span class="string">&#x27;0&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">char2num</span>(<span class="params">s</span>):</span></span><br><span class="line">	<span class="keyword">return</span> DIGIT[s]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2int</span>(<span class="params">s</span>):</span></span><br><span class="line">	<span class="keyword">return</span> reduce(<span class="keyword">lambda</span> x,y: x*<span class="number">10</span>+y, <span class="built_in">map</span>(char2sum, s))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a><code>filter</code></h3><p><code>filter</code> filters out sequence by applying on each element and selecting by return value (True means keep that element)</p>
<ul>
<li><p>Filter out even numbers:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_odd</span>(<span class="params">n</span>):</span></span><br><span class="line">	<span class="keyword">return</span> n%<span class="number">2</span> == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">filter</span>(is_odd, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]))</span><br><span class="line"><span class="comment"># [1,3,5]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Get the prime number</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_odd_iter</span>():</span></span><br><span class="line">	n=<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    	n += <span class="number">2</span></span><br><span class="line">        <span class="keyword">yield</span> n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_not_divisible</span>(<span class="params">n</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">lambda</span> x: x%n &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">primes</span>():</span></span><br><span class="line">	<span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    it = _odd_iter()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    	n = <span class="built_in">next</span>(it)</span><br><span class="line">        <span class="keyword">yield</span> n</span><br><span class="line">        it = <span class="built_in">filter</span>(_not_divisible(n), it)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> primes():</span><br><span class="line">	<span class="keyword">if</span> n&lt;<span class="number">1000</span>:</span><br><span class="line">    	<span class="built_in">print</span>(n)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    	<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a><code>sorted</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sorted</span>([<span class="string">&#x27;bob&#x27;</span>, <span class="string">&#x27;about&#x27;</span>, <span class="string">&#x27;Zoo&#x27;</span>, <span class="string">&#x27;Credit&#x27;</span>], key=<span class="built_in">str</span>.lower, reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># [&#x27;Zoo&#x27;, &#x27;Credit&#x27;, &#x27;bob&#x27;, &#x27;about&#x27;]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Decorator"><a href="#Decorator" class="headerlink" title="Decorator"></a>Decorator</h2><ul>
<li>Print the call time of a function<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_metric</span>(<span class="params">func</span>):</span></span><br><span class="line">	<span class="comment"># print the call time and call counts</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    	<span class="keyword">nonlocal</span> count</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        data = func(*args, **kwargs)</span><br><span class="line">        time_delta = time.time() - start_time</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Call &#123;&#125; times. It costs &#123;&#125; secs&quot;</span>.<span class="built_in">format</span>(count, time_delta))</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@time_metric</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>():</span></span><br><span class="line">    time.sleep(<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h1 id="Deep-copy-and-shallow-copy"><a href="#Deep-copy-and-shallow-copy" class="headerlink" title="Deep copy and shallow copy"></a>Deep copy and shallow copy</h1><ul>
<li><code>=</code> operator only creates a new variable that shares the <strong>reference</strong> of the original object.</li>
<li><code>Deep copy</code> is a process in which the copy process occurs <strong>recursively</strong>. It first construct a new collection object and then recursively populating it with copies of the child objects found in the original. Any changes made to a copy of object do not reflect in the original object.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,[<span class="number">3</span>,<span class="number">4</span>],<span class="number">5</span>]</span><br><span class="line">l_deep = copy.deepcopy(l) <span class="comment"># deep copy</span></span><br><span class="line">l[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">6</span> </span><br><span class="line"><span class="built_in">print</span>(l)      <span class="comment"># [1,2,[6,4],5]</span></span><br><span class="line"><span class="built_in">print</span>(l_deep) <span class="comment"># [1,2,[3,4],5]</span></span><br></pre></td></tr></table></figure></li>
<li><code>Shallow copy</code> constructs a new collection object and then populating it with references to the children objects found in the original. The copying process <strong>does not recurse</strong> and therefore <strong>won’t create copies of the child objects themselves</strong>.</li>
</ul>
<p><img data-src="/notes/images/shallow-copy.png" alt="upload successful"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,[<span class="number">3</span>,<span class="number">4</span>],<span class="number">5</span>]</span><br><span class="line">l_deep = copy.copy(l) <span class="comment"># shallow copy</span></span><br><span class="line">l[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">6</span> </span><br><span class="line"><span class="built_in">print</span>(l)      <span class="comment"># [1,2,[6,4],5]</span></span><br><span class="line"><span class="built_in">print</span>(l_deep) <span class="comment"># [1,2,[6,4],5]</span></span><br></pre></td></tr></table></figure>
<h1 id="multiprocess-v-s-threading"><a href="#multiprocess-v-s-threading" class="headerlink" title="multiprocess v.s. threading"></a>multiprocess v.s. threading</h1><div class="note success">
            <h2 id="multiprocessing"><a href="#multiprocessing" class="headerlink" title="multiprocessing"></a>multiprocessing</h2><p><strong>Pros</strong>:</p><ul><li>Separate memory space</li><li>Takes advantages of multiple CPUs and cores</li><li>Avoids <strong>GIL</strong>(<code>Global Interpreter Lock</code>) limitations for cPython</li><li>Children processes are interruptible</li><li>A must with cPython for <strong>CPU-bound</strong> processing</li></ul><p><strong>Cons</strong>:</p><ul><li>IPC a little more complicated with more overhead</li><li>Larger memory footprint</li></ul>
          </div>
<div class="note info">
            <h2 id="threading"><a href="#threading" class="headerlink" title="threading"></a>threading</h2><p><strong>Pros</strong>:</p><ul><li>Lightweight -&gt; low memory footprint</li><li><strong>Shared memory</strong> -&gt; makes access to state from another context easier</li><li>Allows you to easily make responsive UIs</li><li>cPython C extension modules that properly release the GIL will run in parallel</li><li>Great option for <strong>I/O-bound</strong> applications</li></ul><p><strong>Cons</strong>:</p><ul><li>CPython -&gt; subject to the GIL</li><li>Not interruptible/killable</li></ul>
          </div>
<hr>
<h1 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># approach 1</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.choice(items, p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># approach 2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_select</span>(<span class="params">items, p</span>):</span></span><br><span class="line">    x = random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    cum_p = <span class="number">.0</span></span><br><span class="line">    <span class="keyword">for</span> item, prob <span class="keyword">in</span> <span class="built_in">zip</span>(items, p):</span><br><span class="line">        cum_p += prob</span><br><span class="line">        <span class="keyword">if</span> x &lt; cum_p:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h1 id="Pathlib"><a href="#Pathlib" class="headerlink" title="Pathlib"></a>Pathlib</h1><p>The python std library <code>Pathlib</code> module can be used to manipulate windows paths on a Unix machine(or vice versa). It can also be used to manipulate paths without accessing the <code>os</code> module.<br><img data-src="/notes/images/python-pathlib.png" alt="upload successful"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="comment"># listing subdirectories</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = Path(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x <span class="keyword">for</span> x <span class="keyword">in</span> p.iterdir() <span class="keyword">if</span> x.is_dir()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># listing py src files in the directory tree</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(p.glob(<span class="string">&#x27;**/*.py&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># navigate inside a directory tree</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = Path(<span class="string">&#x27;/etc&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q = p / <span class="string">&#x27;init.d&#x27;</span> / <span class="string">&#x27;reboot&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q</span><br><span class="line">PosixPath(<span class="string">&#x27;/etc/init.d/reboot&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># query path properties</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q.exists()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q.is_dir()</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># open a file</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> q.<span class="built_in">open</span>() <span class="keyword">as</span> f: f.readline()</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.name <span class="comment"># filename</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.stem <span class="comment"># filename without suffix</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.suffix <span class="comment"># file suffix</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.parent <span class="comment"># the dirname</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create directory</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p=Path(<span class="string">r&#x27;./a/b/c&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.mkdir(exist_ok=<span class="literal">True</span>) <span class="comment"># create if the parent directory exists</span></span><br><span class="line"><span class="comment">## iteratively create the directory</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.mkdir(exist_ok=<span class="literal">True</span>, parents=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># file details</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.stat() <span class="comment"># detail information</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.stat().st_size <span class="comment"># file size</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.stat().st_ctime <span class="comment"># create time</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.stat().st_mtime <span class="comment"># modified time</span></span><br></pre></td></tr></table></figure>
<h1 id="itertools"><a href="#itertools" class="headerlink" title="itertools"></a>itertools</h1><h2 id="itertools-count"><a href="#itertools-count" class="headerlink" title="itertools.count()"></a>itertools.count()</h2><p>Iteration over infinite numbers</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">import</span> itertools</span><br><span class="line">$ results = []</span><br><span class="line">$ <span class="keyword">for</span> x <span class="keyword">in</span> itertools.count(start=<span class="number">0</span>):</span><br><span class="line">$     <span class="keyword">if</span> &lt;condition&gt;:</span><br><span class="line">$        <span class="keyword">break</span></span><br><span class="line">$     results.append(&lt;element&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="itertools-permutations"><a href="#itertools-permutations" class="headerlink" title="itertools.permutations()"></a>itertools.permutations()</h2><p><code>itertools.permutations(iterable, r=None)</code></p>
<ul>
<li>Return successive $r$ length permutations of elements in the iterable. If None, then $r$ defaults to the length of the iterable and alll possible full-length permutations.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># permutations(&#x27;ABCD&#x27;, 2) --&gt; AB AC AD BA BC BD CA CB CD DA DB DC</span></span><br><span class="line"><span class="comment"># permutations(range(3)) --&gt; 012 021 102 120 201 210</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="itertools-zip-longest"><a href="#itertools-zip-longest" class="headerlink" title="itertools.zip_longest()"></a>itertools.zip_longest()</h2><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Similar to zip but return the longest length of inputs with filling default values, whereas zip returns the shortest length</span></span><br><span class="line">itertools.zip_longest(*iterables, fillvalue=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></h2><h1 id="File"><a href="#File" class="headerlink" title="File"></a>File</h1><h2 id="os"><a href="#os" class="headerlink" title="os"></a>os</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filepath=<span class="string">&#x27;../test/test/test.txt&#x27;</span></span><br><span class="line"><span class="comment"># split filename</span></span><br><span class="line">os.path.splittext(filepath) <span class="comment"># (&#x27;../test/test&#x27;, &#x27;test.txt&#x27;)</span></span><br><span class="line"><span class="comment"># split extention name</span></span><br><span class="line">os.path.splitext(filepath) <span class="comment"># (&#x27;../test/test/test&#x27;, &#x27;.txt&#x27;)</span></span><br></pre></td></tr></table></figure>
<h2 id="glob"><a href="#glob" class="headerlink" title="glob"></a>glob</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># search for files</span></span><br><span class="line">glob.glob(<span class="string">&#x27;dir/subdir/*.txt&#x27;</span>) <span class="comment"># return *.txt in the directory &#x27;dir/subdir/&#x27;</span></span><br></pre></td></tr></table></figure>
<hr>
<h1 id="My-utils"><a href="#My-utils" class="headerlink" title="My utils"></a>My utils</h1><h2 id="echo-with-colors"><a href="#echo-with-colors" class="headerlink" title="echo with colors"></a>echo with colors</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_format_table</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    prints table of formatted text format options</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> style <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">        <span class="keyword">for</span> fg <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>, <span class="number">38</span>):</span><br><span class="line">            s1 = <span class="string">&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">for</span> bg <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">40</span>, <span class="number">48</span>):</span><br><span class="line">                <span class="built_in">format</span> = <span class="string">&#x27;;&#x27;</span>.join([<span class="built_in">str</span>(style), <span class="built_in">str</span>(fg), <span class="built_in">str</span>(bg)])</span><br><span class="line">                s1 += <span class="string">&#x27;\x1b[%sm %s \x1b[0m&#x27;</span> % (<span class="built_in">format</span>, <span class="built_in">format</span>)</span><br><span class="line">            <span class="built_in">print</span>(s1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cyk_print</span>(<span class="params">content, font_color=<span class="string">&#x27;green&#x27;</span>, bg_color=<span class="literal">None</span>, display=<span class="string">&#x27;highlight&#x27;</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> font_color <span class="keyword">or</span> bg_color <span class="keyword">or</span> display <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    display_map = &#123;<span class="string">&#x27;default&#x27;</span>: <span class="number">0</span>, <span class="string">&quot;highlight&quot;</span>: <span class="number">1</span>, <span class="string">&quot;underline&quot;</span>: <span class="number">4</span>, <span class="string">&quot;flicker&quot;</span>: <span class="number">5</span>, <span class="string">&quot;reverse&quot;</span>: <span class="number">7</span>, <span class="string">&quot;invisible&quot;</span>: <span class="number">8</span>&#125;</span><br><span class="line">    font_map = &#123;<span class="string">&#x27;black&#x27;</span>: <span class="number">30</span>, <span class="string">&#x27;red&#x27;</span>: <span class="number">31</span>, <span class="string">&#x27;green&#x27;</span>: <span class="number">32</span>, <span class="string">&#x27;yellow&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;blue&#x27;</span>: <span class="number">34</span>, <span class="string">&#x27;purple&#x27;</span>: <span class="number">35</span>, <span class="string">&#x27;yank&#x27;</span>: <span class="number">36</span>,</span><br><span class="line">                <span class="string">&#x27;white&#x27;</span>: <span class="number">37</span>&#125;</span><br><span class="line">    bg_map = &#123;<span class="string">&#x27;black&#x27;</span>: <span class="number">40</span>, <span class="string">&#x27;red&#x27;</span>: <span class="number">41</span>, <span class="string">&#x27;green&#x27;</span>: <span class="number">42</span>, <span class="string">&#x27;yellow&#x27;</span>: <span class="number">43</span>, <span class="string">&#x27;blue&#x27;</span>: <span class="number">44</span>, <span class="string">&#x27;purple&#x27;</span>: <span class="number">45</span>, <span class="string">&#x27;yank&#x27;</span>: <span class="number">46</span>,</span><br><span class="line">              <span class="string">&#x27;white&#x27;</span>: <span class="number">47</span>&#125;</span><br><span class="line">    pp = <span class="string">&quot;\033[&#123;&#125;&#123;&#125;&#123;&#125;m&#123;&#125;\033[0m&quot;</span>.<span class="built_in">format</span>(display_map.get(display, <span class="string">&#x27;highlight&#x27;</span>),</span><br><span class="line">                                        <span class="string">&quot;;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(font_map.get(font_color, <span class="number">30</span>)),</span><br><span class="line">                                        <span class="string">&quot;;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(bg_map.get(bg_color, <span class="string">&#x27;&#x27;</span>)), content)</span><br><span class="line">    <span class="built_in">print</span>(pp)</span><br></pre></td></tr></table></figure>
<h2 id="print-name-and-cost-time-of-a-function"><a href="#print-name-and-cost-time-of-a-function" class="headerlink" title="print name and cost time of a function"></a>print name and cost time of a function</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printer</span>(<span class="params">f</span>):</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">f</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">60</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Start to run \033[1;36m&#123;&#125;\033[0m ...&quot;</span>.<span class="built_in">format</span>(f.__name__))</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        r = f(*args, **kwargs)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;the result is \033[1;31m&#123;:.4f&#125;\033[0m&#x27;</span>.<span class="built_in">format</span>(r), end=<span class="string">&#x27;, &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;time cost: &#123;:.5g&#125; \033[1msecs\033[0m&quot;</span>.<span class="built_in">format</span>(time.time() - t1))</span><br><span class="line">        <span class="comment"># return r</span></span><br><span class="line">    <span class="keyword">return</span> wrapped</span><br><span class="line"></span><br><span class="line"><span class="meta">@printer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">...</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    func()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h1><h2 id="Default-argument-is-mutable"><a href="#Default-argument-is-mutable" class="headerlink" title="Default argument is mutable"></a>Default argument is mutable</h2><p>When we assign the default argument values, avoid using mutable object, like list(), dict().</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_f</span>(<span class="params">item, my_list=[]</span>):</span></span><br><span class="line">	my_list.append(item)</span><br><span class="line">    <span class="built_in">print</span>(my_list)</span><br><span class="line">    </span><br><span class="line">error_f(<span class="string">&#x27;a&#x27;</span>) [<span class="string">&#x27;a&#x27;</span>]</span><br><span class="line"></span><br><span class="line">error_f(<span class="string">&#x27;b&#x27;</span>) <span class="comment"># [&#x27;a&#x27;, &#x27;b&#x27;]</span></span><br><span class="line"></span><br><span class="line">error_f(<span class="string">&#x27;c&#x27;</span>) <span class="comment"># [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]</span></span><br></pre></td></tr></table></figure>
<div class="note warning">
            <ul><li>Python’s default arguments are evaluated once when the function is defined, not each time the function is called (like it is in say, Ruby). This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.</li><li><strong>Important warning</strong>: The default value is <strong>evaluated only once</strong>. This makes a difference when the default is a mutable object such as a <em>list</em>, <em>dictionary</em>, or instances of most classes.</li></ul>
          </div>
<hr>
<h1 id="WTF"><a href="#WTF" class="headerlink" title="WTF"></a>WTF</h1><h2 id="Deleting-a-list-item-while-iterating"><a href="#Deleting-a-list-item-while-iterating" class="headerlink" title="Deleting a list item while iterating"></a>Deleting a list item while iterating</h2><div class="note warning">
            <p><strong> Never change the object you’re iterating over!</strong></p>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">l2 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">l3 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">l4 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(l1):</span><br><span class="line">	<span class="keyword">del</span> item</span><br><span class="line">   </span><br><span class="line"><span class="keyword">for</span> idx, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(l2):</span><br><span class="line">	l2.remove(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the correct way to change the object you iterate over</span></span><br><span class="line"><span class="keyword">for</span> idx, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(l3[:]):</span><br><span class="line">	l3.remove(item)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(l4):</span><br><span class="line">	l4.pop(idx)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; l1</span><br><span class="line">[1, 2, 3, 4]</span><br><span class="line">&gt;&gt;&gt; l2</span><br><span class="line">[2, 4]</span><br><span class="line">&gt;&gt;&gt; l3</span><br><span class="line">[]</span><br><span class="line">&gt;&gt;&gt; l4</span><br><span class="line">[2, 4]</span><br></pre></td></tr></table></figure>
<div class="note info">
            <p><strong>Difference between <code>del</code>, <code>remove</code> and <code>pop</code></strong>:</p><ul><li><code>del var_name</code> just removes the binding of the <code>var_name</code> form the local/global namespace</li><li><code>remove</code> removes the fist matching value (raise <strong>ValueError</strong> if the value is not found)</li><li><code>pop</code> removes the element at a specific index and returns it (raise <strong>IndexError</strong> if an invalid index)</li></ul>
          </div>
<div class="note danger">
            <p><strong>Why the output is <code>[2,4]</code> ?</strong> <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[StackOverflow: what-happens-when-you-try-to-delete-a-list-element-while-iterating-over-it](https://stackoverflow.com/questions/45946228/what-happens-when-you-try-to-delete-a-list-element-while-iterating-over-it)">[2]</span></a></sup></p><ul><li>The list iteration is done index by index. When removing <code>1</code> from <code>l1</code> or <code>l4</code>, the contents of lists are <code>[2,3,4]</code>. The remaining elements are shifted down, i.e. <code>2</code> is at index 0, <code>3</code> at index 1. Since the next iteration is going to loo at index 1 (element <code>3</code>), the <code>2</code> is skipped entirely.</li></ul>
          </div>
<h1 id="Run"><a href="#Run" class="headerlink" title="Run"></a>Run</h1><p><code>stderr</code> directly outputs to the screen, whilst the <code>stdout</code> is usually saved in the buffer util full.<br><code>python -u</code> can directly print out the <code>stdout</code> without buffers.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># unbuffered</span></span><br><span class="line">python -u &lt;xxx&gt;.py </span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python">StackOverflow: Multiprocessing vs threading in python</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://stackoverflow.com/questions/45946228/what-happens-when-you-try-to-delete-a-list-element-while-iterating-over-it">StackOverflow: what-happens-when-you-try-to-delete-a-list-element-while-iterating-over-it</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.python.org/3.6/library/pathlib.html">Pathlib module</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://docs.python.org/3.6/library/index.html">Python 3.6 standard library</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Tricks of Latex</title>
    <url>/notes/2019/02/14/Toolkit/Tricks-of-Latex/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Hints on LaTeX.</p>
<span id="more"></span>
<h1 id="Tricks-of-Latex"><a href="#Tricks-of-Latex" class="headerlink" title="Tricks of Latex"></a>Tricks of Latex</h1><h2 id="Symbols"><a href="#Symbols" class="headerlink" title="Symbols"></a>Symbols</h2><p>$\mathcal{D}$<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$\mathcal&#123;D&#125;$</span><br></pre></td></tr></table></figure></p>
<p>$\mathscr{D}$<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% Euscript</span><br><span class="line">$\mathscr&#123;D&#125;$</span><br></pre></td></tr></table></figure></p>
<p>$\Rightarrow$<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$\Rightarrow$</span><br></pre></td></tr></table></figure></p>
<p>$\nRightarrow$<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$\nRightarrow$</span><br></pre></td></tr></table></figure><br><a href="https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols">overleaf link</a></p>
<h2 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h2><h3 id="Left-brace-of-multiple-lines"><a href="#Left-brace-of-multiple-lines" class="headerlink" title="Left brace of multiple lines"></a>Left brace of multiple lines</h3><script type="math/tex; mode=display">
f(x)=\left\{
                \begin{array}{ll}
                  x\\
                  y...\\
                  z...
                \end{array}
    \right.</script><h2 id="Greek-alphabet"><a href="#Greek-alphabet" class="headerlink" title="Greek alphabet"></a>Greek alphabet</h2><p><img data-src="/notes/images/Latex-Greek-symbols.png" alt="upload successful"></p>
<h2 id="Equations"><a href="#Equations" class="headerlink" title="Equations"></a>Equations</h2><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://mohu.org/info/symbols/symbols.htm">math Latex</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.mohu.org/info/lshort-cn.pdf">Latex pdf doc (in Chinese)</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://juejin.im/post/5c0a27ee6fb9a049d05d8b70">Latex blog (in CHN)</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Latex</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title>An Introduction to Bloom Filter</title>
    <url>/notes/2019/12/22/Algorithms/Bloom-filter/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>When we check and filter out the duplicates for a web crawler, bloom filter is a good choice to curtail the memory cost. Here is a brief introduction.<br><span id="more"></span></p>
<h1 id="Bloom-filter"><a href="#Bloom-filter" class="headerlink" title="Bloom filter"></a>Bloom filter</h1><p><strong>Bloom filter</strong> is a space-efficient probabilistic data structure to test the existence of an element. The response of bloom filter for each query is either “possibly in set” or “definitely not in set”.</p>
<ul>
<li>Drawbacks: Elements inside the set can be supplemented, but cannot be removed. “The more items added, the larger the probability of flase positives.”<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[wiki: Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter)
">[1]</span></a></sup></li>
</ul>
<h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><p>An empty bloom filter is a 1d boolean array with a length of $m$. In addition, $m$ different hash function are used to map the input query elements to one of the $m$ bits.</p>
<p>To query for an element, it firstly computes $k$ hash values with different hash functions, and then divides $m$ to get $k$ remainders, indicating $k$ positions in the array. </p>
<ul>
<li>If any of the bits at these positions is 0, it means the element is definitely not in the set. Fill out the corresponding position with 1s when the element is added. </li>
<li>If all of the mod bits are 1, there are two possiblities:<ol>
<li>True positive: the element is already in the set</li>
<li>False positive: bits have by chance been set to 1 during the insertion of other elements.</li>
</ol>
</li>
</ul>
<p><img data-src="/notes/images/Bloom filter.png" width="70%"/></p>
<ul>
<li>Let $n$ be the # of elements, the flase positive probability $\epsilon$, the array length $m$ should be (see <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[wiki: Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter)
">[1]</span></a></sup> for the derivation):<script type="math/tex; mode=display">m = -\frac{n \ln \epsilon}{(\ln 2)^2}</script></li>
</ul>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Bloomfilter</span>(<span class="params"><span class="built_in">filter</span>: <span class="string">&quot;bit array&quot;</span>, value: <span class="string">&quot;elements&quot;</span>, hash_fns: <span class="string">&quot;hash functions&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; insert elemeents &quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="built_in">len</span>(<span class="built_in">filter</span>)</span><br><span class="line">    <span class="keyword">for</span> fn <span class="keyword">in</span> hash_fns:</span><br><span class="line">        idx = fn(value) % m</span><br><span class="line">        <span class="built_in">filter</span>[idx] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">filter</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span>(<span class="params"><span class="built_in">filter</span>, value, hash_fns</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; check the existence &quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="built_in">len</span>(<span class="built_in">filter</span>)</span><br><span class="line">    <span class="keyword">for</span> fn <span class="keyword">in</span> hash_fns:</span><br><span class="line">        idx = fn(value) % m</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">filter</span>[idx]:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span>  <span class="comment"># only true negative</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span>  <span class="comment"># involving false positive + true negative</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Bloom_filter">wiki: Bloom filter</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://hackernoon.com/probabilistic-data-structures-bloom-filter-5374112a7832">blog: Bloom filter</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Bloom filter</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>Bloom filter</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode: partition equal subset sum</title>
    <url>/notes/2019/09/12/Algorithms/LeetCode-partition-equal-subset-sum/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><a href="https://leetcode.com/problems/partition-equal-subset-sum/">LeetCode 416.Partition Equal Subset Sum</a><br><span id="more"></span></p>
<h1 id="LeetCode-416-Partition-Equal-Subset-Sum"><a href="#LeetCode-416-Partition-Equal-Subset-Sum" class="headerlink" title="LeetCode 416.Partition Equal Subset Sum"></a>LeetCode 416.Partition Equal Subset Sum</h1><p>Given a <strong>non-empty</strong> array containing <strong>only positive integers</strong>, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal.</p>
<p>Note:</p>
<ul>
<li>Each of the array element will not exceed 100.</li>
<li>The array size will not exceed 200.</li>
</ul>
<p>This can be regarded as finding a subset whose sum is equal to a specified number (half of the sum here).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canPartition</span>(<span class="params">self, nums</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sum_ = <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span> &amp; <span class="number">1</span>: <span class="comment"># odd number</span></span><br><span class="line">        	<span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        target = sum_ &gt;&gt; <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        dp = [<span class="literal">True</span>] + [<span class="literal">False</span>] * target</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(num, target+<span class="number">1</span>)): </span><br><span class="line">                <span class="comment"># target&lt;=i&lt;=num -&gt; i-num&gt;=0</span></span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i-num]</span><br><span class="line">        <span class="keyword">return</span> dp[target]</span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-494-Target-Sum"><a href="#LeetCode-494-Target-Sum" class="headerlink" title="LeetCode 494. Target Sum"></a>LeetCode 494. Target Sum</h1><p>You are given a list of non-negative integers, a1, a2, …, an, and a target, S. Now you have 2 symbols + and -. For each integer, you should choose one from + and - as its new symbol.</p>
<p>Find out how many ways to assign symbols to make sum of integers equal to target S.<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Examples:</span><br><span class="line">- Input: nums is [1, 1, 1, 1, 1], S is 3. </span><br><span class="line">- Output: 5</span><br><span class="line">- Explanation: </span><br><span class="line">  -1+1+1+1+1 = 3</span><br><span class="line">  +1-1+1+1+1 = 3</span><br><span class="line">  +1+1-1+1+1 = 3</span><br><span class="line">  +1+1+1-1+1 = 3</span><br><span class="line">  +1+1+1+1-1 = 3</span><br><span class="line">  </span><br><span class="line">There are 5 ways to assign symbols to make the sum of nums be target 3.</span><br></pre></td></tr></table></figure></p>
<p>This can be converted to a <code>subset sum</code> problem:<br>Let $P$ and $N$ denote positive subset and negative subset,</p>
<script type="math/tex; mode=display">\begin{align}
\text{sum}(P) - \text{sum}(N) &= \text{target}\\
    \text{sum}(P) + \text{sum}(N) + \text{sum}(P) - \text{sum}(N) &= \text{target} + \text{sum}(P) + \text{sum}(N)\\
    2 * \text{sum}(P) &= \text{target} + \text{sum}(\text{nums})
\end{align}</script><p>So the original problem has been converted to a subset sum problem as follows:<br>Find a subset P of nums such that <code>sum(P) = (target + sum(nums)) / 2</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findTargetSumWays</span>(<span class="params">self, nums, S</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type S: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sum_ = <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> sum_ &lt; S <span class="keyword">or</span> (sum_+S) &amp; <span class="number">1</span>: <span class="comment"># if sum &lt; S or it is odd</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># convert to subset sum problem</span></span><br><span class="line">            target = (sum_ + S) &gt;&gt; <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> self.subsetSum(nums, target)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">subsetSum</span>(<span class="params">self, nums, s</span>):</span></span><br><span class="line">        dp = [<span class="number">1</span>] + [<span class="number">0</span>]*s</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(num, s+<span class="number">1</span>)):</span><br><span class="line">                dp[i] += dp[i-num]</span><br><span class="line">        <span class="keyword">return</span> dp[s]</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://leetcode.com/problems/partition-equal-subset-sum/discuss/90592/01-knapsack-detailed-explanation">LeetCode 416: 0/1 knapsack detailed explanation</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://leetcode.com/problems/target-sum/discuss/97334/Java-(15-ms)-C%2B%2B-(3-ms)-O(ns)-iterative-DP-solution-using-subset-sum-with-explanation">Leetcode 494: iterative DP solution</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Algorithms</category>
        <category>LeetCode</category>
        <category>Dynamic Programming</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Dynamic Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>Sorting Algorithms</title>
    <url>/notes/2019/07/07/Algorithms/sorting-alg/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A summary of sorting algorithms.<br><span id="more"></span></p>
<h1 id="Basic-sorting-algorithms"><a href="#Basic-sorting-algorithms" class="headerlink" title="Basic sorting algorithms"></a>Basic sorting algorithms</h1><div class="table-container">
<table>
<thead>
<tr>
<th>Algorithms</th>
<th>Time complexity(Avg)</th>
<th>Time complexity(worst)</th>
<th>Time complexity(best)</th>
<th>Space complexity</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bubble Sort</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(n)$</td>
<td>$O(1)$</td>
<td>Stable</td>
</tr>
<tr>
<td>Selection Sort</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(1)$</td>
<td>Untable</td>
</tr>
<tr>
<td>Insertion Sort</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(n)$</td>
<td>$O(1)$</td>
<td>Stable</td>
</tr>
<tr>
<td>Shell Sort</td>
<td>$O(n^1.3)$</td>
<td>$O(n^2)$</td>
<td>$O(n)$</td>
<td>$O(1)$</td>
<td>Unstable</td>
</tr>
<tr>
<td>Merge Sort</td>
<td>$O(n \log n)$</td>
<td>$O(n \log n)$</td>
<td>$O(n \log n)$</td>
<td>$O(n)$</td>
<td>stable</td>
</tr>
<tr>
<td>Heap Sort</td>
<td>$O(n \log n)$</td>
<td>$O(n \log n)$</td>
<td>$O(n \log n)$</td>
<td>$O(1)$</td>
<td>Unstable</td>
</tr>
<tr>
<td>Quick Sort</td>
<td>$O(n \log n)$</td>
<td>$O(n^2)$</td>
<td>$O(n \log n)$</td>
<td>$O(n \log n)$</td>
<td>Unstable</td>
</tr>
<tr>
<td>Count Sort</td>
<td>$O(n+k)$</td>
<td>-</td>
<td>-</td>
<td>$O(1)$</td>
<td>Stable</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Bubble-Sort"><a href="#Bubble-Sort" class="headerlink" title="Bubble Sort"></a>Bubble Sort</h2><p>Idea: Bubble the least element to the front position $i=j-1$ at each episode $j$.(j= 1,2,…, len(A))</p>
<ul>
<li>Time complexity: $O(n^2)$</li>
<li>Space Complexity: $O(1)$, in place<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BubbleSort</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Time Complexity: O(n^2)</span></span><br><span class="line"><span class="string">    Space Complexity: O(1), in place</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :return: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)):  <span class="comment"># len(A)-1 episodes</span></span><br><span class="line">    	<span class="comment"># bubble the least element to the most front position, at i-th index</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A) - <span class="number">1</span>, j, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> A[i] &lt; A[i - <span class="number">1</span>]:</span><br><span class="line">                A[i], A[i - <span class="number">1</span>] = A[i - <span class="number">1</span>], A[i]</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Bubble-Sort-with-swap-flag"><a href="#Bubble-Sort-with-swap-flag" class="headerlink" title="Bubble Sort with swap flag"></a>Bubble Sort with swap flag</h3><ul>
<li><strong>Problems</strong>: The above version of bubble sort would always do len(A)-1 times episodes even though all elements are sorted before that.</li>
<li><strong>Solution</strong>: set a flag to record whether there exists swap during the current episode. If there is no swap in the previous iteration, ilustrating that elements are already in order, it is no need to continue the bubble process.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BubbleSort_with_flag</span>(<span class="params">A:<span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Time Complexity: O(n^2)</span></span><br><span class="line"><span class="string">    Space Complexity: O(1), in place</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :return: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)):  <span class="comment"># len(A)-1 episodes</span></span><br><span class="line">        flag = <span class="literal">False</span>  <span class="comment"># flag of whether to swap</span></span><br><span class="line">        <span class="comment"># bubble the least element to the most front position, at i-th index</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A) - <span class="number">1</span>, j, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> A[i] &lt; A[i - <span class="number">1</span>]:</span><br><span class="line">                A[i], A[i - <span class="number">1</span>] = A[i - <span class="number">1</span>], A[i]</span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># if no swap, all following elements are already in order</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> flag: <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> A</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Selection-Sort"><a href="#Selection-Sort" class="headerlink" title="Selection Sort"></a>Selection Sort</h2><p>Process: find the least value from A[i:] and swap it with the front (i-th) element at $i$-th episode. (i= 1,2,…, len(A)).</p>
<ul>
<li>Time complexity: $O(n^2)$</li>
<li>Space Complexity: $O(1)$, in place</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SelectionSort</span>(<span class="params">A: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Time Complexity: O(n^2)</span></span><br><span class="line"><span class="string">    Space Complexity: O(1), in place</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :return: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A) - <span class="number">1</span>):</span><br><span class="line">        min_index = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(A)):</span><br><span class="line">            <span class="keyword">if</span> A[j] &lt; A[min_index]:</span><br><span class="line">                min_index = j</span><br><span class="line">        <span class="comment"># find the least value to swap in each episode</span></span><br><span class="line">        A[i], A[min_index] = A[min_index], A[i]</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
<h2 id="Insertion-Sort"><a href="#Insertion-Sort" class="headerlink" title="Insertion Sort"></a>Insertion Sort</h2><p>Process: keep the former section in order. From the index 1 on, insert the current value at the correct place in previous section.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InsertionSort</span>(<span class="params">A: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Time Complexity: O(n^2)</span></span><br><span class="line"><span class="string">    Space Complexity: O(1), in place</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :return: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(A)):</span><br><span class="line">        key = A[j]</span><br><span class="line">        i = j - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> key &lt; A[i]:</span><br><span class="line">            A[i + <span class="number">1</span>] = A[i]</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">        A[i + <span class="number">1</span>] = key</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure></p>
<h2 id="Shell-Sort"><a href="#Shell-Sort" class="headerlink" title="Shell Sort"></a>Shell Sort</h2><p>Process: divide the array into len(A)//$h$ groups, and sort separately; the reduce the step $h$ and sort, until the step size reaches to 1. </p>
<ul>
<li>It can be seen as an improvement of Insertion Sort.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShellSort</span>(<span class="params">A: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :return: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = <span class="built_in">len</span>(A)</span><br><span class="line">    h = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> h &lt; n / <span class="number">3</span>:</span><br><span class="line">        h = <span class="number">3</span> * h + <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> h &gt;= <span class="number">1</span>:</span><br><span class="line">    	<span class="comment"># insertion sort</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h, n):</span><br><span class="line">            j = i</span><br><span class="line">            <span class="keyword">while</span> j &gt;= h <span class="keyword">and</span> A[j] &lt; A[j - h]:</span><br><span class="line">                A[j], A[j - h] = A[j - h], A[j]</span><br><span class="line">                j -= h</span><br><span class="line">        h //= <span class="number">3</span></span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h2><p>Idea: divide-and-conquer.</p>
<ul>
<li>Time Complexity: $O(n \log n)$</li>
<li>Space Complexity: $O(n)$</li>
</ul>
<p><img data-src="/notes/images/alg-MergeSort.png" alt="upload successful"></p>
<p>The array version of merge sort:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span>(<span class="params">L, R</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param L: List(int)</span></span><br><span class="line"><span class="string">    :param R: List(int)</span></span><br><span class="line"><span class="string">    :return c: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c = []</span><br><span class="line">    i = j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(L) + <span class="built_in">len</span>(R)):</span><br><span class="line">        <span class="comment"># check whether L or R reaches its end</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="built_in">len</span>(L):</span><br><span class="line">            c.append(R[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> j == <span class="built_in">len</span>(R):</span><br><span class="line">            c.append(L[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> L[i] &lt;= R[j]:</span><br><span class="line">            c.append(L[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c.append(R[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MergeSort</span>(<span class="params">A: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    merge sort from top down (with recursion)</span></span><br><span class="line"><span class="string">    Time Complexity: O(NlogN)</span></span><br><span class="line"><span class="string">    Space Complexity: O(n)</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :return: List(int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(A) &lt;= <span class="number">1</span>: <span class="keyword">return</span> A</span><br><span class="line">    mid = <span class="built_in">len</span>(A) &gt;&gt; <span class="number">1</span></span><br><span class="line">    left = MergeSort(A[:mid])</span><br><span class="line">    right = MergeSort(A[mid:])</span><br><span class="line">    <span class="keyword">return</span> merge(left, right)</span><br></pre></td></tr></table></figure></p>
<p>Linked List version <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Leetcode 148. Sort List](https://leetcode.com/problems/sort-list/)">[7]</span></a></sup> :<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.val = x</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sortList</span>(<span class="params">self, head</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type head: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head <span class="keyword">or</span> <span class="keyword">not</span> head.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        <span class="comment"># divide the linked list into two parts</span></span><br><span class="line">        slow, fast = head, head.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">        second = slow.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># cut down the fist part</span></span><br><span class="line">        slow.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">        l = self.sortList(head)</span><br><span class="line">        r = self.sortList(second)</span><br><span class="line">        <span class="keyword">return</span> self.merge(l, r)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge</span>(<span class="params">self, l, r</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        merge sort helper func</span></span><br><span class="line"><span class="string">        :param l: the left part of linked list</span></span><br><span class="line"><span class="string">        :param r: the right part of linked list</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> l <span class="keyword">or</span> <span class="keyword">not</span> r:</span><br><span class="line">            <span class="keyword">return</span> l <span class="keyword">or</span> r</span><br><span class="line">        <span class="keyword">if</span> l.val &gt; r.val:</span><br><span class="line">            l, r = r, l</span><br><span class="line">        head = pre = l  <span class="comment"># define the return head node</span></span><br><span class="line">        l = l.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> l <span class="keyword">and</span> r:</span><br><span class="line">            <span class="keyword">if</span> l.val &lt; r.val:</span><br><span class="line">                pre.<span class="built_in">next</span> = l</span><br><span class="line">                l = l.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pre.<span class="built_in">next</span> = r</span><br><span class="line">                r = r.<span class="built_in">next</span></span><br><span class="line">            pre = pre.<span class="built_in">next</span></span><br><span class="line">        <span class="comment"># at least one of l or r is None</span></span><br><span class="line">        pre.<span class="built_in">next</span> = l <span class="keyword">or</span> r</span><br><span class="line">        <span class="keyword">return</span> head</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="Heap-Sort"><a href="#Heap-Sort" class="headerlink" title="Heap Sort"></a>Heap Sort</h2><p>Process:</p>
<ol>
<li>Build a max heap</li>
<li>Swap the top element of max heap with the end element, heaptify the max heap followed by heapsize -1 , until the array in order.</li>
</ol>
<ul>
<li>Time complexity: $O(n \log n)$</li>
<li>Space complexity: $O(1)$, in place</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Parent</span>(<span class="params">i: <span class="built_in">int</span></span>):</span> <span class="keyword">return</span> (i - <span class="number">1</span>) &gt;&gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Left</span>(<span class="params">i: <span class="built_in">int</span></span>):</span> <span class="keyword">return</span> <span class="number">2</span> * (i + <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Right</span>(<span class="params">i: <span class="built_in">int</span></span>):</span> <span class="keyword">return</span> <span class="number">2</span> * (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_heapify</span>(<span class="params">A: <span class="built_in">list</span>, i: <span class="built_in">int</span>, heap_size: <span class="built_in">int</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    heapify the max heap, so that the root is the maximum</span></span><br><span class="line"><span class="string">    :param A: heap to be heapify</span></span><br><span class="line"><span class="string">    :param i: the parent node index</span></span><br><span class="line"><span class="string">    :param heap_size: size of the heap to heapify</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    l, r = Left(i), Right(i)  <span class="comment"># left and right node</span></span><br><span class="line">    <span class="keyword">if</span> l &lt; heap_size <span class="keyword">and</span> A[l] &gt; A[i]:</span><br><span class="line">        <span class="comment"># if the left node is larger</span></span><br><span class="line">        largest = l</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        largest = i</span><br><span class="line">    <span class="keyword">if</span> r &lt; heap_size <span class="keyword">and</span> A[r] &gt; A[largest]:</span><br><span class="line">        <span class="comment"># if the right node is larger</span></span><br><span class="line">        largest = r</span><br><span class="line">    <span class="keyword">if</span> largest != i:  <span class="comment"># if not satisfies max heap property</span></span><br><span class="line">        A[i], A[largest] = A[largest], A[i]  <span class="comment"># swap the max value with the root value</span></span><br><span class="line">        max_heapify(A, largest, heap_size)  <span class="comment"># recursively heapify the swapped node</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_max_heap</span>(<span class="params">A: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    build the max heap</span></span><br><span class="line"><span class="string">    O(log n)</span></span><br><span class="line"><span class="string">    :param A:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    heap_size = <span class="built_in">len</span>(A)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)//<span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        max_heapify(A, i, heap_size)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HeapSort</span>(<span class="params">A: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Time complexity: O(NlogN)</span></span><br><span class="line"><span class="string">    :param A: array to be sorted</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># build the max heap so that the top element is the maximum</span></span><br><span class="line">    build_max_heap(A)</span><br><span class="line">    heap_size = <span class="built_in">len</span>(A)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A) - <span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>):  <span class="comment"># from the tail of array to traverse</span></span><br><span class="line">        <span class="comment"># swap the maximum with the array tail</span></span><br><span class="line">        A[<span class="number">0</span>], A[i] = A[i], A[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># exclude the final element (i.e. maximum) in the current heap</span></span><br><span class="line">        heap_size -= <span class="number">1</span></span><br><span class="line">        <span class="comment"># heapify thus the maximum float upwards to the top</span></span><br><span class="line">        max_heapify(A, <span class="number">0</span>, heap_size)</span><br></pre></td></tr></table></figure>
<h2 id="Quick-Sort"><a href="#Quick-Sort" class="headerlink" title="Quick Sort"></a>Quick Sort</h2><p>Process: divide-and-conquer</p>
<ol>
<li>Randomly choose one num as the pivot element A[r]</li>
<li>Put elements less than A[r] on the left side, whilst numbers no less than A[r] on the righ side</li>
<li>Repeat the step2, until there is only one element on the left and right side.</li>
</ol>
<ul>
<li>Time complexity: $O(n \log n)$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span>(<span class="params">A: <span class="built_in">list</span>, p: <span class="built_in">int</span>, r: <span class="built_in">int</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    partition for quick sort</span></span><br><span class="line"><span class="string">    :param A: List(int)</span></span><br><span class="line"><span class="string">    :param p: index of the left boundary</span></span><br><span class="line"><span class="string">    :param r: index of the pivot element</span></span><br><span class="line"><span class="string">    :return: index of pivot element separating the left and right,</span></span><br><span class="line"><span class="string">            where the left values are less than or equal to it</span></span><br><span class="line"><span class="string">            whilst the right are greater than it.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = A[r]</span><br><span class="line">    i = p - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(p, r):</span><br><span class="line">        <span class="keyword">if</span> A[j] &lt;= x:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            A[j], A[i] = A[i], A[j]</span><br><span class="line">    A[r], A[i + <span class="number">1</span>] = A[i + <span class="number">1</span>], A[r]</span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QuickSort</span>(<span class="params">A: <span class="built_in">list</span>, p: <span class="built_in">int</span>, r: <span class="built_in">int</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Quick Sort</span></span><br><span class="line"><span class="string">    Time complexity: O(NlogN)</span></span><br><span class="line"><span class="string">    :param A: List(int), array to be sorted</span></span><br><span class="line"><span class="string">    :param p: index of the left boundary</span></span><br><span class="line"><span class="string">    :param r: index of the right boundary</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> p &lt; r:</span><br><span class="line">        q = partition(A, p, r)</span><br><span class="line">        QuickSort(A, p, q - <span class="number">1</span>)</span><br><span class="line">        QuickSort(A, q + <span class="number">1</span>, r)</span><br></pre></td></tr></table></figure>
<h2 id="Bucket-sort"><a href="#Bucket-sort" class="headerlink" title="Bucket sort"></a>Bucket sort</h2><p>Bucket Sort is a kind of non-comparison sorting algorithms.</p>
<h3 id="Count-Sort"><a href="#Count-Sort" class="headerlink" title="Count Sort"></a>Count Sort</h3><p>Process:</p>
<ol>
<li>find the maximum $k$ of the array, and create $k+1$ new buckets</li>
<li>Traverse the array for one pass, put the elements into the buckets</li>
<li>Traverse all the buckets, take the values from buckets.</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CountSort</span>(<span class="params">arr: <span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Time complexity: O(n+k), where k is the max value of array elements.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    k = <span class="built_in">max</span>(arr)</span><br><span class="line">    B = [<span class="literal">None</span>] *(<span class="built_in">len</span>(arr) + <span class="number">1</span>) <span class="comment"># the 1st element is an index placeholder</span></span><br><span class="line">    C = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k + <span class="number">1</span>)] <span class="comment"># save the count (max index) of nums at index num in C</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        C[arr[j]] += <span class="number">1</span> <span class="comment"># count, e.g. A[1]=2, means there is two 1s in total</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        C[i] = C[i] + C[i - <span class="number">1</span>] <span class="comment"># the maximum index with starting index 1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        B[C[arr[j]]] = arr[j] <span class="comment"># B index : C values, B values: C index</span></span><br><span class="line">        C[arr[j]] -= <span class="number">1</span> <span class="comment"># C values -1</span></span><br><span class="line">    <span class="keyword">return</span> B[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<h1 id="Advance"><a href="#Advance" class="headerlink" title="Advance"></a>Advance</h1><h2 id="Dutch-national-flag-problem"><a href="#Dutch-national-flag-problem" class="headerlink" title="Dutch national flag problem"></a>Dutch national flag problem</h2><div class="note info">
            <p><strong>Dutch national flag problem</strong>:</p><ul><li>Given an array with $n$ objects colored red, white or blue, sort them <code>in-place</code> so that objects of the same color are adjacent, with the colors in the order red, white and blue. Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively.<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki:Dutch national flag problem](https://en.wikipedia.org/wiki/Dutch_national_flag_problem)">[1]</span></a></sup><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Leetcode 75 Sort Colors](https://leetcode.com/problems/sort-colors)">[4]</span></a></sup></li></ul>
          </div>
<p><img data-src="/notes/images/Netherlands-flag.png" width=40% height=40% /></p>
<h3 id="Solution-three-way-partitioning"><a href="#Solution-three-way-partitioning" class="headerlink" title="Solution: three-way partitioning"></a>Solution: three-way partitioning</h3><p>Three-way partitioning <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Dutch National Flag, U of Monash](http://users.monash.edu/~lloyd/tildeAlgDS/Sort/Flag/)
">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Three-way quicksort solution](https://www.geeksforgeeks.org/sort-an-array-of-0s-1s-and-2s/)
">[3]</span></a></sup> divides the arrays into four sections:</p>
<ul>
<li>A[:lo]: all of 0s (red)</li>
<li>A[lo:mid]: all of 1s (white)</li>
<li>A[mid:hi]: unclassified</li>
<li>A[hi:]: all of 2s (blue)</li>
</ul>
<p>The pseudocode:</p>
<ol>
<li>Input: array A with values 0,1,2, mid value 1</li>
<li>Initilize  $i \leftarrow 0, j \leftarrow 0, n \leftarrow sizeof(A)-1$</li>
<li>while j&lt;=n:<ol>
<li>if A[j] &lt; mid:<ol>
<li><strong>swap</strong> A[i] and A[j]</li>
<li>$i \leftarrow i+1$</li>
<li>$j \leftarrow j+1$</li>
</ol>
</li>
<li>else if A[j] &gt; mid:<ol>
<li><strong>swap</strong> A[j] and A[n]</li>
<li>$n \leftarrow n-1$</li>
</ol>
</li>
<li>else:<ol>
<li>$j \leftarrow j+1$</li>
</ol>
</li>
</ol>
</li>
</ol>
<ul>
<li>Time complexity: $O(n)$</li>
<li>Space complexity: $O(1)$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortColors</span>(<span class="params">nums:<span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Dutch National Flag Algorithm, 3-way partitioning</span></span><br><span class="line"><span class="string">    O(n)</span></span><br><span class="line"><span class="string">    :type nums: List[int]</span></span><br><span class="line"><span class="string">    :rtype: sorted array nums (in-place)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># divide into four sections:</span></span><br><span class="line">    <span class="comment"># [:lo]: all of 0s</span></span><br><span class="line">    <span class="comment"># [lo:mid]: all of 1s</span></span><br><span class="line">    <span class="comment"># [mid:hi]: unclassified</span></span><br><span class="line">    <span class="comment"># [hi:]: all of 2s</span></span><br><span class="line">    lo, mid, hi = <span class="number">0</span>, <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span> <span class="comment"># red, white, blue</span></span><br><span class="line">    <span class="keyword">while</span> mid &lt;= hi:  <span class="comment"># if still exists unclassified elements</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] == <span class="number">0</span>:  <span class="comment"># if it is 0s</span></span><br><span class="line">            nums[lo], nums[mid] = nums[mid], nums[lo]</span><br><span class="line">            lo += <span class="number">1</span>  <span class="comment"># the 0&#x27;s section grew</span></span><br><span class="line">            mid += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == <span class="number">1</span>:</span><br><span class="line">            mid += <span class="number">1</span>  <span class="comment"># the 1&#x27;s section grew</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># nums[mid]==2</span></span><br><span class="line">            <span class="comment"># swap the 2 (i.e. at the mid index ) with the last unclassified(i.e. at the hi position)</span></span><br><span class="line">            nums[mid], nums[hi] = nums[hi], nums[mid]</span><br><span class="line">            hi -= <span class="number">1</span>  <span class="comment"># the 2&#x27;s section grew</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Dutch_national_flag_problem">Wiki:Dutch national flag problem</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://users.monash.edu/~lloyd/tildeAlgDS/Sort/Flag/">Dutch National Flag, U of Monash</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.geeksforgeeks.org/sort-an-array-of-0s-1s-and-2s/">Three-way quicksort solution</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://leetcode.com/problems/sort-colors">Leetcode 75 Sort Colors
</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.cnblogs.com/chengxiao/p/6129630.html">Heap Sort blog</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Cormen, T.H., Leiserson, C.E., Rivest, R.L., &amp; Stein, C. (2009). Introduction to Algorithms, Third Edition.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://leetcode.com/problems/sort-list/">Leetcode 148. Sort List</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Sorting</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Basics</title>
    <url>/notes/2019/02/14/mlapp/mlapp-notes-ch1-intro/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A short introduction about machine learning.<br><span id="more"></span></p>
<h1 id="Type-of-machine-learning-ML"><a href="#Type-of-machine-learning-ML" class="headerlink" title="Type of machine learning (ML)"></a>Type of machine learning (ML)</h1><h2 id="Predictive-supervised-learning"><a href="#Predictive-supervised-learning" class="headerlink" title="Predictive/ supervised learning"></a>Predictive/ supervised learning</h2><ul>
<li><strong>Goal</strong>: learn a mapping from inputs $x$ to outputs $y$, given a labeled set of input-output pairs <script type="math/tex">\mathscr{D} = \{ (\mathbf{x}_i, y_i) \}_{i=1}^N</script>, a.k.a. <strong>training set</strong>.</li>
<li><strong>Conditional density estimation</strong>, i.e. build models for <script type="math/tex">p(y_i |\mathbf{x}_i, \Theta)</script><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3>a.k.a. pattern recognition.</li>
<li>Binary classification</li>
<li>Multi-class classification</li>
<li>Multi-label classification (viewed as doing multiple binary predictions)</li>
</ul>
<p>The <strong>mode</strong> of the distribution <script type="math/tex">p(y|\mathbf{x}, \mathscr{D})</script>, a.k.a. <strong>MAP (maximum a posteriori) estimate</strong>:</p>
<ul>
<li>Given a probabilistic output, compute the “best guess” as to the “true label”:<script type="math/tex; mode=display">\hat{y} = \hat{f} (\mathbf{x}) = \arg\max_{c=1}^C p(y=c|\mathbf{x},\mathscr{D})</script></li>
</ul>
<h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><h2 id="Descriptive-unsupervised-learning"><a href="#Descriptive-unsupervised-learning" class="headerlink" title="Descriptive/ unsupervised learning"></a>Descriptive/ unsupervised learning</h2><ul>
<li><strong>Goal</strong>: Only given inputs <script type="math/tex">\mathscr{D} = \{\mathbf{x}_i\}_{i=1}^N</script>, find “interesting patterns” in the data (a.k.a. <em>knowledge discovery</em>).</li>
<li><strong>Unconditional density estimation</strong>, i.e. build models for <script type="math/tex">p(\mathbf{x}_i|\Theta)</script></li>
</ul>
<div class="note success">
            <p>Popular deep unsupervised generative models:</p><ul><li>GANs</li><li>VAEs</li><li>Fully visible belief networks (FVBN)</li></ul>
          </div>
<h3 id="Discovering-clusters"><a href="#Discovering-clusters" class="headerlink" title="Discovering clusters"></a>Discovering clusters</h3><p><strong>Dimension reduction</strong>: Clustering data into groups. Let $K$ denote the number of clusters, we estimate the distribution over the number of clusters, $P(K|\mathscr{D})$, which tells us if there are subpopulations within the data.</p>
<h3 id="Discovering-latent-factors"><a href="#Discovering-latent-factors" class="headerlink" title="Discovering latent factors"></a>Discovering latent factors</h3><p>Reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data.</p>
<h3 id="Discovering-graph-structure"><a href="#Discovering-graph-structure" class="headerlink" title="Discovering graph structure"></a>Discovering graph structure</h3><p>Measure a set of correlated variables, discover which ones are most correlated with which others. We learn the graph structure from the data, i.e. compute <script type="math/tex">\hat{G} = \arg\max p(\mathscr{G} | \mathscr{D} )</script>.</p>
<h3 id="Matrix-completion"><a href="#Matrix-completion" class="headerlink" title="Matrix completion"></a>Matrix completion</h3><p>Missing data (<strong>NaN</strong>, “not a number”) completion.</p>
<h4 id="Image-inpainting"><a href="#Image-inpainting" class="headerlink" title="Image inpainting"></a>Image inpainting</h4><p>“Fill in” holes in an image with realistic texture. This can be tackled by building a joint probability model of the pixels, given a set of clean images, and then inferring the unknown variables (pixels) given the known variables (pixels).</p>
<h4 id="Collaborative-filtering"><a href="#Collaborative-filtering" class="headerlink" title="Collaborative filtering"></a>Collaborative filtering</h4><p>Key idea: the prediction is not based on features of the movie or user (although it could be), but merely on a ratings matrix $\mathbf{X}(m,u)$ with user $u$ of movie $m$.</p>
<p><img data-src="/notes/images/collaborative-filtering.png" alt="upload successful"></p>
<h4 id="Market-basket-analysis"><a href="#Market-basket-analysis" class="headerlink" title="Market basket analysis"></a>Market basket analysis</h4><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><ul>
<li><strong>Goal</strong>: learn how to act / behave when given occasional reward or punishment signals (e.g. how a baby learns to walk).</li>
</ul>
<h1 id="Basic-ML-concepts"><a href="#Basic-ML-concepts" class="headerlink" title="Basic ML concepts"></a>Basic ML concepts</h1><h2 id="Parametric-v-s-non-parametric-models"><a href="#Parametric-v-s-non-parametric-models" class="headerlink" title="Parametric v.s. non-parametric models"></a>Parametric v.s. non-parametric models</h2><h3 id="Parametric-models"><a href="#Parametric-models" class="headerlink" title="Parametric models"></a>Parametric models</h3><ul>
<li>Models have a fixed number of parameters.</li>
<li>Cons: strong assumptions about the nature of the data distributions.</li>
</ul>
<h3 id="Non-parametric-models"><a href="#Non-parametric-models" class="headerlink" title="Non-parametric models"></a>Non-parametric models</h3><ul>
<li>The number of model parameters grow with the amount of training set.</li>
<li>Example: $K$-nearest neighbor classifier</li>
<li>The <strong>curse of dimensionality</strong></li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>A Review of Probability</title>
    <url>/notes/2019/02/14/mlapp/mlapp-notes-ch2-probability/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A brief introduction of the probability theory and the information theory.<br><span id="more"></span></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Bayesian</strong> interpretation models the <strong>uncertainty</strong> about the events.</p>
<h1 id="Probability-theory"><a href="#Probability-theory" class="headerlink" title="Probability theory"></a>Probability theory</h1><h2 id="Discrete-random-variables"><a href="#Discrete-random-variables" class="headerlink" title="Discrete random variables"></a>Discrete random variables</h2><p>We denote the probability of the event that $X=x$ by $p(X=x)$, ro just $p(x)$ for short. The expression $p(A)$ denotes the probability that the event $A$ is true. Here $p()$ is called a <strong>probability mass function</strong> (<strong>pmf</strong>). </p>
<h3 id="Fundamental-rules"><a href="#Fundamental-rules" class="headerlink" title="Fundamental rules"></a>Fundamental rules</h3><ul>
<li><p>Union of two event:</p>
<script type="math/tex; mode=display">p(A \cup B) = p(A) + p(B) - p(A \cap B)\\ = p(A) + p(B) \text{ if A and B are mutually exclusive}</script></li>
<li><p>Joint probabilities:</p>
<script type="math/tex; mode=display">p(A,B) = p(A \cap B) = p(A|B) p(B)</script></li>
<li><p>Marginal distribution:</p>
<script type="math/tex; mode=display">p(A) = \sum_b p(A,B) = \sum_b p(A|B=b)p(B=b)</script></li>
<li><p>Chain rule of probability:</p>
<script type="math/tex; mode=display">p(X_{1:D}) = p(X_1) p(X_2|X_1) p(X_3|X_2,X_1) p(X_4|X_3,X_2,X_1) ... p(X_D|X_{1:d-1})</script></li>
<li><p>Conditional probability:</p>
<script type="math/tex; mode=display">p(A|B) = \frac{p(A,B)}{p(B) \text{ if p(B) > 0}}</script></li>
</ul>
<h2 id="Bayes-rule-a-k-a-Bayes-Theorem"><a href="#Bayes-rule-a-k-a-Bayes-Theorem" class="headerlink" title="Bayes rule (a.k.a Bayes Theorem):"></a>Bayes rule (a.k.a Bayes Theorem):</h2><script type="math/tex; mode=display">p(X=x|Y=y) = \frac{p(X=x,Y=y)}{p(Y=y)} = \frac{p(X=x)p(Y=y|X=x)}{\sum_{x'}p(X=x')p(Y=y|X=x')}</script><h2 id="Continuous-random-variables"><a href="#Continuous-random-variables" class="headerlink" title="Continuous random variables"></a>Continuous random variables</h2><p><strong>Cumulative distribution function</strong> (<strong>cdf</strong>) of $X$: define <script type="math/tex">F(q) \triangleq p(X \leq q)</script>. </p>
<script type="math/tex; mode=display">P(a < X \leq b) = F(b) - F(a)</script><p><strong>probability density function</strong> (<strong>pdf</strong>): define <script type="math/tex">f(x) = \frac{d}{dx} F(x)</script></p>
<script type="math/tex; mode=display">P(a < X \leq b) =  \int_a^b f(x)dx</script><h2 id="Mean-and-Variance"><a href="#Mean-and-Variance" class="headerlink" title="Mean and Variance"></a>Mean and Variance</h2><h3 id="Mean-expected-value"><a href="#Mean-expected-value" class="headerlink" title="Mean (expected value)"></a>Mean (expected value)</h3><p>Let $\mu$ denote the mean(expected value).</p>
<ul>
<li>For discrete rv’s:<script type="math/tex; mode=display">\mathbb{E}[X] \triangleq \sum_{x \in \chi} x p(x)</script></li>
<li>For continuous rv’s:<script type="math/tex; mode=display">\mathbb{E}[X] \triangleq \int_{\chi} x p(x) dx</script></li>
</ul>
<h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p>Let <script type="math/tex">\sigma^2</script> denote the measure of the “spread of a distribution”.</p>
<script type="math/tex; mode=display">\text{var}[X]  \triangleq \mathbb{E} (X-\mu)^2 = \int (x-\mu)^2 p(x)dx \\= \int x^2 p(x)dx+ \mu^2 \int p(x)dx - 2 \mu \int x p(x) dx = \mathbb{E}[X^2] - \mu^2</script><p>Derive a useful result:</p>
<script type="math/tex; mode=display">\mathbb{E}[X^2] = \mu^2 + \sigma^2</script><p>The standard deviation is:</p>
<script type="math/tex; mode=display">\text{std}[X] \triangleq \sqrt{\text{var}[X]}</script><h1 id="Common-discrete-distributions"><a href="#Common-discrete-distributions" class="headerlink" title="Common discrete distributions"></a>Common discrete distributions</h1><h2 id="The-binomial-and-Bernoulli-distributions"><a href="#The-binomial-and-Bernoulli-distributions" class="headerlink" title="The binomial and Bernoulli distributions"></a>The binomial and Bernoulli distributions</h2><p>Suppose we toss a coin $n$ times. Let $X \in { 0,…,n }$ be the number of heads. If the probability of heads is $\theta$, then X has a binomial distribution: $\text{X} ~ \text{Bin}(n, \theta)$</p>
<script type="math/tex; mode=display">Bin(k|n, \theta) \triangleq {n \choose k} \theta^k (1-\theta)^{n-k}</script><p>where</p>
<script type="math/tex; mode=display">{n \choose k} \triangleq \frac{n!}{(n-k)!k!}</script><p>is the number of ways to choose $k$ items from $n$ (a.k.a. binomial coefficient, pronounced “n choose k”).</p>
<script type="math/tex; mode=display">\mu = \theta, \quad \sigma^2 = n \theta (1-\theta)</script><p>Suppose we only toss a coin only once. Let <script type="math/tex">X \in \{0,1\}</script> be a binary random variable, with the probability of “success” or “heads” of $\theta$. We say that $X$ has a <strong>Bernoulli distribution</strong>: <script type="math/tex">X \sim \text{Ber}(\theta)</script>, where the pmf is defined as:</p>
<script type="math/tex; mode=display">\text{Ber}(x|\theta) = \theta^{\mathbb{I}(x=1)} (1-\theta)^{\mathbb{I}(x=0)}</script><h2 id="The-multinomial-and-multinoulli-distributions"><a href="#The-multinomial-and-multinoulli-distributions" class="headerlink" title="The multinomial and multinoulli distributions"></a>The multinomial and multinoulli distributions</h2><p>The binomial distribution only model the outcomes of coin tosses (2 results per round). We use <strong>multinomial</strong> distribution to model the outcomes of tossing a $K$-sided die. Let $\mathbf{x} = (x_1,…,x_K)$ be a random vector, where $x_j$ is the number of times side $j$ of the die occurs. The pmd is:</p>
<script type="math/tex; mode=display">\text{Mu}(x|n,\theta) \triangleq {n \choose {x_1...x_K}} \prod_{j=1}^K \theta_j^{x_j}</script><p>where <script type="math/tex">\theta_j</script> is the probability that side $j$ shows up, and the <strong>multinomial coefficient</strong> (<script type="math/tex">n = \sum_{k=1}^K x_k</script>) is:</p>
<script type="math/tex; mode=display">{n \choose {x_1...x_K}}  \triangleq \frac{n!}{x_1!x_2!...x_K!}</script><p>Suppose $n = 1$, we rolling a $K$-sided dice once. This is called <strong>one-hot encoding</strong> .</p>
<table style="border-collapse:collapse;border-spacing:0;border-color:#999" class="tg"><tr><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:left">Name</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:center">$n$</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:center;vertical-align:top">$K$</th><th style="font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#fff;background-color:#26ADE4;text-align:center;vertical-align:top">$x$</th></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Multinomial</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1,...,n \}^K, \sum_{k=1}^K x_k = n$$</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Multinoulli</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1\}^K, \sum_{k=1}^K x_k = 1$$ (1-of-$K$ encoding)</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Binomial</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">-</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1,...,n \}$$</td></tr><tr><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:left">Bernoulli</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">1</td><td style="font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#999;color:#444;background-color:#F7FDFA;text-align:center;vertical-align:top">$$\mathbf{x} \in \{0,1\}$$</td></tr></table>



<h2 id="The-Poisson-distribution"><a href="#The-Poisson-distribution" class="headerlink" title="The Poisson distribution"></a>The Poisson distribution</h2><p>With parameter $\lambda &gt; 0$, $X \sim \text{Poi}(\lambda)$, if its pmf is:</p>
<script type="math/tex; mode=display">\text{Poi}(x|\lambda) = e^{-\lambda} \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(A)</script><p>where the first term is just the normalization constant.</p>
<h2 id="The-empirical-distribution"><a href="#The-empirical-distribution" class="headerlink" title="The empirical distribution"></a>The empirical distribution</h2><p>Given a set of data <script type="math/tex">\mathscr{D} = \{ x_1,...,x_N \}</script>, define the empirical distribution (a.k.a. empirical measure):</p>
<script type="math/tex; mode=display">p_{emp}(A) \triangleq \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(A)</script><p>where <script type="math/tex">\delta_x(A)</script> is the <strong>Dirac measure</strong>, defined by:</p>
<script type="math/tex; mode=display">
    \delta_x(A)=\left\{
                \begin{array}{ll}
                  0 \text{ if } x \notin A \\
                  1 \text{ if } x \in A 
                \end{array}
              \right.</script><h1 id="Common-continuous-distributions"><a href="#Common-continuous-distributions" class="headerlink" title="Common continuous distributions"></a>Common continuous distributions</h1><h2 id="Gaussian-normal-distribution"><a href="#Gaussian-normal-distribution" class="headerlink" title="Gaussian (normal) distribution"></a>Gaussian (normal) distribution</h2><p>Let $X \sim \mathcal{N}(\mu, \sigma^2)$ denote </p>
<script type="math/tex; mode=display">\mathcal{N}(x|\mu, \sigma^2) \triangleq \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2 (x-\mu)^2}}</script><p>The <strong>precision</strong> of a Gaussian, i.e. the inverse variance <script type="math/tex">\lambda = 1/\sigma^2</script>. A high precision means a narrow distribution (low variance) centered on $\mu$.</p>
<ul>
<li>Problems: Gaussian distribution is sensitive to outliers, since the log-probability only decays quadratically with the distance from the center.</li>
</ul>
<p>More robust: <strong>Student</strong> $t$ <strong>distribution</strong>. </p>
<script type="math/tex; mode=display">\tau(x|\mu,\sigma^2,v) \propto [1+ \frac{1}{v} (\frac{x-\mu}{\sigma})^2]^{-\frac{v+1}{2}}</script><script type="math/tex; mode=display">\text{mean} = \mu, \text{mode} = \mu, \text{var} = \frac{v \sigma^2}{(v-2)}</script><h2 id="Laplace-distribution"><a href="#Laplace-distribution" class="headerlink" title="Laplace distribution"></a>Laplace distribution</h2><p>A.k.a. <strong>double-sided exponential</strong> distribution.</p>
<script type="math/tex; mode=display">Lap(x|\mu,b) \triangleq \frac{1}{2b} exp(-\frac{|x-\mu|}{b})</script><p>Here $\mu$ is a location parameter and $b&gt;0$ is a scale parameter.</p>
<script type="math/tex; mode=display">\text{mean} = \mu, \text{mode} = \mu, \text{var} = 2 b^2</script><p><img data-src="/notes/images/dist-cmp.png" alt="upload successful"></p>
<h2 id="Gamma-distribution"><a href="#Gamma-distribution" class="headerlink" title="Gamma distribution"></a>Gamma distribution</h2><p>The <strong>gamma distribution</strong> is a flexible distribution for positive real valued rv’s, $x&gt;0$. </p>
<script type="math/tex; mode=display">Ga(T|\text{shape}=a, \text{rate}=b) \triangleq \frac{b^a}{\Gamma(a)} T^{a-1} e^{-Tb}</script><p>where $\gamma(a)$ is the gamma function:</p>
<script type="math/tex; mode=display">\Gamma(x) \triangleq \int_0^{\infty} \mu^{x-1} e^{-u} du</script><script type="math/tex; mode=display">\text{mean} = \frac{a}{b}, \text{mode} = \frac{a-1}{b}, \text{var} = \frac{a}{b^2}</script><script type="math/tex; mode=display">\text{mean} = \frac{a}{b}, \text{mode} = \frac{a-1}{b}, \text{var} = \frac{a}{b^2}</script><p><img data-src="/notes/images/gamma-dist.png" alt="Gamma distribution."></p>
<h2 id="Beta-distribution"><a href="#Beta-distribution" class="headerlink" title="Beta distribution"></a>Beta distribution</h2><p>The <strong>beta distribution</strong> has support over the interval [0,1]:</p>
<script type="math/tex; mode=display">\text{Beta}(x|a,b) = \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1}</script><p>Here $B(p,q)$ is the beta function,</p>
<script type="math/tex; mode=display">B(a,b) \triangleq \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}</script><script type="math/tex; mode=display">\text{mean} = \frac{a}{a+b}, \text{mode} = \frac{a-1}{a+b-2}, \text{var} = \frac{ab}{(a+b)^2(a+b+1)}</script><p><img data-src="/notes/images/beta-dist.png" alt="upload successful"></p>
<h2 id="Pareto-distribution"><a href="#Pareto-distribution" class="headerlink" title="Pareto distribution"></a>Pareto distribution</h2><p>The <strong>Pareto distribution</strong> is used to model the distribution of quantities that exhibit <strong>long tails</strong> (heavy tails).</p>
<p>For example, the most frequent ward in English occurs approximately twice as often as the second most frequent word, which occurs twice as oten as the fourth most frequent word. This is <strong>Zipf’s law</strong>.</p>
<p>Its pdf:</p>
<script type="math/tex; mode=display">\text{Pareto(x|k,m)} = km^k x^{-(k+1)} \mathbb{I}(x \geq m)</script><script type="math/tex; mode=display">\text{mean} = \frac{km}{k-1} \text{ if k>1}, \text{mode} = m, \text{var} = \frac{m^2 k}{(k-1)^2 (k-2)} \text{ if k>2}</script><p><img data-src="/notes/images/pareto-dist.png" alt="upload successful"></p>
<h1 id="Joint-probability-distributions"><a href="#Joint-probability-distributions" class="headerlink" title="Joint probability distributions"></a>Joint probability distributions</h1><p>A <strong>joint probability distribution</strong> has the form <script type="math/tex">p(x_1,...,x_D)</script> for a set of $D &gt; 1$ variables, and models the (stochastic) relationships between the variables.</p>
<h2 id="Covariance-and-correlation"><a href="#Covariance-and-correlation" class="headerlink" title="Covariance and correlation"></a>Covariance and correlation</h2><p><strong>Covariance</strong> between two rv’s X and Y measures the degree to which X and Y are (linearly) related.</p>
<script type="math/tex; mode=display">\text{cov}[X,Y] \triangleq  \mathbb{E}[(X- \mathbb{E}[X])(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]</script><p>If $\mathbf{x}$ is a $d$-dimensional random vector, its <strong>covariance matrix</strong> is defined to be symmetric, positive definite matrix:</p>
<script type="math/tex; mode=display">\text{cov}[\mathbf{x}] \triangleq \mathbb{E}[(\mathbf{x} - \mathbb{E}[\mathbf{x}])(\mathbf{x} - \mathbb{E}[\mathbf{x}])^T ]]  = 
   \begin{bmatrix}
     \text{var}[X_1] & \text{cov}[X_1,X_2] & \cdots & \text{cov}[X_1,X_d] \\
     \text{cov}[X_2,X_1] & \text{var}[X_2] & \cdots & \text{cov}[X_2,X_d] \\
     \vdots & \vdots & \ddots & \vdots \\
     \text{cov}[X_d,X_1] & \text{cov}[X_d,X_2] & \cdots & \text{var}[X_d]
   \end{bmatrix}</script><p>Covariance $\in [0, \infty]$. <strong>Pearson correlation coefficient</strong> use normalized measure with a finite upper bound:</p>
<script type="math/tex; mode=display">\text{corr}[X, Y] \triangleq \frac{\text{cov}[X,Y]}{\sqrt{\text{var}[X] \text{var}[Y]}}</script><p>A<strong>correlation matrix</strong> has the form:</p>
<script type="math/tex; mode=display">\mathbf{R} =    \begin{bmatrix}
     \text{corr}[X_1, X_1] & \text{corr}[X_1,X_2] & \cdots & \text{corr}[X_1,X_d] \\
     \vdots & \vdots & \ddots & \vdots \\
     \text{corr}[X_d,X_1] & \text{corr}[X_d,X_2] & \cdots & \text{corr}[X_d, X_d]
   \end{bmatrix}</script><p>where $\text{corr}[X,Y] \in [-1, 1]$.</p>
<div class="note primary">
            <p>independent $\Rightarrow$ uncorrelated,<br>uncorrelated $\nRightarrow$ independent.</p><p>Measure the dependence between rv’s: <strong>mutual information</strong>.</p>
          </div>
<h2 id="Multivariate-Gaussian"><a href="#Multivariate-Gaussian" class="headerlink" title="Multivariate Gaussian"></a>Multivariate Gaussian</h2><p>The <strong>Multivariate Gaussian</strong> or <strong>multivariate normal (MVN)</strong> is the most widely used pdf for continuous variables.<br>Its pdf:</p>
<script type="math/tex; mode=display">\mathcal{N}(\mathbf{x}|\mathbf{\mu}, \mathbf{\Sigma}) \triangleq \frac{1}{(2\pi)^{-D/2} |\mathbf{\Lambda}^{1/2}|} \text{exp}[-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \sum^{-1} (\mathbf{x} - \mathbf{\mu}) ]</script><p>where $\mathbf{\mu} = \mathbb{E}[\mathbf{x}] \in \mathbb{R}^D$ is the mean vector, and $\Sigma = \text{cov}[\mathbf{x}]$  is the $D /times D$ covariance matrix. The <strong>precision matrix</strong> or <strong>concentration matrix</strong> is the inverse covariance matrix, <script type="math/tex">\Lambda = \Sigma^{-1}</script>. The normalization constant $(2\pi)^{-D/2} |\mathbf{\Lambda}^{1/2}|$ just ensure that the pdf integrates to 1.</p>
<h2 id="Multivariate-Student-t-distribution"><a href="#Multivariate-Student-t-distribution" class="headerlink" title="Multivariate Student $t$ distribution"></a>Multivariate Student $t$ distribution</h2><h2 id="Dirichlet-distribution"><a href="#Dirichlet-distribution" class="headerlink" title="Dirichlet distribution"></a>Dirichlet distribution</h2><h1 id="Transformations-of-random-varianbles"><a href="#Transformations-of-random-varianbles" class="headerlink" title="Transformations of random varianbles"></a>Transformations of random varianbles</h1><h2 id="Linear-transformation"><a href="#Linear-transformation" class="headerlink" title="Linear transformation"></a>Linear transformation</h2><p>Suppose $f()$ is a linear function:</p>
<script type="math/tex; mode=display">\mathbf{y} = f(\mathbf{x}) = \mathbf{A} \mathbf{x} + \mathbf{b}</script><p><strong>Linearity of expectation</strong>:</p>
<script type="math/tex; mode=display">\mathbb{E}[\mathbf{y}] = \mathbb{E}[\mathbf{A}\mathbf{x} + \mathbf{b}] = \mathbf{A}\mathbf{\mu} + \mathbf{b}</script><p>where $\mathbf{\mu}=\mathbb{E}[\mathbf{x}]$.</p>
<p><strong>Covariance</strong>: </p>
<script type="math/tex; mode=display">\text{cov}[\mathbf{y}] = \text{cov}[\mathbf{A}\mathbf{x}] = \mathbf{A}\Sigma\mathbf{A}^T</script><p>where $\Sigma = \text{cov}[\mathbf{x}]$</p>
<p>If $f()$ is a scalar-valued function, $f(\mathbf{x}) = \mathbf{a}^T \mathbf{x} + b$, the mean is: </p>
<script type="math/tex; mode=display">\mathbb{E}[\mathbf{a}^T \mathbf{x} + b] = \mathbf{a}^T \mathbf{\mu} + b</script><p>The covariance：</p>
<script type="math/tex; mode=display">\text{var}[y] = \text{var}[\mathbf{a}^T \mathbf{x} + b] = \mathbf{a}^T\Sigma \mathbf{a}</script><h2 id="Central-limit-theorem"><a href="#Central-limit-theorem" class="headerlink" title="Central limit theorem"></a>Central limit theorem</h2><p>Consider $N$ random variables with pdf’s $p(x<em>i)$, each with mean $\mu$ and variance $\sigma^2$. We assume each variable is <strong>iid</strong>(independent and identically distributed). Let $$S_N = \sum</em>{i=1}^N X_i$$ be the sum of the rv’s.</p>
<p>As $N$ increases, the distribution of this sum approaches</p>
<script type="math/tex; mode=display">p(S_N = s) = \frac{1}{\sqrt{2\pi N \sigma^2}} \text{exp} (- \frac{(s-N\mu)^2}{2N\sigma^2})</script><p>Hence the distribution of the quantity</p>
<script type="math/tex; mode=display">Z_N \triangleq \frac{S_N - N \mu}{\sigma\sqrt{N}} = \frac{\bar{X} - \mu}{ \sigma / \sqrt{N}}</script><p>converges to the standard normal, where <script type="math/tex">\bar{X} = \frac{1}{N} \sum_{i=1}^N x_i</script> is the sample mean. (<strong>central limit theorem</strong>)</p>
<h1 id="Monte-Carlo-approximation"><a href="#Monte-Carlo-approximation" class="headerlink" title="Monte Carlo approximation"></a>Monte Carlo approximation</h1><p>Computing the distribution of a function of an rv using the change of variables formula is difficult.<br><strong>Monte Carlo</strong> approximation: First generate $S$ samples from the distribution, called them <script type="math/tex">x_1,...,x_S</script>. Given the samples, we approximate the distribution of $f(X)$ by using the empirical distribution of <script type="math/tex">\{f(x_s)\}^S_{s=1}</script>.</p>
<p>Approximate the expected value with the arithmetic mean of the function applied to the samples:</p>
<script type="math/tex; mode=display">\mathbb{E}[f(X)] = \int f(x)p(x)dx \approx \frac{1}{S}\sum_{s=1}^S f(x_s)</script><p>where <script type="math/tex">x_s \sim p(X)</script>. This is called Monte Carlo integration.</p>
<script type="math/tex; mode=display">\bar{x} = \frac{1}{S} \sum_{s=1}^S x_s \rightarrow \mathbb{E}[X]</script><script type="math/tex; mode=display">\frac{1}{S} \sum_{s=1}^S (x_s - \bar{x})^2 \rightarrow \text{var}[X]</script><script type="math/tex; mode=display">\frac{1}{S}\#\{x_s \leq c\} \rightarrow p(X \leq c)</script><script type="math/tex; mode=display">\text{median}\{x_1,...,x_S\} \rightarrow \text{median}(X)</script><h1 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h1><p><strong>Information theory</strong> represents data in a compact fashion, as well as with transmitting and storing it in a way that is robust to errors.</p>
<h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h2><p><strong>Entropy</strong> of a rv $X$ with distribution $p$, denoted by $\mathbb{H}(X)$, is a measure of its uncertainty. For a discrete rv with $K$ states:</p>
<script type="math/tex; mode=display">\mathbb{H}(X) \triangleq -\sum_{k=1}^K p(X=k) log_2 p(X=k)</script><p>Usually we use log base 2, where it is called <strong>bits</strong> (short for binary digits); whereas log base $e$ is called <strong>nats</strong>. The discrete distribution with maimum entropy is the uniform distribution.</p>
<h2 id="KL-divergence"><a href="#KL-divergence" class="headerlink" title="KL divergence"></a>KL divergence</h2><p><strong>Kullback-Leibler (KL) divergence</strong> or <strong>relative entropy</strong>: measures the dissimilarity of two probability distributions, $p$ and $q$.</p>
<script type="math/tex; mode=display">\mathbb{KL}(p||q) \triangleq \sum_{k=1}^K p_k \text{log} \frac{p_k}{q_k}</script><p>where the sum gets replaced by an integral for pdf’s.</p>
<script type="math/tex; mode=display">\mathbb{KL}(p||q) = \sum_k p_k \text{log} p_k - \sum_k p_k \text{log} q_k = - \mathbb{H}(p) + \mathbb{H}(p,q)</script><p>where $\mathbb{H}(p,q)$ is called the <strong>cross entropy</strong>.</p>
<script type="math/tex; mode=display">\mathbb{H}(p,q) \triangleq - \sum_k p_k \text{log}q_k</script><div class="note success">
            <p><strong>Cross entropy</strong>: the average number of bits needed to encoder data coming from a source with distribution $p$ when we use model $q$ to define our codebook, i.e. KL divergence is the avarage number of <em>extra</em> bits needed to encode the data, due to the fact that we use distribution $q$ to encoder the data instead of the true distribution $p$.</p>
          </div>
<p>The “extra number of bits” interpretation implies that $\mathbb{KL}(p||q) \geq 0 $, and KL is only equal to zero iff $q = p$.</p>
<p><strong>Information inequality</strong>: <script type="math/tex">\mathbb{KL}(p||q) \geq 0 \text{ with equality iff } p=q</script></p>
<h2 id="Mutual-information"><a href="#Mutual-information" class="headerlink" title="Mutual information"></a>Mutual information</h2><p><strong>Mutual Information (MI)</strong>: determine how similar the joint distribution $p(X,Y)$ is to the factored distribution $p(X)p(Y)$.</p>
<script type="math/tex; mode=display">\mathbb{I}(X;Y) \triangleq \mathbb{KL}(p(X,Y)||p(X) p(Y)) = \sum_x\sum_y p(x,y) \text{log} \frac{p(x,y)}{p(x)p(y)}</script><p>where $\mathbb{I}(X;Y) \geq 0$ woth equality iff $p(X,Y) = p(X) p(Y)$, i.e. the MI is zero iff the variables are independent.</p>
<script type="math/tex; mode=display">\mathbb{I}(X;Y) = \mathbb{H}(X) - \mathbb{H}(X|Y) = \mathbb{H}(Y) - \mathbb{H}(Y|X)</script><p>where <script type="math/tex">\mathbb{H}(Y|X)</script> is the <strong>conditional entropy</strong>, defined as <script type="math/tex">\mathbb{H}(Y|X) = \sum_x p(x)\mathbb{H}(Y|X=x)</script>.<br>Hence we interpret the MI between $X$ and $Y$ as the reduction in uncertainty about $X$ after observing $Y$, or by symmetry, the reduction in uncertainty about $Y$ after observing $X$.</p>
<p><strong>Pointwise mutual information (PMI)</strong>: measures the discrepancy between events $x$ and $y$ occurring together compared to what would be expected by chance. </p>
<script type="math/tex; mode=display">PMI(x,y) \triangleq \text{log} \frac{p(x,y)}{p(x)p(y)} = \text{log} \frac{p(x|y)}{p(x)} = \text{log} \frac{p(y|x)}{p(y)}</script><div class="note success">
            <p>The MI of $X$ and $Y$ is the expected value of the PMI.</p><script type="math/tex; mode=display">PMI(x,y)= \text{log} \frac{p(x|y)}{p(x)} = \text{log} \frac{p(y|x)}{p(y)}</script><p>We can interpret that PMI is the amount we update the prior $p(x)$ into the posterior $p(x|y)$, or equivalently update the prior $p(y)$ into $p(y|x)$.</p>
          </div>]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Probability theory</category>
      </categories>
      <tags>
        <tag>Probability theory</tag>
      </tags>
  </entry>
  <entry>
    <title>Industrial Tricks for Named Entity Recognition</title>
    <url>/notes/2018/12/19/NLP/NER/Industrial-Named-Entity-Recognition/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Why is NER hard in the industry?</p>
<p>This blog dicusses several frequently occurred problems and possible solutions.</p>
<p><img data-src="/notes/images/NER-illustration.png" width="70%"/></p>
<span id="more"></span>
<div class="note info">
            <p>Related blog: </p><ul><li><strong><a href="/notes/2018/11/29/NLP/NER/NER-overview/">Named Entity Recoginition: an Overview</a></strong></li></ul>
          </div>
<h1 id="Industrial-NER-Problems"><a href="#Industrial-NER-Problems" class="headerlink" title="Industrial NER Problems"></a>Industrial NER Problems</h1><p><strong>Background</strong>: Named Entity Recognition (NER) has always been a fundamental task in the NLP tasks, including information extraction, relation extraction, information retrieval, question answering, <em>etc</em>. The prevalent solution to NER is <code>BiLSTM-CRF</code> model, but there still exist several issues in the real scenario.</p>
<p><img data-src="/notes/images/NER-fig.png" alt="Named Entity Recognition"></p>
<p>Possible problems including:</p>
<ul>
<li>Expensive cost for manual labeling</li>
<li>Incapability of generalization and transferability. For example, transfer between different domains.</li>
<li>Weak interpretability. In certain domains such as medical NER, the “black box” is not relable for decision making.</li>
<li>Low computing resources. E.g., some medical data is confidential and only accessible on the deivices of a hospital, where there is no enough GPU resource for computing.</li>
</ul>
<h2 id="Q1-How-to-quickly-improve-the-NER-performance-in-the-industry"><a href="#Q1-How-to-quickly-improve-the-NER-performance-in-the-industry" class="headerlink" title="Q1. How to quickly improve the NER performance in the industry?"></a>Q1. How to quickly improve the NER performance in the industry?</h2><p>For vertical domain:</p>
<ol>
<li>Adopt BiLSTM-CRF models. </li>
<li>Analyze bad cases;</li>
<li>Consistently build the in-domain lexicon and improve the pattern-based method.</li>
</ol>
<p>For general domain:</p>
<ol>
<li>Construct syntactic features to feed into NER. For Chinese/Japanese NER, also use segmented words.</li>
<li>Combine lexicon.</li>
</ol>
<h2 id="Q2-How-to-improve-towards-neural-models"><a href="#Q2-How-to-improve-towards-neural-models" class="headerlink" title="Q2. How to improve towards neural models?"></a>Q2. How to improve towards neural models?</h2><p>NER focus more on the bottom features. Try to introduce rich features, such as char, bi-gram, lexicon, POS tagging, ElMo, <em>etc</em>. In the vertical domain, we can pretrain a in-domain word embedding or language model. Try to build multiple features from different views.</p>
<h2 id="Q3-How-to-incorporate-lexicon-embedding-into-Chinese-NER"><a href="#Q3-How-to-incorporate-lexicon-embedding-into-Chinese-NER" class="headerlink" title="Q3. How to incorporate lexicon embedding into Chinese NER?"></a>Q3. How to incorporate lexicon embedding into Chinese NER?</h2><ol>
<li>Simple-Lexicon</li>
<li>FLAT</li>
</ol>
<h2 id="Q4-How-to-solve-over-long-entity-span"><a href="#Q4-How-to-solve-over-long-entity-span" class="headerlink" title="Q4. How to solve over-long entity span?"></a>Q4. How to solve over-long entity span?</h2><p>If certain type of spans are too long, try:</p>
<ol>
<li>Use rule for postfix.</li>
<li>Adopt pointer network + CRF for multi-task learning.</li>
</ol>
<h2 id="Q5-How-to-treat-BERT-in-NER"><a href="#Q5-How-to-treat-BERT-in-NER" class="headerlink" title="Q5. How to treat BERT in NER?"></a>Q5. How to treat BERT in NER?</h2><p>In situations with no computing limit, in general domain, or few-shot problems.</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>NER</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>Evaluation Metrics of Named Entity Recognition</title>
    <url>/notes/2018/11/21/NLP/NER/NER-Evaluation-Metrics/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Here we briefly introduce some common evaluation metrics in NER tasks, considering both extracted boundary and entities.<br><span id="more"></span></p>
<h1 id="Scenarios-that-NER-systems-predict"><a href="#Scenarios-that-NER-systems-predict" class="headerlink" title="Scenarios-that-NER-systems-predict"></a>Scenarios-that-NER-systems-predict</h1><h2 id="Exact-Match"><a href="#Exact-Match" class="headerlink" title="Exact Match"></a>Exact Match</h2><ul>
<li>1) Surface entity and type match （Both entity boundary and type are correct）</li>
<li>2) System hypothesized an entity (predict entity that does not exist in ground truth)</li>
<li>3) Systems miss an entity (entity exists in ground truth, but is not predicted by NER system)</li>
</ul>
<h2 id="Partial-Match-Overlapping"><a href="#Partial-Match-Overlapping" class="headerlink" title="Partial Match (Overlapping)"></a>Partial Match (Overlapping)</h2><ul>
<li>4) Wrong entity type ( correct entity boundary, type disagree)</li>
<li>5) Wrong boundaries （boundary overlap）</li>
<li>6) Wrong boundaries and wrong entity type</li>
</ul>
<hr>
<h1 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h1><h2 id="CoNLL-2003-Computational-Natural-Language-Learning"><a href="#CoNLL-2003-Computational-Natural-Language-Learning" class="headerlink" title="CoNLL-2003: Computational Natural Language Learning"></a>CoNLL-2003: Computational Natural Language Learning</h2><ul>
<li>Only considers previous 1,2,3 scenarios</li>
<li>Exact match: precision, recall, f1 measure</li>
<li>See <a href="http://www.aclweb.org/anthology/W03-0419">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</a> for details.</li>
</ul>
<h2 id="Automatic-Content-Extraction-ACE"><a href="#Automatic-Content-Extraction-ACE" class="headerlink" title="Automatic Content Extraction (ACE)"></a>Automatic Content Extraction (ACE)</h2><ul>
<li>Include weighting schema</li>
<li>See <a href="https://pubweb.eng.utah.edu/~cs6961/papers/ACE-2008-description.pdf">Automatic Content Extraction 2008 Evaluation Plan (ACE08) </a></li>
<li>See <a href="https://pdfs.semanticscholar.org/0617/dd6924df7a3491c299772b70e90507b195dc.pdf">The Automatic Content Extraction (ACE) Program: Tasks, Data, and Evaluation </a></li>
</ul>
<h2 id="Message-Understanding-Conference-MUC"><a href="#Message-Understanding-Conference-MUC" class="headerlink" title="Message Understanding Conference (MUC)"></a>Message Understanding Conference (MUC)</h2><ul>
<li>Consider both entity boundary and entity type</li>
<li>Correct (COR): match</li>
<li>Incorrect(INC)：not match</li>
<li>Partial(PAR)：predicted entity boundary overlap with golden annotation，but they are not the same</li>
<li>Missing(MIS)：golden annotation boundary is not identified （predictee do not have, but golden label do）</li>
<li>Spurius(SPU)：predicted entity boundary does not exist in golden annotation（predictee have, but golden label do not）</li>
<li>See <a href="http://www.aclweb.org/anthology/M93-1007">MUC-5 EVALUATION METRICS</a></li>
<li><a href="https://github.com/jantrienes/nereval">Implementation</a> in python version</li>
</ul>
<h2 id="SemEval‘13"><a href="#SemEval‘13" class="headerlink" title="SemEval‘13"></a>SemEval‘13</h2><ul>
<li>Strict：Exact match (Both entity boundary and type are correct)</li>
<li>Exact boundary matching：predicted entity boundary is correct, regardless of entity boundary</li>
<li>Partial boundary matching：entity boundaries overlap, regardless of entity boundary</li>
<li>Type matching：some overlap between the system tagged entity and the gold annotation is required;</li>
</ul>
<p><style type="text/css"><br>.tg  {border-collapse:collapse;border-spacing:0;}<br>.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}<br>.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}<br>.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}<br>.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style></p>
<table class="tg">
  <tr>
    <th class="tg-0pky">Scenario</th>
    <th class="tg-c3ow" colspan="2">Golden Standard</th>
    <th class="tg-c3ow" colspan="2">NER system prediction</th>
    <th class="tg-c3ow" colspan="4">Measure</th>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-c3ow">Entity Type</td>
    <td class="tg-c3ow">Entity Boundary (Surface String)</td>
    <td class="tg-0pky">Entity Type</td>
    <td class="tg-0pky">Entity Boundary (Surface String)</td>
    <td class="tg-0pky">Type</td>
    <td class="tg-0pky">Partial</td>
    <td class="tg-0pky">Exact</td>
    <td class="tg-0pky">Strict</td>
  </tr>
  <tr>
    <td class="tg-0pky">III</td>
    <td class="tg-c3ow">MUSIC_NAME</td>
    <td class="tg-c3ow">告白气球</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">MIS</td>
    <td class="tg-0pky">MIS</td>
    <td class="tg-0pky">MIS</td>
    <td class="tg-0pky">MIS</td>
  </tr>
  <tr>
    <td class="tg-0pky">II</td>
    <td class="tg-c3ow"></td>
    <td class="tg-c3ow"></td>
    <td class="tg-0pky">MUSIC_NAME</td>
    <td class="tg-0pky">年轮</td>
    <td class="tg-0pky">SPU</td>
    <td class="tg-0pky">SPU</td>
    <td class="tg-0pky">SPU</td>
    <td class="tg-0pky">SPU</td>
  </tr>
  <tr>
    <td class="tg-0pky">V</td>
    <td class="tg-c3ow">MUSIC_NAME</td>
    <td class="tg-c3ow">告白气球</td>
    <td class="tg-0pky">MUSIC_NAME</td>
    <td class="tg-0pky">一首告白气球</td>
    <td class="tg-0pky">COR</td>
    <td class="tg-0pky">PAR</td>
    <td class="tg-0pky">INC</td>
    <td class="tg-0pky">INC</td>
  </tr>
  <tr>
    <td class="tg-0pky">IV</td>
    <td class="tg-c3ow">MUSIC_NAME</td>
    <td class="tg-c3ow">告白气球</td>
    <td class="tg-0pky">SINGER</td>
    <td class="tg-0pky">告白气球</td>
    <td class="tg-0pky">INC</td>
    <td class="tg-0pky">COR</td>
    <td class="tg-0pky">COR</td>
    <td class="tg-0pky">INC</td>
  </tr>
  <tr>
    <td class="tg-0pky">I</td>
    <td class="tg-c3ow">MUSIC_NAME</td>
    <td class="tg-c3ow">告白气球</td>
    <td class="tg-0pky">MUSIC_NAME</td>
    <td class="tg-0pky">告白气球</td>
    <td class="tg-0pky">COR</td>
    <td class="tg-0pky">COR</td>
    <td class="tg-0pky">COR</td>
    <td class="tg-0pky">COR</td>
  </tr>
  <tr>
    <td class="tg-0pky">VI</td>
    <td class="tg-c3ow">MUSIC_NAME</td>
    <td class="tg-c3ow">告白气球</td>
    <td class="tg-0pky">SINGER</td>
    <td class="tg-0pky">一首告白气球</td>
    <td class="tg-0pky">INC</td>
    <td class="tg-0pky">PAR</td>
    <td class="tg-0pky">INC</td>
    <td class="tg-0pky">INC</td>
  </tr>
</table>


<p><strong>Number of golden standard</strong>: </p>
<script type="math/tex; mode=display">Possible(POS) = COR+INC+PAR+MIS = TP + FN</script><p><strong>Number of predictee</strong>: </p>
<script type="math/tex; mode=display">Actual(ACT) = COR + INC + PAR + SPU = TP + FP</script><h3 id="Exact-match-i-e-Strict-Exact"><a href="#Exact-match-i-e-Strict-Exact" class="headerlink" title="Exact match(i.e. Strict, Exact)"></a>Exact match(i.e. Strict, Exact)</h3><script type="math/tex; mode=display">Precision = \frac{COR}{ACT} = \frac{TP}{TP+FP}</script><script type="math/tex; mode=display">Recall =\frac{COR}{POS}=\frac{TP}{TP+FN}</script><h3 id="Partial-match-i-e-Partial-Type"><a href="#Partial-match-i-e-Partial-Type" class="headerlink" title="Partial match (i.e. Partial, Type)"></a>Partial match (i.e. Partial, Type)</h3><script type="math/tex; mode=display">Precision = \frac{COR + 0.5\times PAR}{ACT}</script><script type="math/tex; mode=display">Recall = \frac{COR+0.5 \times PAR}{POS}</script><h3 id="F-measure"><a href="#F-measure" class="headerlink" title="F-measure"></a>F-measure</h3><script type="math/tex; mode=display">F_\alpha = \frac{(\alpha^2+1)PR}{\alpha^2P+R}</script><script type="math/tex; mode=display">F_1 = \frac{2PR}{P+R}</script><p><style type="text/css"><br>.tg  {border-collapse:collapse;border-spacing:0;}<br>.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}<br>.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}<br>.tg .tg-t0np{font-family:”Comic Sans MS”, cursive, sans-serif !important;;text-align:left;vertical-align:top}<br>.tg .tg-23iq{font-family:”Comic Sans MS”, cursive, sans-serif !important;;text-align:left}<br>.tg .tg-ww3v{font-family:Tahoma, Geneva, sans-serif !important;;text-align:left}<br>.tg .tg-e6bt{font-family:Arial, Helvetica, sans-serif !important;;text-align:left}<br>.tg .tg-gx32{font-family:Arial, Helvetica, sans-serif !important;;text-align:left;vertical-align:top}<br>.tg .tg-8l38{font-family:Tahoma, Geneva, sans-serif !important;;text-align:left;vertical-align:top}
</style></p>
<table class="tg">
  <tr>
    <th class="tg-e6bt">Measure</th>
    <th class="tg-23iq">Type</th>
    <th class="tg-23iq">Partial</th>
    <th class="tg-ww3v">Exact</th>
    <th class="tg-ww3v">Strict</th>
  </tr>
  <tr>
    <td class="tg-e6bt">Correct</td>
    <td class="tg-23iq">2</td>
    <td class="tg-23iq">2</td>
    <td class="tg-ww3v">2</td>
    <td class="tg-ww3v">1</td>
  </tr>
  <tr>
    <td class="tg-e6bt">Incorrect</td>
    <td class="tg-23iq">2</td>
    <td class="tg-23iq">0</td>
    <td class="tg-ww3v">2</td>
    <td class="tg-ww3v">3</td>
  </tr>
  <tr>
    <td class="tg-e6bt">Partial</td>
    <td class="tg-23iq">0</td>
    <td class="tg-23iq">2</td>
    <td class="tg-ww3v">0</td>
    <td class="tg-ww3v">0</td>
  </tr>
  <tr>
    <td class="tg-e6bt">Missed</td>
    <td class="tg-23iq">1</td>
    <td class="tg-23iq">1</td>
    <td class="tg-ww3v">1</td>
    <td class="tg-ww3v">1</td>
  </tr>
  <tr>
    <td class="tg-e6bt">Spurius</td>
    <td class="tg-23iq">1</td>
    <td class="tg-23iq">1</td>
    <td class="tg-ww3v">1</td>
    <td class="tg-ww3v">1</td>
  </tr>
  <tr>
    <td class="tg-e6bt">Precision</td>
    <td class="tg-23iq">0.4</td>
    <td class="tg-23iq">0.6</td>
    <td class="tg-ww3v">0.4</td>
    <td class="tg-ww3v">0.2</td>
  </tr>
  <tr>
    <td class="tg-e6bt">Recall</td>
    <td class="tg-23iq">0.4</td>
    <td class="tg-23iq">0.6</td>
    <td class="tg-ww3v">0.4</td>
    <td class="tg-ww3v">0.2</td>
  </tr>
  <tr>
    <td class="tg-gx32">F1 score</td>
    <td class="tg-t0np">0.4</td>
    <td class="tg-t0np">0.6</td>
    <td class="tg-8l38">0.4</td>
    <td class="tg-8l38">0.2</td>
  </tr>
</table>


<p>Pypi library <a href="https://github.com/cyk1337/NER-evaluation">eval4ner</a> installation: <code>pip install -U eval4ner</code></p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2021NER-eval,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Evaluation Metrics of Named Entity Recognition&#125;&#125;,</span><br><span class="line">  year = &#123;2021&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2018/11/21/NLP/NER/NER-Evaluation-Metrics/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.aclweb.org/anthology/W03-0419">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://pubweb.eng.utah.edu/~cs6961/papers/ACE-2008-description.pdf">Automatic Content Extraction 2008 Evaluation Plan (ACE08) </a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/">Named-Entity evaluation metrics based on entity-level</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Sequence labeling</category>
        <category>NER</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Survey</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>Named Entity Recognition Overview</title>
    <url>/notes/2018/11/29/NLP/NER/NER-overview/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p><strong>Name Entity Recognition</strong> (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names.</p>

            <i class="fa fa-quote-right"></i>
          </blockquote>
<span id="more"></span>
<p>Approaches:</p>
<ul>
<li>Statistical ML methods: HMM, MEMM, CRF</li>
<li>Deep learning methods: RNN-CRF, CNN-CRF</li>
</ul>
<hr>
<h1 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a>Hidden Markov Model</h1><p><img data-src="/notes/images/HMM.png" alt="HMM"></p>
<p>HMM consists of a discrete-time, discrete-state Markov chain, with hidden states $z_t \in {1,…,K} $<br>, plus an observation model $p(\mathbf{x}_t|z_t)$. </p>
<p>HMM is a <strong>generative model</strong> that optimises the likelihood $P(W|T)$, consisting of three components: </p>
<ul>
<li>Initial probability $\pmb{\pi}$</li>
<li>Transition probability matrix $\pmb{A}$</li>
<li>Emission probability matrix $\pmb{B}$. </li>
</ul>
<p>The joint distribution is:</p>
<script type="math/tex; mode=display">p( \mathbf{z}_{1:T}, \mathbf{x}_{1:T}) =p( \mathbf{z}_{1:T}) p( \mathbf{x}_{1:T}| \mathbf{z}_{1:T}) = \underbrace{p(z_1)}_{\pmb{\pi}} \underbrace{\prod_{t=2}^T p(z_t|z_{t-1})}_{\pmb{A}} \underbrace{[\prod_{t=1}^T p(\mathbf{x}_t|z_t)]}_{\pmb{B}}</script><p>We estimate the posterior by combining the likelihood and the prior P(T).</p>
<script type="math/tex; mode=display">\hat{T} = \mathop{\arg\max}_T P(T|W) \\  = \mathop{\arg\max}_T P(W|T) P(T) \\ = \mathop{\arg\max}_T \underbrace{\prod_i P(\textrm{word}_i | \textrm{tag}_i)}_\textrm{emission probability} \underbrace{\prod_i P(\textrm{tag}_i|tag_{i-1})}_\textrm{transmission probability}</script><p>HMM states only conditions on the previous state.</p>
<div class="note danger">
            <p><strong>HMM cons</strong>: Independence assumptions.</p>
          </div>
<h1 id="Maximum-Entropy-Markov-Model-MEMM"><a href="#Maximum-Entropy-Markov-Model-MEMM" class="headerlink" title="Maximum-Entropy Markov Model (MEMM)"></a>Maximum-Entropy Markov Model (MEMM)</h1><div class="note info">
            <p>HMM based on the probabilities of transmission probability matrix and emission probability matrix. It is <strong>hard to encode the knowledge</strong> into these two matrices. If we include many knowledge sources, like capitalisation, the presence of hyphens, word endings. It is not easy to fit the probability like $P(\textrm{capitalisation} | \textrm{tag}), P(\textrm{hyphen} | \textrm{tag}), P(\textrm{suffix} | \textrm{tag}), P(\textrm{suffix} | \textrm{tag})$ into an HMM-style model.</p><p>HMM assumes the independence between observations $z$, while MEMM does not. However, MEMM suffers from the label bias problem.</p>
          </div>
<p><img data-src="/notes/images/MaxEnt.png" alt="MEMM"></p>
<p>MEMM is a <strong>discriminative model</strong>. It computes the <em>posterior</em> $P(Z=T|X=W)$ directly. MEMM states conditioned on the <em>previous state</em> and <em>current observation</em>.</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(Z \vert X) &{}= \prod_{t=1}^T p(z_t \vert x_t, z_{t-1}) \\
&{}= \prod_{t=1}^T \frac{1}{Z(x_t, z')} \exp \bigg( \sum_t \lambda_t f(x_t, z_t) \bigg)
\end{aligned}</script><p>where $f{\cdot}$ is real-valued feature functions, $Z$ is a normalization term.</p>
<p>Thus, </p>
<script type="math/tex; mode=display">\hat{T} = \mathop{\arg\max}_T P(T|W) \\ =  \mathop{\arg\max}_T \prod_i P(\textrm{tag}_i | \textrm{word}_i, \textrm{tag}_{i-1})</script><p><strong>Decoding</strong>: Viterbi algorithm.</p>
<p><strong>Pros over HMM</strong>: offer increased freedom in choosing features to represent obervations.</p>
<ul>
<li><del>Strict left-to-right word classifier</del>. </li>
<li>Cons: only use left sequence information. It cannot consider future sequence information.</li>
</ul>
<div class="note danger">
            <p><strong>MEMM weakness</strong>: </p><ul><li><strong>Label bias</strong> problem: states with low-entropy transition distributions “effectively ignore their observations”. MEMM normalizes over the set of possible output labels at each time step, which is <strong>locally normalized</strong>. “Conservation of score mass just says that the outgoing scores from a state for a given observation are normalized.”<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[The Label Bias Problem](https://awni.github.io/label-bias/)">[7]</span></a></sup> The result is any inference process will bias towards states with fewer outgoing transitions.</li></ul><p>CRFs were designed to overcome this weakness.</p>
          </div>
<h2 id="vs-HMM"><a href="#vs-HMM" class="headerlink" title="vs HMM"></a>vs HMM</h2><p>unlike the HMM, the MEMM can condition on any useful feature of the input observation. In the HMM this wasn’t possible because the HMM is likelihood-based, hence would have needed to compute the likelihood of each feature of the observation.</p>
<p>$Y$ denotes the state sequence, $X$ denotes the observation.</p>
<p><strong>HMM</strong></p>
<script type="math/tex; mode=display">P(Y|X) = \prod_{i=1}^n P(x_i|y_i) \times \prod_{i=1}^n P(y_i|y_{i-1})</script><p><strong>MEMM</strong></p>
<script type="math/tex; mode=display">P(Y|X) = \prod_{i=1}^n P(y_i|y_{i-1},x_i)</script><p>To estimate the individual probability of a transition from a state $q’$ to a state $q$ producing an observation $o$, build a MaxEnt model:</p>
<script type="math/tex; mode=display">P(y|y',x) = \frac{1}{Z(x,y')} \exp(\sum_i w_i f_i(x,y))</script><hr>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><h2 id="Linear-Chain-CRF"><a href="#Linear-Chain-CRF" class="headerlink" title="Linear-Chain CRF"></a>Linear-Chain CRF</h2><p>Modeling the distribution of a set of ouputs $y_{1:T}$ given an input vector $\mathbf{x}$.</p>
<script type="math/tex; mode=display">p(y_{1:T}|x, \mathbf{\lambda}) = \frac{1}{Z(\mathbf{x}, \mathbf{\lambda})} \prod_{t=2}^T \phi_t (y_t, y_{t-1}, \mathbf{x}, \mathbf{\lambda})</script><p>where $\lambda$ are the free parameters of the potentials, common form:</p>
<script type="math/tex; mode=display">\exp(\sum_{k=1}^K \lambda_k f_{k,t}(y_t,y_{t-1},\mathbf{x}))</script><p>where <script type="math/tex">f_{k,t}(y_t,y_{t-1}, \mathbf{x})</script> are <strong>features</strong>. </p>
<p>Thus,</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y_{1:T}|x, \mathbf{\lambda}) &{}= \frac{1}{Z(\mathbf{x}, \mathbf{\lambda})} \prod_{t=2}^T \exp(\sum_{k=1}^K \lambda_k f_{k,t}(y_t,y_{t-1},\mathbf{x})) \\
&{}= \frac{1}{Z(\mathbf{x}, \mathbf{\lambda})}  \exp(\sum_{t=2}^T \sum_{k=1}^K \lambda_k f_{k,t}(y_t,y_{t-1},\mathbf{x}))
\end{aligned}</script><p>Given a set of input-output sequence pairs, <script type="math/tex">\mathbf{x}^n, y_{1:T}^n, n=\{1,2,...,N\}</script>. We can learn the parameters $\lambda$ by Maximum Likelihood. Under the i.i.d. data assumption, the log likelihood is:</p>
<script type="math/tex; mode=display">L(\mathbf{\lambda}) = \sum_{t,n} \sum_{k} \lambda_k f_k (y^n_t, y^n_{t-1}, x^n) - \sum_n \log Z(\mathbf{x}^n, \mathbf{\lambda})</script><p><strong>Cons</strong>: heavily rely on engineering features</p>
<hr>
<h1 id="BiLSTM-CRF"><a href="#BiLSTM-CRF" class="headerlink" title="BiLSTM-CRF"></a>BiLSTM-CRF</h1><p><strong>Intuition</strong>:<br>Capture both information from history (forward LSTM) and future (backward LSTM) using bi-LSTM; Also use <em>sentence level</em> tag information (after concatenation of both forward and backward LSTM/GRU hidden states in each time step) followed by the CRF model; Each output after concatenation can be regarded as a sentence level output.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Huang, Z., Xu, W., & Yu, K. (2015). [Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/pdf/1508.01991.pdf). arXiv preprint arXiv:1508.01991.
">[2]</span></a></sup> CRF can help learn the boundary constraints, for example, ‘B-‘ is the start of a tag.</p>
<div class="note info">
            <p><strong>Pros</strong>: More robust, less affected by the removal of engineering features</p><p><strong>Cons</strong>: RNNs are not as GPU-efficient as CNNs in terms of training speed and efficiency.</p>
          </div>
<p>Let $y$ be the tag sequence and $x$ an input word sequence. Then, we have</p>
<script type="math/tex; mode=display">
P(y \vert x) = \frac{\exp (\textrm{score}(x,y))}{\sum_{y'} \exp(\textrm{score}(x,y'))}</script><p>where </p>
<script type="math/tex; mode=display">
\begin{aligned}
\textrm{score}(x,y) &{}=\sum_{i} \psi_i (x,y) \\
&{}= \sum_i \log \psi_{textrm{emit}} (y_i \rightarrow x_i) + \log \psi_{\textrm{trans}} (y_{i-1} \rightarrow y_i) \\
&{}= \sum_i h_i [y_i] + \mathbf{P}_{y_i, y_i-1}
\end{aligned}</script><p>In BiLSTM-CRFs, two potentials: emission and transition. The emission potential for the word $i$ comes from the hidden state of the BiLSTM at timestep $i$. The transition scores are stored at $\mathbf{P} \in \mathbb{R}^{|T| \times |T|}$. In the following implementation<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[PyTorch BiLSTM-CRF](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)
">[8]</span></a></sup>, $\mathbf{P}_{y_i, y_i-1}$ is the score of transitioning to tag $i-1$ from tag $i-1$.</p>
<p><img data-src="/notes/images/bilstm-crf.png" alt="BiLSTM-CRF"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># util function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">vec</span>):</span></span><br><span class="line">    <span class="comment"># return the argmax as a python int</span></span><br><span class="line">    _, idx = torch.<span class="built_in">max</span>(vec, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span>(<span class="params">seq, to_ix</span>):</span></span><br><span class="line">    idxs = [to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute log sum exp in a numerically stable way for the forward algorithm</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span>(<span class="params">vec</span>):</span></span><br><span class="line">    max_score = vec[<span class="number">0</span>, argmax(vec)]</span><br><span class="line">    max_score_broadcast = max_score.view(<span class="number">1</span>, -<span class="number">1</span>).expand(<span class="number">1</span>, vec.size()[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> max_score + \</span><br><span class="line">        torch.log(torch.<span class="built_in">sum</span>(torch.exp(vec - max_score_broadcast)))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># BiLSTM CRF</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, tag_to_ix, embedding_dim, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BiLSTM_CRF, self).__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.tag_to_ix = tag_to_ix</span><br><span class="line">        self.tagset_size = <span class="built_in">len</span>(tag_to_ix)</span><br><span class="line"></span><br><span class="line">        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim // <span class="number">2</span>,</span><br><span class="line">                            num_layers=<span class="number">1</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Maps the output of the LSTM into tag space.</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Matrix of transition parameters.  Entry i,j is the score of</span></span><br><span class="line">        <span class="comment"># transitioning *to* i *from* j.</span></span><br><span class="line">        <span class="comment"># $\mathbf&#123;P&#125;_&#123;y_i, y_i-1&#125;$</span></span><br><span class="line">        self.transitions = nn.Parameter(</span><br><span class="line">            torch.randn(self.tagset_size, self.tagset_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># These two statements enforce the constraint that we never transfer</span></span><br><span class="line">        <span class="comment"># to the start tag and we never transfer from the stop tag</span></span><br><span class="line">        self.transitions.data[tag_to_ix[START_TAG], :] = -<span class="number">10000</span></span><br><span class="line">        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -<span class="number">10000</span></span><br><span class="line"></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (torch.randn(<span class="number">2</span>, <span class="number">1</span>, self.hidden_dim // <span class="number">2</span>),</span><br><span class="line">                torch.randn(<span class="number">2</span>, <span class="number">1</span>, self.hidden_dim // <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_alg</span>(<span class="params">self, feats</span>):</span></span><br><span class="line">        <span class="comment"># Do the forward algorithm to compute the partition function</span></span><br><span class="line">        init_alphas = torch.full((<span class="number">1</span>, self.tagset_size), -<span class="number">10000.</span>)</span><br><span class="line">        <span class="comment"># START_TAG has all of the score.</span></span><br><span class="line">        init_alphas[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Wrap in a variable so that we will get automatic backprop</span></span><br><span class="line">        forward_var = init_alphas</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate through the sentence</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">            alphas_t = []  <span class="comment"># The forward tensors at this timestep</span></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> <span class="built_in">range</span>(self.tagset_size):</span><br><span class="line">                <span class="comment"># broadcast the emission score: it is the same regardless of</span></span><br><span class="line">                <span class="comment"># the previous tag</span></span><br><span class="line">                emit_score = feat[next_tag].view(</span><br><span class="line">                    <span class="number">1</span>, -<span class="number">1</span>).expand(<span class="number">1</span>, self.tagset_size)</span><br><span class="line">                <span class="comment"># the ith entry of trans_score is the score of transitioning to</span></span><br><span class="line">                <span class="comment"># next_tag from i</span></span><br><span class="line">                trans_score = self.transitions[next_tag].view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># The ith entry of next_tag_var is the value for the</span></span><br><span class="line">                <span class="comment"># edge (i -&gt; next_tag) before we do log-sum-exp</span></span><br><span class="line">                next_tag_var = forward_var + trans_score + emit_score</span><br><span class="line">                <span class="comment"># The forward variable for this tag is log-sum-exp of all the</span></span><br><span class="line">                <span class="comment"># scores.</span></span><br><span class="line">                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="number">1</span>))</span><br><span class="line">            forward_var = torch.cat(alphas_t).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        alpha = log_sum_exp(terminal_var)</span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lstm_features</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line">        embeds = self.word_embeds(sentence).view(<span class="built_in">len</span>(sentence), <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        lstm_out, self.hidden = self.lstm(embeds, self.hidden)</span><br><span class="line">        lstm_out = lstm_out.view(<span class="built_in">len</span>(sentence), self.hidden_dim)</span><br><span class="line">        lstm_feats = self.hidden2tag(lstm_out)</span><br><span class="line">        <span class="keyword">return</span> lstm_feats</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_score_sentence</span>(<span class="params">self, feats, tags</span>):</span></span><br><span class="line">        <span class="comment"># Gives the score of a provided tag sequence</span></span><br><span class="line">        score = torch.zeros(<span class="number">1</span>)</span><br><span class="line">        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])</span><br><span class="line">        <span class="keyword">for</span> i, feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(feats):</span><br><span class="line">            score = score + \</span><br><span class="line">                self.transitions[tags[i + <span class="number">1</span>], tags[i]] + feat[tags[i + <span class="number">1</span>]]</span><br><span class="line">        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span>(<span class="params">self, feats</span>):</span></span><br><span class="line">        backpointers = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the viterbi variables in log space</span></span><br><span class="line">        init_vvars = torch.full((<span class="number">1</span>, self.tagset_size), -<span class="number">10000.</span>)</span><br><span class="line">        init_vvars[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward_var at step i holds the viterbi variables for step i-1</span></span><br><span class="line">        forward_var = init_vvars</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">            bptrs_t = []  <span class="comment"># holds the backpointers for this step</span></span><br><span class="line">            viterbivars_t = []  <span class="comment"># holds the viterbi variables for this step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> <span class="built_in">range</span>(self.tagset_size):</span><br><span class="line">                <span class="comment"># next_tag_var[i] holds the viterbi variable for tag i at the</span></span><br><span class="line">                <span class="comment"># previous step, plus the score of transitioning</span></span><br><span class="line">                <span class="comment"># from tag i to next_tag.</span></span><br><span class="line">                <span class="comment"># We don&#x27;t include the emission scores here because the max</span></span><br><span class="line">                <span class="comment"># does not depend on them (we add them in below)</span></span><br><span class="line">                next_tag_var = forward_var + self.transitions[next_tag]</span><br><span class="line">                best_tag_id = argmax(next_tag_var)</span><br><span class="line">                bptrs_t.append(best_tag_id)</span><br><span class="line">                viterbivars_t.append(next_tag_var[<span class="number">0</span>][best_tag_id].view(<span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Now add in the emission scores, and assign forward_var to the set</span></span><br><span class="line">            <span class="comment"># of viterbi variables we just computed</span></span><br><span class="line">            forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">            backpointers.append(bptrs_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transition to STOP_TAG</span></span><br><span class="line">        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        best_tag_id = argmax(terminal_var)</span><br><span class="line">        path_score = terminal_var[<span class="number">0</span>][best_tag_id]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Follow the back pointers to decode the best path.</span></span><br><span class="line">        best_path = [best_tag_id]</span><br><span class="line">        <span class="keyword">for</span> bptrs_t <span class="keyword">in</span> <span class="built_in">reversed</span>(backpointers):</span><br><span class="line">            best_tag_id = bptrs_t[best_tag_id]</span><br><span class="line">            best_path.append(best_tag_id)</span><br><span class="line">        <span class="comment"># Pop off the start tag (we dont want to return that to the caller)</span></span><br><span class="line">        start = best_path.pop()</span><br><span class="line">        <span class="keyword">assert</span> start == self.tag_to_ix[START_TAG]  <span class="comment"># Sanity check</span></span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> path_score, best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span>(<span class="params">self, sentence, tags</span>):</span></span><br><span class="line">        feats = self._get_lstm_features(sentence) <span class="comment"># emission scores</span></span><br><span class="line">        forward_score = self._forward_alg(feats) <span class="comment"># partition function</span></span><br><span class="line">        gold_score = self._score_sentence(feats, tags) <span class="comment"># correct score (numerator) $\exp(\psi(\cdot))$</span></span><br><span class="line">        <span class="keyword">return</span> forward_score - gold_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sentence</span>):</span>  <span class="comment"># dont confuse this with _forward_alg above.</span></span><br><span class="line">        <span class="comment"># Get the emission scores from the BiLSTM</span></span><br><span class="line">        lstm_feats = self._get_lstm_features(sentence) <span class="comment"># emission scores</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Find the best path, given the features.</span></span><br><span class="line">        score, tag_seq = self._viterbi_decode(lstm_feats)</span><br><span class="line">        <span class="keyword">return</span> score, tag_seq</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="Stack-LSTM"><a href="#Stack-LSTM" class="headerlink" title="Stack-LSTM"></a>Stack-LSTM</h1><p>Char-based word representation indicates the word internal information, whilst pretrained word embeddings hold contextual text information.<br>Then, concat <strong>character-based word representation</strong> using Bi-LSTM and <strong>pretrained word embeddings</strong> as the final embeddings. <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016). [Neural Architectures for Named Entity Recognition](https://arxiv.org/pdf/1603.01360.pdf). arXiv preprint arXiv:1603.01360.
">[6]</span></a></sup></p>
<p><img data-src="/notes/images/stack-LSTM.png" alt="Stack LSTM"></p>
<hr>
<h1 id="ID-CNN-CRF"><a href="#ID-CNN-CRF" class="headerlink" title="ID-CNN-CRF"></a>ID-CNN-CRF</h1><p><strong>Pros</strong>: fast, resource-efficient</p>
<p><strong>Dilation convolutions</strong>: </p>
<ul>
<li>context does not need to be consecutive</li>
<li><blockquote class="blockquote-center">
            <i class="fa fa-quote-left"></i>
            <p>By stacking layers of dilated convolutions of <strong>exponentially dilation width</strong>, we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers: the size of the effective input width for a token at layer <em>l</em> is now given by 2<sup>l+1</sup>-1 </p>

            <i class="fa fa-quote-right"></i>
          </blockquote></li>
<li>Let x<sub>t</sub> denotes the token in timestep t, W<sub>t</sub> is a sliding window of width $r$ tokens on either side of each token in the sequence, the conventional convolution output output c<sub>t</sub> is:<br>  <script type="math/tex">c_t= W_c \bigoplus_{k=0}^{r} x_{t} \pm k</script>,  where $\bigoplus$ is vector concatenation.</li>
<li>Dilated convolution is defined over a wider effective input width by skpping over \delta inputs, where $\delta$ is the dilation width. The dialated convolution is:  <script type="math/tex">c_t= W_c \bigoplus_{k=0}^{r} x_{t} \pm k \delta</script><br>  When $\delta$ is 1, dialated convolution is equivalent to a simple convolution.</li>
</ul>
<p><strong>Model</strong>: <strong>Stacked dilated CNNs</strong> <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Strubell, E., Verga, P., Belanger, D., & McCallum, A. (2017). [Fast and accurate entity recognition with iterated dilated convolutions](https://arxiv.org/pdf/1702.02098). arXiv preprint arXiv:1702.02098.
">[3]</span></a></sup></p>
<p> Let the <script type="math/tex">j_{th}</script> dilated Conv layer of dilation width $\delta$ as <script type="math/tex">D_{\delta}^{(j)}</script>, <script type="math/tex">L_c</script> layers output:</p>
<script type="math/tex; mode=display">c_t^{(j)} = Relu(D_{2^{L_c-1}}^{(j-1)} c_t^{j-1})</script><p> And add a final dilation-1 layer to the stack:</p>
<script type="math/tex; mode=display">c_t^{L_c+1} = Relu(D_1^{(L_c)} c_t^{L_C})</script><p> Finally, apply the simple affine transformation $W_0$ to the final representation for each token <script type="math/tex">x_t</script>:</p>
<script type="math/tex; mode=display">h_T^{(L_b)} = W_O b_t^{L_b}</script><p>ID-CNNs generate token-wise representation (corresponding to logits for each token in RNNs) for each token. </p>
<p><strong>In comparison with BiLSTM-CRF</strong>  </p>
<p>Bi-LSTM or ID-CNNs calculates the logits for each token, whilst CRF layer capture the transmission probability of sequential inputs, i.e. use NNs to predict each token’s label independently, or apply <em>Viterbi inference</em> in a chain structured graphical model.</p>
<p><img data-src="/notes/images/ID-CNNs.png" alt="IDCNN"></p>
<hr>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>Bidirectional Transformer for Language model, with pretraining methods of Masked Language Models (predicting randomly masked 15% tokens of each sentence) and next sentence prediction (to capture information of ajacent sentences).<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf?fbclid=IwAR3FQiWQzP7stmPWZ4kzrGmiUaN81UpiNeq4GWthrxmwgX0B9f1CvuXJC2E). arXiv preprint arXiv:1810.04805.
">[5]</span></a></sup></p>
<p><img data-src='/notes/images/BERT-NER.png' width='60%'/></p>
<hr>
<h1 id="Chinese-NER"><a href="#Chinese-NER" class="headerlink" title="Chinese NER"></a>Chinese NER</h1><div class="note warning">
            <p><strong>Chinese NER</strong><br><em>Word-based approach</em></p><ul><li>Intuitive pipeline: segmentation &rarr; NER</li><li>Suffer from segmentation error: <em>NE</em> imports OOV errors in segmentation, and incorrectly segmented entity boundaries lead to NER errors.</li></ul><p><em>Char-based approach</em></p><ul><li>Char-based NER outperform word-based approach in <em>Chinese</em> NER.</li><li><strong>Cons</strong>: Explicit word and word sequence information are not fully exploited.</li></ul><p><em>Solution</em>:</p><ul><li>Lattice LSTM: leverage both char sequence and lexicon word information.</li><li>FLAT</li></ul>
          </div>
<h2 id="Char-based-LSTM-CRF-model"><a href="#Char-based-LSTM-CRF-model" class="headerlink" title="Char-based LSTM-CRF model"></a>Char-based LSTM-CRF model</h2><ul>
<li><p>$\mathbf{x}_j^c = \mathbf{e}^c(c_j)$, where $\mathbf{e}^c$ denotes a char embedding lookup table.</p>
</li>
<li><p>Bi-LSTM output: $\mathbf{h_j^c} = [\overrightarrow{\mathbf{h}_j^c} ; \overleftarrow{\mathbf{h}_j^c}]$</p>
</li>
<li><p>CRF model use  $\mathbf{h_1^c}, \mathbf{h_2^c}, …,\mathbf{h_m^c}$ for sequence labelling.</p>
</li>
</ul>
<h3 id="Char-bi-char"><a href="#Char-bi-char" class="headerlink" title="Char + bi-char"></a>Char + bi-char</h3><p> Concat char embeddings with char-bigram embeddings.</p>
<script type="math/tex; mode=display">\mathbf{x_j^c} = [ \mathbf{e}^c(c_j) ; \mathbf{e}^b(c_j, c_{j+1}) ]</script><p>,where $\mathbf{e}^b$ denotes a char bigram lookup table</p>
<h3 id="Char-softword"><a href="#Char-softword" class="headerlink" title="Char + softword"></a>Char + softword</h3><ul>
<li>add segmentation as soft features by concatenating <strong>segmentation label embedding</strong> to char embeddings</li>
</ul>
<script type="math/tex; mode=display">\mathbf{x}_j^c = [\mathbf{e}^c (c_j) ; \mathbf{e^s}(seg(c_j)) ]</script><p>, where $\mathbf{e^s}$ signals the segmentation label on the char $c_j$ given by a word segmentor with <strong>BMES</strong> scheme.</p>
<p><img data-src="/notes/images/char-lstm.png" alt="Char LSTM"></p>
<h2 id="Word-based-model"><a href="#Word-based-model" class="headerlink" title="Word-based model"></a>Word-based model</h2><p>The input for each word $w_i$:  $\mathbf{x_i^w} = \mathbf{e}^w (w_i)$, where $e^w$ denotes the word embedding lookup table.<br>Then feed word embeddings into bi-directional LSTMs.</p>
<p><img data-src="/notes/images/word-lstm.png" alt="Word LSTM"></p>
<h3 id="Integrating-char-representations"><a href="#Integrating-char-representations" class="headerlink" title="Integrating char-representations"></a>Integrating char-representations</h3><blockquote>
<p>concat char-based word representation $\mathbf{x}_i^c$ (from cahr-LSTMs or char-CNNs) with pretrained word embeddings $\mathbf{e}^w(w_i)$:   $\mathbf{x}_i^w = [\mathbf{e}^w(w_i); \mathbf{x}_i^c]$</p>
</blockquote>
<p>1.<strong>Word + char bi-LSTMs</strong>: bi-LSTMs with chars as input in each time step.</p>
<script type="math/tex; mode=display">\mathbf{x}_i^c = [\overrightarrow{\mathbf{h}^c_{t(i,len(i))}} ; \overleftarrow{\mathbf{h}^c_{t(i,1)}}]</script><p>2.<strong>Word + char-uni-LSTM</strong></p>
<p>3.<strong>Word + char-CNNs</strong><br>Use char sequence of each word to obtain its char representation $\mathbf{x}_i^c$:</p>
<script type="math/tex; mode=display">\mathbf{x}_i^c = \max_{t(i,1)\leq j \leq t(i,len(i))}  (\mathbf{W}_{CNN}^T 
\bigl[ \begin{smallmatrix} \mathbf{e}^c (c_{j-\frac{k-1}{2}}) \\ ... \\ \mathbf{e}^c (c_{j+\frac{k-1}{2}}) \end{smallmatrix} \bigr] + \mathbf{b}_{CNN})</script><p>, where $ k=3 $ is the kernel size and $max$ denotes the max-pooling.</p>
<hr>
<h2 id="Lattice-LSTM"><a href="#Lattice-LSTM" class="headerlink" title="Lattice LSTM"></a>Lattice LSTM</h2><p>Let $s$ denotes input sequence.</p>
<ul>
<li><strong>Char-level</strong>: <script type="math/tex">s= c_1,c_2,...,c_m</script>, where <script type="math/tex">c_j</script> denotes the <script type="math/tex">j_{th}</script> character.</li>
<li><strong>Word-level</strong>: <script type="math/tex">s= w_1,w_2,...,w_n</script>, where <script type="math/tex">w_i</script> denotes the <script type="math/tex">i_{th}</script> token in the word sequence.</li>
</ul>
<p>In comparison with char-based model:</p>
<p>Char embedding:   $\mathbf{x}_j^c = \mathbf{e}^c(c_j)$</p>
<p>Basic recurrent LSTM:</p>
<script type="math/tex; mode=display">\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  = \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c)</script><script type="math/tex; mode=display">\mathbf{c}^c_j = \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j}</script><script type="math/tex; mode=display">\mathbf{h}_j^c = \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)</script><p>where $\mathbf{i}^c_j$, $\mathbf{f}^c_j$, $\mathbf{o}^c_j$ denotes a set of input, forget and output gates, respectively. $\mathbf{c}^c_j$ denotes the char cell vector, $\mathbf{h}^c_j$ denotes the hidden vector on each char $c_j$, $\mathbf{W}^{c^T}$, $\mathbf{b}^c$ are parameters.</p>
<p><img data-src="/notes/images/lattice-lstm-2.png" alt="Lattice-LSTM"></p>
<blockquote>
<p>The computation of cell vector <script type="math/tex">\mathbf{c}^c_j</script> considers lexicon subsequence <script type="math/tex">w_{b,e}^d</script> in the sentence<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zhang, Y., & Yang, J. (2018). [Chinese NER Using Lattice LSTM](https://arxiv.org/pdf/1805.02023.pdf). arXiv preprint arXiv:1805.02023.
">[4]</span></a></sup>. </p>
</blockquote>
<p>Each subsequence <script type="math/tex">w_{b,e}^{d}</script> is:</p>
<script type="math/tex; mode=display">\mathbf{x}_{b,e}^w = \mathbf{e}^w (w_{b,e}^d)</script><p>, where $\mathbf{e}^w$ denotes the word embedding lookup table.</p>
<p>The word cell <script type="math/tex">\mathbf{c}^w_{b,e}</script> represents the recurrent state of <script type="math/tex">\mathbf{x}^w_{b,e}</script> from the beginning of the sentence. The <script type="math/tex">\mathbf{c}^w_{b,e}</script> is:</p>
<script type="math/tex; mode=display">\left[\begin{array}{c} \mathbf{i}^w_{b,e}    \\ \mathbf{f}^w_{b,e}    \\ \tilde{c}^w_{b,e} \end{array}\right]  = \left[\begin{array}{c} \sigma    \\ \sigma    \\\tanh \end{array}\right]  (\mathbf{W}^{w^T} \left[\begin{array}{c} \mathbf{x}^w_{b,e}    \\ \mathbf{h}^c_b\end{array}\right] + \mathbf{b}^w)</script><script type="math/tex; mode=display">\mathbf{c}^w_{b,e} = \mathbf{f}^w_{b,e} \odot \mathbf{c}^c_b + \mathbf{i}^w_{b,e} \odot \tilde{c}^w_{b,e}</script><p>where <script type="math/tex">\mathbf{i}^w_{b,e}</script>, <script type="math/tex">\mathbf{f}^w_{b,e}</script> are input and forget gates. There is <strong>no output gate</strong> for word cells since <strong>labelling is performed only at the char level</strong>.</p>
<p>Here <script type="math/tex">\mathbf{c}^w_{b,e}</script> as an additional recurrent path for information flow for each <script type="math/tex">\mathbf{c}^c_{j}</script> . It applies an additional gate <script type="math/tex">\mathbf{i}^c_{b,e}</script> for each subsequence cell <script type="math/tex">\mathbf{c}^w_{b,e}</script> for controlling its contribution into <script type="math/tex">\mathbf{c}^w_{b,e}</script>:</p>
<script type="math/tex; mode=display">\mathbf{i}^c_{b,e} = \sigma (\mathbf{W}^{l^T} \left[\begin{array}{c} \mathbf{x}^c_{e}    \\ \mathbf{c}^w_{b,e} \end{array}\right] + \mathbf{b}^l)</script><p>Then cell vector <script type="math/tex">\mathbf{c}^c_j</script> is:</p>
<script type="math/tex; mode=display">\mathbf{c}^c_j = \mathbf{\alpha}_{j}^c \odot \tilde{\mathbf{c}}_j^c  + \Sigma_{b \in\{b'|w^d_{b',j}\} } \mathbf{\alpha}_{b,j}^c \odot \mathbf{c}^w_{b,j}</script><p>where the gate value <script type="math/tex">\mathbf{i}^c_{b,j}</script>, <script type="math/tex">\mathbf{i}^c_{j}</script> are normalised to <script type="math/tex">\mathbf{\alpha}_{b,j}^c</script> and <script type="math/tex">\mathbf{\alpha}_{j}^c</script> by setting the sum to 1:</p>
<script type="math/tex; mode=display">\mathbf{\alpha}_{b,j}^c  = \frac{\exp(\mathbf{i}^c_{b,j})}{\mathbf{i}^c_{j} + \Sigma_{b' \in {b''|w_{b'',j}^d}} \exp(\mathbf{i}^c_{b',j})}</script><script type="math/tex; mode=display">\mathbf{\alpha}_{j}^c  = \frac{\exp(\mathbf{i}^c_{j})}{\mathbf{i}^c_{j} + \Sigma_{b' \in {b''|w_{b'',j}^d}} \exp(\mathbf{i}^c_{b',j})}</script><script type="math/tex; mode=display">\mathbf{h}_j^c = \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)</script><p><img data-src="/notes/images/lattice-lstm.png" alt="Lattice LSTM"></p>
<p><strong>CRF decoding</strong></p>
<p>On top of <script type="math/tex">\mathbf{h}_1</script>, <script type="math/tex">\mathbf{h}_2</script>, …, <script type="math/tex">\mathbf{h}_{\tau}</script>, where $\tau$ is $n$ for char-based and lattice-based models and $m$ for word-based models. The probability of a label sequence <script type="math/tex">y = l_1, l_2, ... , l_{\tau}</script> is:</p>
<script type="math/tex; mode=display">P(y|s) = \frac{\exp(\Sigma_i (\mathbf{W}_{CRF}^{l_i} \mathbf{h}_i) +b_{CRF}^{(l_{i-1}, l_i)} )}{\Sigma_{y'} \exp(\Sigma_i (\mathbf{W}_{CRF}^{l'_i} \mathbf{h}_i) +b_{CRF}^{(l'_{i-1}, l'_i)} )}</script><p>where $y’$ denotes an arbitrary label sequence, <script type="math/tex">\mathbf{W}_{CRF}^{l_i}</script> is a model parameter specific to <script type="math/tex">l_i</script> and <script type="math/tex">b_{CRF}^{(l_{i-1}, l_i)}</script> is a bias specific to <script type="math/tex">l_{i-1}</script> and <script type="math/tex">l_i</script>.</p>
<p><strong>Sentence-level log-likelihood loss with L2 regularization</strong>:</p>
<script type="math/tex; mode=display">L = \sum_{i=1}^N log(P(y_i|s_i)) + \frac{\lambda}{2}||\Theta||^2</script><p>where $\lambda$ is the $L_2$ regularisation parameter and $\Theta$ represents the parameter set.</p>
<h2 id="FLAT"><a href="#FLAT" class="headerlink" title="FLAT"></a>FLAT</h2><p>Flat-Lattice Transformer (FLAT)</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jurafsky, D., &amp; Martin, J. H. (2014). Speech and language processing (Vol. 3). London: Pearson.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Huang, Z., Xu, W., &amp; Yu, K. (2015). <a href="https://arxiv.org/pdf/1508.01991.pdf">Bidirectional LSTM-CRF Models for Sequence Tagging</a>. arXiv preprint arXiv:1508.01991.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Strubell, E., Verga, P., Belanger, D., &amp; McCallum, A. (2017). <a href="https://arxiv.org/pdf/1702.02098">Fast and accurate entity recognition with iterated dilated convolutions</a>. arXiv preprint arXiv:1702.02098.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang, Y., &amp; Yang, J. (2018). <a href="https://arxiv.org/pdf/1805.02023.pdf">Chinese NER Using Lattice LSTM</a>. arXiv preprint arXiv:1805.02023.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). <a href="https://arxiv.org/pdf/1810.04805.pdf?fbclid=IwAR3FQiWQzP7stmPWZ4kzrGmiUaN81UpiNeq4GWthrxmwgX0B9f1CvuXJC2E">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. arXiv preprint arXiv:1810.04805.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp; Dyer, C. (2016). <a href="https://arxiv.org/pdf/1603.01360.pdf">Neural Architectures for Named Entity Recognition</a>. arXiv preprint arXiv:1603.01360.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://awni.github.io/label-bias/">The Label Bias Problem</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html">PyTorch BiLSTM-CRF</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/97676647">BiLSTM-CRF Explanation (in Chinese)</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
        <category>Sequence labeling</category>
        <category>NER</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Survey</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP Basics</title>
    <url>/notes/2018/06/29/NLP/NER/NLP-Basics-Summary/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A note for NLP Interview.<br><span id="more"></span></p>
<h1 id="Statistical-ML"><a href="#Statistical-ML" class="headerlink" title="Statistical ML"></a>Statistical ML</h1><h2 id="LR-vs-SVM"><a href="#LR-vs-SVM" class="headerlink" title="LR vs SVM"></a>LR vs SVM</h2><p>Difference:</p>
<ol>
<li>LR uses logistic loss, while SVM uses hinge loss.</li>
<li>LR is sensitive to outliers, while SVM is not.</li>
<li>SVM is suitable for small training set, while LR needs much.</li>
<li>LR tries to find a hyperplane that stays far away with all points (all points count), whereas SVM only aims at keeping away support vectors.</li>
<li>LR requires feature enginnering, SVM uses kernel trick.</li>
<li>SVM is non-parametric methods, whereas LR is parametric model.</li>
</ol>
<h3 id="Logistic-Regression-LR"><a href="#Logistic-Regression-LR" class="headerlink" title="Logistic Regression (LR)"></a>Logistic Regression (LR)</h3><p>Logistic Regression (LR) is a linear mapping from features $x$ to labels $y \in \{ 0,1 \}$ with sigmoid function $g(z)=1/(1+\exp(-z))$.</p>
<p>The LR is fomulated as:</p>
<script type="math/tex; mode=display">
\begin{equation}
h_{\theta} = g(\theta^\top x) = \frac{1}{1+\exp(-\theta^\top x)}
\end{equation}</script><p>The derivative of sigmoid function is:</p>
<script type="math/tex; mode=display">
\begin{align}
g^\prime(z) &{}= \frac{d}{dz} \frac{1}{1+\exp(-\theta^\top x)}\\
&{}= \frac{1}{(1+\exp(-\theta^\top x))^2} (\exp(-\theta^\top x))\\
&{}= \frac{1}{1+\exp(-\theta^\top x)} \cdot \bigg( 1- \frac{1}{1+\exp(-\theta^\top x)} \bigg) \\
&{}= g(z)(1-g(z))
\end{align}</script><p>LR can be used for binary classification, thus</p>
<script type="math/tex; mode=display">
\begin{align}
 P(y=1 \vert x; \theta) &{}= h_{\theta} (x)\\
 P(y=0 \vert x; \theta) &{}= (1-h_{\theta} (x))
\end{align}</script><p>That is,</p>
<script type="math/tex; mode=display">
p(y \vert x, \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{(1-y)}</script><p>Given the training data, the features $x = \{ x_1, x_2, \cdots, x_m \}$ and labels $y = \{ y_1, y_2, \cdots, y_m \}$. The maximum likelihood function is:</p>
<script type="math/tex; mode=display">
\begin{align}
\ell (\theta) &{}= \log \mathcal{L}(\theta) \\
&{}= \sum_{i=1}^m y^{(i)} \log h(x^{(i)}) + (1-y^{(i)}) \log (1-h(x^{(i)}))
\end{align}</script><p>With gradient ascend algorithm, we have $\theta : \theta + \alpha \nabla_{\theta}\ell (\theta)$.</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial \theta_j} &{}= \bigg( y \frac{1}{g(\theta^T x)} - (1-y) \frac{1}{1-g(\theta^T x)} \bigg) \frac{\partial}{\partial \theta_j} g(\theta^T x) \\
&{}= \bigg( y \frac{1}{g(\theta^T x)} - (1-y) \frac{1}{1-g(\theta^T x)} \bigg) g(\theta^T x) (1-g(\theta^T x)) \frac{\partial}{\partial \theta_j} \theta^T x \\
&{}= (y (1-g(\theta^T x)) - (1-y)g(\theta^T x)) x_j \\
&{}= (y-h_\theta (x)) x_j
\end{align}</script><p>If we only use one sample to train, the update can be formulated as:</p>
<script type="math/tex; mode=display">
\theta_j: \theta_j + \alpha (y^{(i)} - h_\theta (x^{(i)})) x_{j}^{(i)}</script><p>The loss function is:</p>
<script type="math/tex; mode=display">
\begin{align}
J(w, b) &{}= \frac{1}{m} \sum_{i=1}^m \mathcal{L} (\hat{y}^{(i)}, y^{(i)})\\
&{}= \frac{1}{m} \sum_{i=1}^m (-y \log (\hat{y}^{(i)})- (1-y) \log (1-\hat{y}^{(i)}))
\end{align}</script><h3 id="Linear-SVM"><a href="#Linear-SVM" class="headerlink" title="Linear SVM"></a>Linear SVM</h3><p>Given a training dataset of $m$ points of the form $(\mathbf{x}_1,y_1)， \cdots,(\mathbf{x}_m,y_m)$, where $y \in \{-1,1\}$, each indicating the calss to which the point $x_i$ belongs. We want to find the maximum-margin hyperplane that divides the group of points $\mathbf{x}_i$ into two groups so that the distance between the hyperplane and the nearest point from either group is maximized.</p>
<p>Any hyperplane can be written as the set of points $\mathbf{x}$ satisfying $\mathbf{w}^T\mathbf{x}+\mathbf{b}=0$.</p>
<h4 id="Hard-margin"><a href="#Hard-margin" class="headerlink" title="Hard margin"></a>Hard margin</h4><p>If the training data is linearly separable, we can select to parallel hyperplanes that separate the two clases of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the “margin”, and the maximum-margin hyperplanes is the hyperplane that lies halfway between them.<br>The optimization aims to “minimize $\Vert \mathbf{w} \Vert$ subject to $y_i (w^T x_i + b) \geq 1$ for $\forall i$”:</p>
<ol>
<li>Maximize the margin: $ \min_{\mathbf{w,b}} \frac{1}{2} \Vert \mathbf{w} \Vert^2$</li>
<li>Classify: $y_i (w^T x_i + b) \geq 1, \quad i=1,2,3,\cdots,m$</li>
</ol>
<p>where the $\mathbf{w},\mathbf{b}$ determine our classifier $\mathbf{x} \rightarrow \textrm{sign} (\mathbf{w}^T \mathbf{x} + \mathbf{b})$</p>
<h4 id="Soft-margin"><a href="#Soft-margin" class="headerlink" title="Soft margin"></a>Soft margin</h4><ul>
<li><p>Hinge Loss<br>When the data are not linearly separable, the hinge loss<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: Hinge Loss](https://en.wikipedia.org/wiki/Hinge_loss)
">[1]</span></a></sup> is helpful:</p>
<script type="math/tex; mode=display">
\begin{align}
\max(0, 1- \underbrace{y_i}_\textrm{label} \underbrace{(\mathbf{w}^T \mathbf{x}_i + \mathbf{b})}_\textrm{prediction})
\end{align}</script><p>The hinge loss is zero if the constraint $y_i (w^T x_i + b) \geq 1$ is satisfied, <em>i.e.</em>, if $\mathbf{x}_i$ lies on the correct side of the margin. For data on the wrong side of the margin (-1 vs 1), the hinge loss is proportional to the distance from the margin.</p>
</li>
<li><p>Soft margin objective<br>The optimization goal is to minimize</p>
<script type="math/tex; mode=display">
\begin{align}
\lambda \Vert \mathbf{w} \Vert^2 +  \bigg[ \frac{1}{n} \sum_{i=1}^n \max (0, 1- \underbrace{y_i}_\textrm{label}\underbrace{(\mathbf{w}^T\mathbf{x}_i + \mathbf{b}))}_\textrm{prediction} \bigg]
\end{align}</script><p>where the parameter $\lambda$ determines the trade-off between increasing the margin size and ensuring that the $\mathbf{x}_i$ lie on the correct side of the margin. Thus, for sufficiently small values of $\lambda$, the second term in the loss function will become negligible, hence it will behave similar to the hard-margin SVM.</p>
</li>
</ul>
<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Let $x$ represent the observation, $y$ denote the labels. CRF can be formulated as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y|x) = \frac{\exp(\textrm{score}(x,y))}{\sum_{y'}\exp(\textrm{score}(x,y'))}
\end{aligned}</script><p>where </p>
<script type="math/tex; mode=display">\textrm{score}(x,y) = \sum_{i} T_{y_i, y_{i+1}} + \sum_{i} E_{i, y_i}</script><p>The loss function would be given as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell &{}= -\log (p (y \vert x))\\
&{}= - \textrm{score}(x,y) + \log (\sum_{y'} (\exp(\textrm{score}(x, y'))))
\end{aligned}</script><h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><h3 id="GBDT-Xgboost"><a href="#GBDT-Xgboost" class="headerlink" title="GBDT / Xgboost"></a>GBDT / Xgboost</h3><h2 id="L1-L2-Regularization"><a href="#L1-L2-Regularization" class="headerlink" title="L1/L2 Regularization"></a>L1/L2 Regularization</h2><h3 id="L1-regularization"><a href="#L1-regularization" class="headerlink" title="L1 regularization"></a>L1 regularization</h3><p>The L1 regularization is given as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
C = C_0 + \frac{\lambda}{2n} \sum_w \Vert w \Vert^2 \\
\end{aligned}</script><p>Thus,</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w} = \frac{\partial C}{\partial w} + \frac{\lambda}{n} w</script><p>The weight update is:</p>
<script type="math/tex; mode=display">
\begin{aligned}
w & \rightarrow w - \eta \frac{\partial C_0}{\partial w} - \frac{\eta \lambda}{n} \\
&{}= \underbrace{\big( 1- \frac{\eta \lambda}{n} \big) w }_{\Downarrow decrease }- \eta \frac{\partial C_0}{\partial w}
\end{aligned}</script><h3 id="L2-regularization"><a href="#L2-regularization" class="headerlink" title="L2 regularization"></a>L2 regularization</h3><p>The L2 regularization is given as:</p>
<script type="math/tex; mode=display">
\begin{aligned}
C &{}= C_0 + \frac{\lambda}{2n} \sum_w \vert w \vert 
\end{aligned}</script><p>Thus, </p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial w} = \frac{\partial C}{\partial w} + \frac{\lambda}{n} \textrm{sgn} (w)</script><p>The weight update is:</p>
<script type="math/tex; mode=display">
\begin{aligned}
w & \rightarrow w - \frac{\eta \lambda}{n} \textrm{sgn}(w) - \eta \frac{\partial C_0}{\partial w} 
\end{aligned}</script><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h3><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><h2 id="Class-Imbalance-Long-tailed-Learning"><a href="#Class-Imbalance-Long-tailed-Learning" class="headerlink" title="Class Imbalance / Long-tailed Learning"></a>Class Imbalance / Long-tailed Learning</h2><p>Extant <strong>class imbalance</strong><sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Long-Tail Learning via Logit Adjustment](https://arxiv.org/pdf/2007.07314.pdf)
">[12]</span></a></sup><sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Data Imbalance blog](https://kexue.fm/archives/7615)">[13]</span></a></sup> methods:</p>
<ol>
<li>the <em>input</em> to a model (Data modification)<ul>
<li>Under-sampling</li>
<li>Over-sampling</li>
<li>Feature Transfer</li>
</ul>
</li>
<li>the <em>output</em> of a model (Post-hoc correction of the decision threshold)<ul>
<li>Modify threshold</li>
<li>Normalize weights</li>
</ul>
</li>
<li>the <em>internals</em> of a model (e.g., loss function)<ul>
<li>Loss balancing</li>
<li>Volume weighting</li>
<li>Average top-k loss</li>
<li>Domain adaptation</li>
<li>Label aware margin</li>
</ul>
</li>
</ol>
<h2 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h2><h3 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL Divergence"></a>KL Divergence</h3><p>Kullback-Leibler (KL) divergence<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
">[9]</span></a></sup> (a.k.a, relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. </p>
<p>Consider two probability distributions $P$ and $Q$. Usually, $P$ represents the data, the observations, or a measured probability distribution. Distribution $Q$ represents instead a theory, a model, a description or an approxmation of $P$. The KL divergence is then interpreted as <strong>the average difference of the number of bits required for encoding samples of $P$ using a code optimized for $Q$ rather than one optimized for $P$</strong>.</p>
<p>For discrete probability distributions $P$ and $Q$ defined on the same probability space $\chi$, the <strong>relative entropy from $Q$ to $P$</strong> is defined to be:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{KL}(P \Vert Q) &{}= \sum_{x \in \chi} P(x) \log \bigg( \frac{P(x)}{Q(x)} \bigg) \\
&{}= - \sum_{x \in \chi} P(x) \log \bigg( \frac{Q(x)}{P(x)} \bigg)
\end{align}</script><p>The relative entropy can be interpreted as the expected message-length per datum that must be communicated if a code that is optimal for a given (wrong) distribution $Q$ is used, compared to using a code based on the true distribution $P$.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{KL} (P \Vert Q) &{}= - \sum_{x \in \chi} p(x) \log q(x) + \sum_{x \in \chi} p(x) \log p(x) \\
&{}= \mathbb{H}(P \vert Q) - \mathbb{H}(P)
\end{align}</script><p>where $\mathbb{H}(P \vert Q)$ indicates the cross entropy of P and Q, $\mathbb{H}(P)$ is the entropy of P.</p>
<h4 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Non-negative</li>
<li>Asymmetric</li>
</ol>
<h3 id="JS-Divergence"><a href="#JS-Divergence" class="headerlink" title="JS Divergence"></a>JS Divergence</h3><p>Jensen-Shannon (JS) divergence is a measure of similarity between two probablity distributions.</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{JS}(P \vert Q) = \frac{1}{2} \mathbb{KL}(P \Vert M) + \frac{1}{2} \mathbb{KL}(Q \Vert M)
\end{align}</script><p>where $M = \frac{1}{2}(P+Q)$</p>
<h4 id="Properties-1"><a href="#Properties-1" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Symmetric</li>
<li>Bound $0 \leq JSD \leq 1$</li>
</ol>
<h3 id="Mutual-Information"><a href="#Mutual-Information" class="headerlink" title="Mutual Information"></a>Mutual Information</h3><p>Mutual Information (MI)<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Wiki: Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)
">[10]</span></a></sup> measures the mutual dependence between the two variables. </p>
<script type="math/tex; mode=display">
\begin{align}
I(X; Y) &{}= \mathbb{KL} (P(X,Y) \Vert P(X)P(Y)) \\
&{}= \mathbb{E}_{X} \{\mathbb{KL}(P(Y \vert X) \Vert P(Y))\}\\
&{}= \mathbb{E}_{Y} \{\mathbb{KL}(P(X \vert Y) \Vert P(X))\}
\end{align}</script><p>For discrete variables $X$ and $Y$ the MI is:</p>
<script type="math/tex; mode=display">
\begin{align}
I(X;Y) = \sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p_{(X,Y)} (x,y) \log \bigg( \frac{p_{(X,Y)}(x,y)}{p_X(x)_Yp(y)} \bigg)
\end{align}</script><h4 id="Properties-2"><a href="#Properties-2" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Non-negative: $I(X;Y) \geq 0$</li>
<li>Symmetry: $I(X;Y) = I(Y;X)$</li>
</ol>
<h2 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h2><h3 id="ROC-AUC"><a href="#ROC-AUC" class="headerlink" title="ROC / AUC"></a>ROC / AUC</h3><h4 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h4><p>Reiceiver Operating Characteristic (ROC) Curve is a plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.</p>
<ul>
<li>x-axis: false positive rate (FPR), a.k.a, sensitivity, recall, probability of detection.</li>
<li>y-axis: true positive rate (TPR), a.k.a. probability of false alarm.</li>
</ul>
<p>ROC is a comparison of two operating characteristics (TPR and FPR) as the criterion changes. </p>
<p>True Positive Rate (TPR) is a synonym for <strong>recall</strong> and is therefore defined as follows:</p>
<script type="math/tex; mode=display">
\textrm{TPR} = \frac{\textrm{TP}}{\textrm{TP+FN}}</script><p>False Positive Rate (FPR) is defined as follows:</p>
<script type="math/tex; mode=display">
\textrm{FPR} = \frac{\textrm{FP}}{\textrm{FP+TN}}</script><p>An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. </p>
<p><img data-src="/notes/images/ROC.png" width='40%' /></p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>Area under the ROC Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.</p>
<p><img data-src="/notes/images/AUC.png" width='40%' /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">y_pred = <span class="built_in">list</span>(np.random.uniform(<span class="number">.4</span>, <span class="number">.6</span>, <span class="number">2000</span>)) + <span class="built_in">list</span>(np.random.uniform(<span class="number">.5</span>, <span class="number">.7</span>, <span class="number">8000</span>))</span><br><span class="line">y_true = [<span class="number">0</span>] * <span class="number">2000</span> + [<span class="number">1</span>] * <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_auc</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    pair = <span class="built_in">list</span>(<span class="built_in">zip</span>(y_true, y_pred))</span><br><span class="line">    pair = <span class="built_in">sorted</span>(pair, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    df = pd.DataFrame([[x[<span class="number">0</span>], x[<span class="number">1</span>], i + <span class="number">1</span>] <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(pair)], columns=[<span class="string">&#x27;y_true&#x27;</span>, <span class="string">&#x27;y_pred&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> df[<span class="string">&#x27;y_pred&#x27;</span>].value_counts().items():</span><br><span class="line">        <span class="keyword">if</span> v == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        rank_mean = df[df[<span class="string">&#x27;y_pred&#x27;</span>] == k][<span class="string">&#x27;rank&#x27;</span>].mean()</span><br><span class="line">        df.loc[df[<span class="string">&#x27;y_pred&#x27;</span>] == k, <span class="string">&#x27;rank&#x27;</span>] = rank_mean</span><br><span class="line">    pos_df = df[df[<span class="string">&#x27;y_true&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line">    m = pos_df.shape[<span class="number">0</span>]</span><br><span class="line">    n = df.shape[<span class="number">0</span>] - m</span><br><span class="line">    <span class="keyword">return</span> (pos_df[<span class="string">&#x27;rank&#x27;</span>].<span class="built_in">sum</span>() - m * (m + <span class="number">1</span>) / <span class="number">2</span>) / (m * n)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(calc_auc(y_true, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="built_in">print</span>(roc_auc_score(y_true, y_pred))</span><br></pre></td></tr></table></figure>
<h3 id="F1-Measure"><a href="#F1-Measure" class="headerlink" title="F1-Measure"></a>F1-Measure</h3><ul>
<li>Micro-F1: calculate metrics globally by counting total TP,FN,FP</li>
<li>Macro-F1: calculate metrics for each label =&gt; unweighted mean.</li>
<li>Weighted-F1: calculate metrics for each label =&gt;  average weighted by support (# of true instances for each class)</li>
</ul>
<div class="note warning">
            <p><strong>Comparison between ROC and F1-measure</strong>:</p><ol><li>Both look at the precision scores (TPR): ROC looks at the True Positive Rate (TPR/Recall) and False Positive Rate (FPR) while F1 looks at Positive Predictive Value (PPV/Precision) and True Positive Rate (TPR/Recall).<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[F1 score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?]">[11]</span></a></sup></li><li><strong>F1 score</strong> cares more about the <strong>positive class</strong>, such as highly <strong>imbalanced</strong> dataset where the fraction of positive class is small.</li><li><strong>ROC</strong> cares equally about the <strong>positive and negative class</strong> or the dataset is quite <strong>balanced</strong>.</li></ol>
          </div>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><h2 id="Batch-Norm-vs-Layer-Norm"><a href="#Batch-Norm-vs-Layer-Norm" class="headerlink" title="Batch Norm vs Layer Norm"></a>Batch Norm vs Layer Norm</h2><script type="math/tex; mode=display">
y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</script><ul>
<li>BN normalizes along one batch (first dim), LN does on one sample (last dim). </li>
<li>Refer to <a href="/notes/2019/02/28/NN/Normalization-in-Neural-Networks-a-Summary/">details</a></li>
</ul>
<h2 id="Gradient-Vanishing-Exploding"><a href="#Gradient-Vanishing-Exploding" class="headerlink" title="Gradient Vanishing/Exploding"></a>Gradient Vanishing/Exploding</h2><p>Gradient vanishing/exploding arises from the issues of backpropagation, in other words, the accumulated multiplication of smaller-than-1 or greater-than-1 gradient values.</p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><ol>
<li>Pretraining-Finetuning per layer</li>
<li>Gradient Clip / Weight Regularization</li>
<li>Activation function: avoid to use sigmoid.</li>
<li>Appropriate weight initialization: Xavier-Glorot initialization<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Written Memories: Understanding, Deriving and Extending the LSTM](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)
">[4]</span></a></sup></li>
<li>Batch Norm: reduce the covariant shift of training dataset.</li>
<li>Residual Connection</li>
<li>LSTM: refer to <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Written Memories: Understanding, Deriving and Extending the LSTM](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)
">[4]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[LSTM eased gradient vanishing explanations (in Chinese)](https://www.zhihu.com/question/34878706)
">[5]</span></a></sup>.</li>
</ol>
<h2 id="RNNs"><a href="#RNNs" class="headerlink" title="RNNs"></a>RNNs</h2><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
">[6]</span></a></sup> integrates three gates: input gate, forget gate, and output gate.</p>
<script type="math/tex; mode=display">
\begin{align}
\left[\begin{array}{c} \mathbf{i}^c_j\\ \mathbf{o}^c_j    \\ \mathbf{f}^c_j    \\ \tilde{c}^c_j \end{array}\right]  &{}= \left[\begin{array}{c} \sigma    \\ \sigma    \\ \sigma    \\ \tanh \end{array}\right]  (\mathbf{W}^{c^T} \left[\begin{array}{c} \mathbf{x}^c_j    \\ \mathbf{h}^c_{j-1}\end{array}\right] + \mathbf{b}^c) \\
\mathbf{c}^c_j &{}= \mathbf{f}^c_j \odot \mathbf{c}^c_{j-1} + \mathbf{i}^c_j \odot \tilde{c}^c_{j} \\
\mathbf{h}_j^c &{}= \mathbf{o}_j^c \odot \tanh(\mathbf{c}^c_j)
\end{align}</script><p><img data-src="/notes/images/LSTM.png" width='80%' /><br><!--![LSTM<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
">[6]</span></a></sup>](/notes/images/LSTM.png)--><br><img data-src="/notes/images/LSTM-1.png" alt="LSTM" width='40%' /></p>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>GRU has three gates: update gate (vs input/output gate in LSTM) and reset gate.<br><img data-src="/notes/images/GRU.png" width='80%' /></p>
<p><img data-src="/notes/images/GRU-1.png" alt="GRU" width='40%' /></p>
<!--![GRU<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
">[6]</span></a></sup>](/notes/images/GRU.png)-->
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>See <a href="/notes/2019/10/17/NN/Transformer-variants-a-peek/">Transformer blog</a></p>
<h2 id="Backprop-with-Softmax-XE"><a href="#Backprop-with-Softmax-XE" class="headerlink" title="Backprop with Softmax + XE"></a>Backprop with Softmax + XE</h2><p>Refer to <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Softmax classification with cross-entropy (2/2)](https://peterroelants.github.io/posts/cross-entropy-softmax/)
">[7]</span></a></sup>.</p>
<h3 id="Softmax-Forward"><a href="#Softmax-Forward" class="headerlink" title="Softmax Forward"></a>Softmax Forward</h3><p>Given the softmax written in:</p>
<script type="math/tex; mode=display">
\textrm{softmax}(a_i) = p_i = \frac{\exp(a_i)}{\sum_{j}^N \exp(a_j)}</script><p>where $a_i, i=1,2,\cdots,N$ is the output logits, $p_i$ is the predicted probability of $i$-th class, and</p>
<script type="math/tex; mode=display">\sum_{i=1}^N p_i = 1</script><h4 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h4><p>The computation of softmax will first reduce the maximum value of $A=[a_1, a_2, \cdots, a_N]$ to avoid the overflow of exp(.).</p>
<p>We have </p>
<script type="math/tex; mode=display">
\begin{align}
p_i &{}= \frac{\exp(a_i)}{\sum_j^N \exp(a_i)} \\
&{}= \frac{C \exp(a_i)}{C \sum_j^N \exp(a_i)} \\
&{}= \frac{\exp(\log C) \exp(a_i)}{\exp(\log C) \sum_j^N \exp(a_i)} \\
&{}= \frac{\exp(a_i + \log C)}{\sum_j^N \exp(a_i + \log C)} \\
&{}= \frac{\exp(a_i - max(A))}{\sum_j^N \exp(a_i - max(A))} \\
\end{align}</script><p>where $C$ is constant.</p>
<h3 id="Cross-Entropy-Forward"><a href="#Cross-Entropy-Forward" class="headerlink" title="Cross Entropy Forward"></a>Cross Entropy Forward</h3><p>Denote the Cross Entropy (XE) loss as $H$:</p>
<script type="math/tex; mode=display">
\ell(y_i, p_i) = H(y_i, p_i) = -\sum_{i}^N y_i \cdot \log p_i</script><h3 id="Softmax-Derivative"><a href="#Softmax-Derivative" class="headerlink" title="Softmax Derivative"></a>Softmax Derivative</h3><p>The derivative of softmax w.r.t $a_i$ is:</p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} = \frac{\partial \big( \frac{\exp(a_i)}{\sum_j^N \exp(a_i)} \big)}{\partial a_j}</script><p>For brevity, let $\sum = \sum_j^N \exp(a_j)$.</p>
<ol>
<li><p>When $i=j$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial p_i}{\partial a_j} &{}= \frac{\exp(a_i) \cdot \sum - \exp(a_i)\cdot \exp(a_j)}{\sum\cdot \sum} \\
&{}= \frac{\exp(a_i) (\sum - \exp(a_i))}{\sum\cdot \sum} \\
&{}= p_i (1-p_j)
\end{align}</script></li>
<li><p>When $i \neq j$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial p_i}{\partial a_j} &{}= \frac{0 \cdot \sum - \exp(a_i)\cdot \exp(a_j)}{\sum\cdot \sum} \\
&{}= - p_i \cdot p_j 
\end{align}</script></li>
</ol>
<h3 id="XE-Softmax-Derivative"><a href="#XE-Softmax-Derivative" class="headerlink" title="XE+Softmax Derivative"></a>XE+Softmax Derivative</h3><p>The derivative of XE is:</p>
<script type="math/tex; mode=display">
\begin{equation}
H^\prime(y_i, p_i) = - \sum_i^N y_i \frac{1}{p_i}
\end{equation}</script><p>According to the chain rule, the derivative w.r.t $a_j$ is:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial H}{\partial a_j} &{}= \frac{\partial H}{\partial p_i} \cdot \frac{\partial p_i}{\partial a_j}\\
&{}= \bigg( -\sum_i y_i \frac{1}{p_i} \bigg) \cdot \frac{\partial p_i}{\partial a_j}  \label{eq:xe_derivative}
\end{align}</script><ol>
<li><p>When $i=j$</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{Eq.} \eqref{eq:xe_derivative} &{}= -\sum_{i=j} y_i \frac{1}{p_i}\cdot p_i \cdot (1-p_j) \\
&{}= -\sum_{i=j} y_i \cdot (1 - p_j) \\
&{}= -y_i + y_i p_i  \label{eq:s1}
\end{align}</script></li>
<li><p>When $i \neq j$, the Eq. $\eqref{eq:xe_derivative}$ is:</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{Eq.} \eqref{eq:xe_derivative}  &{}= -\sum_{i \neq j} y_i \frac{1}{p_i}\cdot (- p_i \cdot p_j) \\
&{}= \sum_{i \neq j} y_i p_j  \label{eq:s2}
\end{align}</script></li>
</ol>
<p>Since above two scenarios are independent, combining Eq. $\eqref{eq:s1}$ and $\eqref{eq:s2}$, we have:</p>
<script type="math/tex; mode=display">
\begin{align}
\textrm{Eq.} \eqref{eq:xe_derivative}  &{}= \textrm{Eq.}\eqref{eq:s1} + \textrm{Eq.}\eqref{eq:s2} \\
&{}= -y_i + y_i p_i + \sum_{i \neq j} y_i p_j \\
&{}= -y_i + (\sum_{i=j}y_i p_j + \sum_{i \neq j} y_i p_j)\\
&{}= -y_i + \sum_i^N y_i p_i \label{eq:one_hot} \\
&{}= p_j - y_i \label{eq:ij}\\
&{}= p_j - y_j
\end{align}</script><p>In Eq.$\eqref{eq:one_hot}$, we have $\sum_i^N y_i = 1$;<br>In Eq.$\eqref{eq:ij}$, we have $\sum_i^N y_i = y_j$.</p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><h2 id="Static-Word-Representation"><a href="#Static-Word-Representation" class="headerlink" title="Static Word Representation"></a>Static Word Representation</h2><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><h4 id="Hierarchical-Softmax-Negative-Sampling"><a href="#Hierarchical-Softmax-Negative-Sampling" class="headerlink" title="Hierarchical Softmax / Negative Sampling"></a>Hierarchical Softmax / Negative Sampling</h4><p>Refer to <a href="/notes/2019/12/13/NN/Efficient-Softmax-Explained/">my blog</a></p>
<ul>
<li>Hierarchical Softmax: $|V| =&gt; \log |V|$ using huffman tree</li>
<li>Negative Sampling</li>
</ul>
<h4 id="W2V-vs-GloVe"><a href="#W2V-vs-GloVe" class="headerlink" title="W2V vs GloVe"></a>W2V vs GloVe</h4><h3 id="BPE-vs-WordPiece"><a href="#BPE-vs-WordPiece" class="headerlink" title="BPE vs WordPiece"></a>BPE vs WordPiece</h3><p>Refer to <a href="/notes/2019/03/08/NLP/How-to-handle-Out-Of-Vocabulary-words/">OOV blog</a></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Hinge_loss">Wiki: Hinge Loss</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/76946313">SVM Blog</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.jiqizhixin.com/articles/2018-10-17-20">SVM Derivatives (in Chinese)</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">Written Memories: Understanding, Deriving and Extending the LSTM</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.zhihu.com/question/34878706">LSTM eased gradient vanishing explanations (in Chinese)</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://peterroelants.github.io/posts/cross-entropy-softmax/">Softmax classification with cross-entropy (2/2)</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.csdn.net/MacKendy/article/details/106391817">Softmax+XE Backpropagation (in Chinese)</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Wiki: KL divergence</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Mutual_information">Wiki: Mutual Information</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">[F1 score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?]<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/2007.07314.pdf">Long-Tail Learning via Logit Adjustment</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kexue.fm/archives/7615">Data Imbalance blog</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Q-Networks: A Summary</title>
    <url>/notes/2019/07/04/RL/DRL/DQN-A-Summary/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>A summary of key advances of Deep Q-Networks.</p>
<span id="more"></span>
<h1 id="Nature-DQN-Nature-2015"><a href="#Nature-DQN-Nature-2015" class="headerlink" title="Nature DQN (Nature 2015)"></a>Nature DQN (Nature 2015)</h1><div class="note danger">
            <p><strong>Challenge</strong>:</p><ol><li><code>Sparse, noisy and delayed reward</code>. The scalar reward signal is frequently <strong>sparse, noisy and delayed</strong>. The delay between actions and rewards can be thousands of time steps long.</li><li><code>Highly-correlated data</code>. Deep learning assume the <em>data samples to be independent</em>, whilst in RL one typically encounters sequences of <strong>highly correlated states</strong>.</li><li><code>Non-staionary distribution</code>. In RL, the <strong>data distribution changes as the algorithm learns new behaviours</strong>, while can be problematic for deep learning that assume a <em>fixed underlying distribution</em>.</li></ol>
          </div>
<p>Deep Q-Nerwork <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). [Human-level control through deep reinforcement learning](https://daiwk.github.io/assets/dqn.pdf). Nature, 518(7540), 529.
">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). [Playing atari with deep reinforcement learning](https://arxiv.org/pdf/1312.5602.pdf)). arXiv preprint arXiv:1312.5602.
">[2]</span></a></sup> leveraged <em>convolutional nets</em> and <em>experience replay</em>, receiving raw pixes as the input.</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul>
<li>Input: gray-scale raw pixels.</li>
<li>Output: Q-values of each action.</li>
</ul>
<p><img data-src="/notes/images/rl-dqn-architecture.png" alt="upload successful"></p>
<p><strong>Model architecture</strong>:</p>
<ol>
<li>conv1 + ReLU (32 filters of $8 \times 8$ with stride 4)</li>
<li>conv2 + ReLU (64 filters of $4 \times 4$ with stride 2)</li>
<li>conv3 + ReLU (64 filters of $3 \times 3$ with stride 1)</li>
<li>FC layer + ReLU(512)</li>
<li>FC layer with single output for each action</li>
</ol>
<p><img data-src="/notes/images/rl-dqn-model.png" alt="upload successful"></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><ul>
<li>Optimizer: RMSProp</li>
<li>$\epsilon$-greedy with $\epsilon$ annealed linearly from 1.0 to 0.1 over the 1st 1,000,000 frames, and fixed at 0.1 thereafter.</li>
<li>Experience replay memory: 1,000,000 most recent frames.</li>
<li>Training data: 50 million frames, i.e. ~38 days of game experiences</li>
<li>Frame-skipping technique: select actions on every $k$-th frame instead of every frame, $k=4$.</li>
<li><strong>Tricks</strong> of <code>Error clipping</code>:<br>Clip the error term from the update <script type="math/tex">r+\gamma \max_{a'} Q(s',a';\theta_i^{-}) - Q(s,a;\theta_i)</script> to be in $[-1,1]$.</li>
</ul>
<p>To perform experience replay, store the agents experience <script type="math/tex">e_t = (s_t, a_t, r_t, s_{t+1})</script> at each time step $t$ in a dataset <script type="math/tex">D_t=\{e_1,\cdots,e_t\}</script>. During learning, at each time step, apply Q-learning updates on samples(or minibaches) of experience <script type="math/tex">(s,a,r,s')\sim \text{U(D)}</script>, <em>drawn uniformly at random</em> from the pool of stored samples. The target Q-network do <strong>periodical updates</strong>, so as to reduce the corerlations with the target. The loss function at iteration $i$:</p>
<script type="math/tex; mode=display">L_{i}(\theta_i) = \mathbb{E}_{(s,a,r,s')\sim \text{U(D)}} \left[ \left( \underbrace{r + \gamma \max_{a'} Q(s',a';\theta_i^{-})}_\text{target Q-network} - Q(s,a;\theta_i) \right) \right]</script><p>where the target Q-network parameters <script type="math/tex">\theta_i^{-}</script> are only updated with the Q-network parameters <script type="math/tex">\theta_i</script> <strong>every $C$ steps</strong>, and are held fixed between individual updates.</p>
<p><img data-src="/notes/images/rl-dqn-alg.png" alt="upload successful"></p>
<h1 id="Deep-Recurrent-Q-Network-AAAI-2015"><a href="#Deep-Recurrent-Q-Network-AAAI-2015" class="headerlink" title="Deep Recurrent Q-Network(AAAI 2015)"></a>Deep Recurrent Q-Network(AAAI 2015)</h1><h2 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h2><p>Replace the 1st FC layer with LSTMs<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hausknecht, M., & Stone, P. (2015, September). [Deep recurrent q-learning for partially observable mdps](https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/download/11673/11503). In 2015 AAAI Fall Symposium Series.
">[3]</span></a></sup>.<br><strong>Model architecture</strong>:</p>
<ol>
<li>conv1 + ReLU (32 filters of $8 \times 8$ with stride 4)</li>
<li>conv2 + ReLU (64 filters of $4 \times 4$ with stride 2)</li>
<li>conv3 + ReLU (64 filters of $3 \times 3$ with stride 1)</li>
<li>LSTM layer</li>
<li>FC layer with single output for each action</li>
</ol>
<p><img data-src="/notes/images/rl-drqn.png" alt="upload successful"></p>
<ul>
<li>No explicit improvement in comparison with Nature DQN </li>
<li>Recurrency confers benefits with partial observability, when adapting at evaluation time if the quality of observation changes.</li>
<li>Replacing LSTMs with 1st FC layer in DQN achive the best performance, intuitionally indicating that this allows LSTM direct access to the convolutional features.</li>
</ul>
<h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><ul>
<li>Bootstrapped <strong>squential updates</strong>: randomly select episodes from the replay memory and updates begin <strong>at the beginning of the episode</strong>.</li>
<li>Boostrapped <strong>random updates</strong>: randomly select eposides from the replay memory and updates <strong>begin at randomly points</strong> in the episode and proceed.</li>
</ul>
<p>Both are viable and yield convergent policies with similar performance. Therefore, apply randomized update strategy.</p>
<h1 id="Double-DQN-AAAI-2016"><a href="#Double-DQN-AAAI-2016" class="headerlink" title="Double DQN (AAAI 2016)"></a>Double DQN (AAAI 2016)</h1><div class="note danger">
            <p><strong>Problems</strong>:</p><ul><li>Q-learning algorithms lead to <code>overestimation</code>, since the $\max$ op <em>employs the same values to both select and evaluate an action</em>. </li></ul>
          </div>
<p>In <strong>DQN</strong>, the target Q-function is:</p>
<script type="math/tex; mode=display">y_t^{DQN} = R_{t+1} + \gamma Q(S_{t+1}, \arg\max_a Q(S_{t+1},a;\pmb{\theta}_t); \pmb{\theta}_t)</script><p>In <code>Double DQN</code>, the target is:</p>
<script type="math/tex; mode=display">y_t^{DDQN} = R_{t+1} + \gamma Q(S_{t+1}, \arg\max_a Q(S_{t+1},a;\pmb{\theta}_t); \pmb{\theta}_t^{-})</script><p>The weights of target Q-network <script type="math/tex">\pmb{\theta}_t^{-}</script> stayed unchanged from DQN, and remains a <em>periodic copy</em> of the online network.</p>
<h1 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>Online RL incrementally update the parameters while observing a  stream of experience. This leads to problems:<ul>
<li>strongly correlated updates break the i.i.d assumption of SGD algorithms;</li>
<li>rapid forgetting of possibly rare experiences.</li>
</ul>
</li>
<li><em>Experience replay</em> uses a large sliding window replay memory, uniformly sampling experience transition from a replay memory at random. Experience replay frees online learning agents from processing transitions in the <strong>exact order that they are experienced</strong>, but suffer from sampling transitions with the <strong>same frequency that they are experienced</strong>.</li>
</ul>
<h2 id="Prioritized-Experience-Replay-1"><a href="#Prioritized-Experience-Replay-1" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h2><ul>
<li>Intuition: RL agent can learn more effectively from some transitions than from others. Prioritized replay liberatres agents from considering transitions with the same frequency that they are experienced. </li>
</ul>
<h3 id="greedy-TD-error-prioritization"><a href="#greedy-TD-error-prioritization" class="headerlink" title="greedy TD-error prioritization"></a>greedy TD-error prioritization</h3><ul>
<li><p>Prioritize the experience transitions according to <strong>the magnitude of temporal-difference(TD) error</strong> $\delta$ <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). [Prioritized experience replay](https://arxiv.org/pdf/1511.05952.pdf). arXiv preprint arXiv:1511.05952.">[7]</span></a></sup>, indicating how surprising or unexpected the transition is: </p>
<script type="math/tex; mode=display">\delta = | R_{t+1} + \gamma_{t+1} \max_{a'} q_{\theta}^{-}(S_{t+1,a'}) - q_\theta (S_t, A_t) |</script></li>
<li><p>Implementation: priority queue with binary heap data structure.</p>
<ul>
<li>the complexity of searching for the maximum $O(1)$</li>
<li>Update complexity $O(\log N)$</li>
</ul>
</li>
</ul>
<h3 id="Stochastic-prioritization"><a href="#Stochastic-prioritization" class="headerlink" title="Stochastic prioritization"></a>Stochastic prioritization</h3><ul>
<li><p>The <strong>problems</strong> of greedy TD-error prioritization:</p>
<ol>
<li>TD errors are only updated for replayed transitions, resulting in that transitions with low TD error on first visit may not be replayed for a long time.</li>
<li>It is sensitive to noise spikes(e.g. when rewards are stochastic), which can be exacerbated by bootstrapping where approximation errors appear as another source of noise.</li>
<li>It focuses on a small subset of the experience: error shrink slowly, especially when using function approximation. This means initially high error transitions get replayed frequently, which is lack of diversity, making it prone to overfitting.</li>
</ol>
</li>
<li><p><strong>Stochastic prioritization</strong> interpolates between pure greedy prioritization and uniform random sampling. We ensure the sampling probability is <em>monotonic</em> and guarantee non-zero prob. even for lowest-priority transition.</p>
</li>
<li>Define the probability of sample transition $i$ as:<script type="math/tex; mode=display">P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}</script>where <script type="math/tex">p_i > 0</script> is the priority of transition $i$. $\alpha$ determins how much prioritization is used, $\alpha = 0$ denotes to the uniform case.</li>
</ul>
<h4 id="Proportional-prioritization"><a href="#Proportional-prioritization" class="headerlink" title="Proportional prioritization"></a>Proportional prioritization</h4><script type="math/tex; mode=display">p_i = |\delta_i| + \epsilon</script><p>where $\epsilon$ is a small positive constant that prevents the edge-case of transitions not being revisited once their error is zero.</p>
<p><img data-src="/notes/images/rl-propotional-prioritization.png" alt="upload successful"></p>
<ul>
<li>Implementation: ‘sum-tree’ data structure.</li>
</ul>
<h4 id="Rank-based-prioritization"><a href="#Rank-based-prioritization" class="headerlink" title="Rank-based prioritization"></a>Rank-based prioritization</h4><script type="math/tex; mode=display">p_i = \frac{1}{\text{rank}(i)}</script><p>where $\text{rank}(i)$ denotes the rank of transition $i$ according to <script type="math/tex">|\delta_i|</script></p>
<h1 id="Dueling-DQN-ICML-2016"><a href="#Dueling-DQN-ICML-2016" class="headerlink" title="Dueling DQN (ICML 2016)"></a>Dueling DQN (ICML 2016)</h1><h2 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h2><p>$Q$ measures the value of choosing a particular action when in this state:</p>
<script type="math/tex; mode=display">Q^{\pi}(s,a) = \mathbb{E}[R_t \vert s_t=s, a_t=a, \pi]</script><p>$V$ measures how good it is to be in a particular state $s$:</p>
<script type="math/tex; mode=display">V^{\pi}(s) = \mathbb{E}_{a \sim \pi(s)}[Q^\pi (s,a)]</script><p>The <strong>Advantage function</strong> <script type="math/tex">A^\pi</script> measures the relative measure of the importance of each action:</p>
<script type="math/tex; mode=display">A^\pi(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)</script><div class="note info">
            <p><strong>Intuition</strong>:</p><ul><li>It is unnecessary to estimate the value of each action choice. In some states, it is of paramount importance to know which action to take, but in many other states the coice of action has no repercussion on what happens.</li><li>In bootstrapping-based methods, the estimation of state values if of great importance for every state.</li></ul>
          </div>
<h2 id="Model-architecture-1"><a href="#Model-architecture-1" class="headerlink" title="Model architecture"></a>Model architecture</h2><p>Dueling DQN decouples the <strong>value</strong> and <strong>advantage</strong> functions in separate streams <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Z., Schaul, T., Hessel, M., Hasselt, H.V., Lanctot, M., & Freitas, N.D. (2016). [Dueling network architectures for deep reinforcement learning](https://arxiv.org/pdf/1511.06581). ICML.
">[5]</span></a></sup>.<br><img data-src="/notes/images/rl-dueling dqn.png" alt="upload successful"></p>
<ul>
<li>Replace the 1st FC layer with two sequences(streams) of FC layers, which separately estimate the <strong>scalar state value</strong> <script type="math/tex">V(s;\theta, \beta)</script> and $|\mathcal{A}|$-dimensional vector <strong>advantange</strong> <script type="math/tex">A(s,a;\theta,\alpha)</script>, where $\theta$ denotes parameters of conv layers, $\alpha$ and $\beta$ are the parameters of two streams of FC layers. Afterwards, combine both of them:<script type="math/tex; mode=display">Q(s,a; \theta,\alpha,\beta) = V(s;\theta,\beta) + A(s,a;\theta,\alpha)</script>To form the matrix form of $Q$, we need to replicate the scalar $V(s;\theta,\beta)$ $|\mathcal{A}|$ times, i.e. <strong>broadcasting</strong> the scalar value $V$ to $|\mathcal{A}|$ dimensions.    </li>
</ul>
<p>The above equation lead to identifiability that given $Q$ we cannot recover $V$ and $A$ uniquely. In other words, the resulting value remains the same if adding a constant substracted from $V$, to the advantage $\mathcal{A}$.<br>Thus, they forced the advantage function to zero by substracting the $\max$ value of $\mathcal{A}$:</p>
<script type="math/tex; mode=display">Q(s,a; \theta,\alpha,\beta) = V(s;\theta,\beta) + \left( A(s,a;\theta,\alpha) - \max_{a' \in |\mathcal{A}|} A(s,a'; \theta, \alpha) \right)</script><p>Here, for <script type="math/tex">a^* = \arg\max_{a' \in \mathcal{A}} Q(s,a';\theta,\alpha,\beta) = \arg\max_{a' \in \mathcal{A}} A(s,a';\theta,\alpha)</script>.</p>
<p>An alternative way is replace $\max$ with average op:</p>
<script type="math/tex; mode=display">Q(s,a; \theta,\alpha,\beta) = \underbrace{V(s;\theta,\beta)}_\text{scalar} + \left( \underbrace{A(s,a;\theta,\alpha)}_{|\mathcal{A}|-\text{dimensional vector}} - \underbrace{\frac{1}{|\mathcal{A}|} \sum_{a'} A(s,a'; \theta, \alpha)}_\text{scalar} \right)</script><p>This alternative loses the original semantics of $V$ and $A$ since they are off-target by a constant, but it increases the stabiliity of the optimization.</p>
<p>The $\text{softmax}$ version’s performance matched the above average version.</p>
<h1 id="Rainbow-DQN-AAAI-2018"><a href="#Rainbow-DQN-AAAI-2018" class="headerlink" title="Rainbow DQN (AAAI 2018)"></a>Rainbow DQN (AAAI 2018)</h1><ul>
<li>RainBow DQN integrates the ingredients of Double DQN, Prioritized replay, Dueling DQN, multi-step learning, distributional RL and Noisy Net<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., ... & Silver, D. (2018, April). [Rainbow: Combining improvements in deep reinforcement learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/17204/16680). In Thirty-Second AAAI Conference on Artificial Intelligence.
">[6]</span></a></sup>.</li>
</ul>
<p><img data-src="/notes/images/rl-rainbow-dqn.png" alt="upload successful"></p>
<ul>
<li><p>Apply <strong>Adam</strong> optimizer: less sensitive to the choice of learning rate then RMSProp.</p>
</li>
<li><p>The ablation studies illustrate that <strong>prioritized replay</strong> and <strong>multi-step learning</strong> were the two most crucial components of Rainbow, in that removing either component caused a large drop.</p>
</li>
</ul>
<p><img data-src="/notes/images/rl-rainbow-ablation.png" alt="upload successful"></p>
<ul>
<li>Other ingredients to research: Bootstrapped DQN, instrinsic motivation, count-bsed exploration.</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... &amp; Petersen, S. (2015). <a href="https://daiwk.github.io/assets/dqn.pdf">Human-level control through deep reinforcement learning</a>. Nature, 518(7540), 529.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). <a href="https://arxiv.org/pdf/1312.5602.pdf">Playing atari with deep reinforcement learning</a>). arXiv preprint arXiv:1312.5602.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hausknecht, M., &amp; Stone, P. (2015, September). <a href="https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/download/11673/11503">Deep recurrent q-learning for partially observable mdps</a>. In 2015 AAAI Fall Symposium Series.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Van Hasselt, H., Guez, A., &amp; Silver, D. (2016, March). <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847">Deep reinforcement learning with double q-learning</a>. In Thirtieth AAAI Conference on Artificial Intelligence.<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, Z., Schaul, T., Hessel, M., Hasselt, H.V., Lanctot, M., &amp; Freitas, N.D. (2016). <a href="https://arxiv.org/pdf/1511.06581">Dueling network architectures for deep reinforcement learning</a>. ICML.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., ... &amp; Silver, D. (2018, April). <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/17204/16680">Rainbow: Combining improvements in deep reinforcement learning</a>. In Thirty-Second AAAI Conference on Artificial Intelligence.<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2015). <a href="https://arxiv.org/pdf/1511.05952.pdf">Prioritized experience replay</a>. arXiv preprint arXiv:1511.05952.<a href="#fnref:7" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>RL</category>
        <category>DRL</category>
        <category>DQN</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Policy Gradient: A Summary !</title>
    <url>/notes/2019/07/19/RL/DRL/PG-A-Summary/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>The mathematical foundations of <strong>policy gradient</strong> algorithms.</p>
<span id="more"></span>
<h1 id="Policy-Gradient-preliminaries"><a href="#Policy-Gradient-preliminaries" class="headerlink" title="Policy Gradient preliminaries"></a>Policy Gradient preliminaries</h1><p>Policy gradient estimates the gradients with the form:<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Schulman, J., Moritz, P., Levine, S., Jordan, M.I., & Abbeel, P. (2016). [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438). CoRR, abs/1506.02438.
">[10]</span></a></sup></p>
<script type="math/tex; mode=display">\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \Psi_t \nabla_\theta \log \pi_\theta (a_t \vert s_t)  \right]</script><p>where <script type="math/tex">\Psi_t</script> may be the following forms:</p>
<script type="math/tex; mode=display">
\begin{align}
(1) \quad & \sum_{t=0}^\infty r_t & \text{total reward of the trajectory} &\\
(2)  \quad &  \sum_{t'=t}^\infty r_{t'} & \text{reward following action } a_t &\\
(3)  \quad & \sum_{t'=t}^\infty r_{t'} - b(s_t) & \text{baselined version of (2)} &\\
(4)  \quad & Q^\pi (s_t, a_t) & \text{state-action value function} &\\
(5) \quad & A^\pi(s_t, a_t) & \text{advantage function} &\\
(6) \quad & r_t+V^\pi(s_{t+1}) - V^{\pi}(s_t) & \text{TD residual} &\\
\end{align}</script><p>where (6) yields the lowest possible variance:</p>
<script type="math/tex; mode=display">V^\pi (s_t) := \mathbb{E}_{s_{t+1:\infty}, \pmb{a_{t:\infty}} } \left[ \sum_{l=0}^\infty \gamma^l r_{t+l} \right]</script><script type="math/tex; mode=display">Q^\pi (s_t, a_t) := \mathbb{E}_{s_{t+1:\infty}, \pmb{a_{t+1:\infty}} } \left[ \sum_{l=0}^\infty  \gamma^l r_{t+l} \right]</script><p>The advantage function</p>
<script type="math/tex; mode=display">A^\pi(s_t, a_t) := Q^\pi(s_t, a_t) - V^\pi (s_t)</script><ul>
<li><em>Intuitional interpretation</em>: a step in policy gradient direction should <strong>increase the probability of better-than-average actions and decrease the probability of worse-than-average actions</strong>. The advantage function measures whether or not the action is better or worse than the policy’s default behavior (expection).</li>
</ul>
<h1 id="Vanilla-Policy-Gradient"><a href="#Vanilla-Policy-Gradient" class="headerlink" title="Vanilla Policy Gradient"></a>Vanilla Policy Gradient</h1><h2 id="The-goal-of-RL"><a href="#The-goal-of-RL" class="headerlink" title="The goal of RL"></a>The goal of RL</h2><h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><script type="math/tex; mode=display">\theta^* = \arg \max_\theta E_{\tau \sim p_\theta(\tau)} \big[ \sum_t r\mathbf{(s_t,a_t)} \big]</script><ul>
<li>Infinite horizon<script type="math/tex; mode=display">\theta^{*} = \arg \max_{\theta} E_{\mathbf{(s,a)} \sim p_\theta \mathbf{(s,a)}} [r \mathbf{(s,a)}]</script></li>
<li>Finite horizon<script type="math/tex; mode=display">\theta^* = \arg\max_\theta \sum_{t=1}^T E_{ \mathbf{(s_t,a_t)} \sim p_\theta \mathbf{(s_t,a_t)} } \big[ r \mathbf{(s,a)} \big]</script></li>
</ul>
<h3 id="Evaluating-the-objective"><a href="#Evaluating-the-objective" class="headerlink" title="Evaluating the objective"></a>Evaluating the objective</h3><script type="math/tex; mode=display">\theta^* = \arg\max_\theta \underbrace{E_{\tau \sim p_{\theta}(\tau)} \big[ \sum_t r (\mathbf{s}_t,\mathbf{a}_t) \big]}_{J(\theta)}</script><script type="math/tex; mode=display">J(\theta) = E_{\tau \sim p_{\theta}(\tau)} \big[ \sum_t r (\mathbf{s}_t,\mathbf{a}_t) \big] \approx \frac{1}{N} \underbrace{\sum_i \sum_t r(\mathbf{s}_{i,t}, \mathbf{a}_{i,t}) }_{i \rightarrow \text{sum over samples from } \pi_\theta}</script><p><img data-src="/notes/images/RL_eval-objective.png" alt="upload successful"></p>
<h2 id="Direct-differentiation"><a href="#Direct-differentiation" class="headerlink" title="Direct differentiation"></a>Direct differentiation</h2><script type="math/tex; mode=display">J(\theta) =E_{\tau \sim \pi_{\theta}(\tau)} \big[ r(\tau) \big] = \int \pi_\theta (\tau) r(\tau) d \tau</script><script type="math/tex; mode=display">r(\tau) = \sum_{t=1}^T r(\mathbf{s}_t, \mathbf{a}_t)</script><script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\theta) & = \int \nabla_\theta \pi_\theta (\tau) r(\tau) d \tau \\ &= \int \pi_\theta (\tau) \nabla_\theta \log \pi_\theta (\tau) r(\tau) d \tau \\&= E_{\tau \sim \pi_{\theta}(\tau)} \big[ \nabla_\theta \log \pi_\theta (\tau) r(\tau) \big] 
\end{align}</script><div class="note info">
            <p><strong>A convenient identity</strong>:</p><script type="math/tex; mode=display">\pi_{\theta}(\tau) \nabla_\theta \log \pi_\theta (\tau) = \pi_\theta (\tau) \frac{\nabla_\theta \pi_\theta (\tau)}{\pi_\theta (\tau)} = \nabla_\theta \pi_\theta (\tau)</script>
          </div>
<script type="math/tex; mode=display">
\begin{align}
\log \pi_\theta(\tau) &= \log \pi_\theta(\mathbf{s}_1,\mathbf{a}_1, \cdots, \mathbf{s}_T,\mathbf{a}_T) \\&= \log \big[ p(\mathbf{s}_1) \prod_{t=1}^T \pi_\theta (\mathbf{a}_t \vert \mathbf{s}_t) p(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t ) \big ] 
\\ &= \underbrace{\log p(\mathbf{s}_1)}_\text{initial probability, derivative:0} + \underbrace{\sum_{t=1}^T \log \pi_\theta (\mathbf{a}_t \vert \mathbf{s}_t)}_\text{transition prob.} + \underbrace{\log p(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t )}_\text{emission prob., derivative:0}
\end{align}</script><p>Overall,</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\theta) &= E_{\tau \sim \pi_\theta(\tau)} [\nabla_\theta \log \pi_\theta(\tau) r(\tau)] \\&= E_{\tau \sim \pi_\theta(\tau)} \big[ \big( \sum_{t=1}^T \log \pi_\theta (\mathbf{a}_t \vert \mathbf{s}_t) \big ) \big( \sum_{t=1}^T r(\mathbf{s}_t, \mathbf{a}_t) \big) \big] 
\\& \approx \frac{1}{N} \sum_{i=1}^N \big( \sum_{t=1}^T \nabla_\theta \log \pi_\theta (\mathbf{a}_{i,t} \vert \mathbf{s}_{i,t}) \big) \big( \sum_{t=1}^T r(\mathbf{s}_{i,t} , \mathbf{a}_{i,t}) \big) 
\end{align}</script><ul>
<li>Comparison to Maximum likelihood:<script type="math/tex; mode=display">\nabla_\theta J_{ML}(\theta) \approx \frac{1}{N} \sum_{i=1}^N \big( \sum_{t=1}^T \nabla_\theta \log \pi_\theta (\mathbf{a}_{i,t} \vert \mathbf{s}_{i,t}) \big)</script></li>
</ul>
<h2 id="Drawbacks-variance"><a href="#Drawbacks-variance" class="headerlink" title="Drawbacks: variance"></a>Drawbacks: variance</h2><ul>
<li>Reducing variance</li>
</ul>
<div class="note info">
            <ul><li>Future does not affect the past</li><li>Casuality: policy at time $t’$ cannot affect reard at time t when $t&lt;t’$</li></ul>
          </div>
<script type="math/tex; mode=display">\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta (\mathbf{a}_{i,t} \vert \mathbf{s}_{i,t}) \hat{Q}_{i,t}</script><p>where <script type="math/tex">\hat{Q}_{i,t}</script> denotes the reward to go:</p>
<script type="math/tex; mode=display">\hat{Q}_{i,t} = \sum_{t'=i}^T r(\mathbf{s}_{i,t'}, \mathbf{a}_{i,t'})</script><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><script type="math/tex; mode=display">\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \nabla_\theta \log \pi_\theta (\tau) [r(\tau)-b]</script><p>where <script type="math/tex">b=\frac{1}{N} \sum_{i=1}^N r(\tau)</script></p>
<p>proof:</p>
<ul>
<li>Substracting abseline is <code>unbiased in expectation</code></li>
<li><p>Average reward is not the best baseline, but it’s pretty good.</p>
<script type="math/tex; mode=display">E[\nabla_\theta \log \pi_\theta(\tau) b] = \int \pi_\theta (\tau) \nabla_\theta \log \pi_\theta (\tau) b d\tau = \int \nabla_\theta \pi_\theta (\tau)b d\tau = b \nabla_\theta \int \pi_\theta(\tau) d\tau =b\nabla_\theta 1 = 0</script></li>
<li><p><strong>Optimal baseline</strong>: </p>
<script type="math/tex; mode=display">\text{var}[x] = E[x^2] - E[x]^2</script><script type="math/tex; mode=display">\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta(\tau)} [\nabla_\theta \log \pi_\theta(\tau)(r(\tau)-b)]</script></li>
</ul>
<script type="math/tex; mode=display">\text{var} = E_{\tau \sim \pi_\theta(\tau)} [(\nabla_\theta \log \pi_\theta (\tau) (r(\tau)-b))^2] -  E_{\tau \sim \pi_\theta(\tau)} [\underbrace{\nabla_\theta \log \pi_\theta (\tau) (r(\tau)-b)}_{\text{this is just unbiased baseline in expectation}}]^2</script><p>Hence,</p>
<script type="math/tex; mode=display">\frac{d \text{var}}{db} = \frac{d}{db} E[g(\tau)^2(r(\tau)-b)^2] = \frac{d}{db} \big( E[g(\tau)^2 r(\tau)^2] - 2E[g(\tau)^2 r(\tau) b] + b^2 E[g(\tau)^2] \big)  \\= -2 E[g(\tau)^2 r(\tau)] + 2b E[g(\tau)^2] = 0</script><p>We get:</p>
<script type="math/tex; mode=display">b = \frac{E[g(\tau)^2 r(\tau)]}{E[g(\tau)^2]}</script><p>This is just the <code>expected reward, weighted by gradient magnitudes</code>.</p>
<h2 id="Deriving-the-simplest-policy-gradient-Spinning-Up"><a href="#Deriving-the-simplest-policy-gradient-Spinning-Up" class="headerlink" title="Deriving the simplest policy gradient (Spinning Up)"></a>Deriving the simplest policy gradient (Spinning Up)</h2><p>Consider the case of a stochastic, parameterized policy, <script type="math/tex">\pi_{\theta}</script>. We maximize the expected return </p>
<script type="math/tex; mode=display">J(\theta) = \underset{\tau \sim \pi_\theta}{R(\tau)}</script><p>Optimize the policy by gradient descent:</p>
<script type="math/tex; mode=display">\theta_{k+1} = \theta_k + \alpha \underbrace{\nabla_\theta J(\pi_{\theta})\vert_{\theta_k}}_\text{policy gradient}</script><p>Step by step:</p>
<ol>
<li><p><strong>Probability of a Trajectory</strong>. The probability of a trajectory <script type="math/tex">\tau = (s_0, a_0, \cdots, s_{T+1})</script> given that actions come from <script type="math/tex">\pi_\theta</script> is:</p>
<script type="math/tex; mode=display">P(\tau \vert \theta) = \rho_0(s_0) \prod_{t=0}^T P(s_{t+1} \vert s_t, a_t) \pi_\theta (a_t \vert s_t)</script></li>
<li><p><strong>The log-derivative trick</strong>. </p>
<script type="math/tex; mode=display">\nabla_\theta P(\tau \vert \theta) = P(\tau \vert \theta) \nabla_\theta \text{log} P(\tau \vert \theta)</script></li>
<li><p><strong>Log-probability of a trajectory</strong>:</p>
<script type="math/tex; mode=display">\text{log} P(\tau \vert \theta) = \text{log} \rho_0 (s_0) + \sum_{t=0}^T \big( \text{log}P(s_{t+1} \vert s_t,a_t) + \text{log} \pi_\theta (a_t \vert s_t) \big)</script></li>
<li>Gradients of environment functions<br>The environment has no dependence on $\theta$, so gradients of <script type="math/tex">\rho_0(s_0)</script>, <script type="math/tex">P(s_{t+1} \vert s_t, a_t)</script> and $R(\tau)$ are zero.</li>
<li><strong>Grad-log-prob of a trajectory</strong>.<br>The gradient of the log-prob of a trajectory is thus</li>
</ol>
<p><img data-src="/notes/images/Eq-RL-policy-gradient.png" alt="upload successful"></p>
<p>Overall:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\pi_\theta) & = \underset{\tau \sim \pi_\theta}{E} [R(\tau)] \\ & = \nabla_\theta \int_\tau P(\tau \vert \theta) R(\tau) & \text{expand expectation}\\ 
& = \int_\tau \nabla_\theta P(\tau \vert \theta) R(\tau)  & \text{bring gradient under integral}\\ 
& = \int_\tau P(\tau \vert \theta) \nabla_\theta \log P(\tau \vert \theta) R(\tau)  & \text{log-derivative trick}\\ 
& = \underset{\tau \sim \pi_\theta}{E} \big[ \nabla_\theta \log P(\tau \vert \theta) R(\tau) \big]  & \text{return to expectation form}
\end{align}
\\
\therefore \nabla_\theta J(\pi_\theta)  = \underset{\tau \sim \pi_\theta}{E} \big[ \sum_{t=0}^T \nabla_\theta \log P(\tau \vert \theta) R(\tau) \big]</script><p>We can estimate the expectation with a sample mean. If we collect a set of trajectories <script type="math/tex">\mathcal{D} = \{ \tau_i \}_{i=1,\cdots,N}</script> where each trajectory is obtained by letting the agent act in the environment using the policy <script type="math/tex">\pi_\theta</script>, the policy gradient can be estimated as:</p>
<script type="math/tex; mode=display">\hat{g} = \frac{1}{|D|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^T \nabla_\theta \text{log} \pi_\theta (a_t \vert s_t) R(\tau)</script><p>where $|\mathcal{D}|$ is the # of trajectories in $\mathcal{D}$</p>
<p><img data-src="/notes/images/Vinilla-Policy-Gradient-pseudo-code.png" alt="upload successful"></p>
<h1 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h1><p><strong>REINFORCE</strong>(Monte-Carlo policy gradient) is a Monte-Carlo algorithm using the <em>complete return</em> from the time $t$, which includes future rewards up until the end of episode. <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sutton, R.S., & Barto, A.G. (1988). Reinforcement Learning: An Introduction. IEEE Transactions on Neural Networks, 16, 285-286.
">[3]</span></a></sup> </p>
<script type="math/tex; mode=display">
\begin{align}
\nabla J(\theta) & = \mathbb{E}_\pi \left[ \sum_a \pi(a \vert S_t, \theta) q_\pi (S_t, a) \frac{\nabla \pi(a \vert S_t, \theta)}{\pi(a \vert S_t, \theta)} \right] \\
& = \mathbb{E} \left[ q_\pi(S_t, A_t) \frac{\nabla \pi(A_t \vert S_t,\theta)}{\pi(A_t \vert S_t, \theta)} \right] \\
& = \mathbb{E} \left[ G_t \underbrace{\frac{\nabla \pi(A_t \vert S_t,\theta)}{\pi(A_t \vert S_t, \theta)}}_{\text{eligibility vector:} \\ \nabla \ln \pi(A_t \vert S_t,\theta)} \right]
\end{align}</script><ul>
<li><strong>Intuition</strong>: the update increases the paramer vector in this distribution proportional to the return, and inversely proportional to the action probability.</li>
</ul>
<p>REINFORCE algorithm:</p>
<ol>
<li>Initialize policy parameter $\theta$ at random;</li>
<li>For each episode:<ol>
<li>Generate an episode <script type="math/tex">S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_T</script>, following $\pi(\cdot \vert \cdot, \theta)$</li>
<li>Loop for each step of the episode $t=0,1,\cdots, T-1$:<ol>
<li>$ G \leftarrow \sum_{k=t+1}^T \gamma^{k-t-1} R_k $</li>
<li>$\theta \leftarrow \theta + \alpha \gamma^t G \nabla \ln \pi (A_t \vert S-t, \theta)$</li>
</ol>
</li>
</ol>
</li>
</ol>
<ul>
<li>Drawbacks: REINFORCE has a <strong>high variance</strong> and thus produces slow learning.</li>
</ul>
<h2 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h2><p>A variant of REINFORCE is to substract a baseline value from the return $G_t$ to reduce the variance of policy gradient while keeping the bias unchanged.<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sutton, R.S., & Barto, A.G. (1988). Reinforcement Learning: An Introduction. IEEE Transactions on Neural Networks, 16, 285-286.
">[3]</span></a></sup></p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t + \alpha \left( G_t - \pmb{b(S_t)}\right) \nabla \ln \pi(A_t \vert S_t, \theta_t)</script><h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h1><p>Actor-Critic consists of two components </p>
<p>Actor-Critic algorithms:</p>
<ol>
<li>Initialize policy parameter $\pmb{\theta} \in \mathbb{R}^{d’}$ and state-value weights $\pmb{w} \in \mathbb{R}^d$</li>
<li>For each episode:<ol>
<li>Initilize the first state of episode $ S \leftarrow 1$</li>
<li>While $S$ != TERMINAL (for each time step):<ol>
<li>$A \sim \pi(\cdot \vert S,\theta) $</li>
<li>Take action $A$, observe $S’$, $R$</li>
<li>$\delta \leftarrow R + \gamma \hat{v}(S’,\pmb{w}) - \hat{v}(S,\pmb{w})$ (If $S’$ is terminal, then $\hat{v}(S’, \pmb{w})=0$)</li>
<li>$\pmb{w} \leftarrow \pmb{w} + \alpha^{\pmb{w}} \delta \nabla \hat{v}(S,\pmb{w})$</li>
<li>$\pmb{\theta} \leftarrow \pmb{\theta} + \alpha^{\theta} I \delta \nabla \ln \pi(A \vert S,\theta)$</li>
<li>$I \leftarrow \gamma I$</li>
<li>$S \leftarrow S’$</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="Asynchronous-Advantage-Actor-Critic-A3C"><a href="#Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="Asynchronous Advantage Actor-Critic(A3C)"></a>Asynchronous Advantage Actor-Critic(A3C)</h1><p>Mnih et.al(2016) <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Mnih, V., Badia, A.P., Mirza, M.P., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., & Kavukcuoglu, K. (2016). [Asynchronous Methods for Deep Reinforcement Learning](https://pdfs.semanticscholar.org/6204/12c443e85a1fc2f9ab950ddfada8d18d63b4.pdf). ICML.
">[1]</span></a></sup> proposed an asynchronous gradient descent for optimization of deep neural networks, showing that parallel actor-learners have a stabilizing effect on training, greatly reducing the training time with a <strong>single multi-core CPU</strong> instead of GPU.</p>
<ul>
<li>Instead of experience replay, they asynchronously execute multiple agents in parallel on multiple instances of the environment. This parallelism also decorrelates the agents’ data into a more stationary process, since at any given time-step the parallel agents will be <em>experiencing a variety of different states</em>.</li>
<li>Apply different exploration policies in each actor-learner to maximize the diversity. By running different exploration policies in different threads, the overall updates of parameters are likely to be <strong>less correlated</strong> in time than a single agent applying online updates.</li>
<li>The gradient accumulation in parallelism can be seen as a prarallized minibatch of stochastic gradient update, where the parameters update thread-by-thread in the direction of each thread independently.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Policy gradient algorithms #A3C](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a3c)
">[2]</span></a></sup></li>
</ul>
<p><strong>A3C</strong> pseudocode for each actor-learner thread:</p>
<ol>
<li>Assume global shared parameter vectors $\theta$ and <script type="math/tex">\theta_v</script> and global shared counter $T=0$, thread-specified parameter vectors $\theta’$ and $\theta’_v$</li>
<li>Initialize the thread step count $t \leftarrow 1$</li>
<li>While <script type="math/tex">T<=T_{max}</script>:<ol>
<li>Reset gradients: $d\theta \leftarrow 0$ and $d\theta_v \leftarrow 0$</li>
<li>Synchronize thread-specific parameters $\theta’=\theta$ and $\theta’_v = \theta_v$</li>
<li>$t_\text{start} = t$</li>
<li>sample state <script type="math/tex">s_t</script></li>
<li>while (<script type="math/tex">s_t</script> != TERMINAL and <script type="math/tex">t - t_\text{start} <= t_\text{max}</script>):<ol>
<li>Perform the action <script type="math/tex">a_t \sim \pi(a_t \vert s_t; \theta')</script></li>
<li>Receive reward <script type="math/tex">r_t</script> and new state <script type="math/tex">s_{t+1}</script>;</li>
<li>$t \leftarrow t+1$</li>
<li>$T \leftarrow T+1$</li>
</ol>
</li>
<li>The return estimation: <script type="math/tex; mode=display">R = \begin{cases} 0 & \text{if } s_t \text{ is TERMINAL} \\
V_{w’}(s_t) & \text{otherwise}\end{cases}</script></li>
<li>For <script type="math/tex">i \in \{ t-1, \cdots, t_\text{start} \}</script> do<ol>
<li>$R \leftarrow \gamma R + R_i$; here $R$ is a MC measure of <script type="math/tex">G_i</script></li>
<li>Accumulate gradients w.r.t $\theta’$: <script type="math/tex">d\theta \leftarrow d\theta + \nabla_{\theta'} \log \pi(a_i \vert s_i;\theta') (R-V(s_i;\theta_v'))</script></li>
<li>Accumulate gradients w.r.t <script type="math/tex">d\theta_v \leftarrow d\theta_v + \frac{\partial (R-V(s_i;\theta_v'))^2}{\partial \theta_v'}</script></li>
</ol>
</li>
<li>Perform asynchronous update of $\theta$ using $d\theta$ and of <script type="math/tex">\theta_v</script> using <script type="math/tex">d\theta_v</script></li>
</ol>
</li>
</ol>
<h1 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor-Critic (A2C)"></a>Advantage Actor-Critic (A2C)</h1><p>Removing the first “A”(Asynchronous) from A3C, we get <strong>advantage actor-critic (A2C)</strong>. A3C updates the global parameters independently, thus thread-specific agents updates the policy with different versions and aggregated updates could not be optimal. </p>
<p>A2C <strong>waits for each actor to finish its segment of experience before performing an update</strong>, averaging over all of the actors. In the next iteration, parallel actors starts from the same policy. A2C is <strong>more cost-effective</strong> than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies. <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[A2C](https://openai.com/blog/baselines-acktr-a2c/)
">[4]</span></a></sup></p>
<p><img data-src="/notes/images/rl-A2C.png" alt="upload successful"></p>
<h1 id="Trust-Region-Policy-Optimization-TRPO"><a href="#Trust-Region-Policy-Optimization-TRPO" class="headerlink" title="Trust Region Policy Optimization (TRPO)"></a>Trust Region Policy Optimization (TRPO)</h1><p>Trust Region Policy Optimization (TRPO) enforces a KL divergence constraint at every point in the state space.</p>
<p>TRPO minimizes a certain surrogate obejctive fuction guarateeing policy improvement with non-trivial step sizes, giving monotonic improvements with little tuning of hyperparameters at each update<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Schulman, J., Levine, S., Abbeel, P., Jordan, M.I., & Moritz, P. (2015). [Trust Region Policy Optimization](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf). ICML.
">[5]</span></a></sup>.</p>
<ul>
<li>Let $\tilde{\pi}$ denote the expected return of another policy $\tilde{\pi}$ in terms of the advantage over the policy $\pi$</li>
<li><script type="math/tex">\rho_\pi</script> denotes discounted visitation frequency: <script type="math/tex">\rho_\pi (s) = P(s_0=s) + \gamma P(s_1 = s) + \gamma^2 P(s_2=s)+ \cdot</script>.</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\eta(\tilde{\pi}) &= \eta(\pi) + \mathbb{E}_{s_0,a_0,\cdots \sim \tilde{\pi}} \left[ \sum_{t=0}^\infty \gamma^t A_\pi (s_t, a_t) \right] & \\
& = \eta(\pi) + \sum_s \pmb{\rho_{\tilde{\pi}}}(s) \sum_{a} \tilde{\pi} (a \vert s) A_\pi (s,a) & \text{rewrite with sum over states} \\
L_\pi(\tilde{\pi})& \approx \eta(\pi) + \sum_s \pmb{\rho_{\pi}}(s) \sum_{a} \tilde{\pi} (a \vert s) A_\pi (s,a) & \text{replace } \rho_{\tilde{\pi}} \text{ with } \rho_{\pi} \text{ to approximate} \\ 
\end{align}</script><p>The aforementioned update does not give any guidance on the step size to update. </p>
<ul>
<li><code>Conservative policy iteration</code> provides explicit lower bounds on the improvements of $\eta$. </li>
</ul>
<p>Let <script type="math/tex">\pi_\text{old}</script> denote the current policy and <script type="math/tex">\pi' = \arg \max_{\pi'} L_{\pi_\text{old}} (\pi')</script>, the new policy is:</p>
<script type="math/tex; mode=display">\pi_\text{new}= (1-\alpha)\pi_\text{old} (a \vert s) + \alpha \pi'(a \vert s)</script><p>where the lower bound:</p>
<script type="math/tex; mode=display">\begin{align}
\eta(\pi_\text{new}) &\geq L_{\pi_\text{old}}(\pi_\text{new}) - \frac{2 \epsilon \gamma}{(1-\gamma)^2}\alpha^2 & \\
& \text{where } \epsilon = \max_s \vert \mathbb{E}_{a \sim \pi'(a \vert s)} [A_\pi (s,a)] \vert & \\
\end{align}</script><p>Replace $\alpha$ with the distance measure between $\pi$ and $\tilde{\pi}$, <strong>total vairation divergence</strong>: <script type="math/tex">D_\text{TV}=\frac{1}{2} \sum_i |p_i - q_i|</script> for discrete distributions $p,q$.<br>Define <script type="math/tex">D_\text{TV}^{\max} = \max_{s,a} |A_\pi (s,a)|</script></p>
<script type="math/tex; mode=display">\begin{align}
\eta(\pi_\text{new}) & \geq L_{\pi_\text{old}}(\pi_\text{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2}\alpha^2   & \text{replace } \alpha \text{ with total variation divergence} \\
& \text{where } \epsilon =\max_{s,a} | A_\pi (s,a)| &
\end{align}</script><p>Since <script type="math/tex">D_{TV} (p || q)^2  \leq D_{KL}(p || q)</script>,  Let <script type="math/tex">D_{KL}^{\max} (\pi, \tilde{\pi}) = \max_s D_{KL}(\pi(\cdot \vert s) || \tilde{\pi}(\cdot \vert s))</script>, we get:</p>
<script type="math/tex; mode=display">
\begin{align}
\eta(\pi) & \geq L_\pi (\tilde{\pi} - C D_{KL}^{\max}(\pi, \tilde{\pi})) & \text{replace } \alpha^2 \text{ with } D_{KL}^{\max} (\pi, \tilde{\pi}) \\
& \text{where } C=\frac{4 \epsilon \gamma}{(1-\gamma)^2}
\end{align}</script><p>Let <script type="math/tex">M_i(\pi) = L_{\pi_i} (\pi) - C D_{KL}^{\max}(\pi_i, \pi)</script>, then:</p>
<script type="math/tex; mode=display">
\begin{align}
\eta(\pi_{i+1}) \geq M_i (\pi_{i+1}) & \\
\eta(\pi_i) = M_i(\pi_i) &\\ 
\end{align}</script><p>Therefore,</p>
<script type="math/tex; mode=display">\eta(\pi_{i+1}) - \eta(\pi_i) \geq M_i(\pi_{i+1}) - M (\pi_i)</script><p>This guarantees that the true objective $\eta$ is non-decreasing.</p>
<p>Afterwards, we improve the true objective $\eta$. Let <script type="math/tex">\theta_\text{old}</script> represent <script type="math/tex">\pi_{\theta_\text{old}}</script>, and $\theta$ represent <script type="math/tex">\pi_\theta</script>.</p>
<script type="math/tex; mode=display">\text{maximize}_{\theta} [ L_{\theta_\text{old}}(\theta) - C D_\text{KL}^{\max} (\theta_\text{old}, \theta)]</script><p>Thus, we use a constraint on the KL divergence beween the new policy and the old policy, i.e., a trust region constraint:</p>
<script type="math/tex; mode=display">\begin{align}
\text{maximize}_{\theta} L_{\theta_\text{old}}(\theta) & \\
s.t. D_\text{KL}^{\max} (\theta_\text{old}, \theta) \leq \delta &
\end{align}</script><p>By heuristic approximation, we consider the average KL divergence to replace the $\max$ KL divergence:</p>
<script type="math/tex; mode=display">\begin{align}
& \text{maximize}_{\theta} L_{\theta_\text{old}}(\theta)   \\
 & s.t. \bar{D}_\text{KL}^{\rho_{\theta_\text{old}}} (\theta_\text{old}, \theta) \leq \delta  \\
\end{align}</script><ul>
<li>Expand <script type="math/tex">L_{\theta_\text{old}}</script>:<script type="math/tex; mode=display">\text{maximize}_{\theta} \sum_s \rho_{\theta_\text{old}}(s) \sum_a \pi_\theta (a \vert s) A_{\theta_\text{old}}(s,a)</script></li>
<li>Replace the sum over actions by an important sampling estimator:<script type="math/tex; mode=display">\sum_a \pi_\theta(a \vert s_n) A_{\theta_\text{old}} (s_n, a) = \mathbb{E}_{a \sim q} \left[ \frac{\pi_\theta(a \vert s_n)}{q(a \vert s_n)} A_{\theta_\text{old}}(s_n, a) \right]</script></li>
<li>Replace <script type="math/tex">\sum_s \rho_{\theta_\text{old}}</script> with expectation <script type="math/tex">\mathbb{E}_{s \sim \rho_{\theta_\text{old}}}[\cdots]</script>; replace the advantage values <script type="math/tex">A_{\theta_\text{old}}</script> by the $Q$-values <script type="math/tex">Q_{\theta_\text{old}}</script>. Finally, we get<script type="math/tex; mode=display">
\begin{align}
\text{maximize}_\theta & \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim q} \left[ \frac{\pi_\theta (a \vert s)}{q(a \vert s)} Q_{\theta_\text{old}}(s,a) \right] \\
s.t. & \mathbb{E}_{s \sim \rho_{\theta_\text{old}}} \left[ D_\text{KL} \left(\pi_{\theta_\text{old}}(\cdot \vert s) || \pi_\theta (\cdot \vert s) \right) \right] \leq \delta
\end{align}</script></li>
</ul>
<h1 id="Proximal-Policy-Optimization-PPO"><a href="#Proximal-Policy-Optimization-PPO" class="headerlink" title="Proximal Policy Optimization (PPO)"></a>Proximal Policy Optimization (PPO)</h1><ul>
<li><p>Problems: TRPO is relatively complicated, and is not compatible with architectures that include noise (e.g. dropout) or parameter sharing (between the policy and value function, or with auxiliary tasks).<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347). ArXiv, abs/1707.06347.
">[8]</span></a></sup></p>
</li>
<li><p>PPO with clipped surrogate objective performs better than that with KL penalty.</p>
</li>
</ul>
<h2 id="PPO-clip"><a href="#PPO-clip" class="headerlink" title="PPO-clip *"></a>PPO-clip *</h2><p>Clipped surrogate objective</p>
<ul>
<li>Let <script type="math/tex">r_t(\theta) = \frac{\pi_\theta (a_t \vert s_t)}{\pi_{\theta_\text{old}}(a_t \vert s_t)}</script>, so <script type="math/tex">r(\theta_\text{old})=1</script>. TRPO maimize a surrogate objective with conservative policy iteration (CPI). Without the constraint, maximization of $L^{\text{CPI}}$ would lead to excessively large policy update.<script type="math/tex; mode=display">\begin{align}
L^{\text{CPI}}(\theta) & = \hat{\mathbb{E}}_t \left[ \frac{\pi_\theta (a_t \vert s_t)}{\pi_{\theta_\text{old}}(a_t \vert s_t)} \hat{A}_t \right] = \hat{\mathbb{E}}_t \left[ r_t(\theta) \hat{A}_t \right]   \\
s.t. & \hat{\mathbb{E}}_t \left[ \text{KL} [\pi_{\theta_\text{old}} (\cdot \vert s_t), \pi_\theta (\cdot \vert s_t) ] \right] \leq \delta 
\end{align}</script></li>
<li>PPO pernalize changes to the policy that move <script type="math/tex">r_t(\theta)</script> away from 1:<script type="math/tex; mode=display">L^\text{clip}(\theta) = \hat{\mathbb{E}} \left[ \min \big( r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \big) \right]</script></li>
</ul>
<p>where $\epsilon$ is a hyperparameter, say, $\epsilon=0.2$. The intuition is to take the minimum of the clipped and unclipped objective, thus the final objective is a lower bound (i.e. a <strong>pessimistic bound</strong>) on the unclipped objective.</p>
<h2 id="PPO-penalty"><a href="#PPO-penalty" class="headerlink" title="PPO-penalty"></a>PPO-penalty</h2><p>Adaptive KL pernalty coefficient</p>
<ul>
<li>Variant: add a penalty on KL divergence</li>
<li>With mini-batch SGD, optimize the KL-penalized objective:<script type="math/tex; mode=display">L^\text{KL-penalty} = \hat{\mathbb{E}} \left[ \frac{\pi_\theta (a_t \vert s_t)}{\pi_{\theta_\text{old}}} \hat{A}_t - \beta \text{KL} [ \pi_{\theta_\text{old}} (\cdot \vert s_t), \pi_\theta (\cdot \vert s_t) ] \right]</script></li>
<li>Compute <script type="math/tex">d=\hat{E}_t [\text{KL} [ \pi_{\theta_\text{old}} (\cdot \vert s_t), \pi_\theta (\cdot \vert s_t) ] ]</script><ul>
<li>If $d &lt; d_\text{target}/1.5, \beta \leftarrow \beta/2$</li>
<li>If $d &gt; d_\text{target} \times 1.5,  \beta \leftarrow \beta \times 2$</li>
</ul>
</li>
</ul>
<h2 id="PPO-algorithms"><a href="#PPO-algorithms" class="headerlink" title="PPO algorithms"></a>PPO algorithms</h2><p>Finally, the objective function is augmented with an error term on the value estimation and an entropy term to encourage sufficient exploration.</p>
<script type="math/tex; mode=display">\begin{align}
L^\text{Clip + SE + Entropy}_t (\theta) = \mathbb{E} \left[ L^\text{clip}_t (\theta) - c_1 \underbrace{(V_\theta (s_t) - V_t^\text{target})^2}_\text{squared error loss} + c_2 \underbrace{\mathbb{H}(s_t, \pi_{\theta})}_\text{entropy term}   \right] 
\end{align}</script><p>where <script type="math/tex">c_1</script>, <script type="math/tex">c_2</script> are constant coefficients.</p>
<ul>
<li>Settings:<ul>
<li>RNN</li>
<li>Adam</li>
<li>mini-batch SGD</li>
</ul>
</li>
</ul>
<p>PPO algorithms with Actor-Critic style:</p>
<ol>
<li>for iteration=$1,2,,\cdots$:<ol>
<li>for actor=$1,2,,\cdots, N$:<ol>
<li>run policy <script type="math/tex">\pi_{\theta_\text{old}}</script> in environment for $T$ timesteps;</li>
<li>Compute advantage estimates <script type="math/tex">\hat{A}_1, \cdots, \hat{A}_T</script>;</li>
</ol>
</li>
<li>Optimize surrogate $L$ w.r.t $\theta$, with $K$ epochs and mini-batch size $M \leq NT$</li>
<li>$\theta_\text{old} \leftarrow \theta$</li>
</ol>
</li>
</ol>
<h2 id="Distributed-PPO"><a href="#Distributed-PPO" class="headerlink" title="Distributed PPO"></a>Distributed PPO</h2><p>Let $W$ denote # of workers; $D$ sets a threshold for the # of workers whose gredients must be available to update the parameters; $M$, $B$ is the # of sub-iterations with policy and baseline updates given a batch of datapoints.<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Heess, N., Dhruva, T., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y., Erez, T., Wang, Z., Eslami, S.M., Riedmiller, M.A., & Silver, D. (2017). [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/pdf/1707.02286.pdf). ArXiv, abs/1707.02286.
">[9]</span></a></sup></p>
<p>The distributed PPO-penalty algorithms:</p>
<p><img data-src="/notes/images/rl-DPPO-chief-alg.png" alt="upload successful"><br><img data-src="/notes/images/rl-DPPO-worker-alg.png" alt="upload successful"></p>
<p>Experiments indicates that averaging gradients and applying them <strong>synchronously</strong> leads to better results than asynchronously in practice.</p>
<h1 id="Generalized-Advantage-Estimation-GAE"><a href="#Generalized-Advantage-Estimation-GAE" class="headerlink" title="Generalized Advantage Estimation(GAE)"></a>Generalized Advantage Estimation(GAE)</h1><div class="note danger">
            <p><strong>Challenges</strong>:</p><ol><li>Requires a large number of samples;</li><li>Difficulty of obtaining stable and steady improvement despite the non-stationarity of the incoming data.</li><li><code>credit reward problem</code> in RL (a.k.a distal reward problem in the behavioral literature): long time delay on rewards.</li></ol>
          </div>
<p>Solution:<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Schulman, J., Moritz, P., Levine, S., Jordan, M.I., & Abbeel, P. (2016). [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438). CoRR, abs/1506.02438.
">[10]</span></a></sup></p>
<ol>
<li>Use value functions to reduce the variance of policy gradient estimates at the cost of some bias, with an <strong>exponentially-weighted estimator of advantage function</strong> that is analogous to TD($\lambda$);</li>
<li>Use TRPO for both policy and value function with NNs.</li>
</ol>
<h2 id="Advantage-function-estimation"><a href="#Advantage-function-estimation" class="headerlink" title="Advantage function estimation"></a>Advantage function estimation</h2><p>The advantage has the form:</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{E}_{s_{t+1}} \left[ \delta_t^{V^{\pi,\gamma}} \right] & = \mathbb{E}_{s_{t+1}} [r_t + \gamma V^{\pi, \gamma} (s_{t+1}) - V^{\pi,\gamma}(s_t)]  \\
&= \mathbb{E}_{s_{t+1}} [Q^{\pi, \gamma} (s_t, a_t) - V^{\pi, \gamma}(s_t)] \\
&= A^{\pi, \gamma} (s_t, a_t)
\end{align}</script><p>Now take the form of $k$ of $\delta$ terms, denoted by <script type="math/tex">\hat{A}_t^{(k)}</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
&\hat{A}^{(1)}_t := \delta_t^V & = -V(s_t) + r_t + \gamma V(s_{t+1}) \\
&\hat{A}^{(2)}_t := \delta_t^V + \gamma \delta_{t+1}^V & = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) \\
&\hat{A}^{(3)}_t := \delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V & = -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 r_{t+2}  + \gamma^3 V(s_{t+3}) \\
& \cdots & \cdots\\
& \hat{A}_t^{(k)} := \sum_{l=0}^{k-1} \gamma^l \delta_{t+1}^V & = - V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{k-1} r_{t+k-1} + \gamma^kV(s_{t+k})
\end{align}</script><p>With $k \rightarrow \infty$, the bias generally becomes smaller:</p>
<script type="math/tex; mode=display">\hat{A}^{(\infty)}_t = \sum_{l=0}^\infty \gamma^l \delta_{t+1}^V = -\underbrace{V(s_t)}_\text{value function} + \underbrace{\sum_{l=0}^\infty \gamma^l r_{t+1}}_\text{empirical returns}</script><p>which is simply the empirical returns minus the value function baseline.</p>
<h3 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h3><p>GAE is defined as the exponentially-weighted average of these $k$-step estimators:</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{A}_t^{\text{GAE}(\gamma,\lambda)} & := (1-\lambda)(\hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} +  \lambda^2 \hat{A}_t^{(3)} + \cdots ) \\
& = (1-\lambda) \big(\delta_t^V + \lambda (\delta_t^V + \gamma \delta_{t+1}^V) + \lambda^2 (\delta_t^V + \gamma \delta_{t+1}^V + \gamma^2 \delta_{t+2}^V) + \cdots \big) \\
& = (1-\lambda) \big( \delta_t^V (1+\lambda + \lambda^2 + \cdots) + \gamma \delta_{t+1}^V (\lambda + \lambda^2 + \lambda^3 + \cdots) + \gamma^2 \delta_{t+2}^V (\lambda^2+\lambda^3+\lambda^4+\cdots) + \cdots \big) \\
& = (1-\lambda) \big( \delta_t^V (\frac{1}{1-\lambda}) + \gamma \delta_{t+1}^V (\frac{\lambda}{1-\lambda}) + \gamma^2 \delta_{t+2}^V (\frac{\lambda^2}{1-\lambda}) + \cdots \big) \\
& = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+1}^V
\end{align}</script><p>Now consider two special cases:</p>
<ul>
<li>when $\lambda \rightarrow 0$:<script type="math/tex; mode=display">\text{GAE}(\gamma, 0) \quad \hat{A}_t := \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)</script></li>
<li>when $\lambda \rightarrow 1$:<script type="math/tex; mode=display">\text{GAE}(\gamma, 1) \quad \hat{A}_t := \sum_{t=0}^\infty \gamma^l \delta_{t+l} = \sum_{l=0}^\infty \gamma^l r_{t+1} - V(s_t)</script></li>
</ul>
<p>It shows that GAE($\gamma,1$) has high variance due to the sum of terms; GAE($\gamma,0$) induces bias but with lower variance. The GAE with $\lambda \in (0,1)$ reaches a tradeoff between the bias and variance.</p>
<h2 id="Interpretation-as-Reward-Shaping"><a href="#Interpretation-as-Reward-Shaping" class="headerlink" title="Interpretation as Reward Shaping"></a>Interpretation as Reward Shaping</h2><ul>
<li><p><strong>Reward shaping</strong> refers to the following reward transformation of MDP:</p>
<script type="math/tex; mode=display">\tilde{r}(s,a,s') = r(s,a,s') + \gamma \Phi(s') - \Phi(s)</script><p>where $\Phi: \mathcal{S} \rightarrow \mathbb{R}$ is an arbitrary scalar-valued function on the state space. </p>
</li>
<li><p>Let $\tilde{Q}^{\pi, \gamma}$, $\tilde{V}^{\pi, \gamma}$, $\tilde{A}^{\pi, \gamma}$ be the value and advatage functions of the transformed MDP:</p>
<script type="math/tex; mode=display">\tilde{Q}^{\pi, \gamma} (s,a) = Q^{\pi, \gamma} (s,a) - \Phi(s)</script><script type="math/tex; mode=display">\tilde{V}^{\pi, \gamma} (s,a) = V^{\pi, \gamma} (s,a) - \Phi(s)</script><script type="math/tex; mode=display">\tilde{A}^{\pi, \gamma} (s,a) = \big(Q^{\pi, \gamma} (s,a) - \Phi(s) \big) -\big( V^{\pi, \gamma} (s,a) - \Phi(s) \big) = A^{\pi, \gamma} (s,a)</script></li>
</ul>
<p>Let $\Phi = V$, then</p>
<script type="math/tex; mode=display">\sum_{l=0}^\infty (\gamma \lambda)^l \tilde{r}(s_{t+l}, a_t, s_{t+l+1}) = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}^V = \hat{A}_t^{\text{GAE}(\gamma, \lambda)}</script><h2 id="Value-function-estimation"><a href="#Value-function-estimation" class="headerlink" title="Value function estimation"></a>Value function estimation</h2><p>Constrain the average KL divergence between the previous value function and new value function to be smaller than $\epsilon$:</p>
<script type="math/tex; mode=display">
\begin{align}
\underset{\phi}{\text{minimize}} & \quad \sum_{n=1}^N ||V_\phi(s_n)- \hat{V}_n ||^2 \\
s.t & \quad \frac{1}{N}\sum_{n=1}^N \frac{||V_\phi(s_n)- V_{\phi_\text{old}}(s_n)||^2}{2\sigma^2} \le \epsilon
\end{align}</script><h1 id="Actor-Critic-with-Experience-Replay-ACER"><a href="#Actor-Critic-with-Experience-Replay-ACER" class="headerlink" title="Actor-Critic with Experience Replay(ACER)"></a>Actor-Critic with Experience Replay(ACER)</h1><p>Actor-Critic with Experience Replay(ACER)<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., & de Freitas, N. (2016). [Sample efficient actor-critic with experience replay](https://arxiv.org/pdf/1611.01224). arXiv preprint arXiv:1611.01224.
">[12]</span></a></sup> employs truncated importance sampling with bias correction, stochastic dueling network architectures, and a new TRPO method.</p>
<h2 id="Importance-weight-truncation-with-bias-correction"><a href="#Importance-weight-truncation-with-bias-correction" class="headerlink" title="Importance weight truncation with bias correction"></a>Importance weight truncation with bias correction</h2><script type="math/tex; mode=display">\hat{g}^\text{ACER}_t = \bar{\rho}_t \nabla_\theta \log \pi_\theta (a_t \vert x_t) [Q^\text{retrace}(x_t,a_t) - V_{\theta_v}(x_t)] + \mathbb{E}\big( [\frac{\rho_t(a) - c}{\rho_t(a)}]_+ \nabla_\theta \log \pi_\theta (a \vert x_t) [Q_{\theta_v}(x_t, a) - V_{\theta_v}(x_t)]  \big)</script><p>where <script type="math/tex">\bar{\rho}_t = \min(c,\rho_t)</script>, with importance weight <script type="math/tex">\rho_t = \frac{\pi(a_t \vert x_t)}{\mu (a_t \vert x_t)}</script></p>
<h2 id="Efficient-TRPO"><a href="#Efficient-TRPO" class="headerlink" title="Efficient TRPO"></a>Efficient TRPO</h2><p>ACER maintains an <code>average policy network</code> <script type="math/tex">\phi_{\theta_a}</script> that represents a running average of past policies and forces the updated policy to not deviate far from the average.</p>
<p>Update the parameter <script type="math/tex">\theta_a</script> of the average policy net work “softly” after each update:</p>
<script type="math/tex; mode=display">\theta_a \leftarrow \alpha \theta_a + (1-\alpha)\theta</script><p>The policy gradient w.r.t $\phi$:</p>
<script type="math/tex; mode=display">\hat{g}^\text{ACER}_t = \bar{\rho}_t \nabla_{\phi_\theta(x_t)} \log \pi_\theta (a_t \vert \phi_\theta (x)) [Q^\text{retrace}(x_t,a_t) - V_{\theta_v}(x_t)] + \mathbb{E}\big( [\frac{\rho_t(a) - c}{\rho_t(a)}]_+ \nabla_{\phi_\theta(x_t)} \log \pi_\theta (a \vert \phi_\theta (x)) [Q_{\theta_v}(x_t, a) - V_{\theta_v}(x_t)]  \big)</script><p><img data-src="/notes/images/rl-ACER-master-alg.png" alt="upload successful"></p>
<p><img data-src="/notes/images/rl-ACER-worker-alg.png" alt="upload successful"></p>
<h1 id="ACKTR"><a href="#ACKTR" class="headerlink" title="ACKTR"></a>ACKTR</h1><h1 id="Soft-Q-learning"><a href="#Soft-Q-learning" class="headerlink" title="Soft Q-learning"></a>Soft Q-learning</h1><p>Soft Q-learning(SQL)  expresses the optimal policy via Boltzmann distribution (a.k.a Gibbs distribution). </p>
<ul>
<li>Soft Q-learning fomulates a stochastic policy as a (conditional) energy-based model (EBM), with the energy function corresponding to the “soft” Q-function obtained when optimizing the maximum entropy objective. </li>
<li>“The entropy regularized actor-critic algorithms can be viewed as approaximate Q-learning methods, with the actor serving the role of an approimate sampler from an intrctable posterior” <sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017, August). [Reinforcement learning with deep energy-based policies](https://arxiv.org/pdf/1702.08165). In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1352-1361). JMLR. org.
">[14]</span></a></sup>.</li>
</ul>
<div class="note success">
            <p><strong>Contributions</strong>:</p><ol><li>Improved exploration performance is with <strong>multi-modal</strong> reward landscapes, where conventional deterministic or unimodal methods are at high risk of falling into suboptimal local optima.</li><li>Stochastic energy-based policies can provide a much better initialization for learning new skills than either random policies or policies pretrained with maximum expected reward objectives.</li></ol>
          </div>
<h2 id="Maximum-Entropy-RL"><a href="#Maximum-Entropy-RL" class="headerlink" title="Maximum Entropy RL"></a>Maximum Entropy RL</h2><ul>
<li>Conventional RL objectives to learn a policy <script type="math/tex">\pi(\pmb{a}_t \vert \pmb{s}_t)</script>:<script type="math/tex; mode=display">\pi^*_\text{std} = \arg \max_\pi \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} [r(s_t, a_t)]</script></li>
<li>Maximum entropy RL augments the entropy term to maximize its entropy at each visited state:<script type="math/tex; mode=display">\pi^*_\text{std} = \arg \max_\pi \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} [r(s_t, a_t) + \alpha \mathcal{H} (\pi(\cdot \vert s_t)) ]</script>where $\alpha$ is used to determine the relative importance of entropy and reward.</li>
</ul>
<h2 id="Energy-based-models-EBM"><a href="#Energy-based-models-EBM" class="headerlink" title="Energy-based models (EBM)"></a>Energy-based models (EBM)</h2><p>As the figure below, the conventional RL specifies a unimodal policy distribution, centered at the maimal Q-value and extending toe the neighboring actions to provide noise for exploration (as red distribution). The exploration is biased towards the upper mode, RL ignores the lower mode completely.<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Soft Q-learning, UC Berkeley blog](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
">[15]</span></a></sup><br><img data-src="/notes/images/rl-SQL-unimodal.png" width="50%"/></p>
<p>To ensure the agent to explore all promising states while prioritizing the more promising mode, Soft Q-learning definesthe policy w.r.t exponentiated Q-values (see green distribution):</p>
<script type="math/tex; mode=display">\pi(a_t \vert s_t) \propto \exp(-\mathcal{Q}(s_t, a_t))</script><p>where $\mathcal{Q}$ is an energy function, that could be represented by NNs.</p>
<p><img data-src="/notes/images/rl-SQL-multimodal.png" width="50%"/></p>
<h2 id="Soft-value-functions"><a href="#Soft-value-functions" class="headerlink" title="Soft value functions"></a>Soft value functions</h2><p>The soft Bellman equation:</p>
<script type="math/tex; mode=display">Q(s_t, a_t) = \mathbb{E} \left[ r_t + \gamma \text{softmax}_a Q(s_{t+1}, a) \right]</script><p>where</p>
<script type="math/tex; mode=display">\text{softmax}_a f(a) := \log \int_a \exp f(a) da</script><p>The soft Q-function is:</p>
<script type="math/tex; mode=display">Q^*_\text{soft} (s_t, a_t) = r_t + \mathbb{E}_{(s_{t+1,\cdots}) \sim \rho_\pi} \mathcal{H} (\pi^*_\text{MaxEnt}(\cdot \vert s_{t+l}))</script><p>The soft value function:</p>
<script type="math/tex; mode=display">V^*_\text{soft} (s_t) = \alpha \log \int_\mathcal{A} \exp \big( \frac{1}{\alpha} Q^*_\text{soft}(s_t, a') \big) da'</script><p>Then the optimal policy is:</p>
<script type="math/tex; mode=display">\begin{align} 
\pi^*_\text{MaxEnt}(a_t \vert s_t) &= \exp \big( \frac{1}{\alpha} (Q^*_\text{soft} (\underbrace{s_t, a_t) - V^*_\text{soft}(s_t)}_{\text{advantage function}}) \big) & \\
&=\frac{\exp(\frac{1}{\alpha}Q^*_\text{soft}(s_t,a_t))}{\int_\mathcal{A} \exp(\frac{1}{\alpha} Q^*_\text{soft}(s_t,a')) da'} & \\
&= \frac{\exp(\frac{1}{\alpha}Q^*_\text{soft}(s_t,a_t))}{\exp(\frac{1}{\alpha}V^*_\text{soft}(s_t))} &

\end{align}</script><h2 id="Proofs"><a href="#Proofs" class="headerlink" title="Proofs"></a>Proofs</h2><ul>
<li>Define the optimal policy with the EBM form:<script type="math/tex; mode=display">\pi^*(a \vert s) = \frac{\exp(Q(s,a))}{Z}</script>where $Z$ is the sum of the numerator.</li>
<li>Minimize the KL divergence:</li>
</ul>
<script type="math/tex; mode=display">\begin{align} 
\min \mathbb{KL} (\pi || \tilde{\pi}) & = \sum \pi(a \vert s) \log \frac{\pi(a \vert s)}{\tilde{\pi}(a \vert s)} & \\
& = \underbrace{\sum \pi(a \vert s) \log \pi(a \vert s)}_{- \mathbb{H}(\cdot \vert \pi)} - \sum \pi(a \vert s) \log \tilde{\pi}(a \vert s) & \text{expand KL divergence}\\
&= - \mathbb{H}(\cdot \vert \pi) - \sum \pi(a \vert s) \left[ Q(s_t,a_t) - \underbrace{log Z}_{V(s)} \right] \\
\text{here, } & & \\
V(s) & = \log Z & \\
    & = \log \int_\mathcal{A} \exp \left( \frac{1}{\alpha} Q(s',a') da' \right)    
\end{align}</script><h2 id="Soft-Q-learning-1"><a href="#Soft-Q-learning-1" class="headerlink" title="Soft Q-learning"></a>Soft Q-learning</h2><ul>
<li>Soft Bellman-backup<script type="math/tex; mode=display">\begin{align*}
Q_\text{soft}(s_t,a_t) &\leftarrow r_t+\gamma\mathbb{E}_{s_{t+1}\sim p_s}[V_\text{soft}(s_{t+1})] \\
V_\text{soft}(s_t) &\leftarrow \alpha\log\int_\mathcal{A} \exp\left( \frac{1}{\alpha} Q_\text{soft}(s_t,a') \right)\mathrm{d}a'
\end{align*}</script></li>
</ul>
<p>This cannot be performed exactly in countinuous or large state and action spaces. Sampling from the energy-based model is intractable in general.</p>
<h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><h4 id="Importance-sampling"><a href="#Importance-sampling" class="headerlink" title="Importance sampling"></a>Importance sampling</h4><script type="math/tex; mode=display">V^\theta_\text{soft} (s_t) = \alpha \log \mathbb{E}_{q_{a'}} \left[ \frac{\exp(\frac{1}{\alpha} Q_\text{soft}^\theta(s_t, a') )}{q_{a'}(a')} \right]</script><p>where <script type="math/tex">q_{a'}</script> can be an arbitrary distribution over the action space.<br>It can also be equivalent as minimizing:</p>
<script type="math/tex; mode=display">J_Q(\theta) = \mathbb{E}_{s_t \sim q_{s_t}, a_t \sim q_{a_t}} \left[ \frac{1}{2} \big( \hat{Q}^{\bar{\theta}}_\text{soft} (s_t,a_t) - Q_\text{soft}^\theta (s_t,a_t) \big)^2 \right]</script><p>where <script type="math/tex">\hat{Q}^{\bar{\theta}}_\text{soft} (s_t, a_t) = r_t + \gamma \mathbb{E}_{s_{t+1} \sim p_s} [V_\text{soft}^{\bar{\theta}}(s_{t+1})]</script> is the target $Q$-value.</p>
<h4 id="Stein-Variational-Gradient-Descent-SVGD"><a href="#Stein-Variational-Gradient-Descent-SVGD" class="headerlink" title="Stein Variational Gradient Descent (SVGD)"></a>Stein Variational Gradient Descent (SVGD)</h4><ul>
<li>How to approximately sample from the soft Q-funtion?<ol>
<li>MCMC based sampling</li>
<li>learn a stochastic sampling network trained to output approximate samples from the target distribution</li>
</ol>
</li>
</ul>
<p>Soft Q-learning applies the sampling network based on <strong>Stein variational gradient descent</strong> (SVGD) and <strong>amortized SVGD</strong>.</p>
<ol>
<li>Learn a state-conditioned stochastic NN <script type="math/tex">a_t = f^\phi(\xi; s_t)</script> that maps noise samples $\xi$ drawn from an arbitrary distribution into unbiased action samples from the target EBM of <script type="math/tex">Q_\text{soft}^\theta</script>. </li>
<li>The induced distribution of the actions <script type="math/tex">\pi^\phi (a_t \vert s_t)</script> approximates the energy-based distribution w.r.t KL divergence:<script type="math/tex; mode=display">J_\pi(\phi^\phi \big(\cdot \vert s_t) || \exp(\frac{1}{\alpha} (Q_\text{soft}^\theta(s_t, \cdot) - V_\text{soft}^\theta ) ) \big)</script></li>
<li>SVGD provides the most greedy directions as a functional:<script type="math/tex; mode=display">\Delta f^\phi(\cdot ; s_t) = \mathbb{E}_{a_t \sim \pi^\phi} \big[\kappa (a_t, f^\phi(\cdot;s_t)) \nabla_{a'}Q^\theta_\text{soft}(s_t, a') |_{a'=a_t} + \alpha \nabla_{a'}\kappa (a',f^\phi(\cdot;s_t))|_{a'=a_t} \big]</script>Update the policy networks:<script type="math/tex; mode=display">\frac{\partial J_\pi (\phi; s_t)}{\partial \phi}  \propto \mathbb{E}_\xi \left[ \Delta f^\phi(\xi; s_t)\frac{\partial f^\phi (\xi; s_t)}{\partial \phi} \right]</script></li>
</ol>
<h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h3><p>$\mathcal{D} \leftarrow$ empty replay memory<br>Assign target parameters: $\bar{\theta} \leftarrow \theta$, $ \bar{\phi} \leftarrow \phi $</p>
<ol>
<li>for each epoch:<ol>
<li>for each t do:<ol>
<li><strong>Collect experience</strong><br> Sample an action for <script type="math/tex">s_t</script> using $f^\phi$: <script type="math/tex">a_t f^\phi(\xi;s_t)</script> where $\xi \sim (0; I)$<br> Sample next state from the environment: <script type="math/tex">s_{t+1} \sim p_s(s_{t+1} \vert s_t, a_t)</script><br> Save the new experience in the replay memory: <script type="math/tex">\mathcal{D} \leftarrow \mathcal{D} \cup \{ (s_t,a_t,r(s_t,a_t), s_{t+1}) \}</script></li>
<li><strong>Sample a mini-batch from the replay memory</strong> <script type="math/tex">\{(s_t^{(i)}, a_t^{(i)}, r_t^{(i)},s_{t+1}^{(i)})\}_{i=0}^N \sim \mathcal{D}</script></li>
<li><strong>Update the soft Q-function parameters</strong>:<br> Sample <script type="math/tex">\{ a^{(i,j)} \}_{j=0}^M \sim q_{a'}</script>  for each <script type="math/tex">s_{t+1}^{(i)}</script><br> Compute empirical soft values <script type="math/tex">\hat{V}_\text{soft}^\bar{\theta} (s_{t+1}^{(i)})</script> in <script type="math/tex; mode=display">V^\theta_\text{soft} (s_t) = \alpha \log \mathbb{E}_{q_{a'}} \left[ \frac{\exp(\frac{1}{\alpha} Q_\text{soft}^\theta(s_t, a') )}{q_{a'}(a')} \right]</script> Compute empirical gradient <script type="math/tex">\hat{\nabla}_\theta J_Q</script> of <script type="math/tex; mode=display">J_Q(\theta) = \mathbb{E}_{s_t \sim q_{s_t}, a_t \sim q_{a_t}} \left[ \hat{Q}^{\bar{\theta}}_\text{soft} (s_t,a_t) - Q_\text{soft}^\theta (s_t,a_t)^2 \right]</script> Update $\theta$ according to <script type="math/tex">\hat{\nabla}_\theta J_Q</script> using Adam.</li>
<li><strong>Update policy</strong><br> Sample <script type="math/tex">\{ \xi^{(i,j)} \}_{j=0}^{M} \sim \mathcal{N} (\pmb{0},\pmb{I})</script> for each <script type="math/tex">s_t^{(i)}</script><br> Compute actions <script type="math/tex">a_t^{(i,j)}= f^\phi(\xi^{(i,j)}, s_t^{(i)})</script><br> Compute <script type="math/tex">\Delta f^\phi</script> using empirical estimate of <script type="math/tex; mode=display">\Delta f^\phi(\cdot ; s_t) = \mathbb{E}_{a_t \sim \pi^\phi} \big[\kappa (a_t, f^\phi(\cdot;s_t)) \nabla_{a'}Q^\theta_\text{soft}(s_t, a') |_{a'=a_t} + \alpha \nabla_{a'}\kappa (a',f^\phi(\cdot;s_t))|_{a'=a_t} \big]</script> Compute empirical estimtate of <script type="math/tex">\hat{\nabla}_\phi J_\pi</script> of <script type="math/tex; mode=display">\frac{\partial J_\pi (\phi; s_t)}{\partial \phi}  \propto \mathbb{E}_\xi \left[ \Delta f^\phi(\xi; s_t)\frac{\partial f^\phi (\xi; s_t)}{\partial \phi} \right]</script> Update $\phi$ according to <script type="math/tex">\hat{\nabla}_\phi J_\pi</script> using Adam</li>
</ol>
</li>
<li>If epoch $%$ update_interval == 0:<br> $\bar{\theta} \leftarrow \theta, \quad \bar{\phi} \leftarrow \phi $</li>
</ol>
</li>
</ol>
<h3 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h3><ul>
<li>Better exploration. SQL provides an implicit exploration stategy by assgining each action a non-zero probability, shaped by the current belief about its value, effectively combining exploration and exploitation in a natural way.<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Soft Q-learning, UC Berkeley blog](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
">[15]</span></a></sup></li>
<li>Fine-tuning maximum entropy policies: <code>general-to-specific transfer</code>.Pre-train policies for general purpose tasks, then use them as templates or initializations for more specific tasks.</li>
<li>Compositionality. Compose new skills from existing policies—even without any fine-tuning—by intersecting different skills.<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Soft Q-learning, UC Berkeley blog](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
">[15]</span></a></sup></li>
</ul>
<p><img data-src="/notes/images/rl-SQL-compositionality.gif" /></p>
<ul>
<li>Robustness. Maximum entropy formulation encourages to try all possible solutions, the agents learn to explore a large portion of the state space. Thus they learn to act in various situations, more robust against perturbations in the environment.</li>
</ul>
<p><img data-src="/notes/images/rl-SQL-robustness.gif" /></p>
<h1 id="Soft-Actor-Critic-SAC"><a href="#Soft-Actor-Critic-SAC" class="headerlink" title="Soft Actor-Critic(SAC)"></a>Soft Actor-Critic(SAC)</h1><p>Soft Actor-Critic (SAC)<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). [Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor](https://arxiv.org/pdf/1801.01290). arXiv preprint arXiv:1801.01290.
">[13]</span></a></sup> is an off-policy actor-critic algorithm based on the maximum entropy framework. Like SQL, SAC augments the conventional RL objectives with a maximum entropy objective:</p>
<script type="math/tex; mode=display">J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t,a_t) \sim \rho_\pi} \left[ r(s_t,a_t) + \alpha \mathcal{H}(\pi(\cdot \vert s_t)) \right]</script><p>where the $\alpha$ determines the relative importance of the entropy term against the reward, and thus controls the stochasticity of the optimal policy.</p>
<p>The maximum entropy terms </p>
<ol>
<li>Encourage to explore more widely, while giving up on clearly unpromising avenues;</li>
<li>Capture multiple modes of near-optimal behavior.</li>
<li>Improve learning speed and exploration.</li>
</ol>
<h2 id="Soft-policy-iteration"><a href="#Soft-policy-iteration" class="headerlink" title="Soft policy iteration"></a>Soft policy iteration</h2><h3 id="Policy-evaluation"><a href="#Policy-evaluation" class="headerlink" title="Policy evaluation"></a>Policy evaluation</h3><p>The soft-Q value of the policy $\pi$:</p>
<script type="math/tex; mode=display">\tau^\pi Q(s_t,a_t) \triangleq r(s_t,a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V(s_{t+1}) ]</script><p>where the soft state value function:</p>
<script type="math/tex; mode=display">V(s_t) = \mathbb{E}_{a_t \sim \pi} [Q(s_t, a_t) - \log \pi(a_t \vert s_t)]</script><h3 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h3><p>For each state, we update the policy according to:</p>
<script type="math/tex; mode=display">\pi_\text{new} = \arg \min_{\pi'} D_\mathbb{KL}(\pi'(\cdot \vert s_t) || \frac{\exp(Q^\text{old}(s_t, \cdot))}{Z^{\pi_\text{old}(s_t)}})</script><p>where the partitioning function <script type="math/tex">Z^{\pi_\text{old}}(s_t)</script> normalizes the distribution.</p>
<h2 id="SAC"><a href="#SAC" class="headerlink" title="SAC"></a>SAC</h2><p>Consider a parameterized state value function <script type="math/tex">V_\Psi (s_t)</script>, soft Q-function <script type="math/tex">Q_\theta (s_t,a_t)</script> and a tractable policy <script type="math/tex">\pi_\phi (a_t \vert s_t)</script>.</p>
<p>The soft value function is trained to minimize the squared residual error:</p>
<script type="math/tex; mode=display">J_{V(\Psi)} = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ \frac{1}{2} \big(V_\Psi (s_t)-\mathbb{E}_{a_t \sim \pi_\phi} [Q_\theta (s_t, a_t)+ \log \pi_\phi (a_t \vert s_t)] \big) \right]</script><p>The soft Q-function can be trained to minimize the soft Bellman residual:</p>
<script type="math/tex; mode=display">J_Q (\theta) = \mathbb{E}_{(s_t,a_t) \sim \mathcal{D}} \left[ \frac{1}{2}\big( Q_\theta(s_t,a_t)-\hat{Q}(s_t,a_t) \big)^2 \right]</script><p>with</p>
<script type="math/tex; mode=display">\hat{Q}(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1}\sim p} [V_\bar{\psi}(s_{t+1})]</script><p>The policy can be optimized by the expected KL-divergence:</p>
<script type="math/tex; mode=display">J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ D_\text{KL} \big( \pi_\phi(\cdot \vert s_t) || \frac{\exp(Q_\theta(s_t,\cdot))}{Z_\theta(s_t)} \big) \right]</script><p>Minimizing <script type="math/tex">J_\pi</script> with <strong>reparameterization trick</strong>. </p>
<ul>
<li>Reparameterize the policy with an NN transformation:<script type="math/tex; mode=display">a_t = f_\phi(\epsilon_t; s_t)</script>where $\epsilon$ is an input noise vector, sampled from some fixed distribution.</li>
<li><p>Rewrite the previous objective as:</p>
<script type="math/tex; mode=display">J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}} [\log \pi_\phi (f_\phi(\epsilon_t;s_t) \vert s_t) - Q_\theta(s_t, f_\phi(\epsilon_t;s_t))]</script></li>
<li><p>Employ two Q-functions to mitigate positive bias in the policy improvement step, using the minimum of the Q-functions for the value gradient and policy gradient.</p>
</li>
</ul>
<h2 id="Automatically-adjusted-temperature"><a href="#Automatically-adjusted-temperature" class="headerlink" title="Automatically adjusted temperature"></a>Automatically adjusted temperature</h2><p>SAC is brittle w.r.t the temperature parameter. Choosing the optimal temperature $\alpha$ is non-trival, and the temperature needs to be tuned for each task. <sup id="fnref:17"><a href="#fn:17" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... & Levine, S. (2018). [Soft actor-critic algorithms and applications](https://arxiv.org/pdf/1812.05905). arXiv preprint arXiv:1812.05905.">[17]</span></a></sup></p>
<p>SAC finds a stochastic policy with maximal expected return that satisfies a minimum expected entropy constraint.</p>
<script type="math/tex; mode=display">\max_{\pi_{o:T}} \mathbb{E}_{\rho_\pi} \left[ \sum_{t=0}^T r(s_t,a_t) \right] \quad s.t. \quad  \mathbb{E}_{(s_t,a_t)\sim \rho_\pi} [- \log(\pi_t(a_t \vert s_t))] gleq \mathcal{H} \forall t</script><p>where $\mathcal{H}$ is a desired minimum expected entropy.</p>
<ul>
<li>Rewite the objective as an iterated maximization<script type="math/tex; mode=display">\max_{\pi_0} \big( \mathbb{E} [r(s_0,a_0)] + \max_{\pi_1}\big( \mathbb{E}[\cdots] + \max_{\pi_T} \mathbb{E}[r(s_T,a_T)] \big) \big)</script></li>
<li>Finally we get:<script type="math/tex; mode=display">\alpha_t^* = \arg\min_{\alpha_t} \mathbb{E}_{a_t \sim \pi_t^*} [-\alpha_t \log \pi_t^* (a_t \vert s_t; \alpha_t) -\alpha_t \bar{\mathcal{H}}]</script></li>
</ul>
<h2 id="Algorithms-1"><a href="#Algorithms-1" class="headerlink" title="Algorithms"></a>Algorithms</h2><p>Update $\alpha$ with following objective:</p>
<script type="math/tex; mode=display">J(\alpha) = \mathbb{E}_{a_t \sim \pi_t} [-\alpha \log \pi_t(a_t \vert s_t) -\alpha \bar{\mathcal{H}}]</script><p><img data-src="/notes/images/rl-SAC-alg.png" alt="upload successful"></p>
<h1 id="Deterministic-Policy-Gradient-DPG"><a href="#Deterministic-Policy-Gradient-DPG" class="headerlink" title="Deterministic Policy Gradient (DPG)"></a>Deterministic Policy Gradient (DPG)</h1><h1 id="Deep-DPG-DDPG"><a href="#Deep-DPG-DDPG" class="headerlink" title="Deep DPG (DDPG)"></a>Deep DPG (DDPG)</h1><h1 id="Distributed-Distributional-DDPG-D4PG"><a href="#Distributed-Distributional-DDPG-D4PG" class="headerlink" title="Distributed Distributional DDPG(D4PG)"></a>Distributed Distributional DDPG(D4PG)</h1><h1 id="Multi-Agent-DDPG-MADDPG"><a href="#Multi-Agent-DDPG-MADDPG" class="headerlink" title="Multi-Agent DDPG(MADDPG)"></a>Multi-Agent DDPG(MADDPG)</h1><h1 id="Twin-Delayed-Deep-Deterministic-PG-TD3"><a href="#Twin-Delayed-Deep-Deterministic-PG-TD3" class="headerlink" title="Twin Delayed Deep Deterministic PG(TD3)"></a>Twin Delayed Deep Deterministic PG(TD3)</h1><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Mnih, V., Badia, A.P., Mirza, M.P., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., &amp; Kavukcuoglu, K. (2016). <a href="https://pdfs.semanticscholar.org/6204/12c443e85a1fc2f9ab950ddfada8d18d63b4.pdf">Asynchronous Methods for Deep Reinforcement Learning</a>. ICML.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a3c">Policy gradient algorithms #A3C</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">- Actor: updates the policy parameters $\theta$ for $\pi_\theta(a \vert s)$ in the direction suggested by the critic.
- Critic: updates the value function parameter $w$ and could be action-value $Q_w(a \vert s)$ or state-value $V_w(s)$<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sutton, R.S., &amp; Barto, A.G. (1988). Reinforcement Learning: An Introduction. IEEE Transactions on Neural Networks, 16, 285-286.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://openai.com/blog/baselines-acktr-a2c/">A2C</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Schulman, J., Levine, S., Abbeel, P., Jordan, M.I., &amp; Moritz, P. (2015). <a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf">Trust Region Policy Optimization</a>. ICML.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://bluefisher.github.io/2018/06/30/Trust-Region-Policy-Optimization/">TRPO blog 1</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/26308073">TRPO blog 2</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). <a href="https://arxiv.org/pdf/1707.06347">Proximal Policy Optimization Algorithms</a>. ArXiv, abs/1707.06347.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Heess, N., Dhruva, T., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y., Erez, T., Wang, Z., Eslami, S.M., Riedmiller, M.A., &amp; Silver, D. (2017). <a href="https://arxiv.org/pdf/1707.02286.pdf">Emergence of Locomotion Behaviours in Rich Environments</a>. ArXiv, abs/1707.02286.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Schulman, J., Moritz, P., Levine, S., Jordan, M.I., &amp; Abbeel, P. (2016). <a href="https://arxiv.org/pdf/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>. CoRR, abs/1506.02438.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wu, Y., Mansimov, E., Grosse, R. B., Liao, S., &amp; Ba, J. (2017). <a href="http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation.pdf">Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation</a>. In Advances in neural information processing systems (pp. 5279-5288).<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., &amp; de Freitas, N. (2016). <a href="https://arxiv.org/pdf/1611.01224">Sample efficient actor-critic with experience replay</a>. arXiv preprint arXiv:1611.01224.<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). <a href="https://arxiv.org/pdf/1801.01290">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</a>. arXiv preprint arXiv:1801.01290.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Haarnoja, T., Tang, H., Abbeel, P., &amp; Levine, S. (2017, August). <a href="https://arxiv.org/pdf/1702.08165">Reinforcement learning with deep energy-based policies</a>. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1352-1361). JMLR. org.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">Soft Q-learning, UC Berkeley blog</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/">Notes on the Generalized Advantage Estimation Paper</a><a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... &amp; Levine, S. (2018). <a href="https://arxiv.org/pdf/1812.05905">Soft actor-critic algorithms and applications</a>. arXiv preprint arXiv:1812.05905.<a href="#fnref:17" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>RL</category>
        <category>DRL</category>
        <category>Policy Gradient</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Reinforcement Learning</title>
    <url>/notes/2019/02/23/RL/David%20Silver/RL-notes-1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Notes of lectures by D. Silver. A brief introduction of RL.<br><span id="more"></span></p>
<h1 id="Introduction-to-Reinforcement-Learning"><a href="#Introduction-to-Reinforcement-Learning" class="headerlink" title="Introduction to Reinforcement Learning"></a>Introduction to Reinforcement Learning</h1><h2 id="Characteristics-v-s-ML"><a href="#Characteristics-v-s-ML" class="headerlink" title="Characteristics v.s. ML"></a>Characteristics v.s. ML</h2><ul>
<li>No supervisor, only a <strong>reward signal</strong></li>
<li>Feedback is delayed, not instantaneous</li>
<li>Time really matters (sequential, non i.i.d data)</li>
<li>Agent’s action affect the subsequent data it receives</li>
</ul>
<h2 id="RL-problem"><a href="#RL-problem" class="headerlink" title="RL problem"></a>RL problem</h2><h3 id="Rewards"><a href="#Rewards" class="headerlink" title="Rewards"></a>Rewards</h3><ul>
<li>A reward <script type="math/tex">R_t</script> is a <strong>scalar</strong> feedback signal</li>
<li>Indicates how well agent is doing at step $t$</li>
<li>The agent’s job is to <strong>maximize cumulative reward</strong></li>
</ul>
<p>RL is based on <em>reward hypothesis</em>, i.e. All goals can be described by the maximization of expected cumulative reward.</p>
<h3 id="Sequential-Decision-making"><a href="#Sequential-Decision-making" class="headerlink" title="Sequential Decision making"></a>Sequential Decision making</h3><ul>
<li>Goal: select actions to <strong>maximize total future reward</strong></li>
<li>Actions may have long term consequences</li>
<li>Reward may be delayed</li>
<li>It may be better to sacrifice immediate reward to gain more long-term reward</li>
</ul>
<h3 id="Environments"><a href="#Environments" class="headerlink" title="Environments"></a>Environments</h3><ul>
<li>At each time step $t$ the agent:<ul>
<li>executed action <script type="math/tex">A_t</script></li>
<li>Receives observations <script type="math/tex">O_t</script></li>
<li>Receives scalar reward <script type="math/tex">R_t</script></li>
</ul>
</li>
<li>The environment:<ul>
<li>receives action <script type="math/tex">A_t</script></li>
<li>emits observation <script type="math/tex">O_{t+1}</script></li>
<li>emits scalar reward <script type="math/tex">R_{t+1}</script></li>
</ul>
</li>
<li>$t$ increments at env. step</li>
</ul>
<p><img data-src="/notes/images/rl-env.png" alt="upload successful"></p>
<h3 id="State"><a href="#State" class="headerlink" title="State"></a>State</h3><p>The history is the sequence of observations, actions, rewards</p>
<script type="math/tex; mode=display">H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t</script><ul>
<li>i.e. all observable variables up to time $t$</li>
<li>i.e. the sensorimotor stream of a robot or embodied agent</li>
<li>What happens depends on the history:<ul>
<li>The agent selects actions</li>
<li>The environment selects observations / rewards</li>
</ul>
</li>
<li><strong>State</strong> is the information used to determine what happens next. Formally, state is a function of the history:<script type="math/tex; mode=display">S_t = f(H_t)</script></li>
</ul>
<h4 id="Environment-state"><a href="#Environment-state" class="headerlink" title="Environment state"></a>Environment state</h4><p>The environment state <script type="math/tex">S_t^e</script> is the environment’s private representation, i.e. whatever data the environment uses to pick the next observation / reward.</p>
<ul>
<li>The environment state is <strong>not usually visible</strong> to the agent. Even if <script type="math/tex">S_t^e</script> is visible, it may contain irrelevant information.</li>
</ul>
<h4 id="Agent-state"><a href="#Agent-state" class="headerlink" title="Agent state"></a>Agent state</h4><p>The agent state <script type="math/tex">S_t^a</script> is the agent’s internal representation.</p>
<ul>
<li>i.e. whatever information the agent uses to pick the next action.</li>
<li>i.e. it is the information used by RL algorithms.</li>
<li>It can be any function of history<script type="math/tex; mode=display">S_t^a = f(H_t)</script></li>
</ul>
<h4 id="Information-state"><a href="#Information-state" class="headerlink" title="Information state"></a>Information state</h4><p>An information state (a.k.a.Markov state) contains all useful information from the history.</p>
<p><em>Definition</em>: A state <script type="math/tex">S_t</script> is Markov if and only if</p>
<script type="math/tex; mode=display">\mathbb{P}(S_{t+1}|S_T) = \mathbb{P}(S_{t+1 |S_1, ...,S_t})</script><ul>
<li>“The future is independent of the past given the present”<script type="math/tex; mode=display">H_{1:t} \Rightarrow S_t \Rightarrow H_{t+1: \infty}</script></li>
<li>Once the state is known, the history may be thrown away, i.e. the state is a sufficient statistic of the future</li>
<li>The environment state <script type="math/tex">S_t^e</script> is Markov</li>
<li>The history <script type="math/tex">H_t</script> is Markov</li>
</ul>
<h4 id="Fully-observable-environment"><a href="#Fully-observable-environment" class="headerlink" title="Fully observable environment"></a>Fully observable environment</h4><p><strong>Fully observability</strong>: agent directly observes environment state;</p>
<script type="math/tex; mode=display">O_t = S_t^a = S_t^e</script><ul>
<li>Agent state = environment state = information state</li>
<li>Formally, this is a <strong>Markov decision process</strong> (MDP)</li>
</ul>
<h4 id="Partially-observable-environments"><a href="#Partially-observable-environments" class="headerlink" title="Partially observable environments"></a>Partially observable environments</h4><p><strong>Partially observability</strong>: agent indirectly observes environment. e.g.:</p>
<ul>
<li>A robot with camera vision is not told its absolute location.</li>
<li>A trading agent only observes current prices.</li>
</ul>
<p>Not agent state $\neq$ environment state.</p>
<ul>
<li>Formally, this is a <strong>partially observable Markov decision process</strong> (POMDP).</li>
<li>Agent must construct its own state representation <script type="math/tex">S_t^a</script>, e.g.<ul>
<li>Complete history <script type="math/tex">S_t^a = H_t</script></li>
<li><strong>Beliefs</strong> of environment state: <script type="math/tex">S_t^a = (\mathbb{P}(S_t^e = s^1),...,\mathbb{P}(S_t^e = s^n))</script></li>
<li>Recurrent neural net: <script type="math/tex">S_t^a = \sigma(S^a_{t-1} W_s + O_t W_t)</script></li>
</ul>
</li>
</ul>
<h2 id="RL-agent"><a href="#RL-agent" class="headerlink" title="RL agent"></a>RL agent</h2><h3 id="Major-component"><a href="#Major-component" class="headerlink" title="Major component"></a>Major component</h3><ul>
<li>Policy: agent’s behavior function</li>
<li>Value function: how good is each state and/or action</li>
<li>Model: agent’s representation of the environment</li>
</ul>
<h4 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h4><ul>
<li>A policy is the agent’s behavior</li>
<li>it is a map from state to action, e.g.<ul>
<li>Deterministic policy: <script type="math/tex">a=\pi(s)</script></li>
<li>Stochastic policy: <script type="math/tex">\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]</script></li>
</ul>
</li>
</ul>
<h4 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h4><ul>
<li>Value function is a <strong>prediction of future reward</strong></li>
<li>Used to evaluate the goodness / badness of states</li>
<li>Therefore to select between actions, e.g.<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{pi} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S-t = s]</script></li>
</ul>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>A model predicts what the environment will do next</p>
<ul>
<li>$\mathcal{P}$ predicts the next state</li>
<li>$\mathcal{R}$ predicts the next (immediate) reward, e.g.<script type="math/tex; mode=display">\mathcal{P}_{ss'}^{a} = \mathbb{P}[S_{t+1}=s' | S_t = s, A_t =a]</script><script type="math/tex; mode=display">\mathcal{R}_s^a = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]</script></li>
</ul>
<h3 id="RL-agent-category-1"><a href="#RL-agent-category-1" class="headerlink" title="RL agent category 1"></a>RL agent category 1</h3><ul>
<li>Value based<ul>
<li><strong>No policy (implicit)</strong></li>
<li>Value function</li>
</ul>
</li>
<li>Policy based<ul>
<li>Policy</li>
<li><strong>No value function</strong></li>
</ul>
</li>
<li>Actor Critic<ul>
<li>Policy</li>
<li>Value function</li>
</ul>
</li>
</ul>
<h3 id="RL-agent-category-2"><a href="#RL-agent-category-2" class="headerlink" title="RL agent category 2"></a>RL agent category 2</h3><ul>
<li>Model Free<ul>
<li>Policy and/or Value Function</li>
<li><strong>No Model</strong></li>
</ul>
</li>
<li>Model based<ul>
<li>Policy and/or Value Function</li>
<li>Model</li>
</ul>
</li>
</ul>
<p><img data-src="/notes/images/rl-category.png" alt="upload successful"></p>
<h2 id="Problems-within-RL"><a href="#Problems-within-RL" class="headerlink" title="Problems within RL"></a>Problems within RL</h2><h3 id="Leaning-and-Planning"><a href="#Leaning-and-Planning" class="headerlink" title="Leaning and Planning"></a>Leaning and Planning</h3><p>Two fundamental problems in sequential decision making</p>
<ul>
<li>Reinforcement learning<ul>
<li>The environment is initially unknown</li>
<li>The agent interacts with the environment</li>
<li>The agent improves its policy</li>
</ul>
</li>
<li>Planning<ul>
<li>A model of the environment is known</li>
<li>The agent performs computations with its model (without any external interaction)</li>
<li>The agents improves its policy</li>
<li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li>
</ul>
</li>
</ul>
<h3 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h3><ul>
<li>RL is like <strong>trial-and-error learning</strong></li>
<li>The agent should discover a good policy</li>
<li>From its experiences of the environment</li>
<li><p>Without losing too much reward along the way</p>
</li>
<li><p><strong>Exploration</strong> finds more information about the environment; <strong>exploitation</strong> exploits known information to maximize reward</p>
</li>
<li>It is usually important to explore as well as exploit.</li>
</ul>
]]></content>
      <categories>
        <category>RL</category>
        <category>Lecture</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Markov Decision Process</title>
    <url>/notes/2019/02/23/RL/David%20Silver/RL-notes-2/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Notes of lectures by D. Silver.A brief introduction of MDPs.<br><span id="more"></span></p>
<h1 id="Markov-Decision-Processes"><a href="#Markov-Decision-Processes" class="headerlink" title="Markov Decision Processes"></a>Markov Decision Processes</h1><h2 id="Intro-to-MDPs"><a href="#Intro-to-MDPs" class="headerlink" title="Intro to MDPs"></a>Intro to MDPs</h2><p>MDP formally describe an environment for RL, where the environment is <strong>fully observable</strong>. i.e. the current state completely characterizes the process.</p>
<p>Almost all RL problems can be formalized as MDPs, e.g.</p>
<ul>
<li>Optimal control primarily deals with continuous MDPs</li>
<li>Partially observable problems can be converted into MDPs</li>
<li>Bandits are MDPs with one state</li>
</ul>
<h2 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h2><h3 id="Markov-property"><a href="#Markov-property" class="headerlink" title="Markov property"></a>Markov property</h3><p>“The future is independent of the past given the present”</p>
<p><strong>Definition</strong>: a state <script type="math/tex">S_t</script> is Markov iff <script type="math/tex">\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1,...,S_t]</script></p>
<ul>
<li>The state captures all relevant information from the history.</li>
<li>Once the state is known, the history maybe thrown away, i.e. the state is sufficient statistic of the future.</li>
</ul>
<h4 id="State-Transition-Matrix"><a href="#State-Transition-Matrix" class="headerlink" title="State Transition Matrix"></a>State Transition Matrix</h4><p>For a Markov state $s$ and successor state $s’$, the state transition probability is :</p>
<script type="math/tex; mode=display">\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s'|S_t=s]</script><p>Sate transition matrix $\mathcal{P}$ defines transition probabilities from all sates $s$ to all successor states $s’$</p>
<script type="math/tex; mode=display">\mathcal{P} = \text{from } \overset{\text{to}}{\begin{bmatrix}
    \mathcal{P}_{11}       & \dots & \mathcal{P}_{1n} \\
    \mathcal{P}_{21}       \\
    \vdots \\
    \mathcal{P}_{n1}       & \dots & \mathcal{P}_{nn}
\end{bmatrix}}</script><p>where each row of the matrix sums to 1.</p>
<h3 id="Markov-Process-1"><a href="#Markov-Process-1" class="headerlink" title="Markov Process"></a>Markov Process</h3><p>A Markov process is a memoryless random process, i.e. a sequence of random state $S_1$, $S_2$, … with the Markov property.</p>
<p>Definition: a Markov Process (or Markov Chain)  is a tuple $&lt;\mathcal{S}, \mathcal{P}&gt;$, where $\mathcal{S}$ is a (finite) set of states, $\mathcal{P}$ is a state transition probability matrix, <script type="math/tex">\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s'|S_t=s]</script></p>
<h2 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h2><p>A Markov reward process is a Markov chain with values.</p>
<p><strong>Definition</strong>: A MRP is a tuple $\mathcal{S}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}$</p>
<ul>
<li>$\mathcal{S}$ is a finite set of states</li>
<li>$\mathcal{P}$ is a state transition probability matrix, <script type="math/tex">\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s' | S_t = s]</script></li>
<li>$\mathcal{R}$ is a <strong>reward function</strong>, <script type="math/tex">\mathcal{R} = \mathbb{E}[R_{t=1 |S_t=s}]</script></li>
<li>$\mathcal{\gamma}$ is a <strong>discount factor</strong>, <script type="math/tex">\mathcal{\gamma} \in [0,1]</script></li>
</ul>
<h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><p>The return <script type="math/tex">G_t</script> is the total discounted reward from time-step $t$.</p>
<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}</script><ul>
<li>The discount $\gamma \in [0,1]$ is the present value of future rewards.</li>
<li>The value of receiving reward $R$ after $k+1$ time stems is $\gamma^k R$.</li>
<li>This values immediate reward above delayed reward<ul>
<li>$\gamma$ close to 0 leads to “myopic” evaluation</li>
<li>$\gamma$ close to 1 leads to “far-sighted” evaluation</li>
</ul>
</li>
</ul>
<h4 id="Why-discount"><a href="#Why-discount" class="headerlink" title="Why discount ?"></a><strong>Why discount ?</strong></h4><p>Most MRP, MDP are discounted. Why?</p>
<ul>
<li>Mathematically convenient to discount rewards.</li>
<li>Avoids infinite returns in cyclic Markov processes.</li>
<li>Uncertainty about the future may not be fully represented.</li>
<li>If the reward is financial, immediate rewards may earn more interest than delayed rewards</li>
<li>Animal/human behavior shows preference for immediate reward</li>
<li>It is sometimes possible to use undiscounted MRP (i.e. $\gamma = 1$), e.g. if all sequences terminate.</li>
</ul>
<h3 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h3><p>The value function $v(s)$ gives the long-term value of state $s$<br><strong>Definition</strong>: the state value function $v(s)$ of an MRP is the expected return starting from state $s$:</p>
<script type="math/tex; mode=display">v(s) = \mathbb{E}[G_t | S_t = s]</script><h3 id="Bellman-Equation-for-MRPs"><a href="#Bellman-Equation-for-MRPs" class="headerlink" title="Bellman Equation for MRPs"></a>Bellman Equation for MRPs</h3><p>The value function can be decomposed into two parts:</p>
<ul>
<li>immediate reward <script type="math/tex">R_{t+1}</script></li>
<li>discounted value of successor state <script type="math/tex">\gamma v(S_{t+1})</script><script type="math/tex; mode=display">v(s) = \mathbb{E}[G_t|S_t = s] 
\\ = \mathbb{E}[R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3}+ \dots | S_t =s] \\ = \mathbb{E}[R_{t+1}+\gamma (R_{t+2} + \gamma R_{t+3}+ \dots) | S_t =s] \\ = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\ = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]</script></li>
</ul>
<p><img data-src="/notes/images/bellman-eq.png" alt="upload successful"></p>
<script type="math/tex; mode=display">v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]</script><script type="math/tex; mode=display">\Rightarrow v(s) = \mathcal{R}_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'} v(s')</script><p>Bellman equation in <strong>matrix form</strong></p>
<script type="math/tex; mode=display">v = \mathcal{R} + \gamma \mathcal{P}v</script><p>where $v$ is a column vector with one entry per state</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
= \begin{bmatrix}
\mathcal{R}(1) \\
\vdots \\
\mathcal{R}(n)
\end{bmatrix} 
+ \gamma
\begin{bmatrix}
    \mathcal{P}_{11} & \dots  & \mathcal{P}_{1n} \\
    \vdots \\
    \mathcal{P}_{n1} & \dots  & \mathcal{P}_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}</script><p>Solving the Bellman equation</p>
<ul>
<li><p>The Bellman equation is a <strong>linear equation</strong></p>
<script type="math/tex; mode=display">v = \mathcal{R} + \gamma \mathcal{P}v</script><script type="math/tex; mode=display">(1- \gamma \mathcal{P})v = \mathcal{R}</script><script type="math/tex; mode=display">\mathcal{v} = (1- \gamma \mathcal{P})^{-1} \mathcal{R}</script></li>
<li><p>Computational complexity $\rightarrow O(n^3)$ for $n$ states</p>
</li>
<li>Direct solution is only possible for small MRPs</li>
<li>For large MRPs, iterative methods:<ul>
<li>Dynamic programming</li>
<li>Monte-Carlo evaluation</li>
<li>Temporal-Difference learning</li>
</ul>
</li>
</ul>
<h2 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h2><p>Markov Decision Process(MDP): a Markov reward process <strong>with decisions</strong>.</p>
<p>An MDP is a tuple $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}&gt;$</p>
<ul>
<li>$\mathcal{S}$ is a finite set of states</li>
<li>$\mathcal{A}$ is a finite set of actions</li>
<li>$\mathcal{P}$ is a state transition probability matrix<script type="math/tex; mode=display">\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s'|S_t = s, A_t =a]</script></li>
<li>$\mathcal{R}$ is a reward function, <script type="math/tex; mode=display">\mathcal{R}_s^a = \mathbb{E} [R_{t+1} | S_t=s, A_t=a]</script></li>
<li>$\gamma$ is a discount factor $\gamma \in [0,1]$</li>
</ul>
<h3 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h3><p>A policy $\pi$ is a distribution over actions given states</p>
<script type="math/tex; mode=display">\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]</script><ul>
<li>A policy fully defines the behavior of an agent.</li>
<li>MDP policies depends on the current state (not the history), i.e. Policies are stationary (time-independent)<script type="math/tex; mode=display">A_t \sim \pi(\cdot \vert S_t), \forall t > 0</script></li>
</ul>
<p>Given an MDP $\mathcal{M} = &lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}&gt;$ and a policy $\pi$</p>
<ul>
<li>The state sequence $S_1, S_2,…$ is a Markov process $&lt;\mathcal{S}, \mathcal{P}^{\pi}&gt;$</li>
<li>The state and reward sequence <script type="math/tex">S_1,R_2,S_2,\cdots</script> is a markov reward process $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$<br>where <script type="math/tex; mode=display">\mathcal{P}^{\pi}_{s,s'} = \sum_{a \in \mathcal{A}} \pi (a|s) \mathcal{P}_{ss'}^a</script><script type="math/tex; mode=display">\mathcal{R}^{\pi}_s = \sum_{a \in \mathcal{A}} \pi (a|s) \mathcal{R}_s^a</script></li>
</ul>
<h3 id="Value-Function-1"><a href="#Value-Function-1" class="headerlink" title="Value Function"></a>Value Function</h3><p>The <strong>state-value function</strong> <script type="math/tex">v_{\pi}(s)</script> of an MDP is the expected return starting from state $s$, and then following policy $\pi$</p>
<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]</script><p>The <strong>action-value function</strong> <script type="math/tex">q_{\pi}(s,a)</script> is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$</p>
<script type="math/tex; mode=display">q_{\pi}(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a]</script><h3 id="Bellman-Expectation-Equation"><a href="#Bellman-Expectation-Equation" class="headerlink" title="Bellman Expectation Equation"></a>Bellman Expectation Equation</h3><p>The <strong>state-value function</strong> can again be decomposed into immediate reward plus discounted value of successor state:</p>
<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \vert S_t = s]</script><p>The <strong>action-value function</strong> can similarly be decomposed:</p>
<script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \vert S_t=s, A_t=a ]</script><h4 id="Bellman-Expectation-equation-for-V-pi"><a href="#Bellman-Expectation-equation-for-V-pi" class="headerlink" title="Bellman Expectation equation for $V^{\pi}$"></a>Bellman Expectation equation for $V^{\pi}$</h4><p>(Eq. 1)</p>
<script type="math/tex; mode=display">v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a|s) q_{\pi}(s,a)</script><p><img data-src="/notes/images/bellman-v.png" alt="upload successful"></p>
<p>(Eq. 2)</p>
<script type="math/tex; mode=display">v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a \vert s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_{\pi}(s') )</script><p><img data-src="/notes/images/bellman-v2.png" alt="upload successful"></p>
<h4 id="Bellman-Expectation-equation-for-Q-pi"><a href="#Bellman-Expectation-equation-for-Q-pi" class="headerlink" title="Bellman Expectation equation for $Q^{\pi}$"></a>Bellman Expectation equation for $Q^{\pi}$</h4><p>(Eq. 1)</p>
<script type="math/tex; mode=display">q_{\pi}(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{\pi}(s')</script><p><img data-src="/notes/images/bellman-q.png" alt="upload successful"></p>
<p>(Eq. 2)</p>
<script type="math/tex; mode=display">q_{\pi}(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} \sum_{a' \in \mathcal{A}} \pi (a' \vert s') q_{\pi}(s', a')</script><p><img data-src="/notes/images/bellman-q2.png" alt="upload successful"></p>
<h4 id="Matrix-form"><a href="#Matrix-form" class="headerlink" title="Matrix form"></a>Matrix form</h4><p>The Bellman expectation equation can be expressed concisely using the induced MRP:</p>
<script type="math/tex; mode=display">v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} v_{\pi}</script><p>with direct solution</p>
<script type="math/tex; mode=display">v_{\pi} = (1- \gamma \mathcal{P}^{\pi} )^{-1} \mathcal{R}^{\pi}</script><h3 id="Optimal-Value-Function"><a href="#Optimal-Value-Function" class="headerlink" title="Optimal Value Function"></a>Optimal Value Function</h3><p>The <strong>optimal state-value function</strong> <script type="math/tex">v_{*}(s)</script> is the maximum value function over all policies:</p>
<script type="math/tex; mode=display">v_{*}(s) = \max_{\pi} v_{\pi}(s)</script><p>The <strong>optimal action-value function</strong> <script type="math/tex">_{*}(s,a)</script> is the maximum action-value function over all policies</p>
<script type="math/tex; mode=display">q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a)</script><p>An MDP is solved when we know the optimal value fn.</p>
<h4 id="Optimal-Policy"><a href="#Optimal-Policy" class="headerlink" title="Optimal Policy"></a>Optimal Policy</h4><p>Define a partial ordering over policies</p>
<script type="math/tex; mode=display">\pi \leq \pi' \text{ if } v_{\pi}(s) \leq v_{\pi'}(s), \forall s</script><p>Finding an optimal policy, by maximizing over <script type="math/tex">q_{*}(s,a)</script></p>
<script type="math/tex; mode=display">
\pi_{*}(a \vert s)=\left\{
                \begin{array}{ll}
                  1 \quad \text{if} \quad a=\underset{a \in \mathcal{A}}{\mathrm{argmax}} q_*(s,a) \\
                 0 \quad \text{otherwise}
                \end{array}
              \right.</script><h4 id="Solving-the-Bellman-Optimality-Equation"><a href="#Solving-the-Bellman-Optimality-Equation" class="headerlink" title="Solving the Bellman Optimality Equation"></a>Solving the Bellman Optimality Equation</h4><ul>
<li>Bellman Optimal Equation is non-linear</li>
<li>No closed form solution (in general)</li>
<li>Many iterative solution methods<ul>
<li>Value iteration</li>
<li>Policy iteration</li>
<li>Q-learning</li>
<li>Sarsa</li>
</ul>
</li>
</ul>
<h2 id="Extensions-to-MDPs"><a href="#Extensions-to-MDPs" class="headerlink" title="Extensions to MDPs"></a>Extensions to MDPs</h2><h3 id="Infinite-MDPs"><a href="#Infinite-MDPs" class="headerlink" title="Infinite MDPs"></a>Infinite MDPs</h3><ul>
<li>Countably infinite state and/or action spaces: straightforward</li>
<li>Continuous state and/or action spaces<ul>
<li>Closed form for linear quadratic model (LQR)</li>
</ul>
</li>
<li>Continuous time<ul>
<li>Requires partial differential equations</li>
<li>Hamilton-Jacobi-Bellman (HJB) equation</li>
<li>LImiting case of bellman equation as time-step $\rightarrow 0$</li>
</ul>
</li>
</ul>
<h3 id="POMDPs"><a href="#POMDPs" class="headerlink" title="POMDPs"></a>POMDPs</h3><p>A POMPD is an MDP <strong>with hidden states</strong>. It is a hidden Markov model with actions.</p>
<p>A POMDP is a tuple $&lt;\mathcal{S}, \mathcal{A},\mathcal{O},\mathcal{P},\mathcal{R},\mathcal{Z},\mathcal{\gamma}&gt;$</p>
<ul>
<li>$\mathcal{S}$ is a finite set of states</li>
<li>$\mathcal{A}$ is a finite set of actions</li>
<li>$\mathcal{O}$ is a finite set of observations</li>
<li>$\mathcal{P}$ is a state transition probability matrix<script type="math/tex; mode=display">\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s' \vert S_t=s, A_t=a]</script></li>
<li>\mathcal{R} is a reward function, <script type="math/tex; mode=display">\mathcal{R}_s^a = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]</script></li>
<li>$\mathcal{Z}$ is an observation function<script type="math/tex; mode=display">\mathcal{Z}_{s'o}^a = \mathbb{P}[O_{t=1} =o \vert S_{t=1}=s', A_t=a]</script></li>
<li>$\gamma$ is a discount factor $\gamma \in [0,1]$</li>
</ul>
<h3 id="Average-Reward-MDPs"><a href="#Average-Reward-MDPs" class="headerlink" title="Average Reward MDPs"></a>Average Reward MDPs</h3>]]></content>
      <categories>
        <category>RL</category>
        <category>Lecture</category>
        <category>MDP</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Planning by Dynamic Programming (RL)</title>
    <url>/notes/2019/02/23/RL/David%20Silver/RL-notes-3-Planning-by-Dynamic-Programming-RL/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Notes of lectures by D. Silver.<br><span id="more"></span></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><strong>Dynamic</strong>: sequential or temporal components to the problem</li>
<li><p><strong>Programming</strong>: optimizing a “program”, i.e. a policy<br>c.f. linear programming</p>
</li>
<li><p>Breaking down the complex problems into subproblems</p>
<ul>
<li>Solve the problems</li>
<li>Combine solutions to subproblems</li>
</ul>
</li>
</ul>
<h3 id="Requirements-for-dynamic-programming"><a href="#Requirements-for-dynamic-programming" class="headerlink" title="Requirements for dynamic programming"></a>Requirements for dynamic programming</h3><p>Dynamic programming(DP) is general solution for problems with two properties:</p>
<ul>
<li><strong>Optimal substructure</strong><ul>
<li>Principle of optimality applies</li>
<li>Optimal solution can be decomposed into subproblems</li>
</ul>
</li>
<li>Overlapping subproblems<ul>
<li>Subproblems recur many times</li>
<li>Solutions can be cached and reused</li>
</ul>
</li>
</ul>
<p>MDP satisfy both properties:</p>
<ul>
<li>Bellman equation gives recursive decomposition</li>
<li>Value function stores and reuses solutions</li>
</ul>
<h3 id="Planning-by-DP"><a href="#Planning-by-DP" class="headerlink" title="Planning by DP"></a>Planning by DP</h3><ul>
<li>DP assumes <strong>full knowledge</strong> of the MDP</li>
<li>It is used for planning in an MDP</li>
<li>For prediction:<ol>
<li>Input: DMP $&lt;\mathcal{S}, \mathcal{A},\mathcal{P},\mathcal{R},\mathcal{\gamma}&gt;$ and policy $\pi$, or MRP $&lt;\mathcal{S},\mathcal{P}^{\pi},\mathcal{R}^{\pi},\mathcal{\gamma}&gt;$</li>
<li>Output: value function <script type="math/tex">v_{\pi}</script></li>
</ol>
</li>
<li>For control:<ol>
<li>Input: MDP $&lt;\mathcal{S}, \mathcal{A},\mathcal{P},\mathcal{R},\mathcal{\gamma}&gt;$</li>
<li>Output: optimal value function <script type="math/tex">v_{*}</script>, and optimal policy <script type="math/tex">\pi_{*}</script></li>
</ol>
</li>
</ul>
<h3 id="Other-applications-by-DP"><a href="#Other-applications-by-DP" class="headerlink" title="Other applications by DP"></a>Other applications by DP</h3><ul>
<li>Scheduling algorithms</li>
<li>String algorithms (e.g. sequence alignment)</li>
<li>Graph algorithms (e.g. shortest path algorithms)</li>
<li>Graphical models (e.g. Viterbi algorithm)</li>
<li>Bioinformatics (e.g. lattice model)</li>
</ul>
<h2 id="Iterative-policy-evaluation"><a href="#Iterative-policy-evaluation" class="headerlink" title="Iterative policy evaluation"></a>Iterative policy evaluation</h2><ul>
<li>Problem: evaluate a given policy $\pi$</li>
<li>Solution: iterative application of Bellman expectation backup</li>
</ul>
<script type="math/tex; mode=display">v_1 \rightarrow v_2 \rightarrow ... \rightarrow v_{\pi}</script><p>Using synchronous backups:</p>
<ul>
<li>At each iteration $k+1$</li>
<li>For all states $s \in \mathcal{S}$</li>
<li>Update <script type="math/tex">v_{k+1}(s)</script>  from <script type="math/tex">v_{k}(s')</script>, where $s’$ is a successor state of $s$</li>
</ul>
<p><img data-src="/notes/images/iterative-policy-evaluation.png" alt="upload successful"></p>
<script type="math/tex; mode=display">v_{k+1}(s) = sum_{a \in \mathcal{A}} \pi(a \vert s) \Big( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_k(s') \Big)</script><script type="math/tex; mode=display">\pmb{v}^{k+1} = \pmb{\mathcal{R}^{\pi}} + \gamma \pmb{\mathcal{P}^{\pi}v}^k</script><h2 id="Policy-iteration"><a href="#Policy-iteration" class="headerlink" title="Policy iteration"></a>Policy iteration</h2><h3 id="How-to-improve-a-policy"><a href="#How-to-improve-a-policy" class="headerlink" title="How to improve a policy"></a>How to improve a policy</h3><p> Given a policy $\pi$:</p>
<ul>
<li>Evaluate the policy $\pi$<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + ... | S_t = s]</script></li>
<li>Improve the policy by acting greedily w.r.t <script type="math/tex">v_{\pi}</script><script type="math/tex; mode=display">\pi' = \text{greedy}(v_{\pi})</script></li>
</ul>
<p><img data-src="/notes/images/policy-eval.png" alt="upload successful"></p>
<p><img data-src="/notes/images/policy-improvement.png" alt="upload successful"></p>
<ul>
<li><p><strong>Policy evaluation</strong>: estimate <script type="math/tex">v_{\pi}</script> (iterative policy evaluation)</p>
</li>
<li><p><strong>Policy improvement</strong>: generate <script type="math/tex">\pi' \leq \pi</script> (greedy policy improvement)</p>
</li>
</ul>
<h3 id="Policy-improvement"><a href="#Policy-improvement" class="headerlink" title="Policy improvement"></a>Policy improvement</h3><p>Consider a deterministic policy $a=\pi(s)$</p>
<ul>
<li>Improve the policy by acting greedily<script type="math/tex; mode=display">\pi'(s) = \arg\max_{a \in \mathcal{A}} q_{\pi}(s,a)</script></li>
</ul>
<p>This improves the value from any state $s$ over one step</p>
<script type="math/tex; mode=display">q_{\pi}(s, \pi'(s)) = \max_{a \in \mathcal{A}}q_{\pi}(s,a) \leq q_{\pi}(s, \pi(s)) = v_{\pi}(s)</script><p>It therefore improves the value function <script type="math/tex">v_{\pi'}(s) \leq v_{\pi}(s)</script></p>
<script type="math/tex; mode=display">v_{\pi}(s) \leq q_{\pi}(s, \pi'(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = t ]  
\\ \leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi'(S_{t+1}))|S_t=s] \\  \leq  \mathbb{E}_{\pi'}[R_{t+1}+\gamma R_{t=2}+ \gamma^2 q_{\pi} (S_{t+2}, \pi'(S_{t+2}))|S_t = s] 
\\ \leq \mathbb{E}_{\pi'}[R_{t+1}+ \gamma R_{t+2}+... | S_t=s] = v_{\pi'}(s)</script><ul>
<li>If improvements stop,<script type="math/tex; mode=display">q_{\pi} (s, \pi'(s)) = \max_{a \in \mathcal{A}} q_{\pi}(s, a) = q_{\pi}(s, \pi(s)) = v_{\pi}(s)</script></li>
<li><p>Then the Bellman optimality equation has been satisfied</p>
<script type="math/tex; mode=display">v_{\pi}(s) = \max_{a \in \mathcal{A}} q_{\pi}(s,a)</script></li>
<li><p>Therefore <script type="math/tex">v_{\pi}(s) = v_{*}(s)</script> for all $s \in \mathcal{S}$. So $\pi$ is an optimal policy</p>
</li>
</ul>
<h2 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h2><h3 id="Value-iteration-in-MDPs"><a href="#Value-iteration-in-MDPs" class="headerlink" title="Value iteration in MDPs"></a>Value iteration in MDPs</h3><h4 id="Principle-of-Optimality"><a href="#Principle-of-Optimality" class="headerlink" title="Principle of Optimality"></a>Principle of Optimality</h4><p>An optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first action <script type="math/tex">A_{*}</script></li>
<li>Followed by an optimal policy from successor state $S’$</li>
</ul>
<h4 id="Deterministic-value-iteration"><a href="#Deterministic-value-iteration" class="headerlink" title="Deterministic value iteration"></a>Deterministic value iteration</h4><p>If we know the solution to subproblems <script type="math/tex">v_{*}(s')</script>, then solution <script type="math/tex">v_{*}(s)</script> can be found by one-step lookahead:</p>
<script type="math/tex; mode=display">v_{*}(s) \leftarrow \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{*}(s')</script><ul>
<li>The idea of value iteration is to apply these update iteratively</li>
<li>Intuition: start with final rewards and work backwards</li>
<li>Still works loopy, stochastic MDPs</li>
</ul>
<h4 id="Value-iteration-1"><a href="#Value-iteration-1" class="headerlink" title="Value iteration"></a>Value iteration</h4><ul>
<li>Problems: find optimal policy $\pi$</li>
<li><p>Solution: iterative application of Bellman optimality backup</p>
<script type="math/tex; mode=display">v_1 \rightarrow v_2 \rightarrow ... \rightarrow v_{*}</script></li>
<li><p>Using synchronous backups at each iteration $k+1$, for all states <script type="math/tex">s \in \mathcal{S}</script>, update <script type="math/tex">v_{k+1}(s)</script> from <script type="math/tex">v_{k}(s')</script></p>
</li>
<li>Unlike policy iteration , there is <strong>no explicit policy</strong></li>
</ul>
<p><img data-src="/notes/images/value-iteration.png" alt="upload successful"></p>
<script type="math/tex; mode=display">v_{k+1}(s) = \max_{a \in \mathcal{A}} \big( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_{k}(s')  big)</script><script type="math/tex; mode=display">\pmb{v}_{k+1} = \max_{a \in \mathcal{A}} \pmb{R^a} + \gamma \pmb{\mathcal{P}^a v}_k</script><h2 id="Synchronous-DP-algorithms"><a href="#Synchronous-DP-algorithms" class="headerlink" title="Synchronous DP algorithms"></a>Synchronous DP algorithms</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Problem</th>
<th>Bellman equation</th>
<th>Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prediction</td>
<td>Bellman Expectation Equation</td>
<td>Iterative Policy Evaluation</td>
</tr>
<tr>
<td>Control</td>
<td>Bellman Expectation Equation + Greedy Policy Improvement</td>
<td>Policy Iteration</td>
</tr>
<tr>
<td>Control</td>
<td>Bellman Optimality Equation</td>
<td>Value Iteration</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Asynchronous-DP"><a href="#Asynchronous-DP" class="headerlink" title="Asynchronous DP"></a>Asynchronous DP</h2><h3 id="In-place-DP"><a href="#In-place-DP" class="headerlink" title="In-place DP"></a>In-place DP</h3><ul>
<li><p>Synchronous value iteration stores two copies of value function for all $s$ in $\mathcal{S}$:</p>
<script type="math/tex; mode=display">\pmb{v_{new}(s)} \leftarrow \max{a \in \mathcal{A}} \big( \mathcal{R}_s^a + \gamma \sum{s' \in \mathcal{S}} P_{ss'}^a \pmb{v_{old}(s')} \big)</script><script type="math/tex; mode=display">\pmb{v_{old}} \leftarrow \pmb{v_{new}}</script></li>
<li><p><strong>In-place</strong> value iteration only stores one copy of value function for all  $s$ in $\mathcal{S}$:</p>
<script type="math/tex; mode=display">\pmb{v(s)} \leftarrow \max{a \in \mathcal{A}} \big( \mathcal{R}_s^a + \gamma \sum{s' \in \mathcal{S}} P_{ss'}^a \pmb{v(s')} \big)</script></li>
</ul>
<h3 id="Prioritized-sweeping"><a href="#Prioritized-sweeping" class="headerlink" title="Prioritized sweeping"></a>Prioritized sweeping</h3><p>Use magnitude of Bellman error to select state</p>
<script type="math/tex; mode=display">\big| \max_{a \in \mathcal{A}} \big( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v(s') \big) - v(s)  \big|</script><p>Backup the state with the largest remaining Bellman error</p>
<h3 id="Real-time-DP"><a href="#Real-time-DP" class="headerlink" title="Real-time DP"></a>Real-time DP</h3><ul>
<li>Idea: only states that are relevant to agent</li>
<li>Use agent’s experience to guide the selection of states</li>
<li>After each time-step <script type="math/tex">S_t, A_t, R_{t+1}</script></li>
<li>Backup the state <script type="math/tex">S_t</script></li>
</ul>
<script type="math/tex; mode=display">\pmb{v(S_t)} \leftarrow \max_{a \in \mathcal{A}} \big( \mathcal{R}_{\pmb{S_t}}^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{\pmb{S_t}s'}^a \pmb{v(s')}  \big)</script>]]></content>
      <categories>
        <category>RL</category>
        <category>Lecture</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Deciphering AlphaStar on StarCraft II</title>
    <url>/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>On 19 December 2018, AlphaStar has decisively beaten human player MaNa with 5-0 on StarCraft II, one of the most challenging Real-Time-Strategy (RTS) games.</p>
<p><img data-src="/notes/images/alphaStarVIsualization.gif" /></p>
<p><a href="https://cyk1337.github.io/slides/2019-07-23-AlphaStar.pdf">2019-07-23 slides</a><br><a href="https://cyk1337.github.io/slides/2019-11-12-AlphaStarII.pdf">2019-11-12 slides</a><br><span id="more"></span></p>
<h1 id="How-AlphaStar-is-trained"><a href="#How-AlphaStar-is-trained" class="headerlink" title="How AlphaStar is trained"></a>How AlphaStar is trained</h1><ul>
<li>Imitation learning (supervised)<ul>
<li>AlphaStar is initially trained with <strong>imitation learning</strong> with anonymous game replays from human experts</li>
<li>This offers a good initialization for neural networks</li>
<li>This initial agent beats the built-in “Elite” level AI ($\approx$ human golden level)</li>
</ul>
</li>
<li>RL<ul>
<li>Seed a multi-agent reinforcement learning process with a continuous league</li>
</ul>
</li>
</ul>
<h2 id="AlphaStar-League"><a href="#AlphaStar-League" class="headerlink" title="AlphaStar League"></a>AlphaStar League</h2><p>Multi-agent RL in the presence of strategy cycles</p>
<ul>
<li>The league contains $m$ agents and $n$ competitors<ul>
<li>Agent: actively learning agent, updating a policy</li>
<li>Competitor: passive policy, not learning</li>
</ul>
</li>
<li>Each agent plays games against other agents/competitors and learns by RL<ul>
<li>Ensuring robust performance against a wide diversity of counter-strategies</li>
<li>Several mechanisms encourage diversity among the league</li>
</ul>
</li>
<li>Agents periodically replicate into a competitor<ul>
<li>The new competitor is added to the league</li>
<li>Ensure that agents do not forget how to defeat old selves</li>
</ul>
</li>
</ul>
<p><img data-src="/notes/images/alphaStar-league.png" alt="AlphaStar League Training"></p>
<h3 id="Agent-Diversity"><a href="#Agent-Diversity" class="headerlink" title="Agent Diversity"></a>Agent Diversity</h3><p>Each agent’s personalized objective is described by:</p>
<ul>
<li>Matchmaking distribution over opponent to play in each game</li>
<li>Intrinsic reward function specifying preferences (e.g. over unit types)</li>
</ul>
<h3 id="Matchmaking-Strategies"><a href="#Matchmaking-Strategies" class="headerlink" title="Matchmaking Strategies"></a>Matchmaking Strategies</h3><p>Prioritised Fictious Self-Play (pFSP)</p>
<ul>
<li>Matchmaking distribution is proportional to win-rate of opponents</li>
<li>Encourages monotonic progress against set of competitors</li>
</ul>
<p>Exploiters</p>
<ul>
<li>Matchmaking distribution is uniform over individual agents</li>
<li>The sole goal of an exploiter is to identify the weaknesses in agents</li>
<li>The agents can then learn to defend against those weakness</li>
</ul>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p>Agent parameters updated by <strong>off-policy</strong> RL from replayed subsequence</p>
<ul>
<li>Advantage Actor-Critic</li>
<li>V-Trace + TD($\lambda$)</li>
<li>Self-imitation learning</li>
<li>Entropy regularisation</li>
<li>Policy distillation</li>
</ul>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><p><strong>Exploration and diversity</strong></p>
<ul>
<li>Solution 1: use <strong>human data</strong> to aid in exploration and to preserve strategic diversity throughout training. After initialization with SL, AlphaStar continually <strong>minimizes the KL divergence between the supervised and current policy</strong>.</li>
<li>Solution 2: apply <strong>pseudo-rewards</strong> to follow a strategy statistic $z$, from randomly sampled human data. The pseudo-rewards measure the edit distance between sampled and execeuted build orders; and the Hamming distances between sampled and executed cumulative statistics.</li>
<li>It shows that the utilzation of human data is critical in final results.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hamming_distance</span>(<span class="params">s1, s2</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot; Return the Hamming distance between equal-length sequences &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(s1)!=<span class="built_in">len</span>(s2): <span class="keyword">raise</span> ValueError(<span class="string">&quot;Non-equal length given!&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(e1!=e2 <span class="keyword">for</span> e1,e2 <span class="keyword">in</span> <span class="built_in">zip</span>(s1,s2))</span><br></pre></td></tr></table></figure>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>Three reasons for supervised learning (SL)</p>
<ul>
<li>Provide simpler evaluation metric than multi-agent RL<ul>
<li>A good network architecture for SL is likely to also be good for RL</li>
</ul>
</li>
<li><strong>Initialization</strong>. Starting from human behaviour speeds up learning<ul>
<li>Initialize all policy weights <script type="math/tex">\theta = \theta^\text{SL}</script></li>
</ul>
</li>
<li>Maintain <strong>diverse exploration</strong>. Staying close to human behaviour ensures exploration of reasonable strategies<ul>
<li>Add <script type="math/tex">\text{KL}(\theta \vert\vert \theta^\text{SL})</script> cost to RL update</li>
<li>This is the solution to a<strong>void naive exploration in micro-tactics</strong> of ground units, e.g. naively build and use air units. The agents are also penalized whenever their action probabilties differ from the supervised policy.</li>
</ul>
</li>
</ul>
<h2 id="AlphaStar-architecture"><a href="#AlphaStar-architecture" class="headerlink" title="AlphaStar architecture"></a>AlphaStar architecture</h2><p><img data-src="/notes/images/alphaStar_NN.png" alt="upload successful"></p>
<p>Nature AlphaStar detailed model. It uses <strong>scatter connection</strong> to combine spatial and non-spatial features.</p>
<p><img data-src="/notes/images/nature-alphastar-model.png" alt="upload successful"></p>
<!--![upload successful](/notes/images/alphaStar-architecture.png)-->
<hr>
<h3 id="Network-Inputs"><a href="#Network-Inputs" class="headerlink" title="Network Inputs"></a>Network Inputs</h3><ul>
<li><code>prev_state</code>: the previous LSTM state<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[AlphaStar Nature paper supplemental data](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1724-z/MediaObjects/41586_2019_1724_MOESM2_ESM.zip)
">[19]</span></a></sup></li>
<li><code>entity_list</code>: entities within the game</li>
<li><code>map</code>: game map</li>
<li><code>scalar_features</code>: player data, game statistics, <strong>build orders</strong>.</li>
<li><code>opponent_observations</code>: opponent’s observations (only used for baselines, not for inference during play).</li>
<li><code>cumulative_score</code>: various score metrics of the game (only used for baselines, not for inference during play). “It includes <em>score, idle production and work time, total value of units and structure, total destroyed value of units and structures, total collected minerals and vespene, rate of minerals and vespene collection, and total spent minerals and vespene</em>“<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[AlphaStar Nature paper supplemental data](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1724-z/MediaObjects/41586_2019_1724_MOESM2_ESM.zip)
">[19]</span></a></sup>.</li>
</ul>
<h3 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h3><h4 id="Entity-encoder"><a href="#Entity-encoder" class="headerlink" title="Entity encoder"></a>Entity encoder</h4><ul>
<li>Input: <code>entity_list</code></li>
<li>Output:<br>  <code>embedded_entity</code> - a 1D tensor (one embedding for all entities)<br>  <code>entity_embeddings</code> - one embedding for each entity, including lots of fields, involving unit_type, unit_attr, alliance, current_health, was_selected, etc.</li>
</ul>
<p>The preprocessed entities (features) and biased are passed into a <strong>transformer</strong> (2 layer with 2-headed self attention, with embedding size 128). Then pass the aggregated values to a <strong>Conv1D</strong> with kernel size 1 to double the number of channels (to 256). Sum the head results and passed to a 2-layer MLP with hidden size 1024 and output size 256.</p>
<ul>
<li><code>entity_embeddings</code>: pass the transformer output through a ReLU, a Conv1D with kernel size 1 and 256 channels, and another ReLU.  </li>
<li><code>embedded_entity</code>: The mean of tansformer output across the units is fed through an MLP of size 256 and a ReLU.</li>
</ul>
<h4 id="Spatial-encoder"><a href="#Spatial-encoder" class="headerlink" title="Spatial encoder"></a>Spatial encoder</h4><ul>
<li>Input: <code>map</code>, <code>entity_embeddings</code></li>
<li>Output:<br>  <code>embedded_spatial</code> - A 1D tensor of the embedded map<br>  <code>map_skip</code> - output tensors of intermediate computation, used for skip connections.</li>
</ul>
<p><code>map</code>: add two features</p>
<ul>
<li>cameral: whether a location is inside/outside the virtual camera;</li>
<li>scattered entities. Pass <code>entity_embeddings</code> through a size 32 conv1D followed by a ReLU, then scattered into a map layer so that the 32 vector at a specific location corresponds to the units placed there.</li>
</ul>
<p>Concatenated all planes including camera, scattered_entities, vasibility, entity_owners, buildable, etc. Project to 32 channels by 2D conv with kernel size 1, followed by a ReLU. Then downsampled from 128x128 to 16x16 through 3 conv2D and ReLUs with different channel sizes (i.e., 64, 128, and 128). </p>
<p><code>embedded_spatial</code>: The ResBlock output is embedded into a 1D tensor of size 256 by a MLP and a ReLU.</p>
<h4 id="Scalar-encoder"><a href="#Scalar-encoder" class="headerlink" title="Scalar encoder"></a>Scalar encoder</h4><ul>
<li>Input: <code>scalar_features</code>, <code>entity_list</code></li>
<li>Output:<br>  <code>embedded_scalar</code> - 1D tensor of embedded scalar features<br>  <code>scalar_context</code> - 1D tensor of certain scalar features as context to use for gating</li>
</ul>
<h3 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h3><ul>
<li>Input: <code>prev_state</code>, <code>embedded_entity</code>, <code>embedded_spatial</code>, <code>embedded_scalar</code></li>
<li>Output:<br>  <code>next_state</code> - The LSTM state for the next step<br>  <code>lstm_output</code> - The output of the LSTM</li>
</ul>
<p>Concatenate <code>embedded_entity</code>, <code>embedded_spatial</code>, and <code>embedded_scalar</code> into a single 1D tensor, and feeds that tensor along with <code>prev_state</code> into an LSTM with 3 hidden layers each of size 384. No projection is used.</p>
<h3 id="Heads"><a href="#Heads" class="headerlink" title="Heads"></a>Heads</h3><h4 id="Action-type-head"><a href="#Action-type-head" class="headerlink" title="Action type head"></a>Action type head</h4><ul>
<li>Input: <code>lstm_output</code>, <code>scalar_context</code></li>
<li>Output:<br>  <code>action_type_logits</code> - action type logits<br>  <code>action_type</code> - The action_type sampled from the action_type_logits using a multinomial with temperature 0.8. During supervised learning, <code>action_type</code> will be the ground truth human action type, and temperature is 1.0<br>  <code>autoregressive_embedding</code> - Embedding that combines information from <code>lstm_output</code> and all previous sampled arguments.</li>
</ul>
<p>It embeds <code>lstm_output</code> into a 1D tensor of size 256, passes it through 16 ResBlocks with LayerNorm each of size 256, and applies a ReLU. The output is converted to a tensor with one logit for each possible action type through a <code>GLU</code> gated by <code>scalar_context</code>.</p>
<p><code>autoregressive_embedding</code>: apply a ReLU and a MLP of size 256 to the one-hot version of <code>action_type</code>, and project to a 1D tensor of size 1024 through a <strong>GLU</strong> gated by <code>scalar_context</code>. The projection is added to <code>lstm_output</code> projection gated by <code>scalar_context</code> to yield <code>autoregressive_embedding</code>.</p>
<h4 id="Delay-head"><a href="#Delay-head" class="headerlink" title="Delay head"></a>Delay head</h4><ul>
<li>Input: <code>autoregressive_embedding</code></li>
<li>Output:<br>  <code>delay_logits</code> - The logits corresponding to the probabilities of each delay<br>  <code>delay</code> - The sampled delay using a multinomial with no teperature.<br>  <code>autoregressive_embedding</code> - Embedding that combines information from <code>lstm_output</code> and all previous sampled arguments. Similarly, project the delay to size 1024 1D tensor through 2-layer MLP with ReLUsk and add to <code>autoregressive_embedding</code>.</li>
</ul>
<h4 id="Queued-head"><a href="#Queued-head" class="headerlink" title="Queued head"></a>Queued head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>embedded_entity</code></li>
<li>Output:<br>  <code>queued_logits</code> - 2-dimensional logits corresponding to the probabilities of queueing and not queueing.<br>  <code>queued</code> - Whether or no to queue this action.<br>  <code>autoregressive_embedding</code> - Embedding that combines information from <code>lstm_output</code> and all previous sampled arguments. Queuing information is not added if queuing is not possible for the chosen <code>action_type</code>.</li>
</ul>
<h4 id="Selected-units-head"><a href="#Selected-units-head" class="headerlink" title="Selected units head"></a>Selected units head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>entity_embeddings</code></li>
<li>Output:<br>  <code>units_logits</code> - The logits corresponding to the probabilities of selecting each unit, repeated for each of the possible 64 unit selections<br>  <code>units</code> - The units selected for this action.<br>  <code>autoregressive_embedding</code></li>
</ul>
<p>If the selected <code>action_type</code> does not require units, then ignore this head.</p>
<p>Otherwise, create one-hot version of entities that satisfy the selected action type, pass it to an MLP and a ReLU, denoted as <code>func_embed</code>. </p>
<ul>
<li>Compute the masked of which units can be selected, initialized to allow selected entities that exist (including enemy units)</li>
<li>Compute a <strong>key</strong> for each entity by feeding <code>entity_embeddings</code> through a conv1D with 32channels and kernel size 1.</li>
<li>Then repeated for selecting up to 64 units, pass <code>autoregressive_embedding</code> through an MLP (size 256), add <code>func_embed</code>, pass through a ReLU and a linear with size 32. The result is fed into a LSTM with size 32 and zero initial state to get a <strong>query</strong>. </li>
<li>The entity <strong>keys</strong> are multiplied by the <strong>query</strong>, and are sampled using the mask and temperature 0.8 to decide which entity to select.</li>
<li>The one-hot position of the selected entity is multiplied by the keys, reduced by the mean across the entities, passed through an MLP of size 1024, and added to subsequent <code>autoregressive_embedding</code>. If <code>action_type</code> does not involve selecting units, skip this head.</li>
</ul>
<h4 id="Target-unit-head"><a href="#Target-unit-head" class="headerlink" title="Target unit head"></a>Target unit head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>entity_embeddings</code></li>
<li>Output:<br>  <code>target_unit_logits</code><br>  <code>target_unit</code> - sampled from <code>target_unit_logits</code> using a multinomial with temperature 0.8</li>
</ul>
<p>No need to return <code>autoregressive_embeddings</code> as the one of the two terminal arguments (as Location Head). </p>
<h4 id="Location-head"><a href="#Location-head" class="headerlink" title="Location head"></a>Location head</h4><ul>
<li>Input: <code>autoregressive_embedding</code>, <code>action_type</code>, <code>map_skip</code></li>
<li>Output:<br>  <code>target_location_logits</code><br>  <code>target_location</code></li>
</ul>
<p><code>autoregressive_embedding</code> is reshaped as the same as the final skip as <code>map_skip</code> and concatenate together along the channel dimension, pass through a ReLU and conv2D with 128 channels and kernel size 1, then another ReLU. the 3D tensor is then passed through <strong>gated ResBlocks</strong> with 128 channels, kernel size 3, and <strong>FiLM</strong>, gated on <code>autoregressive_embedding</code> and using the <code>map_skip</code> elements in the reverse order. </p>
<p>Afterwards, upsample 2x by each of the transposed 2D convolutions with kernel size 4 and channel sizes 128, 64, 16 and 1 respectively. Those final logits are flattened and sampled with the temperature 0.3 (masking out invalid locations using <code>action_type</code>) to get the actual target position.</p>
<hr>
<p><strong>Another NN trained with camera-based interface</strong> lost the follow-up game against MaNa.</p>
<ul>
<li>Partial observability: only see information in <strong>camera view</strong>, <strong>“saccade” actions</strong></li>
<li>Imperfect information: only see opponent unit within range of own units</li>
<li>Large action space: simutaneous control of hundreds of units</li>
<li>Strategy cycles: counterstrategies discovered by pro players over 20 years</li>
</ul>
<p><img data-src="/notes/images/AlphaStar-using-camera-based-model.png" alt="upload successful"></p>
<h1 id="Related-methods"><a href="#Related-methods" class="headerlink" title="Related methods"></a>Related methods</h1><h2 id="Relational-inductive-biases"><a href="#Relational-inductive-biases" class="headerlink" title="Relational inductive biases"></a>Relational inductive biases</h2><p>Vinicius et. al(2019)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., ... & Shanahan, M. (2018). [Deep reinforcement learning with relational inductive biases](https://pdfs.semanticscholar.org/9ea9/2ebeb7462f2db346cfa3281ad7497b1063d6.pdf?_ga=2.187374248.1439184430.1563761069-43402279.1542977082). ICLR 2019
">[2]</span></a></sup> augmented model-free DRL with a mechanism for relational reasoning over structured representations via <strong>self-attention mechanisms</strong>, improving performance, learning efficiency, generalization and interpretability.</p>
<p>It incorporates relational inductive baises for entity- and relation- centric state representations, and iterated reasoning into the RL agent based on a distributed advantage actor-critic (A2C).</p>
<p>The agent receives the raw visual input pixels as the input, employing the front-end of CNNs to compute the entity embeddings, without depending on any priori knowledge, similar to Visual QA, video understanding tasks. </p>
<h3 id="Embedded-state-representation"><a href="#Embedded-state-representation" class="headerlink" title="Embedded state representation"></a>Embedded state representation</h3><ul>
<li>The agent creates an embedded state representation $S$ from its input observation, which is a spatial feature map returned by a CNN.</li>
</ul>
<h3 id="Relational-module"><a href="#Relational-module" class="headerlink" title="Relational module"></a>Relational module</h3><ul>
<li><code>Feature-to-entity transformation</code>: the relational module reshapes the feature map $S$ (with shape $m \times n \times f$) to entity vectors $E$ (with shape $N \times f$, where $N=m \cdot n$). Each row of $E$, denoted as <script type="math/tex">\mathbf{e}_i</script>, consists of a feature vector <script type="math/tex">\pmb{s}_{x,y}</script> at a particular $x$,$y$ location in each feature map. This allows for non-local computation between entities, unconstrained by their coordinates in the spatial feature map.</li>
<li><code>Self-attention mechanism</code>: apply multi-head dot-product attention(MHDPA) compute the pairwise interactions between each entity and all others (include itself).<script type="math/tex; mode=display">A = \text{softmax}(d^{-1/2}QK^T)V</script>where $d$ is the dimensionality of the query and key vectors.</li>
<li>Finally pass them to a multi-layer perceptron (MLP), with a residual connection.<script type="math/tex; mode=display">\tilde{\pmb{e}}_i = g_\theta (\pmb{a}_i^{h=1:H})</script></li>
</ul>
<h3 id="Output-module"><a href="#Output-module" class="headerlink" title="Output module"></a>Output module</h3><p>$\tilde{E}$ with the shape $N \times f$, is reduced to an $f$-dimensional vector by max-pooling over the entity dimension, followed by an MLP to output $(c+1)$-dimensional vector. The vector contains $c$-dimensional vector of $\pi$’s logits where $c$ is the # of discrete actions, plus a scalar baseline value estimate $B$.</p>
<p><img data-src="/notes/images/rl-relational-inductive-biases.png" alt="upload successful"></p>
<h2 id="Auto-regressive-policy-head"><a href="#Auto-regressive-policy-head" class="headerlink" title="Auto-regressive policy head"></a>Auto-regressive policy head</h2><p>Vinyals et. al (2017)<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... & Quan, J. (2017). [Starcraft II: A new challenge for reinforcement learning](https://arxiv.org/pdf/1708.04782). arXiv preprint arXiv:1708.04782.
">[3]</span></a></sup> represented the policy with the <strong>auto-regressive</strong> manner, i.e. predict each action conditioned on previous actions:</p>
<script type="math/tex; mode=display">\pi_\theta(a \vert s) = \prod_{l=0}^L \pi_\theta (a^l \vert a^{<l}, s)</script><p>The auto-regressive policy transforms the problem of choosing a full action $a$ to a sequence of decisions for each argument $a^l$.</p>
<h2 id="Pointer-Networks"><a href="#Pointer-Networks" class="headerlink" title="Pointer Networks"></a>Pointer Networks</h2><p>It can be inferred that the Pointer Net is used to <strong>output the action for each unit</strong>, since the StarCraft involves many units in concert and the # of units changes over time. <sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://www.alexirpan.com/2019/02/22/alphastar-part2.html](https://www.alexirpan.com/2019/02/22/alphastar-part2.html)
">[6]</span></a></sup></p>
<p><img data-src="/notes/images/Pointer-nets.png" alt="upload successful"></p>
<p><code>Pointer Net</code> is employed to handle the variablity of the output length:</p>
<ul>
<li><p>Applies the attention mechanism:</p>
<script type="math/tex; mode=display">u_j^i = v^T \tanh (W_1 e_j + W_2 d_i) \quad j \in (1,\cdots,n)</script><script type="math/tex; mode=display">p(C_i \vert C_1, \cdots, C_{i-1}, \mathcal{P}) = \text{softmax}(u^i)</script><p>  where softmax normalizes the vector $u^i$ (of length $n$) to be an output distribution over the dictionary of inputs. And $v$,<script type="math/tex">W_1</script>, <script type="math/tex">W_2</script> are learnable parameters of the output model.</p>
<p>  Here, we do not blend the encoder state <script type="math/tex">e_j</script> to propagate extra information to the decoder $d_i$. Instead, we use <script type="math/tex">u_j^i</script> as <code>pointers to the input elements</code>. </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ptr</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot; Pointer Nets &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Attn, self).__init__()</span><br><span class="line">        self.h_size = h_size</span><br><span class="line">        self.W1 = nn.Linear(h_size, h_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W2 = nn.Linear(h_size, h_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.vt = nn.Linear(h_size, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.tanh = nn.Tanh()</span><br><span class="line">        self.score = nn.Softmax(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec, enc, m</span>):</span></span><br><span class="line">        attn = self.vt(self.tanh(self.W1(enc) + self.W2(dec))).squeeze(-<span class="number">1</span>)</span><br><span class="line">        logits = attn.masked_fill_(m, -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">		<span class="keyword">return</span> self.score(logits)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><code>Ptr Nets</code> can be seen as an application of <strong><code>content-based attention mechanisms</code></strong>.</li></ul>
          </div>
<h2 id="Gated-ResNet"><a href="#Gated-ResNet" class="headerlink" title="Gated ResNet"></a>Gated ResNet</h2><p>Gated Residual Network (Gated ResNet)<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Savarese, P.H. (2017). [Learning Identity Mappings with Residual Gates](https://arxiv.org/pdf/1611.01260.pdf). ICLR
">[20]</span></a></sup> adds a linear gating mechanism to shortcut connections using a scalar parameter to control each gate.</p>
<ul>
<li>Let Residual layer <script type="math/tex">u = g(k) f(x, W) = f_r(x,W)+x</script>, the gated Highway network is:<script type="math/tex; mode=display">
\begin{align}
u &= g(k) f(x,W) + (1- g(k)) x \\
&= g(k) (\underbrace{f_r(x,W)+x}_\text{ResNet}) + (1-g(k))x \\
&= g(k)f_r(x,W) + x
\end{align}</script></li>
</ul>
<p><img data-src="/notes/images/GatedResNet.png" alt="upload successful"></p>
<h2 id="FiLM"><a href="#FiLM" class="headerlink" title="FiLM"></a>FiLM</h2><p><strong>F</strong>eature-w<strong>i</strong>se <strong>L</strong>iner <strong>M</strong>odulation (FiLM)<sup id="fnref:21"><a href="#fn:21" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Perez, E., Strub, F., Vries, H.D., Dumoulin, V., & Courville, A.C. (2017). [FiLM: Visual Reasoning with a General Conditioning Layer](https://arxiv.org/pdf/1709.07871.pdf). AAAI.">[21]</span></a></sup> applies the feature-wise affine transormation to the intermediate features of networks, based on some conditioning inputs. </p>
<p><img data-src="/notes/images/FiLM.png" width="20%" /></p>
<p>FiLM learns functions $f$ and $h$ which output <script type="math/tex">\gamma_{i,c}</script> and <script type="math/tex">\beta_{i,c}</script> as a function of input <script type="math/tex">\mathbf{x}_i</script>:</p>
<script type="math/tex; mode=display">
\begin{align}
\gamma_{ic} & = f_c(\mathbf{x}_i) & \text{coefficient} \\
\beta_{i,c} &= h_c(\mathbf{x}_i) & \text{intercept} \\
\text{FiLM}(\mathbf{F}_{i,c} \vert y_{i,c}, \beta_{i,c}) &= \gamma_{i,c} \mathbf{F}_{i,c} + \beta_{i,c} & \text{FiLM generator}
\end{align}</script><p>where</p>
<ul>
<li>$f$ and $h$ can be arbitary functions such as dense networks, sigmoid/tanh/exponential functions, etc.</li>
<li>Modulation of the target NN can be applied on the same input to that NN or some other inputs. For CNNs, $f$ and $h$ modulate the per-feature-map distribution of activations based on <script type="math/tex">\mathbf{x}_i</script>.</li>
</ul>
<ul>
<li>FiLM-ed network architecture:<br><img data-src="/notes/images/FiLM-ed-Net.png" width="60%" /></li>
</ul>
<h2 id="Centralized-value-baseline"><a href="#Centralized-value-baseline" class="headerlink" title="Centralized value baseline"></a>Centralized value baseline</h2><h3 id="Centralized-critic"><a href="#Centralized-critic" class="headerlink" title="Centralized critic"></a>Centralized critic</h3><ul>
<li><p><strong>Problems</strong>: Conventional <em>independent actor-critic</em> (IAC) independently trains each agent, but the lack of information sharing impedes to learn coordinated strategies that depend on interactions between multiple agents, or to estimate the contribution of single agent’s action to the global rewards. </p>
</li>
<li><p>Solution: use a <strong>centralized critic</strong> that conditions on the true global state $s$, or the joint action-observation histories $\tau$ otherwise. (See the figure below)</p>
</li>
</ul>
<p><img data-src="/notes/images/rl-COMA.png" alt="upload successful"></p>
<p>The critic (red parts in the figure) is used only during learning and only the actor is needed during execution. </p>
<h3 id="Counterfactual-baseline"><a href="#Counterfactual-baseline" class="headerlink" title="Counterfactual baseline"></a>Counterfactual baseline</h3><ul>
<li><strong>Problems</strong>: A naive way is to follow the gradient based on the TD error:<script type="math/tex; mode=display">g = \nabla_{\theta^\pi} \log \pi (\mu \vert \tau_t^a) (r+ \gamma V(s_{t+1})- V(s_t))</script>It fails to address the key credit assignment problem since the TD error only considers global rewards, the gradient from each actor does not explicitly considered based on their respective contribution.</li>
<li>Solution: <strong>counterfactual baseline</strong>.<br>It is inspired by <em>difference reward</em> by computing the change of global reward when the action $a$ of an individual agent is replaced by a default action $c^a$:<script type="math/tex; mode=display">D^a = r(s,\pmb{u}) - r(s, (\pmb{u}^{-a}, c^a))</script></li>
</ul>
<p>But difference baseline requires 1) the access to a simulator <script type="math/tex">r(s, (\pmb{u}^{-a}, c^a))</script> and 2) a use-specific default action $c^a$.</p>
<ul>
<li><strong>counterfactual baseline</strong>. Compute the agent $a$ we can compute the advantage function that compares the $Q$-value for the current action $\mu^a$ to a counterfactual baseline that marginalize out $\mu^a$, while keeping the other agents’ actions $\mu^{-a}$ fixed:<script type="math/tex; mode=display">A^a(s, \mu) = Q(s, \mu) - \sum_{\mu^{'a}} \pi^a(\mu^{'a} \vert \tau^a) Q(s, (\mu^{-a}, \mu^{'a}))</script>where $A^a(s,\mu^a)$ measures the difference when only $a$’s action changes, learn directly from agents’ experiences rather than on extra simulations, a reward model or a use-designed default action.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018, April). [Counterfactual multi-agent policy gradients](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17193/16614). In Thirty-Second AAAI Conference on Artificial Intelligence.
">[5]</span></a></sup></li>
</ul>
<h3 id="Critic-representation"><a href="#Critic-representation" class="headerlink" title="Critic representation"></a>Critic representation</h3><p>The output dimension of networks would be equal to $|U|^n$, where $n$ is the # of agents. COMA uses critic representation in which it also takes the action of other agents <script type="math/tex">u_t^{-a}</script> as part of the input, and output a $Q$-value for each of agent $a$’s action, with the # of output nodes $|U|$. (see Fig.(c) below)</p>
<p><img data-src="/notes/images/rl-COMA-ac.png" alt="upload successful"><br>    Fig. (b) and (c) are the architectures of the actor and critic.</p>
<h2 id="Self-Imitation-Learning"><a href="#Self-Imitation-Learning" class="headerlink" title="Self-Imitation Learning"></a>Self-Imitation Learning</h2><p>Self-Imitation Learning (SIL)<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Oh, J., Guo, Y., Singh, S., & Lee, H. (2018). [Self-imitation learning](https://arxiv.org/pdf/1806.05635). arXiv preprint arXiv:1806.05635.
">[8]</span></a></sup> learns to imitate the agent’s own past good experiences in the actor-critic framework. It stores experiences with cumulative rewards in a replay buffer: <script type="math/tex">{\mathcal{D}=\{ (s_t, a_t, R_t) \}}</script>, where <script type="math/tex">s_t</script>,<script type="math/tex">a_t</script> are a state and an action at time-step $t$, and <script type="math/tex">R_t = \sum_{k=t}^\infty y^{k-t} r_k</script> is the discounted sum of rewards with a discount factor $\gamma$, learns to imitate state-action pairs in the replay buffer only when the return in the past episode is greater than the agent’s value estimate.</p>
<h3 id="Off-policy-actor-critic-loss"><a href="#Off-policy-actor-critic-loss" class="headerlink" title="Off-policy actor-critic loss"></a>Off-policy actor-critic loss</h3><script type="math/tex; mode=display">
\begin{align*}
\mathcal{L}^\text{sil} &= \mathbb{E}_{s,a,R \in \mathcal{D}} [ \mathcal{L}^\text{sil}_\text{policy} + \beta^\text{sil} \mathcal{L}_\text{value}^\text{sil}] \\
\mathcal{L}_\text{policy}^\text{sil} &= -\log \pi_\theta (a \vert s) \max(R-V_\theta(s), 0) \\
\mathcal{L}_\text{value}^\text{sil} &= \frac{1}{2} || \max(R-V_\theta(s), 0) ||^2
\end{align*}</script><p>where <script type="math/tex">\pi_\theta</script>, <script type="math/tex">V_\theta(s)</script> are the policy (i.e. actor) and the value function, $\beta^\text{sil} \in \mathbb{R}^+$ is a hyperparameter for the value loss.</p>
<p>The <script type="math/tex">\mathcal{L}_\text{policy}^\text{sil}</script> can be interpreted as cross entropy loss with sample weights proportional to the gap between the return and the agent’s value estimate <script type="math/tex">(R-V_\theta)</script>:</p>
<ol>
<li>If the past return is greater than the agent’s value estimate, i.e. <script type="math/tex">R>V_\theta</script>, the agent learns to choose the action chosen in the poast in the given state.</li>
<li>Otherwise (<script type="math/tex">R < V_\theta</script>), such a state-action pair is not used to update due to the $\max$ op.<br>This encourages the agent to imitate its own decisions in the past only when such decisions resulted in larger returns than expected. <script type="math/tex">\mathcal{L}_\text{value}^\text{sil}</script> updates the value estimate towards the off-policy return $R$.</li>
</ol>
<h3 id="Prioritized-experience-replay"><a href="#Prioritized-experience-replay" class="headerlink" title="Prioritized experience replay:"></a><strong>Prioritized experience replay</strong>:</h3><p>Sample transitions from the replay buffer using the clipped advantage <script type="math/tex">\max(R-V_\theta(s),0)</script> as priority, i.e. sampling probablity is prop. to <script type="math/tex">\max(R-V_\theta(s),0)</script>.</p>
<h3 id="Advantage-Actor-Critic-with-SIL-A2C-SIL"><a href="#Advantage-Actor-Critic-with-SIL-A2C-SIL" class="headerlink" title="Advantage Actor-Critic with SIL (A2C + SIL)"></a><strong>Advantage Actor-Critic with SIL (A2C + SIL)</strong></h3><p>A2C + SIL objective: </p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{L}^\text{a2c} &= \mathbb{E}_{s,a\sim \pi_\theta} [\mathcal{L}_\text{policy}^\text{a2c} + \beta^\text{a2c} \mathcal{L}_\text{value}^\text{a2c}] \\
\mathcal{L}_\text{policy}^\text{a2c} &= - \log \pi_\theta (a_t \vert s_t) (V_t^n - V_\theta(s_t)) - \alpha \mathcal{H}_t^{\pi_\theta}\\
\mathcal{L}_\text{value}^\text{a2c} &= \frac{1}{2} ||V_\theta(s_t) - V_t^n||^2
\end{align*}</script><h3 id="SIL-algorithms"><a href="#SIL-algorithms" class="headerlink" title="SIL algorithms"></a>SIL algorithms</h3><ol>
<li>Initialize parameter $\theta$</li>
<li>Initialize replay buffer $\mathcal{D} \leftarrow \emptyset$</li>
<li>Initialize episode buffer $\mathcal{E} \leftarrow \emptyset$</li>
<li>For each iteration do:<ol>
<li><strong>## Collect on-policy samples</strong></li>
<li>for each step do:<ol>
<li>execute an action <script type="math/tex">s_t, a_t, r_{t+1} \sim \pi_\theta(a_t \vert s_t)</script></li>
<li>store transition $\mathcal{E} \leftarrow \mathcal{E} \cup { (s_t, a_t, r_t) }$</li>
</ol>
</li>
<li>if $s_{t+1}$ == TERMINAL:<ol>
<li><strong>## Update replay buffer </strong></li>
<li>compute returns $R_t = \sum_k^\infty \gamma^{k-t} r_k$ for all $t$ in $\mathcal{E}$</li>
<li>$\mathcal{D} \leftarrow \mathcal{D} \cup { (s_t,a_t,R_t) }$ for all $t$ in $\mathcal{E}$</li>
<li>clear episode buffer $\mathcal{E} \leftarrow \emptyset$</li>
</ol>
</li>
<li><strong>## perform actor-critic using on-policy samples</strong></li>
<li>$\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}^\text{a2c}$</li>
<li><em>*## Perform self-imitation learning</em></li>
<li>for m = 1 to $M$:<ol>
<li>Sample a mini-batch ${ (s,a,R) }$ from $\mathcal{D}$</li>
<li>$\theta \leftarrow theta - \eta \nabla_\theta \mathcal{L}^\text{sil}$</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Policy-distillation"><a href="#Policy-distillation" class="headerlink" title="Policy distillation"></a>Policy distillation</h2><p><strong>Policy distillation</strong> is iused to train a new network that performs at the expert level while being dramatically smaller and more efficient <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., ... & Hadsell, R. (2015). [Policy distillation](https://arxiv.org/pdf/1511.06295). arXiv preprint arXiv:1511.06295.
">[9]</span></a></sup>. Andrei <em>et. al</em>(2016) demonstrated that the <strong>multi-task</strong> distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent in the Atari domain.</p>
<h3 id="Distillation"><a href="#Distillation" class="headerlink" title="Distillation"></a>Distillation</h3><p>Distillation is proposed for supervised model compression, by creating a single network from an ensemble model. Model compression trains a student network using the output of a teacher network, compressing a large ensemble model into a single shallow network.</p>
<h3 id="Single-game-policy-distillation"><a href="#Single-game-policy-distillation" class="headerlink" title="Single-game policy distillation"></a>Single-game policy distillation</h3><p>Distillation is to transfer knowledge frm a <em>teacher</em> model $T$ to a <em>student</em> model $S$. </p>
<ul>
<li>The distillation targets from a classification targets from a classification network are typically obtained by passing the weighted sum of the last network layer through a softmax function.</li>
<li>In order to transfer more knowledge of the network, the teacher outputs can be softened by passing the network output through a relaxed (higher temperature) softmax than one that was used for training: <script type="math/tex">\text{softmax}(\frac{q^T}{\tau})</script>, where $q^T$ is the vector of $Q$-values of $T$.</li>
</ul>
<p><img data-src="/notes/images/rl-policy-distillation.png" alt="upload successful"></p>
<p>When transferring $Q$-value rather than a classifier, the scale of the $Q$-values may be hard to learn since it is not bounded and can be quite unstable. Training $S$ to predict only the single best action from $T$ is problematic, since multiple actions may have similar $Q$-values. </p>
<p>Consider policy distillation from $T$ to $S$, hwere the teacher $T$ generates a dataset <script type="math/tex">\mathcal{D}^T = \{ (s_i, q_i) \}_{i=0}^N</script>, where each sample consists of a short observation sequence <script type="math/tex">s_i</script> and unnormalized $Q$-value vector <script type="math/tex">q_i</script>. Here is three approaches:</p>
<ol>
<li>Only use the highest valued action from the teacher <script type="math/tex">a_\text{i, best} = \arg \max(q_i)</script>. $T$ is trained with a negative log likelihood(NLL) to predict the same action:<script type="math/tex; mode=display">L_\text{NLL}(\mathcal{D}^T, \theta_S) = - \sum_{i=1}^{|D|} \log P(a_i=a_\text{i, best} \vert x_i, theta_S)</script></li>
<li>Train with mean-squared-error loss (MSE). It preserves the full set of action-values:<script type="math/tex; mode=display">L_\text{MSE} (\mathcal{D}^T, \theta_S) = \sum_{i=1}^{|D|} || q_i^T - q_i^S ||_2^2</script></li>
<li>KL divergence with temperature $\tau$:<script type="math/tex; mode=display">
\begin{align}
L_\text{KL} (\mathcal{D}^T, \theta_S) &= \text{KL} (\text{softmax}(\frac{q_i^T}{\tau}) || \text{softmax} (q_i^S) ) \\
&= \sum_{i=1}^{|D|} \text{softmax}(\frac{q_i^T}{\tau}) \ln \frac{\text{softmax}(\frac{q_i^T}{\tau})}{\text{softmax}(q_i^S)}
\end{align}</script></li>
</ol>
<h3 id="Multi-task-policy-distillation"><a href="#Multi-task-policy-distillation" class="headerlink" title="Multi-task policy distillation"></a>Multi-task policy distillation</h3><p>Multi-task policy distillation uses $n$ DQN single-game experts, each trained separatedly, providing inputs and targets for $S$. The data is stored in separate memory buffers. The distillation agent $S$ learns from the $n$ data stores sequentially, and different tasks have different output layer(i.e. controller layer). The KL and NLL loss functions are used for multi-task distillation.</p>
<p><img data-src="/notes/images/rl-policy-distillation-multi-task.png" alt="upload successful"></p>
<h2 id="IMPALA"><a href="#IMPALA" class="headerlink" title="IMPALA"></a>IMPALA</h2><p><strong>Imp</strong>ortance Weighted <strong>A</strong>ctor-<strong>L</strong>earning <strong>A</strong>rchitecture (IMPALA) can scale to thousands of machines without reducing data efficiency or resouce utilization. IMPALA achieves exceptionally high data throughput rates of 250,000 frames per second, over 30 times faster than single-machine A3C.<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... & Legg, S. (2018). [Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures](https://arxiv.org/pdf/1802.01561). arXiv preprint arXiv:1802.01561.
">[10]</span></a></sup></p>
<h3 id="IMPALA-archtecture"><a href="#IMPALA-archtecture" class="headerlink" title="IMPALA archtecture"></a>IMPALA archtecture</h3><p><img data-src="/notes/images/rl-IMPALA-fig.png" alt="upload successful"></p>
<p>IMPALA applies an actor-critic setup to laern a policy $\pi$ and a baseline function $V^\pi$. Each actor updates its own local policy $\mu$ (i.e. behavior policy) to the latest learner policy $\pi$ (i.e. target policy), and runs $n$ steps in the environment. Afterwards, store the trajectory of states, actions, rewards, <script type="math/tex">\{(x_i,a_i,r_i) \}_{i=1}^n</script> and policy distributions <script type="math/tex">\mu(a_t \vert x_t)</script> as well as the initial LSTM state to a <strong>queue</strong>(as in left Fig.). This could lead to the <em>policy-lag</em> between actors and the learner. <strong>V-trace</strong> is proposed to correct the lag to get high data throughput while keeping data efficiency.</p>
<p>Also, IMPALA can employ synchronized parameter update (right fig.).<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... & Legg, S. (2018). [Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures](https://arxiv.org/pdf/1802.01561). arXiv preprint arXiv:1802.01561.
">[10]</span></a></sup></p>
<p><img data-src="/notes/images/rl-IMPALA-paper-fig.png" alt="upload successful"></p>
<h3 id="V-trace"><a href="#V-trace" class="headerlink" title="V-trace"></a>V-trace</h3><h4 id="V-trace-target"><a href="#V-trace-target" class="headerlink" title="V-trace target"></a>V-trace target</h4><p>Consider the trajectory <script type="math/tex">(x_t, a_t, r_t)_{t=s}^{t=s+n}</script> generated by the actor following the some policy $\mu$, define $n$-step V-trace target for <script type="math/tex">V(s_s)</script>, the value approximation at state <script type="math/tex">x_s</script>:</p>
<script type="math/tex; mode=display">
\begin{align*}
v_s &\triangleq V(x_s) + \sum_{t=s}^{s+n-1} \gamma^{t-s}(\prod_{i=s}^{t-1}c_i) \pmb{\delta_t V} & \text{value approximation}\\
\pmb{\delta_t V} &\triangleq \rho_t (r_t + \gamma V(x_{t+1}) - V(x_t)) & \text{temporal difference for }V \\
\end{align*}</script><p>where <script type="math/tex">\rho_t \triangleq \min(\bar{\rho}, \frac{\pi(a_t \vert x_t)}{\mu(a_t \vert x_t)})</script> and <script type="math/tex">c_i \triangleq \min(\bar{c}, \frac{\pi(a_i \vert x_i)}{\mu(a_i \vert x_i)})</script> are truncated improtance sampling(IS) weights.</p>
<ul>
<li>The truncated IS weight <script type="math/tex">c_i</script> define the <strong>fixed point</strong> of this update rule.</li>
<li>The weights <script type="math/tex">c_i</script> are similar to the “trace cutting” coefficients in Retrace. The product <script type="math/tex">c_s \cdots c_{t-1}</script> measures the temporal difference <script type="math/tex">\delta_t V</script> observed at time $t$ impacts the value function update at previous time $s$.</li>
</ul>
<h4 id="V-trace-actor-critic-algorithms"><a href="#V-trace-actor-critic-algorithms" class="headerlink" title="V-trace actor-critic algorithms"></a>V-trace actor-critic algorithms</h4><ul>
<li>At training time $s$, the value prameters $\theta$ are updated by gradient descent on $l2$ loss to the target <script type="math/tex">v_s</script>, in the direction of:<script type="math/tex; mode=display">(v_s - V_\theta(x_s)) \nabla_\theta V_\theta(x_s)</script></li>
<li>update the policy params:<script type="math/tex; mode=display">\rho_s \nabla_\omega \log \pi_\omega (a_s \vert x_s) (r_s + \gamma v_{s+1} - V-\theta(x_s))</script></li>
<li>To prevent premature convergence, add an entropy term, along the direction:<script type="math/tex; mode=display">- \nabla_\omega \sum_a \pi_\omega(a \vert x_s) \log \pi_\omega (a \vert x_s)</script></li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>Left: small, 2 Conv layers, 1.2 million parameters; </li>
<li>Right: large, 15 Conv layers, 1.6 million parameters.<br><img data-src="/notes/images/rl-IMPALA-model.png" alt="upload successful"></li>
</ul>
<h2 id="Population-based-training-PBT"><a href="#Population-based-training-PBT" class="headerlink" title="Population-based training(PBT)"></a>Population-based training(PBT)</h2><p>Two common tracks of hyperparameter tuning:</p>
<ol>
<li>parallel search:<ul>
<li>grid search</li>
<li>random search</li>
</ul>
</li>
<li>sequential optimization: requires multiple sequential training runs.<ul>
<li>Bayesian optimization</li>
</ul>
</li>
</ol>
<p><img data-src="/notes/images/PBT-fig.png" alt="upload successful"></p>
<p>PBT starts like parallel search, randomly sampling hyperparameters and weight initializations. However, each training run asynchronously evaluates its performance periodically. If a model in the population is under-performing, it will <em>exploit</em> the rest of the polulation by replacing itself with a better performing model, and it will <em>explore</em> new hyperparameters by modifying a better model’s hyperparameters before training is continued.<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., ... & Fernando, C. (2017). [Population based training of neural networks](https://arxiv.org/abs/1711.09846). arXiv preprint arXiv:1711.09846.
">[13]</span></a></sup></p>
<h3 id="PBT-algorithms"><a href="#PBT-algorithms" class="headerlink" title="PBT algorithms"></a>PBT algorithms</h3><p>Population-based Training(PBT) can be used to optimize neural networks for RL, supervised learning, GAN. PBT offers a way to optimize both the <strong>parameters</strong> $\theta$ and the <strong>hyperparameters</strong> $h$ jointly on the actual metric $\mathcal{Q}$ that we care about.</p>
<p>Training $N$ models <script type="math/tex">\{ \theta^i \}_{i=1}^N</script> forming a population $\mathcal{P}$ which are optimized with different hyperparameters <script type="math/tex">\{ \mathbf{h}^i \}_{i=1}^N</script>. Then use the  collection of partial solutions in the population to perform <em>meta-optimization</em>, where the hyperparameters $h$ and weights $\theta$ are additionally adapted w.r.t the entire population. For each worker (member) in the population, we apply two functions independently:</p>
<ol>
<li><em>expoit</em>: decide whether the worker abandon the current solutions and copy a better one.</li>
<li><em>explore</em>: propose new ones to better explore the solution space.</li>
</ol>
<p>Each worker of the population is trained in paraleel, with iterative calls of the repeated cycle of local iterative training (with <em>step</em>) and exploitation and exploration with the rest of the population (with <em>exploit</em> and <em>explore</em>) until convergence of the model.</p>
<ul>
<li><em>step</em>: a step of gradient descent</li>
<li><em>eval</em>: mean episodic return or validation set performance of the metric to optimize</li>
<li><em>exploit</em>: select another member of the population to copy the weights and hyperparameters from</li>
<li><em>explore</em>: create new hyperparameters for the next steps of gradient based learning by either <strong>perturbing the copied hyperparameters</strong> or <strong>resampling hyperparameters from the original defined prior distribution</strong>.</li>
</ul>
<p>PBT is asynchronous and does not require a centralized process to orchestrate the training of the members of the population. “PBT is an online evolutionary process that adapts internal rewards and hyperparameters and performs model selection by replacing underperforming agents with mutated version of better agents”.<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., ... & Sonnerat, N. (2019). [Human-level performance in 3D multiplayer games with population-based reinforcement learning](https://science.sciencemag.org/content/sci/364/6443/859.full.pdf). Science, 364(6443), 859-865.
">[14]</span></a></sup></p>
<p>Silimar to genetic algorithms: local optimization by SGD -&gt; periodic model selection -&gt; hyperparameter refinement</p>
<p><img data-src="/notes/images/PBT-alg.png" alt="upload successful"></p>
<h4 id="PBT-for-RL"><a href="#PBT-for-RL" class="headerlink" title="PBT for RL"></a>PBT for RL</h4><ul>
<li><strong>Hyperparameters</strong>: learning rate, entropy cost, unroll length for LSTM,…</li>
<li><strong>Step</strong>: each iteration does a step of gradient descent with RMSProp on the model weights</li>
<li><strong>Eval</strong>: evaluate the model with the last 10 episodic rewards during training</li>
<li><strong>Ready</strong>: 1e6 ~ 1e7 agent steps finished</li>
</ul>
<div class="note success">
            <p><strong>Exploit</strong></p><ol><li><strong>T-selection</strong>: uniformly sample another agent in the population, and compare the last 10 episodic rewards using Welch’s t-test. If the sampled agent has a higher mean episodic reward and satisfies the t-test, the <em>weights and hyperparameters</em> are copied to replace the current agent.</li><li><strong>Truncation selection</strong>: <strong>rank all agents</strong> in the population by episodic reward. Replace the bottom 20% agents with unformly sampled agent from the top 20% of the population, by copying the <em>weights and hyperparameters</em>.</li></ol>
          </div>
<div class="note warning">
            <p><strong>Explore</strong> the hyperparameter space:</p><ol><li><strong>Perturb</strong>: randomly perturb each hyperparameter by a factor of 0.8 or 1.2 ($\pm 20\%$)</li><li><strong>Resample</strong>: each hyperparameter is resampled from the original prior distribution defined with some probability.</li></ol>
          </div>
<h4 id="For-The-Win-FTW"><a href="#For-The-Win-FTW" class="headerlink" title="For The Win (FTW)"></a>For The Win (FTW)</h4><p>For The Win (FTW) network architecture:</p>
<ul>
<li>Use a hirarchical RNN consisting of two RNNs, operating on two different timescales. The fast timescale RNN generates the hidden state <script type="math/tex">h_t^q</script> at each time step $t$, while the slow timescale RNN produces the hidden state <script type="math/tex">h_t^p = h^p_{\tau \lfloor \frac{t}{\tau} \rfloor }</script> every $\tau$ time steps.</li>
<li>The observation is encoded with CNNs.</li>
</ul>
<p><img data-src="/notes/images/rl-pbt-network-architecture.png" alt="upload successful"></p>
<p><strong>PBT</strong>:</p>
<ul>
<li>Optimize the hyperparameter $\phi$ of learning rate, slow LSTM time scale $\tau$, the weight of <script type="math/tex">D_\text{KL}</script> term, entropy cost</li>
<li>In FTW, for each agent $i$ periodically sampled any agent $j$ and estimated the win probability of a team $i$ versus a team $j$. If the probabilty to win is less than 70%, the losing agent was replaced by the winner.</li>
<li>The exploration is perturbing the inherited value by $\pm 20\%$ with a probability of $5\%$, except that they uniformly sample the slow LSTM time scale $\tau$ from the integer range $[5,20)$.</li>
</ul>
<h2 id="Evolutionary-computation"><a href="#Evolutionary-computation" class="headerlink" title="Evolutionary computation"></a>Evolutionary computation</h2><h3 id="Lamarckian-Evolution"><a href="#Lamarckian-Evolution" class="headerlink" title="Lamarckian Evolution"></a>Lamarckian Evolution</h3><p>PBT is a memetric algorithm that uses Lamarckian evolution (LE):</p>
<ul>
<li>Innner loop: NNs are trained with backpropagation for individual solutions</li>
<li>Outer loop: evolution is run as the optimization algorithm, where NNs are picked with selection methods, with the winner’s parameters overwriting the loser’s.<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arulkumaran, K., Cully, A., & Togelius, J. (2019). [Alphastar: An evolutionary computation perspective](https://arxiv.org/pdf/1902.01724). arXiv preprint arXiv:1902.01724.
">[16]</span></a></sup></li>
</ul>
<h3 id="Co-evolution"><a href="#Co-evolution" class="headerlink" title="Co-evolution"></a>Co-evolution</h3><p>Competitive co-evolutionary algorithms(CCEAs) can be seen as a superset of self-play, it keep and evluate against an entire population of solutions, rather than keeping only one solution.</p>
<h3 id="Quality-diversity"><a href="#Quality-diversity" class="headerlink" title="Quality diversity"></a>Quality diversity</h3><p>Quality diversity (QD) algorithms explicitly optimize for a single objective(quality), but also searches for a large variety of solution types, via behaviour descriptors (i.e, solution phenotypes), to encourage greater diversity in the population.<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Arulkumaran, K., Cully, A., & Togelius, J. (2019). [Alphastar: An evolutionary computation perspective](https://arxiv.org/pdf/1902.01724). arXiv preprint arXiv:1902.01724.
">[16]</span></a></sup></p>
<p>For attribution in academic contexts, please cite this work as:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;chai2019Decipher-AlphaStar-on-StarCraft-II,</span><br><span class="line">  author = &#123;Chai, Yekun&#125;,</span><br><span class="line">  title = &#123;&#123;Deciphering AlphaStar on StarCraft II&#125;&#125;,</span><br><span class="line">  year = &#123;2019&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://cyk1337.github.io/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/&#125;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., ... &amp; Shanahan, M. (2018). <a href="https://pdfs.semanticscholar.org/9ea9/2ebeb7462f2db346cfa3281ad7497b1063d6.pdf?_ga=2.187374248.1439184430.1563761069-43402279.1542977082">Deep reinforcement learning with relational inductive biases</a>. ICLR 2019<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... &amp; Quan, J. (2017). <a href="https://arxiv.org/pdf/1708.04782">Starcraft II: A new challenge for reinforcement learning</a>. arXiv preprint arXiv:1708.04782.<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Fortunato, M., &amp; Jaitly, N. (2015). <a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf">Pointer networks</a>. In Advances in Neural Information Processing Systems (pp. 2692-2700).<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., &amp; Whiteson, S. (2018, April). <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17193/16614">Counterfactual multi-agent policy gradients</a>. In Thirty-Second AAAI Conference on Artificial Intelligence.<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.alexirpan.com/2019/02/22/alphastar-part2.html">https://www.alexirpan.com/2019/02/22/alphastar-part2.html</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/Whiteson.pdf">COMA slides 2017, University of Oxford</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Oh, J., Guo, Y., Singh, S., &amp; Lee, H. (2018). <a href="https://arxiv.org/pdf/1806.05635">Self-imitation learning</a>. arXiv preprint arXiv:1806.05635.<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., ... &amp; Hadsell, R. (2015). <a href="https://arxiv.org/pdf/1511.06295">Policy distillation</a>. arXiv preprint arXiv:1511.06295.<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... &amp; Legg, S. (2018). <a href="https://arxiv.org/pdf/1802.01561">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</a>. arXiv preprint arXiv:1802.01561.<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/">https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepmind.com/blog/population-based-training-neural-networks/">https://deepmind.com/blog/population-based-training-neural-networks/</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., ... &amp; Fernando, C. (2017). <a href="https://arxiv.org/abs/1711.09846">Population based training of neural networks</a>. arXiv preprint arXiv:1711.09846.<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., ... &amp; Sonnerat, N. (2019). <a href="https://science.sciencemag.org/content/sci/364/6443/859.full.pdf">Human-level performance in 3D multiplayer games with population-based reinforcement learning</a>. Science, 364(6443), 859-865.<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepmind.com/blog/capture-the-flag-science/">https://deepmind.com/blog/capture-the-flag-science/</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Arulkumaran, K., Cully, A., &amp; Togelius, J. (2019). <a href="https://arxiv.org/pdf/1902.01724">Alphastar: An evolutionary computation perspective</a>. arXiv preprint arXiv:1902.01724.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">DeepMind AlphaStar: Mastering the Real-Time Strategy Game StarCraft II</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li><li id="fn:18"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., ... &amp; Oh, J. (2019). <a href="https://www.nature.com/articles/s41586-019-1724-z.pdf">Grandmaster level in StarCraft II using multi-agent reinforcement learning</a>. Nature, 1-5.<a href="#fnref:18" rev="footnote"> ↩</a></span></li><li id="fn:19"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1724-z/MediaObjects/41586_2019_1724_MOESM2_ESM.zip">AlphaStar Nature paper supplemental data</a><a href="#fnref:19" rev="footnote"> ↩</a></span></li><li id="fn:20"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Savarese, P.H. (2017). <a href="https://arxiv.org/pdf/1611.01260.pdf">Learning Identity Mappings with Residual Gates</a>. ICLR<a href="#fnref:20" rev="footnote"> ↩</a></span></li><li id="fn:21"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Perez, E., Strub, F., Vries, H.D., Dumoulin, V., &amp; Courville, A.C. (2017). <a href="https://arxiv.org/pdf/1709.07871.pdf">FiLM: Visual Reasoning with a General Conditioning Layer</a>. AAAI.<a href="#fnref:21" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>RL</category>
      </categories>
      <tags>
        <tag>RL</tag>
        <tag>StarCraft II</tag>
      </tags>
  </entry>
  <entry>
    <title>Model-Free Prediction (RL)</title>
    <url>/notes/2019/03/25/RL/David%20Silver/RL-notes-4-Model-Free-Prediction/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p><img data-src="/notes/images/RL-model-free-comp.png" alt="upload successful"><br>Notes of lectures by D. Silver.<br><span id="more"></span></p>
<h1 id="Monte-Carlo-Learning"><a href="#Monte-Carlo-Learning" class="headerlink" title="Monte-Carlo Learning"></a>Monte-Carlo Learning</h1><ul>
<li>MC methods learn directly from episodes of experience</li>
<li>MC is <strong>model-free</strong>: no knowledge of MDP transitions / rewards.</li>
<li>MC learns from <strong>complete episodes</strong>: no bootstrapping</li>
<li>MC uses the simplest possible idea: value = mean return</li>
<li>Caveat: can only apply MC to episodic MDPs<ul>
<li>All episodes must terminate</li>
</ul>
</li>
</ul>
<h2 id="MC-policy-evaluation"><a href="#MC-policy-evaluation" class="headerlink" title="MC policy evaluation"></a>MC policy evaluation</h2><ul>
<li><strong>Goal</strong>： learn <script type="math/tex">v_{\pi}</script> from episodes of experience under policy $\pi$<script type="math/tex; mode=display">S_1, A_1, R_2, \cdots , S_k \sim \pi</script></li>
<li>The return is the total discounted reward:<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T</script></li>
<li>The value function is the expected return:<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]</script></li>
<li>Monte-Carlo policy evaluation uses <strong>empirical mean return</strong> instead of expected return</li>
</ul>
<h3 id="First-visit-MC-policy-evaluation"><a href="#First-visit-MC-policy-evaluation" class="headerlink" title="First-visit MC policy evaluation"></a>First-visit MC policy evaluation</h3><p>To evaluate state $s$</p>
<ul>
<li>The <span class="label danger">first</span> time-step $t$ that state $s$ is visited in an episode</li>
<li>Increment counter $N(s) \leftarrow N(s) + 1$</li>
<li>Increment counter <script type="math/tex">S(s) \leftarrow S(s) + G_t</script></li>
<li>Value is estimated by mean return $V(s) = S(s) / N(s)$</li>
<li>By law of large numbers, $V(s) \rightarrow v_{\pi}(s)$ as $N(s) \rightarrow \infty$</li>
</ul>
<h3 id="Every-visit-MC-policy-evaluation"><a href="#Every-visit-MC-policy-evaluation" class="headerlink" title="Every-visit MC policy evaluation"></a>Every-visit MC policy evaluation</h3><p>To evaluate state $s$</p>
<ul>
<li>The <span class="label danger">every</span> time-step $t$ that state $s$ is visited in an episode</li>
<li>Increment counter $N(s) \leftarrow N(s) + 1$</li>
<li>Increment counter <script type="math/tex">S(s) \leftarrow S(s) + G_t</script></li>
<li>Value is estimated by mean return $V(s) = S(s) / N(s)$</li>
<li>By law of large numbers, $V(s) \rightarrow v_{\pi}(s)$ as $N(s) \rightarrow \infty$</li>
</ul>
<h2 id="Incremental-Monte-Carlo"><a href="#Incremental-Monte-Carlo" class="headerlink" title="Incremental Monte-Carlo"></a>Incremental Monte-Carlo</h2><h3 id="Incremental-Mean"><a href="#Incremental-Mean" class="headerlink" title="Incremental Mean"></a>Incremental Mean</h3><p>The mean <script type="math/tex">\mu_1, \mu_2, \cdots</script> of  a sequence <script type="math/tex">x_1, x_2, \cdots</script> can be computed incrementally</p>
<script type="math/tex; mode=display">\mu_k = \frac{1}{k} \sum_{j=1}^k x_j \\ =\frac{1}{k} \big( x_k + \sum_{j=1}^{k-1}x_j \big) \\ =\frac{1}{k} \big( x_k + (k-1) \mu_{k-1} \big) \\ = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1})</script><h3 id="Incremental-MC-updates"><a href="#Incremental-MC-updates" class="headerlink" title="Incremental MC updates"></a>Incremental MC updates</h3><ul>
<li>Update $V(s)$ incrementally after episode $S_1, A_1, R_2, \cdots, S_T$</li>
<li>For each state <script type="math/tex">S_t</script> with return <script type="math/tex">G_t</script><script type="math/tex; mode=display">N(S_t) \leftarrow N(S_t) + 1</script><script type="math/tex; mode=display">V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t))</script></li>
<li>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes<script type="math/tex; mode=display">V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))</script></li>
</ul>
<h1 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal-Difference Learning"></a>Temporal-Difference Learning</h1><ul>
<li>TD methods learn directly from episodes of experience</li>
<li>TD is <strong>model-free</strong>: no knowledge of MDP transitions / rewards</li>
<li>TD learns from incomplete episodes, by bootstrapping</li>
<li>TD updates a guess towards a guess</li>
</ul>
<h2 id="MC-and-TD"><a href="#MC-and-TD" class="headerlink" title="MC and TD"></a>MC and TD</h2><ul>
<li><strong>goal</strong>: learn <script type="math/tex">v_{\pi}</script> online from experience under policy $\pi$</li>
<li><p>Incremental every-visit MC</p>
<ul>
<li><p>Update value <script type="math/tex">V(S_t)</script> toward actual return <script type="math/tex">G_t</script></p>
<script type="math/tex; mode=display">V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))</script></li>
</ul>
</li>
<li>Simplest temporal-difference learning algorithm: TD(0)<ul>
<li>Update <script type="math/tex">V(S_t)</script> toward estimated return <script type="math/tex">R_{t+1} + \gamma V(S_{t+1})</script><script type="math/tex; mode=display">V(S_t) \leftarrow V(S_t) +\alpha \underbrace{(\overbrace{\pmb{R_{t+1} + \gamma V(S_{t+1})}}^\text{TD target} - V(S_t) )}_\text{TD error}</script></li>
</ul>
</li>
</ul>
<h1 id="MC-v-s-TD"><a href="#MC-v-s-TD" class="headerlink" title="MC v.s TD"></a>MC v.s TD</h1><h2 id="Pros-and-cons"><a href="#Pros-and-cons" class="headerlink" title="Pros and cons"></a>Pros and cons</h2><p>1.Process</p>
<ul>
<li><p>TD can learn before knowing the final outcome</p>
<ul>
<li>TD can learn online after every step</li>
<li>MC must wait until end of episode before return is known</li>
</ul>
</li>
<li><p>TD can learn without the final outcome</p>
<ul>
<li>TD can learn from incomplete sequences</li>
<li>MC can only learn from complete sequences</li>
<li>TD works in continuing (non-terminating) environments</li>
<li>MC only works for episodic (terminating) environments</li>
</ul>
</li>
</ul>
<p>2.Statistics</p>
<ul>
<li>MC has <strong>high variance</strong>, <strong>zero bias</strong><ul>
<li>Good convergence properties (even with function approximation)</li>
<li>Not very sensitive to initial value</li>
<li>Very simple to understand and use</li>
</ul>
</li>
<li>TD has <strong>low variance</strong>, <strong>some bias</strong><ul>
<li>Usually more efficient than MC</li>
<li>TD(0) converges to <script type="math/tex">v_{\pi}(s)</script> but not always with function approximation</li>
<li>More sensitive to initial value</li>
</ul>
</li>
</ul>
<p>3.Markov property</p>
<ul>
<li>TD exploits Markov property<ul>
<li>Usually more efficient in Markov environments</li>
</ul>
</li>
<li>MC does not exploit Markov property<ul>
<li>Usually more efficient in non-Markov environments</li>
</ul>
</li>
</ul>
<h2 id="Bias-Variance-trade-off"><a href="#Bias-Variance-trade-off" class="headerlink" title="Bias / Variance trade-off"></a>Bias / Variance trade-off</h2><ul>
<li>Return <script type="math/tex">G_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_{T}</script> is unbiased estimate of <script type="math/tex">v_{\pi}(S_t)</script></li>
<li>True TD target <script type="math/tex">T_{t+1} + \gamma v_{\pi}(S_{t+1})</script> is <strong>unbiased</strong> estimate of <script type="math/tex">v_{\pi}(S_t)</script></li>
<li>TD target <script type="math/tex">R_{t+1} + \gamma V(S_{t=1})</script> is <strong>biased</strong> estimate of <script type="math/tex">v_{pi}(S_t)</script></li>
<li>TD target is much lower variance that the return:<ul>
<li>Return depends on many random actions, transitions, rewards</li>
<li>TD target depends on one random action, transition, reward</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>RL</category>
        <category>Lecture</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Model-Free Control (RL)</title>
    <url>/notes/2019/03/26/RL/David%20Silver/RL-notes-5-Model-Free-control/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>Notes of lectures by D. Silver.<br><span id="more"></span></p>
<p>For problems like elevator, robot walking and the game of Go, MDP model is unknown, but experience can be sampled; or MDP model is known, but is too big to use, except by samples. Model-free control could solve this.</p>
<div class="note info">
            <ul><li>On-policy learning<ul><li>“learn <strong>on the job</strong>“</li><li>Learn about policy $\pi$ from experience sampled from $\pmb{\pi}$</li></ul></li><li>Off-policy learning<ul><li>“learn <strong>over someone’s shoulder</strong>“</li><li>Learn about policy $\pi$ from experience sampled from $\pmb{\mu}$</li></ul></li></ul>
          </div>
<h1 id="On-policy"><a href="#On-policy" class="headerlink" title="On-policy"></a>On-policy</h1><h2 id="On-policy-Monte-Carlo-control"><a href="#On-policy-Monte-Carlo-control" class="headerlink" title="On-policy Monte-Carlo control"></a>On-policy Monte-Carlo control</h2><ul>
<li>Greedy policy improvement over $V(s)$ requires model of MDP:<script type="math/tex; mode=display">\pi'(s) = \arg\max_{a \in \mathcal{A}} \mathcal{R}_s^a + \mathcal{P}_{ss'}^a V(s')</script></li>
<li>Greedy policy improvement over $Q(s,a)$ is model-free:<script type="math/tex; mode=display">\pi'(s) = \arg\max_{a \in \mathcal{A}} Q(s,a)</script></li>
</ul>
<h3 id="epsilon-greedy-exploration"><a href="#epsilon-greedy-exploration" class="headerlink" title="$\epsilon$-greedy exploration"></a>$\epsilon$-greedy exploration</h3><ul>
<li>Simplest idea for ensuring continual exploration</li>
<li>All $m$ actions are tried with non-zero probability</li>
<li>With probability $1-\epsilon$ choose the greedy action</li>
<li>With probability $\epsilon$ choose an action at random</li>
</ul>
<script type="math/tex; mode=display">
\pi(a \vert s) = \left\{
                    \begin{array}{ll}
                    \frac{\epsilon}{m} + 1 - \epsilon \quad \text{if } a^* =\arg\max_{a \in \mathcal{A}} Q(s,a) \\
                    \frac{\epsilon}{m} \quad \text{ otherwise}
                \end{array}
              \right.</script><h2 id="On-policy-Temporal-Difference-learning"><a href="#On-policy-Temporal-Difference-learning" class="headerlink" title="On-policy Temporal-Difference learning"></a>On-policy Temporal-Difference learning</h2><h3 id="MC-vs-TD-control"><a href="#MC-vs-TD-control" class="headerlink" title="MC vs. TD control"></a>MC vs. TD control</h3><ul>
<li>TD learning has several advantages over MC:<ul>
<li>Lower variance</li>
<li>Online</li>
<li>Incomplete sequences</li>
</ul>
</li>
<li>Natural idea: use TD instead of MC in out control loop<ul>
<li>Apply TD to $Q(S,A)$</li>
<li>Use $\epsilon$-greedy  policy improvement</li>
<li>Update every time-step</li>
</ul>
</li>
</ul>
<h3 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa($\lambda$)"></a>Sarsa($\lambda$)</h3><h4 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA:"></a><strong>SARSA</strong>:</h4><script type="math/tex; mode=display">Q(S,A) \leftarrow Q(S,A) + \alpha (R+ \gamma Q(S',A') - Q(S,A))</script><p><img data-src="/notes/images/RL-Sarsa.png" alt="upload successful"></p>
<p>Every time-step:</p>
<ul>
<li>Policy evaluation <strong>Sarsa</strong>: <script type="math/tex">Q \approx q_{\pi}</script></li>
<li>Policy improvement $\epsilon$-greedy policy improvement</li>
</ul>
<p><img data-src="/notes/images/RL-sarsa-graph.png" alt="upload successful"></p>
<p><img data-src="/notes/images/RL-Sarsa-algorithm.png" alt="upload successful"></p>
<h4 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="$n$-step Sarsa"></a>$n$-step Sarsa</h4><ul>
<li>Consider the following $n$-step returns for $n=1,2,\infty$</li>
</ul>
<script type="math/tex; mode=display">
\begin{bmatrix}
n=1 & (Sarsa) & q_t^{(1)} = R_{t+1} + \gamma Q(S_{t+1}) \\
n=2 &  & q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2})  \\
\vdots &  & \vdots \\
n=\infty & (MC) &  q_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \cdots +\gamma^{T-1} R_T
\end{bmatrix}</script><ul>
<li><p>Define the $n$-step Q-return</p>
<script type="math/tex; mode=display">q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n Q(S_{t+n})</script></li>
<li><p>$n$-step Sarsa updates $Q(s,a)$ towards the n-step Q-return</p>
<script type="math/tex; mode=display">Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha \big( q_t^{(n)} - Q(S_t, A_t) \big)</script></li>
</ul>
<h4 id="Forward-view-Sarsa-lambda"><a href="#Forward-view-Sarsa-lambda" class="headerlink" title="Forward-view Sarsa($\lambda$)"></a>Forward-view Sarsa($\lambda$)</h4><ul>
<li>The $q^{\pi}$ return combines all $n$-step Q-returns <script type="math/tex">q_t^{(n)}</script></li>
<li>Using weight $(1-\lambda) \lambda^{(n-1)}$<script type="math/tex; mode=display">q_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)}</script></li>
<li>Forward-view Sarsa($\lambda$)<script type="math/tex; mode=display">Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha \big( q_t^{(n)} - Q(S_t, A_t) \big)</script></li>
</ul>
<p><img data-src="/notes/images/RL-forward-view-Sarsa-lambda.png" alt="upload successful"></p>
<h4 id="Back-view-Sarsa-lambda"><a href="#Back-view-Sarsa-lambda" class="headerlink" title="Back-view Sarsa($\lambda$)"></a>Back-view Sarsa($\lambda$)</h4><ul>
<li>Like TD($\lambda$), we use <strong>eligibility traces</strong></li>
<li>Sarsa($\lambda$) has one eligibility trace for each state-action pair</li>
</ul>
<script type="math/tex; mode=display">E_0(s,a) = 0</script><script type="math/tex; mode=display">E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb{I}(S_t = s, A_t = a)</script><ul>
<li>$Q(s,a)$ is updated for every state $s$ and action $a$</li>
<li>In proportion to TD-error <script type="math/tex">\delta_t</script> and eligibility trace <script type="math/tex">R_t(s,a)</script><script type="math/tex; mode=display">\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)</script><script type="math/tex; mode=display">Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t (s,a)</script></li>
</ul>
<p><img data-src="/notes/images/RL-Sarsa-lambda-algorithm.png" alt="upload successful"></p>
<h1 id="Off-policy-learning"><a href="#Off-policy-learning" class="headerlink" title="Off-policy learning"></a>Off-policy learning</h1><h2 id="Off-policy-control-with-Q-learning"><a href="#Off-policy-control-with-Q-learning" class="headerlink" title="Off-policy control with Q-learning"></a>Off-policy control with Q-learning</h2><p><img data-src="/notes/images/q-learning.png" alt="upload successful"></p>
<script type="math/tex; mode=display">Q(S,A) \leftarrow Q(S,A) + \alpha \big( R + \gamma \max_{a'} Q(S', a') - Q(S,A) \big)</script><p><img data-src="/notes/images/Q-learning-algorithm.png" alt="upload successful"></p>
]]></content>
      <categories>
        <category>RL</category>
        <category>Lecture</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Key Concepts in RL</title>
    <url>/notes/2019/04/01/RL/SpinningUp/RL-key-concept-and-terminology/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>An introduction to key concepts and terminology in reinforcement learning.</p>
<span id="more"></span>
<p><img data-src="/notes/images/rl-interact-loop.png" alt="upload successful"></p>
<h1 id="Environment-and-agent"><a href="#Environment-and-agent" class="headerlink" title="Environment and agent"></a>Environment and agent</h1><p>The main components of RL are <strong>environment</strong> and <strong>agent</strong>. </p>
<ul>
<li>The <strong>environment</strong> is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the word, then decide on an action to take. The environment changes when the agent acts on it, but my also change on its own.</li>
</ul>
<ul>
<li>The agent also perceives a <strong>reward</strong> signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to <span class="label danger">maximize its cumulative reward</span>, called <strong>return</strong>.</li>
</ul>
<h1 id="State-and-observations"><a href="#State-and-observations" class="headerlink" title="State and observations"></a>State and observations</h1><div class="note info">
            <p><strong>State</strong>:</p><ul><li>A <strong>state</strong> $s$ is a complete description of the state of the world. There is no information which is hidden from the state.</li></ul><p><strong>Observation</strong>:</p><ul><li>An observation $o$ is a partial description of a state, which may omit information.</li></ul>
          </div>
<p>State and observations are almost a <code>real-valued vector, matrix, higher-order tensor</code> in deep RL.</p>
<ul>
<li>When the agent can observe the complete <strong>state</strong> of the environment, we say the environment is <code>fully observed</code>.</li>
<li>When the agent can only see a partial observation, the environment is <code>partially observed</code> (c.f. POMDP).</li>
</ul>
<div class="note warning">
            <p>In practice, RL state $s$ is more appropriate to use observation $o$. Specifically, we often signal in notation that the action is conditioned on the state, when <code>in practice, the action is conditioned on the observation</code> because the agent does not have the access to the state. In notation, also use standard notation $s$, rather than $o$.</p>
          </div>
<h1 id="Action-spaces"><a href="#Action-spaces" class="headerlink" title="Action spaces"></a>Action spaces</h1><ul>
<li><strong>Action space</strong>: the set of all valid actions in a given environment.</li>
<li><strong>Discrete action space</strong>: only a finite number of moves are available to the agent, e.g. Atari, Go.</li>
<li><strong>Continuous action space</strong>: actions are <em>real-valued vectors</em>, e.g. robot walk control.</li>
</ul>
<h1 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h1><p>A <strong>policy</strong> is a rule used by an agent to decide what actions to take. It is the agent’s brain. It is common to substitute the word “policy” for “agent”, e.g. saying “The policy is trying to maximize the reward”.</p>
<h2 id="Deterministic-policies"><a href="#Deterministic-policies" class="headerlink" title="Deterministic policies"></a>Deterministic policies</h2><ul>
<li><code>deterministic</code> (denoted by $\mu$)<script type="math/tex; mode=display">a_t = \mu(s_t)</script></li>
</ul>
<p>Tensorflow code snippet:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">obs = tf.placeholder(shape=(<span class="literal">None</span>, obs_dim), dtype=tf.float32)</span><br><span class="line">net = mlp(obs, hidden_dims=(<span class="number">64</span>,<span class="number">64</span>), activation=tf.tanh)</span><br><span class="line">actions = tf.layers.dense(net, units=act_dim, activation=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><br>where <code>mlp</code> represents MLP layers.</p>
<h3 id="Stochastic-policies"><a href="#Stochastic-policies" class="headerlink" title="Stochastic policies"></a>Stochastic policies</h3><ul>
<li><code>stochastic</code> (denoted by $\pi$)<script type="math/tex; mode=display">a_t \sim \pi(\cdot \vert s_t)</script></li>
</ul>
<p>Two most common kinds of stochastic policies:</p>
<h3 id="Categorical-policies"><a href="#Categorical-policies" class="headerlink" title="Categorical policies"></a>Categorical policies</h3><ul>
<li>used in discrete action spaces</li>
</ul>
<p>A categorical policy is like a classifier over discrete actions:</p>
<ul>
<li><p>build the NN (the same as a classifier): input is the observation, followed by some layers (CNNs FC layers, depending on the kind of input). Then one dense layer gives the logits for each action, followed by a softmax to convert the logits to probabilities.</p>
</li>
<li><p><strong>Sampling</strong>: Given probabilities for each action, frameworks like tensorflow has builtin tools for sampling. E.g. <code>tf.distributions.Categorical</code> or <code>tf.multinomial</code></p>
</li>
<li><p><strong>Log-likelihood</strong>: Denote the last layer of probabilities as <script type="math/tex">P_{\theta}(s)</script>. Treat the actions as the indices of the vector. The log likeligood for an action $a$ can then be obtained by indexing into the vector.</p>
<script type="math/tex; mode=display">\text{log} \pi_{\theta} (a \vert s) = \text{log} [P_{\theta}(s)]_a</script></li>
</ul>
<h3 id="Diagonal-Gaussian-policies"><a href="#Diagonal-Gaussian-policies" class="headerlink" title="Diagonal Gaussian policies"></a><strong>Diagonal Gaussian policies</strong></h3><ul>
<li>used in continuous action spaces</li>
</ul>
<p>A diagonal Gaussian distribution is a special case of multivariate Gaussians where the covariance matrix only has entries on the diagonal, which can be represented as a vector.</p>
<p>NN maps from observations to mean actions, <script type="math/tex">\mu_{\theta}(s)</script>, in two different ways:</p>
<ol>
<li><strong>The first way</strong>: there is a single vector of log standard deviations, $\text{log} \sigma$ are standalone parameters.</li>
<li><strong>The second way</strong>: NN maps from states to log standard deviations, <script type="math/tex">\text{log} \sigma_{\theta}(s)</script>. It may optionally share some layers with the mean network.</li>
</ol>
<p>Both output <code>log standard deviations</code> instead of std deviations directly. Since log stds are free to take any values in $(-\infty, \infty)$, while stds must be non-negative. It’s easier to train parameters without such constraints.</p>
<ul>
<li><p><strong>Sampling</strong>: Given the mean action <script type="math/tex">\mu_{\theta}(s)</script> and std deviation <script type="math/tex">\sigma_{\theta}(s)</script>, and a vector $z$ of noise from a spherical Gaussian $(z \sim ~ \mathcal{N}(0, \mathcal{I}))$, an action sample can be computed:</p>
<script type="math/tex; mode=display">a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z</script><p>where $\odot$ denotes the element-wise product.</p>
</li>
<li><p><strong>Log-likelihood</strong>: the log-likelihood of a $k$-dimensional action $a$, for a diagonal Gaussian with mean <script type="math/tex">\mu = \mu_{\theta}(s)</script> and std dev <script type="math/tex">\sigma = \sigma_{\theta}(s)</script> is:</p>
<script type="math/tex; mode=display">\text{log} \pi_{\theta}(a \vert s) = - \frac{1}{2} \big( \sum_{i=1}^k (\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \text{log} \sigma_i) + k \text{log} 2 \pi \big)</script></li>
</ul>
<p><strong>Parameterized policies</strong>: </p>
<ul>
<li><p>In deep RL, policies whose output are computable functions that depend on a set of parameters (e.g. the weights and biases in NNs).</p>
</li>
<li><p>Let $\theta$ or $\phi$ denotes the parameters, written as a subscript:</p>
<script type="math/tex; mode=display">a_t = \mu_{\theta}(s_t)</script><script type="math/tex; mode=display">a_t \sim \pi_{\theta}(\cdot \vert s_t)</script></li>
</ul>
<h1 id="Trajectories-a-k-a-Episodes-Rollouts"><a href="#Trajectories-a-k-a-Episodes-Rollouts" class="headerlink" title="Trajectories(a.k.a Episodes, Rollouts)"></a>Trajectories(a.k.a Episodes, Rollouts)</h1><p>A trajectory $\tau$ is a sequence of states and actions in the world:</p>
<script type="math/tex; mode=display">\tau = (s_0, a_0, s_1, a_1, \cdots)</script><p>The very first state of the world, <script type="math/tex">s_0</script> is randomly sampled from the <strong>start-state distribution</strong>, sometimes denoted by <script type="math/tex">\rho_0</script></p>
<script type="math/tex; mode=display">s_0 \sim \rho(\cdot)</script><p>State transitions are governed by the natural laws of the environment, and depend only the most recent action <script type="math/tex">a_t</script>. It is either deterministic，</p>
<script type="math/tex; mode=display">s_{t+1} = f(s_t, a_t)</script><p>or stochastic</p>
<script type="math/tex; mode=display">s_{t=1} \sim P(\cdot \vert s_t, a_t)</script><h1 id="Reward-and-return"><a href="#Reward-and-return" class="headerlink" title="Reward and return"></a>Reward and return</h1><p>The reward function $R$ depends on the current state of the world, the action just taken, and the next state of the world:</p>
<script type="math/tex; mode=display">r_t = R(s_t, a_t, s_{t+1})</script><p>Although frequently this is simplified to just a dependence on the current state, <script type="math/tex">r_t = R(s_t)</script>, or state-action pair <script type="math/tex">r_t = R(s_t, a_t)</script></p>
<p>The goal of the agent is to maximize some notation of <strong>cumulative reward over a trajectory</strong>, $R(\tau)$.</p>
<p>Two kinds of returns:</p>
<ol>
<li><strong>Finite-horizon undiscounted return</strong>: the sum of rewards obtained in a fixed window of steps:<script type="math/tex; mode=display">R(\tau) = \sum_{t=0}^T r_t</script></li>
<li><p><strong>Infinite-horizon discounted return</strong>: the sumof all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained. This includes a discounted factor $\gamma \in (0,1)$:</p>
<script type="math/tex; mode=display">R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t</script><ul>
<li>Intuition: “cash now is better than cash later”</li>
<li>Mathematically: more convenient to converge. An infinite-horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.</li>
</ul>
</li>
</ol>
<h1 id="The-RL-Problem"><a href="#The-RL-Problem" class="headerlink" title="The RL Problem"></a>The RL Problem</h1><p>Whatever the choice of return measure and policy, the goal of RL is to select a policy which maximize <strong>expected return</strong> when the agent acts accordingly.</p>
<p>Let us suppose the environment transitions and the policy are stochastic. The probability of a $T$-step trajectory is:</p>
<script type="math/tex; mode=display">P(\tau \vert \pi) = \rho_0 (s_0) \sum_{t=0}^{T-1} P(s_{t+1} \vert s_t, a_t) \pi(a_t \vert s_t)</script><p>The expected return denoted by $J(\pi)$ is:</p>
<script type="math/tex; mode=display">J(\pi) = \int_\tau P(\tau \vert \pi) R(\tau) = \underset{\tau \sim \pi}{E} [R(\tau)]</script><p>The central optimization problem in RL is expressed as:</p>
<script type="math/tex; mode=display">\pi^* = \arg\max_\pi J(\pi)</script><p>where <script type="math/tex">\pi^{*}</script> being the <strong>optimal policy</strong></p>
<h1 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h1><h2 id="Q-function"><a href="#Q-function" class="headerlink" title="Q-function"></a>Q-function</h2><ul>
<li>Q-function: total reward from taking <script type="math/tex">\pmb{a}_t</script> in <script type="math/tex">\pmb{s}_t</script><script type="math/tex; mode=display">Q^{\pi}(\pmb{s}_t, \pmb{a}_t) = \sum_{t=t'}^T E_{\pi_{\theta}} [r(\pmb{s}_{t'}, \pmb{a}_{t'}) \vert \pmb{s}_t, \pmb{a}_t]</script></li>
</ul>
<h2 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h2><ul>
<li>Value function: total reward from <script type="math/tex">\pmb{s}_t</script><script type="math/tex; mode=display">V^{\pi}(\pmb{s}_t) = \sum_{t=t'}^T E_{\pi_{\theta}}[r(\pmb{s}_{t'}, \pmb{a}_{t'}) \vert \pmb{s}_t]</script><script type="math/tex; mode=display">V^{\pi}(\pmb{s}_t) = E_{\pmb{a}_t \sim \pi(\pmb{a}_t \vert \pmb{s}_t)} [Q^{\pi} (\pmb{s}_t, \pmb{a}_t)]</script></li>
</ul>
<div class="note info">
            <p><script type="math/tex">E_{\pmb{s_1 \sim p(\pmb{s}_1)}}[V^{\pi}(\pmb{s}_1)]</script> is the RL objective!</p>
          </div>
<div class="note warning">
            <p><strong>Idea:</strong> compute gradient to increase the probability of good actions <strong>a</strong>:</p><ul><li>If <script type="math/tex">Q^{\pi}(\pmb{s}, \pmb{a}) > V^{\pi}(\pmb{s})</script>, then $\pmb{a}$ is better than average. (recall <script type="math/tex">V^{\pi}(s) = E[Q^\pi (\pmb{s}, \pmb{a})]</script> under <script type="math/tex">\pi(\pmb{a} \vert \pmb{s})</script>) </li><li>modify <script type="math/tex">\pi(\pmb{a} \vert \pmb{s})</script> to increase the probability of $\pmb{a}$ if <script type="math/tex">Q^{\pi}(\pmb{s}, \pmb{a}) > V^{\pi}(\pmb{s})</script></li></ul>
          </div>
<p>By value, we mean the expected return if you start in that state or state-action pair, and then act according to a particular policy forever after. <strong>Value functions</strong> are used in almost every RL algorithm.</p>
<p>Four main functions:</p>
<ol>
<li><p><strong>On-policy value function</strong>, $V^\pi(s)$: give the expected return if you <strong>start in state $s$</strong> and always act according to policy $\pi$</p>
<script type="math/tex; mode=display">V^\pi(s) = \underset{\tau \sim \pi}{E}[R(\tau) \vert s_0 = s]</script></li>
<li><p><strong>On-policy action-value function</strong>, $Q^\pi(s,a)$: gives the expected return if you <strong>start in state $s$, take an arbitrary action $a$</strong>, and then forever after act according to policy $\pi$:</p>
<script type="math/tex; mode=display">Q^{\pi}(s,a) = \underset{\tau \sim \pi}{E}[R(\tau) \vert s_0=s, a_0=a]</script></li>
<li><strong>Optimal value function</strong>, $V^<em>(s)$: give the expected reutrn if you <em>*start in state $s$</em></em>, and always act according to the optimal policy in the environment:<script type="math/tex; mode=display">V^*(s) = \max_\pi \underset{\tau \sim \pi}{E}[R(\tau) \vert s_0 = s]</script></li>
<li><strong>Optimal action-value function</strong>, $Q^<em>(s,a)$: give the expected return if you <strong>start in state $s$, take and arbitrary action $a$</strong>, and then forever after act according to the </em>optimal* policy in the environment:<script type="math/tex; mode=display">Q^*(s,a) = \max_\pi \underset{\tau \sim \pi}{E} [R(\tau) \vert s_0=s, a_0=a]</script></li>
</ol>
<h1 id="The-optimal-Q-function-and-the-optimal-action"><a href="#The-optimal-Q-function-and-the-optimal-action" class="headerlink" title="The optimal Q-function and the optimal action"></a>The optimal Q-function and the optimal action</h1><script type="math/tex; mode=display">a^*(s) = \arg\max_{a} Q^*(s,a)</script><h1 id="Bellman-equations"><a href="#Bellman-equations" class="headerlink" title="Bellman equations"></a>Bellman equations</h1><ul>
<li><p>Basic idea: the value of your starting point is <strong>the reward you expect to get from being there, plus the value of wherever you land next</strong>.</p>
</li>
<li><p>Bellman equation for the <strong>on-policy value functions</strong>:</p>
<script type="math/tex; mode=display">V^{\pi}(s) = \underset{\underset{s' \sim P}{a \sim \pi}}{E} [r(s,a) + \gamma V^{\pi}(s')]</script><script type="math/tex; mode=display">Q^\pi (s,a) = \underset{s' \sim P}{E} \big[r(s,a) + \gamma \underset{a' \sim \pi}{E} [Q^\pi (s', a')] \big]</script></li>
</ul>
<p>where $s’ \sim P$ is shorthand for $s’ \sim P(\cdot \vert s,a)$, indicating that the next state $s’$ is sampled from the environment’s transition rules; $a \sim \pi$ is shorthand for $a \sim \pi(\cdot \vert s)$ and $a’ \sim \pi$ is shorthand for $a’ \sim \pi(\cdot \vert s’)$</p>
<ul>
<li>Bellman equation for the <strong>optimal value functions</strong>:<script type="math/tex; mode=display">V^*(s) = \max_a \underset{s' \sim P}{E} [r(s,a) + \gamma V^*(s')]</script><script type="math/tex; mode=display">Q^*(s,a) = \underset{s' \sim P}{E} \big[ r(s,a) + \gamma \max_{a'} Q^*(s', a') \big]</script></li>
</ul>
<div class="note default">
            <p><strong>Difference</strong> between the Bellman equations for the on-policy value functions and the optimal value functions: the absence or presence of the $\max$ over actions.</p>
          </div>
<h1 id="Advantage-functions"><a href="#Advantage-functions" class="headerlink" title="Advantage functions"></a>Advantage functions</h1><ul>
<li><p>Intuition: In RL, we want to know how much better it is than other on average, i.e the <strong>relative advantage</strong> of that action.</p>
</li>
<li><p>The advantage function $A^\pi (s,a)$ corresponding to a policy $\pi$ describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to <script type="math/tex">\pi(\cdot \vert s)</script>, assuming you act according to $\pi$ forever after.</p>
<script type="math/tex; mode=display">A^\pi(s,a) = Q^\pi (s,a) - V^\pi(s)</script></li>
</ul>
<h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>An MDP is a 5-tuple <script type="math/tex"><S,A,R,P, \rho_0></script>, where</p>
<ul>
<li>$S$ is the set of all valid states</li>
<li>$A$ is the set of all valid actions</li>
<li>$R$: $S \times A \times S \rightarrow \mathbb{R}$ is the reward function, with <script type="math/tex">r_t = R(s_t, a_t, s_{t+1})</script></li>
<li>$P$: $S \times A \rightarrow \mathcal{P}(S)$ is the transition probability function, with $P(S’ \vert s,a)$ being the prob of transitioning into state $s’$ if you start in state $s$ and take action $a$</li>
<li><script type="math/tex">\rho_0</script> is the starting state distribution.</li>
</ul>
<p><strong>Markov property</strong>:</p>
<ul>
<li>transitions only depend on the most recent state and action, and no prior history.</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">OpenAI, Spinning Up, Part 1: Key Concepts in RL</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>RL</category>
        <category>SpinningUp</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Kinds of RL algorithms</title>
    <url>/notes/2019/04/02/RL/SpinningUp/RL-taxonomy/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/2.6.0/hint.min.css"><p>The landscape of algorithms in modern RL.<br><span id="more"></span></p>
<h1 id="A-taxonomy-of-RL-algorithms-OpenAI-SpinningUp"><a href="#A-taxonomy-of-RL-algorithms-OpenAI-SpinningUp" class="headerlink" title="A taxonomy of RL algorithms (OpenAI SpinningUp)"></a>A taxonomy of RL algorithms (OpenAI SpinningUp)</h1><p><img data-src="/notes/images/RL-taxonomy.png" alt="upload successful"></p>
<h1 id="Types-of-RL-algorithms-UCB-CS294-112"><a href="#Types-of-RL-algorithms-UCB-CS294-112" class="headerlink" title="Types of RL algorithms (UCB CS294-112)"></a>Types of RL algorithms (UCB CS294-112)</h1><script type="math/tex; mode=display">\theta^{*} = \arg \max_{\theta} E_{\tau \sim p_{\theta}(\tau)} \big[\sum_t r(\pmb{s}_t, \pmb{a}_t)\big]</script><ul>
<li><strong>Policy gradient</strong>: directly differentiate the above objective</li>
<li><strong>Value-based</strong>: estimate value function or Q-funtion of the optimal policy (no explicit policy)</li>
<li><strong>Actor-critic</strong>: estimate value function or Q-function of the curent policy, use it to improve policy</li>
<li><strong>Model-based RL</strong>: estimate the transition model, and then<ul>
<li>Use it for planning (no explicit policy)</li>
<li>Use it to improve a policy</li>
<li>Something else</li>
</ul>
</li>
</ul>
<h2 id="Model-free-v-s-model-based-RL"><a href="#Model-free-v-s-model-based-RL" class="headerlink" title="Model-free v.s model-based RL"></a>Model-free v.s model-based RL</h2><ul>
<li>Branching point: <code>whether the agent has access to (or learns) a model of the environment</code>. By a model of the environment, we mean <strong>a function which predicts state transition and rewards</strong>.</li>
</ul>
<p><code>Advantages</code> of having a model:</p>
<ul>
<li><strong>Allows the agent to plan</strong> by thinking ahead, seeing what would happen for a range of possible choices, and explicitly deciding between its options. Agents can then distill the results from planning ahead into a learned policy. E.g. AlphaZero <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[AlphaZero](https://arxiv.org/abs/1712.01815) : Silver et al, 2017
">[1]</span></a></sup>.</li>
</ul>
<p><code>Disadvantages</code> of having a model:</p>
<ul>
<li>A ground-truth model of the environment is usually not available to the agent. Agents have to learn he model purely from the experience, and the model-learning is fundamentally hard.</li>
</ul>
<h1 id="What-to-learn"><a href="#What-to-learn" class="headerlink" title="What to learn"></a>What to learn</h1><p>Branching point: what to learn, include</p>
<ul>
<li>Policies, (stochastic or deterministic)</li>
<li>Action-value functions (Q-functions)</li>
<li>Value functions</li>
<li>and/or environment models</li>
</ul>
<h2 id="What-to-learn-in-model-free-RL"><a href="#What-to-learn-in-model-free-RL" class="headerlink" title="What to learn in model-free RL"></a>What to learn in <code>model-free</code> RL</h2><h3 id="Policy-optimization"><a href="#Policy-optimization" class="headerlink" title="Policy optimization"></a>Policy optimization</h3><p>The policy is denoted as <script type="math/tex">\pi_{\theta}(a \vert s)</script>.  Optimize the parameters $\theta$ either </p>
<ol>
<li>directly by <code>gradient ascent</code> on the performance objective <script type="math/tex">J(\pi_{\theta})</script><br>or </li>
<li>indirectly by <code>maximizing local approximation</code> of <script type="math/tex">J(\pi_\theta)</script></li>
</ol>
<ul>
<li><code>on-policy</code>: each update uses data collected from the most recent version of the policy. Policy optimization also usually involves learning an approximator <script type="math/tex">V_{\phi}(s)</script> for the on-policy value function $V^{\pi}(s)$.</li>
</ul>
<p>Example algorithms:</p>
<ul>
<li>A2C / A3C: performs gradient ascent to directly maximize the performance <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[A2C / A3C](https://arxiv.org/abs/1602.01783) (Asynchronous Advantage Actor-Critic): Mnih et al, 2016
">[2]</span></a></sup></li>
<li>PPO: updates indirectly maximize performance, by instead maximizing a surrogate objective function, which gives a conservative estimate for how much $J(\pi_{\theta})$ will change as a result of the update. <sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[PPO](https://arxiv.org/abs/1707.06347) (Proximal Policy Optimization): Schulman et al, 2017
">[3]</span></a></sup></li>
</ul>
<h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>Methods learn an appoximator <script type="math/tex">Q_{\theta}(s,a)</script> for the optimal action-value function, $Q^*(s,a)$. </p>
<ul>
<li>Typically the objective function is based on the Bellman equation</li>
<li><code>Off-policy</code>: each update can use data collected at any point during training. regardless of how the agent was choosing to explore the environment when the data is obtained.</li>
</ul>
<p>The actions taken by the Q-learning:</p>
<script type="math/tex; mode=display">a(s) = \arg\max_a Q_{\theta}(s,a)</script><p>Example algorithms:</p>
<ul>
<li>DQN <sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) (Deep Q-Networks): Mnih et al, 2013
">[8]</span></a></sup></li>
<li>C51: a variant that learns a distribution over return whose expectation is $Q^*$ <sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[C51](https://arxiv.org/abs/1707.06887) (Categorical 51-Atom DQN): Bellemare et al, 2017
">[9]</span></a></sup></li>
</ul>
<h3 id="Trade-offs-between-policy-gradient-and-Q-learning"><a href="#Trade-offs-between-policy-gradient-and-Q-learning" class="headerlink" title="Trade-offs between policy gradient and Q-learning"></a>Trade-offs between policy gradient and Q-learning</h3><p>Policy optimization:</p>
<ul>
<li>Strength: directly optimize the thing you want(i.e. policy); stable and reliable</li>
</ul>
<p>Q-learning</p>
<ul>
<li>Indirectly optimize for agent performance by training <script type="math/tex">Q_{\theta}</script> to satisfy a self-consistency equation</li>
<li>less stable </li>
<li>more sample efficient</li>
</ul>
<p>Trade-of algorithms:</p>
<ul>
<li>DDPG: concurrently learns a deterministic policy and a Q-function by using each to improve the other <sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[DDPG](https://arxiv.org/abs/1509.02971) (Deep Deterministic Policy Gradient): Lillicrap et al, 2015
">[5]</span></a></sup></li>
<li>SAC: a variant using stochastic policies, entropy regularization, and other tricks to stablize learning <sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[SAC](https://arxiv.org/abs/1801.01290) (Soft Actor-Critic): Haarnoja et al, 2018
">[7]</span></a></sup></li>
</ul>
<h2 id="What-to-learn-in-model-based-RL"><a href="#What-to-learn-in-model-based-RL" class="headerlink" title="What to learn in model-based RL"></a>What to learn in model-based RL</h2><h3 id="Pure-planning"><a href="#Pure-planning" class="headerlink" title="Pure planning"></a>Pure planning</h3><ul>
<li>MBMF <sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[MBMF](https://sites.google.com/view/mbmf) (Model-Based RL with Model-Free Fine-Tuning): Nagabandi et al, 2017
">[14]</span></a></sup></li>
</ul>
<h3 id="Expert-iteration"><a href="#Expert-iteration" class="headerlink" title="Expert iteration"></a>Expert iteration</h3><ul>
<li>ExIt <sup id="fnref:16"><a href="#fn:16" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Anthony, T., Tian, Z., & Barber, D. (2017). [Thinking Fast and Slow with Deep Learning and Tree Search
](https://arxiv.org/abs/1705.08439). NIPS.
">[16]</span></a></sup>.</li>
<li>AlphaZero</li>
</ul>
<h3 id="Data-augmentation-for-Model-free-methods"><a href="#Data-augmentation-for-Model-free-methods" class="headerlink" title="Data augmentation for Model-free methods"></a>Data augmentation for Model-free methods</h3><ul>
<li>MBVE <sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[MBVE](https://arxiv.org/abs/1803.00101) (Model-Based Value Expansion): Feinberg et al, 2018
">[15]</span></a></sup></li>
<li>World Models <sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[World Models](https://worldmodels.github.io/): Ha and Schmidhuber, 2018
">[12]</span></a></sup></li>
</ul>
<h3 id="Embedding-planning-loops-into-policies"><a href="#Embedding-planning-loops-into-policies" class="headerlink" title="Embedding planning loops into policies"></a>Embedding planning loops into policies</h3><ul>
<li>I2A <sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[I2A](https://arxiv.org/abs/1707.06203) (Imagination-Augmented Agents): Weber et al, 2017
">[13]</span></a></sup></li>
</ul>
<h1 id="RL-problems"><a href="#RL-problems" class="headerlink" title="RL problems"></a>RL problems</h1><h2 id="a-Different-tradeoffs"><a href="#a-Different-tradeoffs" class="headerlink" title="a. Different tradeoffs"></a>a. Different tradeoffs</h2><h3 id="Sample-efficiency"><a href="#Sample-efficiency" class="headerlink" title="Sample efficiency"></a>Sample efficiency</h3><ul>
<li>Sample efficiency = how many samples do we need to get a good policy?<br><img data-src="/notes/images/sample-efficiency.png" alt="upload successful"></li>
<li>Most important question: is the algorithm off policy?<ul>
<li>Off policy: able to improve the policy without generating new samples from that policy</li>
<li>On policy: each time the policy is changed, even a little bit, we need to generate new samples</li>
</ul>
</li>
</ul>
<h3 id="Stability-amp-ease-of-use"><a href="#Stability-amp-ease-of-use" class="headerlink" title="Stability &amp; ease of use"></a>Stability &amp; ease of use</h3><ul>
<li>Does it converge?</li>
<li>And if it converges, to what?</li>
<li>And does it converge every time?</li>
</ul>
<div class="note info">
            <ul><li>Supervised learning: almost <code>always</code> gradient descent</li><li>Reinforcement learning: often <code>not</code> gradient descent<ul><li>Q-learning: fixed point iteration</li><li>Model-based RL: model is not optimized for expected reward</li><li>Policy gradient: <code>is</code> gradient descent, but also often the least efficient!</li></ul></li></ul>
          </div>
<ol>
<li>Value-function fitting<ul>
<li>At best, minimizes error of fit <code>&quot;Bellman error&quot;</code><ul>
<li>Not the same as expected reward</li>
</ul>
</li>
<li>At worst, doesnot optimize anything<ul>
<li>Many popular deep RL value fitting algorithms are not guaranteed to converge to anything in the non-linear case</li>
</ul>
</li>
</ul>
</li>
<li>Model-based RL<ul>
<li>Model minimizes error of fit<ul>
<li>This will converge</li>
</ul>
</li>
<li>No gurantee that better model = better policy</li>
</ul>
</li>
<li>Policy gradiuent<ul>
<li>The only one that actually performs gradient descent(ascent) on the true objective</li>
</ul>
</li>
</ol>
<h2 id="b-Different-assumptions"><a href="#b-Different-assumptions" class="headerlink" title="b. Different assumptions"></a>b. Different assumptions</h2><ul>
<li>Stochastic or deterministic?</li>
<li>Continuous or discrete?</li>
<li>Episodic or infinite horizon?</li>
</ul>
<ol>
<li>Full observability<ul>
<li>Generally assumed by value function fitting methods</li>
<li>Can be mitigated by adding recurrence</li>
</ul>
</li>
<li>Episodic learning<ul>
<li>Often assumed by pure policy gradient methods</li>
<li>Assumed by some model-based RL methods</li>
</ul>
</li>
<li>Continuous or smoothness<ul>
<li>Assumed by some continuous value function learning methods</li>
<li>Often assumed by some model-based RL methods</li>
</ul>
</li>
</ol>
<h2 id="c-Different-things-are-easy-or-hard-in-different-settings"><a href="#c-Different-things-are-easy-or-hard-in-different-settings" class="headerlink" title="c. Different things are easy or hard in different settings?"></a>c. Different things are easy or hard in different settings?</h2><ul>
<li>Easier to represent the policy?</li>
<li>Easier to represent the model?</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1712.01815">AlphaZero</a> : Silver et al, 2017<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1602.01783">A2C / A3C</a> (Asynchronous Advantage Actor-Critic): Mnih et al, 2016<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1707.06347">PPO</a> (Proximal Policy Optimization): Schulman et al, 2017<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1502.05477">TRPO</a> (Trust Region Policy Optimization): Schulman et al, 2015<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1509.02971">DDPG</a> (Deep Deterministic Policy Gradient): Lillicrap et al, 2015<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1802.09477">TD3</a> (Twin Delayed DDPG): Fujimoto et al, 2018<a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1801.01290">SAC</a> (Soft Actor-Critic): Haarnoja et al, 2018<a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a> (Deep Q-Networks): Mnih et al, 2013<a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1707.06887">C51</a> (Categorical 51-Atom DQN): Bellemare et al, 2017<a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1710.10044">QR-DQN</a> (Quantile Regression DQN): Dabney et al, 2017<a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1707.01495">HER</a> (Hindsight Experience Replay): Andrychowicz et al, 2017<a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://worldmodels.github.io/">World Models</a>: Ha and Schmidhuber, 2018<a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1707.06203">I2A</a> (Imagination-Augmented Agents): Weber et al, 2017<a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://sites.google.com/view/mbmf">MBMF</a> (Model-Based RL with Model-Free Fine-Tuning): Nagabandi et al, 2017<a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1803.00101">MBVE</a> (Model-Based Value Expansion): Feinberg et al, 2018<a href="#fnref:15" rev="footnote"> ↩</a></span></li><li id="fn:16"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Anthony, T., Tian, Z., &amp; Barber, D. (2017). <a href="https://arxiv.org/abs/1705.08439">Thinking Fast and Slow with Deep Learning and Tree Search
</a>. NIPS.<a href="#fnref:16" rev="footnote"> ↩</a></span></li><li id="fn:17"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">OpenAI, Spinning Up, Part 2: Kinds of RL algorithms</a><a href="#fnref:17" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>RL</category>
        <category>SpinningUp</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
</search>
